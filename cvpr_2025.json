[
    {
        "Title": "Neural Hierarchical Decomposition for Single Image Plant Modeling",
        "Authors": "Zhihao Liu \u00b7 Zhanglin Cheng \u00b7 Naoto Yokoya",
        "Abstract": "Obtaining high-quality, practically usable 3D models of biological plants remains a significant challenge in computer vision and graphics. In this paper, we present a novel method for generating realistic 3D plant models from single-view photographs. Our approach employs a neural decomposition technique to learn a lightweight hierarchical box representation from the image, effectively capturing the structures and botanical features of plants. Then, this representation can be subsequently refined through a shape-guided parametric modeling module to produce complete 3D plant models. By combining hierarchical learning and parametric modeling, our method generates structured 3D plant assets with fine geometric details. Notably, through learning the decomposition in different levels of detail, our method can adapt to two distinct plant categories: outdoor trees and houseplants, each with unique appearance features. Within the scope of plant modeling, our method is the first comprehensive solution capable of reconstructing both plant categories from single-view images.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a novel method for generating high-quality 3D plant models from single-view photographs. The approach utilizes a neural decomposition technique to create a hierarchical box representation that captures plant structures and botanical features. This representation is refined through a shape-guided parametric modeling module to produce detailed 3D plant models. The method adapts to two distinct plant categories\u2014outdoor trees and houseplants\u2014by learning decomposition at different levels of detail. It is the first comprehensive solution capable of reconstructing both categories from single-view images.",
        "Tags": [
            "3D Reconstruction",
            "3D Generation",
            "Hierarchical Learning",
            "Parametric Modeling",
            "Single-View Reconstruction"
        ]
    },
    {
        "Title": "ROICtrl: Boosting Instance Control for Visual Generation",
        "Authors": "Yuchao Gu \u00b7 Yipin Zhou \u00b7 Yunfan Ye \u00b7 Yixin Nie \u00b7 Licheng Yu \u00b7 Pingchuan Ma \u00b7 Kevin Qinghong Lin \u00b7 Mike Zheng Shou",
        "Abstract": "Natural language often struggles to accurately associate positional and attribute information with multiple instances, which limits current text-based visual generation models to simpler compositions featuring only a few dominant instances. To address this limitation, this work enhances diffusion models by introducing regional instance control, where each instance is governed by a bounding box paired with a free-form caption. Previous methods in this area typically rely on implicit position encoding or explicit attention masks to separate regions of interest (ROIs), resulting in either inaccurate coordinate injection or large computational overhead. Inspired by ROI-Align in object detection, we introduce a complementary operation called ROI-Unpool. Together, ROI-Align and ROI-Unpool enable explicit, efficient, and accurate ROI manipulation on high-resolution feature maps for visual generation. Building on ROI-Unpool, we propose ROICtrl, an adapter for pretrained diffusion models that enables precise regional instance control. ROICtrl is compatible with community-finetuned diffusion models, as well as with existing spatial-based add-ons (\\eg, ControlNet, T2I-Adapter) and embedding-based add-ons (\\eg, IP-Adapter, ED-LoRA), extending their applications to multi-instance generation. Experiments show that ROICtrl achieves superior performance in regional instance control while significantly reducing computational costs.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces ROICtrl, a method to enhance diffusion models for visual generation by enabling precise regional instance control. Traditional text-based models often fail to accurately associate positional and attribute information with multiple instances, limiting their use to simpler compositions. ROICtrl addresses this by using bounding boxes paired with captions to control each instance, improving accuracy and efficiency. The method introduces ROI-Unpool, which works alongside ROI-Align to manipulate high-resolution feature maps effectively. ROICtrl is compatible with existing diffusion models and add-ons, extending their capabilities to multi-instance generation. The approach significantly reduces computational costs while achieving superior performance in regional instance control.",
        "Tags": [
            "Diffusion Models",
            "Image Generation",
            "ROI-Unpool",
            "Regional Instance Control",
            "Multi-Instance Generation"
        ]
    },
    {
        "Title": "Rectification-specific Supervision and Constrained Estimator for Online Stereo Rectification",
        "Authors": "Rui Gong \u00b7 Kim-Hui Yap \u00b7 Weide Liu \u00b7 Xulei Yang \u00b7 Jun Cheng",
        "Abstract": "Online stereo rectification is critical for autonomous vehicles and robots in dynamic environments, where factors such as vibration, temperature fluctuations, and mechanical stress can affect rectification accuracy and severely degrade downstream stereo depth estimation. Current dominant approaches for online stereo rectification involve estimating relative camera poses in real time to derive rectification homographies. However, they do not directly optimize for rectification constraints, which leads to a gap. Additionally, the general-purpose correspondence matchers used in these methods are not trained for stereo rectification, while training of these matchers typically requires ground-truth correspondences which are not available in stereo rectification datasets. To address these limitations, we propose a matching-based stereo rectification framework that is directly optimized for rectification and does not require ground-truth correspondence annotations for training. Our framework incorporates a rectification-constrained estimator and applies multi-level, rectification-specific supervision that trains the matcher network for rectification without relying on ground-truth correspondences. Additionally, we create a new rectification dataset with ground-truth optical flow annotations, eliminating bias from evaluation metrics used in prior work that relied on pretrained keypoint matching or optical flow models. Extensive experiments show that our approach outperforms both state-of-the-art matching-based and matching-free methods in vertical flow metric by $10.7$% on the Carla dataset and $21.3$% on the Semi-Truck Highway dataset, offering superior rectification accuracy. The codes and the created dataset will be released once the paper is published.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces a novel matching-based stereo rectification framework designed for online applications in dynamic environments, such as autonomous vehicles and robots. The framework addresses the limitations of current methods by directly optimizing for rectification constraints and eliminating the need for ground-truth correspondence annotations during training. It incorporates a rectification-constrained estimator and employs multi-level, rectification-specific supervision to train the matcher network effectively. Additionally, the authors create a new rectification dataset with ground-truth optical flow annotations to improve evaluation accuracy. The proposed method demonstrates superior performance, outperforming state-of-the-art methods on benchmark datasets.",
        "Tags": [
            "Stereo Matching",
            "Autonomous Driving",
            "Rectification-constrained Estimator",
            "Multi-level Supervision",
            "Rectification-specific Supervision",
            "Optical Flow Annotations",
            "Dynamic Environment Adaptation"
        ]
    },
    {
        "Title": "Alignment, Mining and Fusion: Representation Alignment with Hard Negative Mining and Selective Knowledge Fusion for Medical Visual Question Answering",
        "Authors": "Yuanhao Zou \u00b7 Zhaozheng Yin",
        "Abstract": "Medical Visual Question Answering (Med-VQA) is a challenging task that requires a deep understanding of both medical images and textual questions. Although recent works leveraging Medical Vision-Language Pre-training (Med-VLP) have shown strong performance on the Med-VQA task, there is still no unified solution for modality alignment, and the issue of hard negatives remains under-explored. Additionally, commonly used knowledge fusion techniques for Med-VQA may introduce irrelevant information. In this work, we propose a framework to address these challenges through three key contributions: (1) a unified solution for heterogeneous modality alignments across multiple levels, modalities, views, and stages, leveraging methods such as contrastive learning and optimal transport theory; (2) a hard negative mining method that employs soft labels for multi-modality alignments and enforces the hard negative pair discrimination; and (3) a Gated Cross-Attention Module for Med-VQA that integrates the answer vocabulary as prior knowledge and select relevant information from it. Our framework outperforms the previous state-of-the-art on widely used Med-VQA datasets like RAD-VQA, SLAKE, PathVQA and VQA-2019. The code will be publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Medical Visual Question Answering (Med-VQA) is a complex task requiring deep understanding of medical images and textual questions. This paper introduces a framework addressing key challenges in Med-VQA through three main contributions: a unified solution for heterogeneous modality alignment across multiple levels, modalities, views, and stages; a hard negative mining method using soft labels for multi-modality alignments; and a Gated Cross-Attention Module that integrates answer vocabulary as prior knowledge, selectively fusing relevant information. The framework demonstrates superior performance on major Med-VQA datasets.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Medical Image Analysis",
            "Contrastive Learning",
            "Optimal Transport Theory",
            "Gated Cross-Attention"
        ]
    },
    {
        "Title": "Derivative-Free Diffusion Manifold-Constrained Gradient for Unified XAI",
        "Authors": "Won Jun Kim \u00b7 Hyungjin Chung \u00b7 Jaemin Kim \u00b7 Sangmin Lee \u00b7 Byeongsu Sim \u00b7 Jong Chul Ye",
        "Abstract": "Gradient-based methods are a prototypical family of \"explainability for AI\" (XAI) techniques, especially for image-based models.Nonetheless, they have several shortcomings in that they (1) require white-box access to models, (2) are vulnerable to adversarial attacks, and (3) produce attributions that lie off the image manifold, leading to explanations that are not actually faithful to the model and do not align well with human perception. To overcome these challenges, we introduce \"Derivative-Free  Diffusion Manifold-Contrained Gradients (FreeMCG)\", a novel method that serves as an improved basis for explainability of a given neural network than the traditional gradient. Specifically, by leveraging ensemble Kalman filters and diffusion models, we derive a derivative-free approximation of the model\u2019s gradient projected onto the data manifold, requiring access only to the model\u2019s outputs (i.e., in a completely black-box setting). We demonstrate the effectiveness of FreeMCG by applying it to both counterfactual generation and feature attribution, which have traditionally been treated as distinct tasks. Through comprehensive evaluation on both tasks - counterfactual explanation and feature attribution - we show that our method yields state-of-the-art results while preserving the essential properties expected of XAI tools.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces 'Derivative-Free Diffusion Manifold-Constrained Gradients (FreeMCG)', a novel method for improving the explainability of neural networks. Traditional gradient-based XAI techniques face challenges such as requiring white-box access, vulnerability to adversarial attacks, and producing explanations that do not align well with human perception. FreeMCG addresses these issues by using ensemble Kalman filters and diffusion models to approximate the model's gradient in a derivative-free manner, projecting it onto the data manifold and operating in a black-box setting. The method is applied to both counterfactual generation and feature attribution, demonstrating state-of-the-art results while maintaining the essential properties of XAI tools.",
        "Tags": [
            "Explainable AI (XAI)",
            "Diffusion Models",
            "Ensemble Kalman Filters",
            "Black-Box Optimization",
            "Counterfactual Explanation"
        ]
    },
    {
        "Title": "Event-based Video Super-Resolution via State Space Models",
        "Authors": "Zeyu Xiao \u00b7 Xinchao Wang",
        "Abstract": "Exploiting temporal correlations is crucial for video super-resolution (VSR). Recent approaches enhance this by incorporating event cameras. In this paper, we introduce MamEVSR, an Mamba-based network for event-based VSR that leverages the selective state space model, Mamba. MamEVSR stands out by offering global receptive field coverage with linear computational complexity, thus addressing the limitations of convolutional neural networks and Transformers. The key components of MamEVSR include: (1) The interleaved Mamba (iMamba) block, which interleaves tokens from adjacent frames and applies multi-directional selective state space modeling, enabling efficient feature fusion and propagation across bi-directional frames while maintaining linear complexity. (2) The cross-modality Mamba (cMamba) block facilitates further interaction and aggregation between event information and the output from the iMamba block. The cMamba block can leverage complementary spatio-temporal information from both modalities and allows MamEVSR to capture finer motion details. Experimental results show that the proposed MamEVSR achieves superior performance on various datasets quantitatively and qualitatively.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces MamEVSR, a novel Mamba-based network for event-based video super-resolution (VSR) that leverages the selective state space model, Mamba. MamEVSR addresses the limitations of convolutional neural networks and Transformers by offering global receptive field coverage with linear computational complexity. The network includes two key components: the interleaved Mamba (iMamba) block, which enables efficient feature fusion and propagation across bi-directional frames, and the cross-modality Mamba (cMamba) block, which facilitates interaction and aggregation between event information and the iMamba block output. This allows MamEVSR to capture finer motion details and achieve superior performance on various datasets.",
        "Tags": [
            "Super-Resolution",
            "Mamba",
            "Event Cameras",
            "Selective State Space Model",
            "Linear Computational Complexity"
        ]
    },
    {
        "Title": "EntitySAM: Segment Everything in Video",
        "Authors": "Mingqiao Ye \u00b7 Seoung Wug Oh \u00b7 Lei Ke \u00b7 Joon-Young Lee",
        "Abstract": "Automatically tracking and segmenting every video entity remains a significant challenge. Despite rapid advancements in video segmentation, even state-of-the-art models like SAM 2 struggle to consistently track all entities across a video\u2014a task we refer to as Video Entity Segmentation.We propose EntitySAM, a framework for zero-shot video entity segmentation. EntitySAM extends SAM 2 by removing the need for explicit prompts, allowing automatic discovery and tracking of all entities, including those appearing in later frames. We incorporate query-based entity discovery and association into SAM 2, inspired by transformer-based object detectors. Specifically, we introduce an entity decoder to facilitate inter-object communication and an automatic prompt generator using learnable object queries. Additionally, we add a semantic encoder to enhance SAM 2's semantic awareness, improving segmentation quality. Trained on image-level mask annotations without category information from the COCO dataset, EntitySAM demonstrates strong generalization on four zero-shot video segmentation tasks: Video Entity, Panoptic, Instance, and Semantic Segmentation. Results on six popular benchmarks show that EntitySAM outperforms previous unified video segmentation methods and strong baselines, setting new standards for zero-shot video segmentation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "EntitySAM is introduced as a framework for zero-shot video entity segmentation, addressing the challenge of tracking and segmenting all entities in a video without explicit prompts. It extends SAM 2 by incorporating query-based entity discovery and association, an entity decoder for inter-object communication, and an automatic prompt generator using learnable object queries. Enhanced with a semantic encoder, EntitySAM improves segmentation quality and demonstrates strong generalization across four zero-shot video segmentation tasks, outperforming previous methods on six benchmarks.",
        "Tags": [
            "Video Object Segmentation",
            "Zero-Shot Learning",
            "Transformer-based Object Detection",
            "Semantic Encoder",
            "Learnable Object Queries"
        ]
    },
    {
        "Title": "From Zero to Detail: Deconstructing Ultra-High-Definition Image Restoration from Progressive Spectral Perspective",
        "Authors": "Chen Zhao \u00b7 Zhizhou Chen \u00b7 Yunzhe Xu \u00b7 Enxuan Gu \u00b7 Jian Li \u00b7 Zili Yi \u00b7 qian Wang \u00b7 Jian Yang \u00b7 Ying Tai",
        "Abstract": "Ultra-high-definition (UHD) image restoration faces significant challenges due to its high resolution, complex content, and intricate details. To cope with these challenges, we analyze the restoration process in depth through a progressive spectral perspective, and deconstruct the complex UHD restoration problem into three progressive stages: zero-frequency enhancement, low-frequency restoration, and high-frequency refinement. Building on this insight, we propose a novel framework, ERR, which comprises three collaborative  sub-networks: the zero-frequency enhancer (ZFE), the low-frequency restorer (LFR), and the high-frequency refiner (HFR). Specifically, the ZFE integrates global priors to learn global mapping, while the LFR restores low-frequency information, emphasizing reconstruction of coarse-grained content. Finally, the HFR employs our designed  frequency-windowed Kolmogorov-Arnold Networks (FW-KAN) to refine textures and details, producing high-quality image restoration. Our approach significantly outperforms previous UHD methods across various tasks, with extensive ablation studies validating the effectiveness of each component.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenges of ultra-high-definition (UHD) image restoration by deconstructing the process into three progressive stages: zero-frequency enhancement, low-frequency restoration, and high-frequency refinement. The authors propose a novel framework, ERR, consisting of three sub-networks: the zero-frequency enhancer (ZFE), the low-frequency restorer (LFR), and the high-frequency refiner (HFR). The ZFE integrates global priors for global mapping, the LFR focuses on coarse-grained content reconstruction, and the HFR uses frequency-windowed Kolmogorov-Arnold Networks (FW-KAN) to refine textures and details. The framework demonstrates superior performance in UHD image restoration tasks.",
        "Tags": [
            "Super-Resolution",
            "Low-Level Vision",
            "Frequency-Windowed Kolmogorov-Arnold Networks",
            "Progressive Spectral Perspective",
            "UHD Image Restoration"
        ]
    },
    {
        "Title": "Chain of Attack: On the Robustness of Vision-Language Models Against Transfer-Based Adversarial Attacks",
        "Authors": "Peng Xie \u00b7 Yequan Bie \u00b7 Jianda Mao \u00b7 Yangqiu Song \u00b7 Yang Wang \u00b7 Hao Chen \u00b7 Kani Chen",
        "Abstract": "Pre-trained vision-language models (VLMs) have showcased remarkable performance in image and natural language understanding, such as image captioning and response generation. As the practical applications of vision-language models become increasingly widespread, their potential safety and robustness issues raise concerns that adversaries may evade the system and cause these models to generate toxic content through malicious attacks. Therefore, evaluating the robustness of open-source VLMs against adversarial attacks has garnered growing attention, with transfer-based attacks as a representative black-box attacking strategy. However, most existing transfer-based attacks neglect the importance of the semantic correlations between vision and text modalities, leading to sub-optimal adversarial example generation and attack performance. To address this issue, we present Chain of Attack (CoA), which iteratively enhances the generation of adversarial examples based on the multi-modal semantic update using a series of intermediate attacking steps, achieving superior adversarial transferability and efficiency. A unified attack success rate computing method is further proposed for automatic evasion evaluation. Extensive experiments conducted under the most realistic and high-stakes scenario, demonstrate that our attacking strategy is able to effectively mislead models to generate targeted responses using only black-box attacks without any knowledge of the victim models. The comprehensive robustness evaluation in our paper provides insight into the vulnerabilities of VLMs and offers a reference for the safety considerations of future model developments. The code will be made publically available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Pre-trained vision-language models (VLMs) have demonstrated strong performance in tasks like image captioning and response generation, but their robustness against adversarial attacks remains a concern. This paper introduces Chain of Attack (CoA), a method that iteratively improves adversarial example generation by leveraging multi-modal semantic updates, enhancing transferability and efficiency. The approach includes a unified attack success rate computation for evaluating evasion. Experiments show that CoA effectively misleads VLMs to produce targeted responses using black-box attacks, highlighting vulnerabilities and offering insights for future model safety.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Adversarial Attacks",
            "Multi-modal Semantic Update",
            "Black-box Attack Strategy",
            "Adversarial Transferability"
        ]
    },
    {
        "Title": "EditSplat: Multi-View Fusion and Attention-Guided Optimization for View-Consistent 3D Scene Editing with 3D Gaussian Splatting",
        "Authors": "Dong In Lee \u00b7 Hyeongcheol Park \u00b7 Jiyoung Seo \u00b7 Eunbyung Park \u00b7 Hyunje Park \u00b7 Ha Dam Baek \u00b7 Shin sangheon \u00b7 sangmin kim \u00b7 Sangpil Kim",
        "Abstract": "Recent advancements in 3D editing have highlighted the potential of text-driven methods in real-time, user-friendly AR/VR applications. However, current methods rely on 2D diffusion models without adequately considering multi-view information, resulting in multi-view inconsistency. While 3D Gaussian Splatting (3DGS) significantly improves rendering quality and speed, its 3D editing process encounters difficulties with inefficient optimization, as pre-trained Gaussians retain excessive source information, hindering optimization. To address these limitations, we propose EditSplat, a novel 3D editing framework that integrates Multi-view Fusion Guidance (MFG) and Attention-Guided Trimming (AGT). Our MFG ensures multi-view consistency by incorporating essential multi-view information into the diffusion process, leveraging classifier-free guidance from the text-to-image diffusion model and the geometric properties of 3DGS. Additionally, our AGT leverages the explicit representation of 3DGS to selectively prune and optimize 3D Gaussians, enhancing optimization efficiency and enabling precise, semantically rich local edits. Through extensive qualitative and quantitative evaluations, EditSplat achieves superior multi-view consistency and editing quality over existing methods, significantly enhancing overall efficiency.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "EditSplat introduces a novel 3D editing framework that addresses multi-view inconsistency and inefficient optimization in 3D Gaussian Splatting (3DGS). By integrating Multi-view Fusion Guidance (MFG) and Attention-Guided Trimming (AGT), EditSplat ensures multi-view consistency and enhances optimization efficiency. MFG incorporates multi-view information into the diffusion process, while AGT selectively prunes and optimizes 3D Gaussians for precise local edits. The framework demonstrates superior multi-view consistency and editing quality, significantly improving efficiency over existing methods.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "3D Generation",
            "Multi-view Fusion",
            "Attention-Guided Optimization",
            "3D Scene Editing"
        ]
    },
    {
        "Title": "DeRS: Towards Extremely Efficient Upcycled Mixture-of-Experts Models",
        "Authors": "Yongqi Huang \u00b7 Peng Ye \u00b7 Chenyu Huang \u00b7 Jianjian Cao \u00b7 Lin Zhang \u00b7 Baopu Li \u00b7 Gang Yu \u00b7 Tao Chen",
        "Abstract": "Upcycled Mixture-of-Experts (MoE) models have shown great potential in various tasks by converting the original Feed-Forward Network (FFN) layers in pre-trained dense models into MoE layers. However, these models still suffer from significant parameter inefficiency due to the introduction of multiple experts. In this work, we propose a novel DeRS (Decompose, Replace, and Synthesis) paradigm to overcome this shortcoming, which is motivated by our observations about the unique redundancy mechanisms of upcycled MoE experts. Specifically, DeRS decomposes the experts into one expert-shared base weight and multiple expert-specific delta weights, and subsequently represents these delta weights in lightweight forms. Our proposed DeRS paradigm can be applied to enhance parameter efficiency in two different scenarios, including: 1) DeRS Compression for inference stage, using sparsification or quantization to compress vanilla upcycled MoE models; and 2) DeRS Upcycling for training stage, employing lightweight sparse or low-rank matrixes to efficiently upcycle dense models into MoE models. Extensive experiments across three different tasks show that the proposed methods can achieve extreme parameter efficiency while maintaining the performance for both training and compression of upcycled MoE models.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces DeRS (Decompose, Replace, and Synthesis), a novel paradigm aimed at enhancing the parameter efficiency of upcycled Mixture-of-Experts (MoE) models. Traditional upcycled MoE models, which convert Feed-Forward Network layers into MoE layers, often suffer from parameter inefficiency due to the multiple experts they introduce. DeRS addresses this by decomposing experts into a shared base weight and expert-specific delta weights, represented in lightweight forms. This approach is applicable in two scenarios: DeRS Compression for inference, which uses sparsification or quantization to compress MoE models, and DeRS Upcycling for training, which employs lightweight matrices to efficiently convert dense models into MoE models. The method demonstrates significant parameter efficiency while maintaining performance across various tasks.",
        "Tags": [
            "Mixture-of-Experts (MoE)",
            "Model Compression",
            "Parameter Efficiency",
            "Sparsification",
            "Quantization",
            "Low-Rank Matrix Representation"
        ]
    },
    {
        "Title": "Learning Audio-guided Video Representation with Gated Attention for Video-Text Retrieval",
        "Authors": "Boseung Jeong \u00b7 Jicheol Park \u00b7 Sungyeon Kim \u00b7 Suha Kwak",
        "Abstract": "Video-text retrieval, the task of retrieving videos based on a textual query or vice versa, is of paramount importance for video understanding and multimodal information retrieval. Recent methods in this area rely primarily on visual and textual features and often ignore audio, although it helps enhance overall comprehension of video content.Moreover, traditional models that incorporate audio blindly utilize the audio input regardless of whether it is useful or not, resulting in suboptimal video representation. To address these limitations, we propose a novel video-text retrieval framework, Audio-guided VIdeo representation learning with GATEd attention (AVIGATE), that effectively leverages audio cues through a gated attention mechanism that selectively filters out uninformative audio signals.In addition, we propose an adaptive margin-based contrastive loss to deal with the inherently unclear positive-negative relationship between video and text, which facilitates learning better video-text alignment.Our extensive experiments demonstrate that AVIGATE achieves state-of-the-art performance on all the public benchmarks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces AVIGATE, a novel framework for video-text retrieval that leverages audio cues through a gated attention mechanism to selectively filter uninformative audio signals, enhancing video representation. It also proposes an adaptive margin-based contrastive loss to improve video-text alignment by addressing unclear positive-negative relationships. The framework demonstrates state-of-the-art performance on public benchmarks.",
        "Tags": [
            "Multimodal Learning",
            "Video Understanding",
            "Gated Attention Mechanism",
            "Adaptive Margin-based Contrastive Loss",
            "Audio-guided Video Representation"
        ]
    },
    {
        "Title": "Distilling Long-tailed Datasets",
        "Authors": "Zhenghao Zhao \u00b7 Haoxuan Wang \u00b7 Yuzhang Shang \u00b7 Kai Wang \u00b7 Yan Yan",
        "Abstract": "Dataset distillation (DD) aims to synthesize a small information-rich dataset from a large dataset for efficient neural network training. However, existing dataset distillation methods struggle with long-tailed datasets, which are prevalent in real-world scenarios. By investigating the reasons behind this unexpected result, we identified two main causes: 1) The distillation process on imbalanced datasets develops biased gradients, leading to the synthesis of similarly imbalanced distilled datasets. 2) The experts trained on such datasets perform suboptimally on tail classes, resulting in misguided distillation supervision and poor-quality soft-label initialization. To address these issues, we first propose Distribution-agnostic Matching to avoid directly matching the biased expert trajectories. It reduces the distance between the student and the biased expert trajectories and prevents the tail class bias from being distilled to the synthetic dataset. Moreover, we improve the distillation guidance with Expert Decoupling, which jointly matches the decoupled backbone and classifier to improve the tail class performance and initialize reliable soft labels. This work pioneers the field of long-tailed dataset distillation (LTDD), marking the first effective effort to distill long-tailed datasets.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Dataset distillation (DD) synthesizes compact datasets from larger ones for efficient neural network training but struggles with long-tailed datasets due to biased gradients and suboptimal expert performance on tail classes. This paper introduces Distribution-agnostic Matching to mitigate biased expert trajectories and Expert Decoupling to enhance distillation guidance by improving tail class performance and soft-label initialization. These contributions establish the first effective approach to long-tailed dataset distillation (LTDD).",
        "Tags": [
            "Long-Tail Learning",
            "Knowledge Distillation",
            "Dataset Distillation",
            "Expert Decoupling",
            "Distribution-agnostic Matching"
        ]
    },
    {
        "Title": "uCO3D: UnCommon Objects in 3D",
        "Authors": "Xingchen Liu \u00b7 Piyush Tayal \u00b7 Jianyuan Wang \u00b7 Jesus Zarzar \u00b7 Tom Monnier \u00b7 Konstantinos Tertikas \u00b7 Jiali Duan \u00b7 Antoine Toisoul \u00b7 Jason Y. Zhang \u00b7 Natalia Neverova \u00b7 Andrea Vedaldi \u00b7 Roman Shapovalov \u00b7 David Novotny",
        "Abstract": "We introduce Uncommon Objects in 3D (uCO3D), a new object-centric dataset for 3D deep learning and 3D generative AI.uCO3D is the largest publicly-available collection of  high-resolution videos of objects with 3D annotations that ensures full-360$^{\\circ}$ coverage.uCO3D is significantly more diverse than MVImgNet and CO3Dv2, covering more than 1,000 object categories.It is also of higher quality, due to extensive quality checks of both the collected videos and the 3D annotations.Similar to analogous datasets, uCO3D contains annotations for 3D camera poses, depth maps and sparse point clouds.In addition, each object is equipped with a caption and a 3D Gaussian Splat reconstruction.We train several large 3D models on MVImgNet, CO3Dv2, and uCO3Dand obtain superior results using the latter, showing that uCO3D is better for learning applications.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Uncommon Objects in 3D (uCO3D), a novel object-centric dataset designed for 3D deep learning and generative AI. uCO3D stands out as the largest publicly-available collection of high-resolution videos featuring objects with comprehensive 3D annotations, ensuring full 360-degree coverage. It surpasses existing datasets like MVImgNet and CO3Dv2 in diversity, encompassing over 1,000 object categories, and in quality, thanks to rigorous checks on video and annotation quality. The dataset includes 3D camera poses, depth maps, sparse point clouds, captions, and 3D Gaussian Splat reconstructions for each object. Training large 3D models on uCO3D yields superior results compared to other datasets, highlighting its effectiveness for learning applications.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "3D Reconstruction",
            "Datasets and Benchmarks",
            "3D Annotations",
            "Object-Centric Dataset",
            "High-Resolution Videos"
        ]
    },
    {
        "Title": "CraftsMan: High-fidelity Mesh Generation with 3D Native Generation and Interactive Geometry Refiner",
        "Authors": "Weiyu Li \u00b7 Jiarui Liu \u00b7 Hongyu Yan \u00b7 Rui Chen \u00b7 Yixun Liang \u00b7 Xuelin Chen \u00b7 Ping Tan \u00b7 Xiaoxiao Long",
        "Abstract": "We present a novel generative 3D modeling system, coined CraftsMan, which can generate high-fidelity 3D geometries with highly varied shapes, regular mesh topologies, and detailed surfaces, and, notably, allows for refining the geometry in an interactive manner. Despite the significant advancements in 3D generation, existing methods still struggle with lengthy optimization processes, self-occlusion, irregular mesh topologies, and difficulties in accommodating user edits, consequently impeding their widespread adoption and implementation in 3D modeling softwares. Our work is inspired by the craftsman, who usually roughs out the holistic figure of the work first and elaborates the surface details subsequently. Specifically, we first introduce a robust data preprocessing pipeline that utilizes visibility check and winding mumber to maximize the use of existing 3D data. Leveraging this data, we employ a 3D-native DiT model that directly models the distribution of 3D data in latent space, generating coarse geometries with regular mesh topology in seconds. Subsequently, a normal-based geometry refiner enhances local surface details, which can be applied automatically or interactively with user input. Extensive experiments demonstrate that our method achieves high efficacy in producing superior quality 3D assets compared to existing methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces CraftsMan, a novel generative 3D modeling system capable of producing high-fidelity 3D geometries with varied shapes, regular mesh topologies, and detailed surfaces. The system uniquely supports interactive geometry refinement, addressing common challenges in 3D generation such as lengthy optimization processes, self-occlusion, and irregular mesh topologies. CraftsMan employs a robust data preprocessing pipeline and a 3D-native DiT model to generate coarse geometries efficiently. A normal-based geometry refiner then enhances surface details, which can be automated or adjusted interactively by users. The method demonstrates superior efficacy in generating high-quality 3D assets compared to existing approaches.",
        "Tags": [
            "3D Generation",
            "3D Reconstruction",
            "Interactive Geometry Refinement",
            "Normal-based Surface Enhancement",
            "3D-native DiT Model"
        ]
    },
    {
        "Title": "ViCaS: A Dataset for Combining Holistic and Pixel-level Video Understanding using Captions with Grounded Segmentation",
        "Authors": "Ali Athar \u00b7 Xueqing Deng \u00b7 Liang-Chieh Chen",
        "Abstract": "Recent advances in multimodal large language models (MLLMs) have expanded research in video understanding, primarily focusing on high-level tasks such as video captioning and question-answering. Meanwhile, a smaller body of work addresses dense, pixel-precise segmentation tasks, which typically involve category-guided or referral-based object segmentation. Although both research directions are essential for developing models with human-level video comprehension, they have largely evolved separately, with distinct benchmarks and architectures. This paper aims to unify these efforts by introducing ViCaS, a new dataset containing thousands of challenging videos, each annotated with detailed, human-written captions and temporally consistent, pixel-accurate masks for multiple objects with phrase grounding. Our benchmark evaluates models on both holistic/high-level understanding and language-guided, pixel-precise segmentation. We also present carefully validated evaluation measures and propose an effective model architecture that can tackle our benchmark. All annotations, as well as the code and model weights will be made public.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces ViCaS, a novel dataset designed to bridge the gap between high-level video understanding and dense, pixel-level segmentation tasks. ViCaS contains thousands of challenging videos annotated with detailed captions and temporally consistent, pixel-accurate masks for multiple objects, enabling evaluation of both holistic understanding and language-guided segmentation. The paper also proposes a model architecture tailored for this benchmark and provides validated evaluation measures. All annotations, code, and model weights will be publicly available.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Video Understanding",
            "Semantic Segmentation",
            "Datasets and Benchmarks",
            "Phrase Grounding",
            "Temporally Consistent Masks",
            "Language-Guided Segmentation"
        ]
    },
    {
        "Title": "BlobGEN-Vid: Compositional Text-to-Video Generation with Blob Video Representations",
        "Authors": "Weixi Feng \u00b7 Chao Liu \u00b7 Sifei Liu \u00b7 William Yang Wang \u00b7 Arash Vahdat \u00b7 Weili Nie",
        "Abstract": "Existing video generation models struggle to follow complex text prompts and synthesize multiple objects, raising the need for additional grounding input for improved controllability. In this work, we propose to decompose videos into visual primitives -- blob video representation, a general representation for controllable video generation. Based on blob conditions, we develop a blob-grounded video diffusion model named BlobGEN-Vid that allows users to control object motions and fine-grained object appearance. In particular, we introduce a masked 3D attention module that effectively improves regional consistency across frames. In addition, we introduce a learnable module to interpolate text embeddings so that users can control semantics in specific frames and obtain smooth object transitions. We show that our framework is model-agnostic and build BlobGEN-Vid based on both U-Net and DiT-based video diffusion models. Extensive experimental results show that BlobGEN-Vid achieves superior zero-shot video generation ability and state-of-the-art layout controllability on multiple benchmarks. When combined with a Large Language Model for layout planning, our framework even outperforms proprietary text-to-video generators in terms of compositional accuracy.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces BlobGEN-Vid, a novel framework for compositional text-to-video generation using blob video representations. This approach addresses the limitations of existing video generation models in handling complex text prompts and synthesizing multiple objects by decomposing videos into visual primitives. The framework includes a blob-grounded video diffusion model that enhances controllability over object motions and appearances. Key innovations include a masked 3D attention module for improving regional consistency across frames and a learnable module for interpolating text embeddings to control semantics and ensure smooth object transitions. BlobGEN-Vid is model-agnostic and has been implemented using both U-Net and DiT-based video diffusion models. The framework demonstrates superior zero-shot video generation capabilities and state-of-the-art layout controllability, outperforming proprietary text-to-video generators when combined with a Large Language Model for layout planning.",
        "Tags": [
            "Video Generation",
            "Diffusion Models",
            "Blob Video Representations",
            "Masked 3D Attention",
            "Text Embedding Interpolation"
        ]
    },
    {
        "Title": "Synthetic-to-Real Self-supervised Robust Depth Estimation via Learning with Motion and Structure Priors",
        "Authors": "Weilong Yan \u00b7 Weilong Yan \u00b7 Ming Li \u00b7 Li Haipeng \u00b7 Shuwei Shao \u00b7 Robby T. Tan",
        "Abstract": "Self-supervised depth estimation from monocular cameras in diverse outdoor conditions, such as daytime, rain, and nighttime, is challenging due to the difficulty of learning universal representations and the severe lack of labeled real-world adverse data.Previous methods either rely on synthetic inputs and pseudo-depth labels or directly apply daytime strategies to adverse conditions, resulting in suboptimal results.In this paper, we present the first synthetic-to-real robust depth estimation framework, incorporating motion and structure priors to capture real-world knowledge effectively. In the synthetic adaptation, we transfer motion-structure knowledge inside cost volumes for better robust representation, using a frozen daytime model to train a depth estimator in synthetic adverse conditions.In the innovative real adaptation which targets to fix synthetic-real gaps, models trained earlier identify the weather-insensitive regions with a designed consistency-reweighting strategy to emphasize valid pseudo-labels.We further introduce a new regularization by gathering explicit depth distribution prior to constrain the model facing real-world data.Experiments show that our method outperforms the state-of-the-art across diverse conditions in multi-frame and single-frame settings. We achieve improvements of 7.5\\% in AbsRel and 4.3\\% in RMSE on average for nuScenes and Robotcar datasets (daytime, nighttime, rain). In zero-shot evaluation on DrivingStereo (rain, fog), our method generalizes better than previous ones. Our code will be released soon.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a synthetic-to-real robust depth estimation framework that leverages motion and structure priors to enhance depth estimation from monocular cameras across diverse outdoor conditions, including daytime, rain, and nighttime. The framework consists of two main adaptations: synthetic adaptation, which uses a frozen daytime model to train a depth estimator in synthetic adverse conditions, and real adaptation, which employs a consistency-reweighting strategy to identify weather-insensitive regions and emphasize valid pseudo-labels. Additionally, a new regularization technique based on explicit depth distribution prior is introduced to better constrain the model when dealing with real-world data. The method demonstrates superior performance over state-of-the-art approaches in both multi-frame and single-frame settings, showing significant improvements in accuracy metrics on the nuScenes and Robotcar datasets, and better generalization in zero-shot evaluations on the DrivingStereo dataset.",
        "Tags": [
            "Self-Supervised Learning",
            "Depth Estimation",
            "Synthetic-to-Real Learning",
            "Consistency-Reweighting Strategy",
            "Depth Distribution Prior"
        ]
    },
    {
        "Title": "V2X-R: Cooperative LiDAR-4D Radar Fusion with Denoising Diffusion for 3D Object Detection",
        "Authors": "Xun Huang \u00b7 Jinlong Wang \u00b7 Qiming Xia \u00b7 Siheng Chen \u00b7 Bisheng Yang \u00b7 Xin Li \u00b7 Cheng Wang \u00b7 Chenglu Wen",
        "Abstract": "Current Vehicle-to-Everything (V2X) systems have significantly enhanced 3D object detection using LiDAR and camera data. However, they face performance degradation in adverse weather. Weather-robust 4D radar, with Doppler velocity and additional geometric information, offers a promising solution to this challenge. To this end, we present V2X-R, the first simulated V2X dataset incorporating LiDAR, camera, and 4D radar modalities. V2X-R contains 12,079 scenarios with 37,727 frames of LiDAR and 4D radar point clouds, 150,908 images, and 170,859 annotated 3D vehicle bounding boxes. Subsequently, we propose a novel cooperative LiDAR-4D radar fusion pipeline for 3D object detection and implement it with multiple fusion strategies. To achieve weather-robust detection, we additionally propose a Multi-modal Denoising Diffusion (MDD) module in our fusion pipeline. MDD utilizes weather-robust 4D radar feature as a condition to guide the diffusion model in denoising noisy LiDAR features.Experiments show that our LiDAR-4D radar fusion pipeline demonstrates superior performance in the V2X-R dataset. Over and above this, our MDD module further improved the foggy/snowy performance of the basic fusion model by up to  5.73\\%/6.70\\% and barely disrupting normal performance. The dataset and code will be publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces V2X-R, a novel simulated V2X dataset that integrates LiDAR, camera, and 4D radar data to enhance 3D object detection, particularly in adverse weather conditions. The dataset includes extensive data points across various scenarios. A new cooperative fusion pipeline for LiDAR and 4D radar is proposed, featuring a Multi-modal Denoising Diffusion (MDD) module that uses 4D radar features to improve the robustness of LiDAR data against weather-induced noise. The fusion pipeline, enhanced by the MDD module, shows significant improvements in detection accuracy under foggy and snowy conditions without compromising normal performance.",
        "Tags": [
            "3D Object Detection",
            "Autonomous Driving",
            "4D Radar",
            "Denoising Diffusion",
            "Weather-Robust Detection"
        ]
    },
    {
        "Title": "EventGPT: Event Stream Understanding with Multimodal Large Language Models",
        "Authors": "shaoyu liu \u00b7 Jianing Li \u00b7 guanghui zhao \u00b7 Yunjian Zhang \u00b7 Xin Meng \u00b7 Fei Richard Yu \u00b7 Xiangyang Ji \u00b7 Ming Li",
        "Abstract": "Event cameras record visual information as asynchronous pixel change streams, excelling at scene perception under unsatisfactory lighting or high-dynamic conditions. Existing multimodal large language models (MLLMs) concentrate on natural RGB images, failing in scenarios where event data fits better. In this paper, we introduce EventGPT, the first MLLM for event stream understanding, to the best of our knowledge, marking a pioneering attempt to integrate large language models (LLMs) with event stream comprehension. Our EventGPT comprises an event encoder, followed by a spatio-temporal aggregator, a linear projector, an event-language adapter, and an LLM. Firstly, RGB image-text pairs generated by GPT are leveraged to warm up the linear projector, referring to LLaVA, as the gap between natural image and language modalities is relatively smaller. Secondly, we construct a synthetic yet large dataset, N-ImageNet-Chat, consisting of event frames and corresponding texts to enable the use of the spatio-temporal aggregator and to train the event-language adapter, thereby aligning event features more closely with the language space. Finally, we gather an instruction dataset, Event-Chat, which contains extensive real-world data to fine-tune the entire model, further enhancing its generalization ability. We construct a comprehensive evaluation benchmark, and extensive experiments demonstrate that EventGPT outperforms previous state-of-the-art MLLMs in generation quality, descriptive accuracy, and reasoning capability.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "EventGPT is introduced as the first multimodal large language model (MLLM) designed for event stream understanding, integrating large language models (LLMs) with event stream comprehension. The model includes an event encoder, spatio-temporal aggregator, linear projector, event-language adapter, and an LLM. It leverages RGB image-text pairs for initial training and uses a synthetic dataset, N-ImageNet-Chat, to align event features with the language space. An instruction dataset, Event-Chat, is also used for fine-tuning. EventGPT demonstrates superior performance in generation quality, descriptive accuracy, and reasoning capability compared to existing MLLMs.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Event Stream Understanding",
            "Event Cameras",
            "Spatio-Temporal Aggregation",
            "Event-Language Alignment"
        ]
    },
    {
        "Title": "A Theory of Learning Unified Model via Knowledge Integration from Label Space Varying Domains",
        "Authors": "Dexuan Zhang \u00b7 Thomas Westfechtel \u00b7 Tatsuya Harada",
        "Abstract": "Existing domain adaptation systems can hardly be applied to real-world problems with new classes presenting at deployment time, especially regarding source-free scenarios where multiple source domains do not share the label space despite being given a few labeled target data. To address this, we define a novel problem setting: multi-source semi-supervised open-set domain adaptation and propose a learning theory via joint error, effectively tackling strong domain shift. To generalize the algorithm into source-free cases, we introdcue a computationally efficient and architecture-flexible attention-based feature generation module. Extensive experiments on various data sets demonstrate the significant improvement of our proposed algorithm over baselines.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the challenge of domain adaptation in real-world scenarios where new classes emerge at deployment time, particularly in source-free settings with multiple source domains that do not share label spaces. The authors propose a novel problem setting called multi-source semi-supervised open-set domain adaptation and introduce a learning theory based on joint error to handle strong domain shifts. Additionally, they develop an attention-based feature generation module that is computationally efficient and architecture-flexible, enabling generalization to source-free cases. The proposed approach demonstrates significant improvements over baseline methods across various datasets.",
        "Tags": [
            "Domain Adaptation",
            "Open-Set Learning",
            "Knowledge Integration",
            "Attention Mechanism",
            "Multi-Source Learning",
            "Semi-Supervised Learning",
            "Feature Generation"
        ]
    },
    {
        "Title": "Leveraging Perturbation Robustness to Enhance Out-of-Distribution Detection",
        "Authors": "Wenxi Chen \u00b7 Raymond A. Yeh \u00b7 Shaoshuai Mou \u00b7 Yan Gu",
        "Abstract": "Out-of-distribution (OOD) detection is the task of identifying inputs that deviate from the training data distribution. This capability is essential for the safe deployment of deep computer vision models in open-world environments. In this work, we propose a post-hoc method, Perturbation-Rectified OOD detection (PRO), based on the insight that prediction confidence for OOD inputs is more susceptible to reduction under perturbation than IND inputs. From this observation, we proposed a meta-score function that searches for local minimum scores near original inputs by applying gradient descent. This procedure enhances the separability between in-distribution (IND) and OOD samples. Importantly, the approach improves OOD detection performance without complex modifications to the underlying model architectures or training protocol. To validate our approach, we conduct extensive experiments using the OpenOOD benchmark. Our approach further pushes the limit of softmax-based OOD detection and is the leading post-hoc method for small-scale models. On a CIFAR-10 model with adversarial training, PRO effectively detects near-OOD inputs, achieving a reduction of more than 10% on FPR@95 compared to state-of-the-art methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Perturbation-Rectified OOD detection (PRO), a post-hoc method for enhancing out-of-distribution (OOD) detection in deep computer vision models. The method leverages the observation that OOD inputs' prediction confidence is more susceptible to reduction under perturbation compared to in-distribution (IND) inputs. By applying gradient descent to search for local minimum scores near original inputs, PRO improves the separability between IND and OOD samples without requiring complex modifications to model architectures or training protocols. The approach demonstrates superior performance on the OpenOOD benchmark, particularly in detecting near-OOD inputs, and sets a new standard for softmax-based OOD detection in small-scale models.",
        "Tags": [
            "Out-of-Distribution Detection",
            "Data Augmentation",
            "Gradient Descent",
            "Softmax-based Detection",
            "Post-hoc Method"
        ]
    },
    {
        "Title": "Advancing Generalizable Tumor Segmentation with Anomaly-Aware Open-Vocabulary Attention Maps and Frozen Foundation Diffusion Models",
        "Authors": "Yankai Jiang \u00b7 Peng Zhang \u00b7 Donglin Yang \u00b7 Yuan Tian \u00b7 Hai Lin \u00b7 Xiaosong Wang",
        "Abstract": "We explore Generalizable Tumor Segmentation, aiming to train a single model for zero-shot tumor segmentation across diverse anatomical regions. Existing methods face limitations related to segmentation quality, scalability, and the range of applicable imaging modalities. In this paper, we uncover the potential of the internal representations within frozen medical foundation diffusion models as highly efficient zero-shot learners for tumor segmentation by introducing a novel framework named DiffuGTS. DiffuGTS creates anomaly-aware open-vocabulary attention maps based on text prompts to enable generalizable anomaly segmentation without being restricted by a predefined training category list. To further improve and refine anomaly segmentation masks, DiffuGTS leverages the diffusion model, transforming pathological regions into high-quality pseudo-healthy counterparts through latent space inpainting, and applies a novel pixel-level and feature-level residual learning approach, resulting in segmentation masks with significantly enhanced quality and generalization. Comprehensive experiments on four datasets and seven tumor categories demonstrate the superior performance of our method, surpassing current state-of-the-art models across multiple zero-shot settings. The codes will be made publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces DiffuGTS, a novel framework for generalizable tumor segmentation that leverages frozen medical foundation diffusion models for zero-shot learning across diverse anatomical regions. The method utilizes anomaly-aware open-vocabulary attention maps generated from text prompts, enabling segmentation without predefined categories. DiffuGTS enhances segmentation quality by transforming pathological regions into pseudo-healthy images through latent space inpainting and employs a residual learning approach at both pixel and feature levels. The framework demonstrates superior performance in zero-shot settings across multiple datasets and tumor categories.",
        "Tags": [
            "Medical Image Segmentation",
            "Diffusion Models",
            "Zero-Shot Learning",
            "Anomaly-Aware Attention Maps",
            "Latent Space Inpainting",
            "Residual Learning"
        ]
    },
    {
        "Title": "A Kernel Perspective on Few-Shot Adaptation of Large Vision-Language Models",
        "Authors": "Yassir Bendou \u00b7 Amine Ouasfi \u00b7 Vincent Gripon \u00b7 Adnane Boukhayma",
        "Abstract": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has led to its widespread application in various visual downstream tasks. To enhance CLIP's effectiveness, efficient few-shot adaptation techniques have been widely adopted. Among these approaches, training-free methods, particularly caching methods exemplified by Tip-Adapter, have gained attention for their lightweight adaptation without the need for additional fine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective, showing that caching methods function as local adapters and are connected to a well-established kernel literature. Leveraging this insight, we offer a theoretical understanding of how these methods operate and suggest multiple avenues for enhancing over the Tip-Adapter baseline. Notably, our analysis shows the importance of incorporating global information in local adapters. Therefore, we subsequently propose a global method that learns a proximal regularizer in a reproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our method, that we call ProKeR (Proximal Kernel ridge Regression), has a closed form solution and achieves state-of-the-art performance across 11 datasets in the standard few-shot adaptation benchmark.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper explores few-shot adaptation techniques for Contrastive Language-Image Pretraining (CLIP) models, focusing on training-free methods like Tip-Adapter. By analyzing these methods from a kernel perspective, the authors demonstrate their connection to kernel literature and propose enhancements. They introduce ProKeR (Proximal Kernel ridge Regression), a global method that leverages a reproducing kernel Hilbert space (RKHS) and achieves state-of-the-art performance on 11 datasets in few-shot adaptation benchmarks.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Few-Shot Learning",
            "Kernel Methods",
            "Reproducing Kernel Hilbert Space (RKHS)",
            "Proximal Regularizer"
        ]
    },
    {
        "Title": "Adaptive Part Learning for Fine-Grained Generalized Category Discovery: A Plug-and-Play Enhancement",
        "Authors": "Qiyuan Dai \u00b7 Hanzhuo Huang \u00b7 Yu Wu \u00b7 Sibei Yang",
        "Abstract": "Generalized Category Discovery (GCD) aims to recognize unlabeled images from known and novel classes by distinguishing novel classes from known ones, while also transferring knowledge from another set of labeled images with known classes. Existing GCD methods rely on self-supervised vision transformers such as DINO for representation learning. However, focusing solely on the global representation of the DINO CLS token introduces an inherent trade-off between discriminability and generalization. In this paper, we introduce an adaptive part discovery and learning method, called APL, which generates consistent object parts and their correspondences across different similar images using a set of shared learnable part queries and DINO part priors, without requiring any additional annotations. More importantly, we propose a novel all-min contrastive loss to learn discriminative yet generalizable part representation, which adaptively highlights discriminative object parts to distinguish similar categories for enhanced discriminability while simultaneously sharing other parts to facilitate knowledge transfer for improved generalization. Our APL can easily be incorporated into different GCD frameworks by replacing their CLS token feature with our part representations, showing significant enhancements on fine-grained datasets.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Adaptive Part Learning (APL), a method designed to enhance Generalized Category Discovery (GCD) by generating consistent object parts and their correspondences across similar images using shared learnable part queries and DINO part priors. APL employs a novel all-min contrastive loss to learn discriminative yet generalizable part representations, which adaptively highlight discriminative object parts to distinguish similar categories and share other parts to facilitate knowledge transfer. The method can be seamlessly integrated into existing GCD frameworks by replacing their CLS token features with part representations, demonstrating significant improvements on fine-grained datasets.",
        "Tags": [
            "Generalized Category Discovery (GCD)",
            "Vision Transformer (ViT)",
            "Adaptive Part Learning",
            "All-Min Contrastive Loss",
            "Fine-Grained Classification"
        ]
    },
    {
        "Title": "GeoAvatar: Geometrically-Consistent Multi-Person Avatar Reconstruction from Sparse Multi-View Videos",
        "Authors": "SooHyun Lee \u00b7 SeoYeon Kim \u00b7 HeeKyung Lee \u00b7 Won-Sik Cheong \u00b7 Jooho Lee",
        "Abstract": "Multi-person avatar reconstruction from sparse multiview videos is challenging.The independent reconstruction of individual avatars often fails to capture the geometric relationships among multiple instances, resulting in inter-penetrations between avatars.While some researchers have resolved this issue using neural volumetric rendering techniques, these approaches suffer from huge computational costs for rendering and training.In this paper, we propose a multi-person avatar reconstruction method that reconstructs 3D avatars while preserving the geometric relations between people.Our 2D Gaussian Splatting (2DGS)-based avatar representation allows us to represent geometrically accurate surfaces of multiple instances that support sharp inside-outside tests.To efficiently influence the occluded instances, we design a differentiable multi-layer alpha blending system compatible with the GS rendering pipeline.We mitigate inter-penetrations among avatars by penalizing segmentation discrepancies and seeing through near-contact regions to reveal penetrating parts.We also utilize monocular priors to enhance quality in less-observed and textureless surfaces.Our proposed method achieves fast reconstruction while maintaining state-of-the-art performance in terms of geometry and rendering quality.We demonstrate the efficiency and effectiveness of our method on a multi-person dataset containing close interactions.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of multi-person avatar reconstruction from sparse multi-view videos, focusing on maintaining geometric consistency among avatars to avoid inter-penetrations. The authors propose a novel method using 2D Gaussian Splatting (2DGS) for avatar representation, which ensures geometrically accurate surfaces and supports sharp inside-outside tests. A differentiable multi-layer alpha blending system is introduced to efficiently handle occluded instances, and inter-penetrations are mitigated through segmentation discrepancy penalties and visibility adjustments in near-contact regions. Monocular priors are utilized to improve the quality of less-observed and textureless surfaces. The method achieves fast reconstruction with state-of-the-art geometry and rendering quality, demonstrated on a dataset featuring close interactions.",
        "Tags": [
            "Avatars",
            "3DGS (Gaussian Splatting)",
            "Differentiable Rendering",
            "Geometric Consistency",
            "Multi-Person Interaction"
        ]
    },
    {
        "Title": "Weakly Supervised Semantic Segmentation via Progressive Confidence Region Expansion",
        "Authors": "Xiangfeng Xu \u00b7 Pinyi Zhang \u00b7 Wenxuan Huang \u00b7 Yunhang Shen \u00b7 Haosheng Chen \u00b7 Jingzhong Lin \u00b7 Wei Li \u00b7 Gaoqi He \u00b7 Jiao Xie \u00b7 Shaohui Lin",
        "Abstract": "Weakly supervised semantic segmentation (WSSS) has garnered considerable attention due to its effective reduction of annotation costs. Most approaches utilize Class Activation Maps (CAM) to produce pseudo-labels, thereby localizing target regions using only image-level annotations. However, the prevalent methods relying on vision transformers (ViT) encounter an \"over-expansion\" issue, i.e., CAM incorrectly expands high activation value from the target object to the background regions, as it is difficult to learn pixel-level local intrinsic inductive bias in ViT from weak supervisions. To solve this problem, we propose a Progressive Confidence Region Expansion (PCRE) framework for WSSS, it gradually learns a faithful mask over the target region and utilizes this mask to correct the confusion in CAM. PCRE has two key components: \"Confidence Region Mask Expansion\" (CRME) and \"Class-Prototype Enhancement\" (CPE). CRME progressively expands the mask in the small region with the highest confidence, eventually encompassing the entire target, thereby avoiding unintended CPE aims to enhance mask generation in CRME by leveraging the similarity between the learned, dataset-level class prototypes and patch features as supervision to optimize the mask output from CRME. Extensive experiments demonstrate that our method outperforms the existing single-stage and multi-stage approaches on the PASCAL VOC and MS COCO benchmark datasets.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Weakly supervised semantic segmentation (WSSS) reduces annotation costs by using image-level annotations to generate pseudo-labels via Class Activation Maps (CAM). However, vision transformers (ViT) often suffer from over-expansion, where CAM incorrectly extends high activation values to background regions. To address this, the authors propose a Progressive Confidence Region Expansion (PCRE) framework, which includes two key components: Confidence Region Mask Expansion (CRME) and Class-Prototype Enhancement (CPE). CRME gradually expands a mask from the most confident region to cover the entire target, while CPE improves mask generation by leveraging class prototypes and patch feature similarities. The method outperforms existing approaches on PASCAL VOC and MS COCO benchmarks.",
        "Tags": [
            "Semantic Segmentation",
            "Weakly Supervised Learning",
            "Vision Transformer (ViT)",
            "Class Activation Maps (CAM)",
            "Progressive Learning",
            "Class-Prototype Enhancement"
        ]
    },
    {
        "Title": "PreciseCam: Precise Camera Control for Text-to-Image Generation",
        "Authors": "Edurne Bernal-Berdun \u00b7 Ana Serrano \u00b7 Belen Masia \u00b7 Matheus Gadelha \u00b7 Yannick Hold-Geoffroy \u00b7 Xin Sun \u00b7 Diego Gutierrez",
        "Abstract": "Images as an artistic medium often rely on specific camera angles and lens distortions to convey ideas or emotions; however, such precise control is missing in current text-to-image models. We propose an efficient and general solution that allows precise control over the camera when generating both photographic and artistic images. Unlike prior methods that rely on predefined shots, we rely solely on four simple extrinsic and intrinsic camera parameters, removing the need for pre-existing geometry, reference 3D objects, and multi-view data.We also present a novel dataset with more than 57,000 images, along with their text prompts and ground-truth camera parameters. Our evaluation shows precise camera control in text-to-image generation, surpassing traditional prompt engineering approaches.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces PreciseCam, a method for achieving precise camera control in text-to-image generation, addressing the limitations of current models that lack such control. The approach uses four simple camera parameters to generate both photographic and artistic images without requiring predefined shots, pre-existing geometry, reference 3D objects, or multi-view data. A novel dataset of over 57,000 images with text prompts and ground-truth camera parameters is also presented. The method demonstrates superior camera control compared to traditional prompt engineering approaches.",
        "Tags": [
            "Text-to-Image Generation",
            "Image Editing",
            "Camera Parameter Control",
            "Artistic Image Generation",
            "Dataset Creation"
        ]
    },
    {
        "Title": "TAPT: Test-Time Adversarial Prompt Tuning for Robust Inference in Vision-Language Models",
        "Authors": "Xin Wang \u00b7 Kai Chen \u00b7 Jiaming Zhang \u00b7 Jingjing Chen \u00b7 Xingjun Ma",
        "Abstract": "Large pre-trained Vision-Language Models (VLMs) such as CLIP have demonstrated excellent zero-shot generalizability across various downstream tasks. However, recent studies have shown that the inference performance of CLIP can be greatly degraded by small adversarial perturbations, especially its visual modality, posing significant safety threats. To mitigate this vulnerability, in this paper, we propose a novel defense method called Test-Time Adversarial Prompt Tuning (TAPT) to enhance the inference robustness of CLIP against visual adversarial attacks. TAPT is a test-time defense method that learns defensive bimodal (textual and visual) prompts to robustify the inference process of CLIP. Specifically, it is an unsupervised method that optimizes the defensive prompts for each test sample by minimizing a multi-view entropy and aligning adversarial-clean distributions. We evaluate the effectiveness of TAPT on 11 benchmark datasets, including ImageNet and 10 other zero-shot datasets, demonstrating that it enhances the zero-shot adversarial robustness of the original CLIP by at least 48.9\\% against AutoAttack (AA), while largely maintaining performance on clean examples. Moreover, TAPT outperforms existing adversarial prompt tuning methods across various backbones, achieving an average robustness improvement of at least 36.6\\%.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Large pre-trained Vision-Language Models (VLMs) like CLIP exhibit strong zero-shot generalizability but are vulnerable to adversarial perturbations, particularly in the visual modality. To address this, the paper introduces Test-Time Adversarial Prompt Tuning (TAPT), a defense method that enhances CLIP's robustness against visual adversarial attacks. TAPT learns defensive bimodal (textual and visual) prompts at test time by minimizing multi-view entropy and aligning adversarial-clean distributions. Evaluated on 11 benchmark datasets, including ImageNet, TAPT improves CLIP's zero-shot adversarial robustness by at least 48.9% against AutoAttack while maintaining performance on clean examples. It also surpasses existing adversarial prompt tuning methods, achieving an average robustness improvement of at least 36.6%.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Adversarial Robustness",
            "Test-Time Defense",
            "Bimodal Prompt Tuning",
            "Adversarial-Clean Distribution Alignment"
        ]
    },
    {
        "Title": "A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation",
        "Authors": "Andrew Z Wang \u00b7 Songwei Ge \u00b7 Tero Karras \u00b7 Ming-Yu Liu \u00b7 Yogesh Balaji",
        "Abstract": "Both text-to-image generation and large language models (LLMs) have made significant advancements. However, many text-to-image models still employ the somewhat outdated T5 and CLIP as their text encoders.In this work, we investigate the effectiveness of using modern decoder-only LLMs as text encoders for text-to-image diffusion models. We build a standardized training and evaluation pipeline that allows us to isolate and evaluate the effect of different text embeddings. We train a total of 22 text-to-image models with 12 different text encoders to analyze the critical aspects of LLMs that could impact text-to-image generation, including the approaches to extract embeddings, different LLMs variants, and model sizes.Our experiments reveal that the de facto way of using last-layer embeddings as conditioning leads to inferior performance.Instead, we explore embeddings from various layers and find that usinglayer-normalized averaging across all layers significantly improves alignment with complex prompts. LLMs with this conditioning outperform the baseline T5 model, showing enhanced performance in advanced visio-linguistic reasoning skills.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This study explores the use of modern decoder-only large language models (LLMs) as text encoders for text-to-image diffusion models, addressing the limitations of traditional text encoders like T5 and CLIP. A standardized training and evaluation pipeline was developed to assess the impact of different text embeddings on text-to-image generation. The research involved training 22 models with 12 text encoders, focusing on embedding extraction methods, LLM variants, and model sizes. Findings indicate that using last-layer embeddings for conditioning is suboptimal, while layer-normalized averaging across all layers significantly improves alignment with complex prompts. This approach enhances performance in visio-linguistic reasoning, outperforming the baseline T5 model.",
        "Tags": [
            "Text-to-Image Generation",
            "Large Language Models (LLMs)",
            "Diffusion Models",
            "Layer-Normalized Averaging",
            "Visio-Linguistic Reasoning"
        ]
    },
    {
        "Title": "Ref-GS: Directional Factorization for 2D Gaussian Splatting",
        "Authors": "Youjia Zhang \u00b7 Anpei Chen \u00b7 Yumin Wan \u00b7 Zikai Song \u00b7 Junqing Yu \u00b7 Yawei Luo \u00b7 Wei Yang",
        "Abstract": "In this paper, we introduce $\\textit{Ref-GS}$, a novel approach for directional light factorization in 2D Gaussian splatting, which enables photorealistic view-dependent appearance rendering and precise geometry recovery. $\\textit{Ref-GS}$ builds upon the deferred rendering of Gaussian splatting and applies directional encoding to the deferred-rendered surface, effectively reducing the ambiguity between orientation and viewing angle. Next, we introduce a spherical mip-grid to capture varying levels of surface roughness, enabling roughness-aware Gaussian shading. Additionally, we propose a simple yet efficient geometry-lighting factorization that connects geometry and lighting via the vector outer product, significantly reducing renderer overhead when integrating volumetric attributes. Our method achieves superior photorealistic rendering for a range of open-world scenes while also accurately recovering geometry.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper presents Ref-GS, a novel method for directional light factorization in 2D Gaussian splatting, enabling photorealistic view-dependent rendering and precise geometry recovery. Ref-GS leverages deferred rendering and directional encoding to reduce ambiguity between orientation and viewing angle. It introduces a spherical mip-grid for roughness-aware Gaussian shading and a geometry-lighting factorization technique that connects geometry and lighting via vector outer products, reducing renderer overhead. The method achieves high-quality photorealistic rendering and accurate geometry recovery in open-world scenes.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Neural Rendering",
            "Directional Encoding",
            "Roughness-Aware Shading",
            "Geometry-Lighting Factorization"
        ]
    },
    {
        "Title": "TSAM: Temporal SAM Augmented with Multimodal Prompts for Referring Audio-Visual Segmentation",
        "Authors": "Abduljalil Radman \u00b7 Jorma Laaksonen",
        "Abstract": "Referring audio-visual segmentation (Ref-AVS) aims to segment objects within audio-visual scenes using multimodal cues embedded in text expressions. While the Segment Anything Model (SAM) has revolutionized visual segmentation, its applicability to Ref-AVS, where multimodal cues act as novel prompts, remains unexplored. SAM\u2019s limitation to single-frame segmentation also hinders its ability to capture essential temporal context needed for multi-frame audio-visual segmentation. To address this gap, we propose TSAM, a novel extension of SAM designed to leverage multimodal cues for precise segmentation in dynamic audio-visual scenes. TSAM enhances SAM\u2019s image encoder with a temporal modeling branch, enabling spatio-temporal learning and deep multimodal fusion across video frames, while retaining SAM\u2019s pre-trained knowledge. Additionally, TSAM replaces SAM\u2019s user-interactive prompting mechanism with sparse and dense data-driven prompts, enabling more effective integration of audio-visual inputs and reference text expressions. Extensive experiments on the Ref-AVS dataset demonstrate the superiority of our proposed TSAM over state-of-the-art methods, underscoring its effectiveness in accurately segmenting objects in audio-visual scenes guided by text-based multimodal cues and its strong generalization to unseen objects.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces TSAM, an extension of the Segment Anything Model (SAM) tailored for referring audio-visual segmentation (Ref-AVS). TSAM addresses SAM's limitations by incorporating a temporal modeling branch for spatio-temporal learning and deep multimodal fusion across video frames, while preserving SAM's pre-trained knowledge. It replaces SAM's user-interactive prompting with data-driven prompts to better integrate audio-visual inputs and text expressions. TSAM demonstrates superior performance on the Ref-AVS dataset, showcasing its ability to accurately segment objects in dynamic audio-visual scenes guided by multimodal cues and its generalization to unseen objects.",
        "Tags": [
            "Multimodal Learning",
            "Semantic Segmentation",
            "Temporal Modeling",
            "Multimodal Fusion",
            "Data-Driven Prompts"
        ]
    },
    {
        "Title": "Q-PART: Quasi-Periodic Adaptive Regression with Test-time Training for Pediatric  Left Ventricular Ejection Fraction Regression",
        "Authors": "Jie Liu \u00b7 Tiexin Qin \u00b7 Hui Liu \u00b7 Yilei Shi \u00b7 Lichao Mou \u00b7 Xiao Xiang Zhu \u00b7 Shiqi Wang \u00b7 Haoliang Li",
        "Abstract": "In this work, we address the challenge of adaptive pediatric Left Ventricular Ejection Fraction (LVEF) assessment. While Test-time Training (TTT) approaches show promise for this task, they suffer from two significant limitations. Existing TTT works are primarily designed for classification tasks rather than continuous value regression, and they lack mechanisms to handle the quasi-periodic nature of cardiac signals. To tackle these issues, we propose a novel  \\textbf{Q}uasi-\\textbf{P}eriodic \\textbf{A}daptive \\textbf{R}egression with \\textbf{T}est-time Training (Q-PART) framework. In the training stage, the proposed Quasi-Period Network decomposes the echocardiogram into periodic and aperiodic components within latent space by combining parameterized helix trajectories with Neural Controlled Differential Equations. During inference, our framework further employs a variance minimization strategy across image augmentations that simulate common quality issues in echocardiogram acquisition, along with differential adaptation rates for periodic and aperiodic components. Theoretical analysis is provided to demonstrate that our variance minimization objective effectively bounds the regression error under mild conditions. Furthermore, extensive experiments across three pediatric age groups demonstrate that Q-PART not only significantly outperforms existing approaches in pediatric LVEF prediction, but also exhibits strong clinical screening capability with high mAUROC scores (up to 0.9747) and maintains gender-fair performance across all metrics, validating its robustness and practical utility in pediatric echocardiography analysis. The relevant dataset and code will be released upon acceptance of this paper.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Q-PART, a novel framework for adaptive pediatric Left Ventricular Ejection Fraction (LVEF) assessment, addressing limitations in existing Test-time Training (TTT) approaches. Q-PART leverages a Quasi-Period Network to decompose echocardiograms into periodic and aperiodic components using parameterized helix trajectories and Neural Controlled Differential Equations. During inference, it employs a variance minimization strategy across image augmentations and differential adaptation rates for these components. Theoretical analysis supports the framework's effectiveness, and extensive experiments across pediatric age groups demonstrate superior performance in LVEF prediction, high clinical screening capability, and gender-fair performance, highlighting its robustness and practical utility in pediatric echocardiography analysis.",
        "Tags": [
            "Medical Image Analysis",
            "Test-time Training",
            "Quasi-Periodic Signal Processing",
            "Variance Minimization Strategy",
            "Neural Controlled Differential Equations"
        ]
    },
    {
        "Title": "One2Any: One-Reference 6D Pose Estimation for Any Object",
        "Authors": "Mengya Liu \u00b7 Siyuan Li \u00b7 Ajad Chhatkuli \u00b7 Prune Truong \u00b7 Luc Van Gool \u00b7 Federico Tombari",
        "Abstract": "6D object pose estimation remains challenging for many applications due to dependencies on complete 3D models, multi-view images, or training limited to specific object categories. These requirements make generalization to novel objects difficult for which neither 3D models nor multi-view images may be available. To address this, we propose a novel method One2Any that estimates the relative 6-degrees of freedom (DOF) object pose using only a single reference-single query RGB-D image, without prior knowledge of its 3D model, multi-view data, or category constraints.We treat object pose estimation as an encoding-decoding process: first, we obtain a comprehensive Reference Object Pose Embedding (ROPE) that encodes an object\u2019s shape, orientation, and texture from a single reference view. Using this embedding, a U-Net-based pose decoding module produces Reference Object Coordinate (ROC) for new views, enabling fast and accurate pose estimation. This simple encoding-decoding framework allows our model to be trained on any pair-wise pose data, enabling large-scale training and demonstrating great scalability.Experiments on multiple benchmark datasets demonstrate that our model generalizes well to novel objects, achieving state-of-the-art accuracy and robustness even rivaling methods that require multi-view or CAD inputs, at a fraction of compute.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces One2Any, a novel method for 6D object pose estimation that requires only a single reference-single query RGB-D image, eliminating the need for complete 3D models, multi-view images, or category-specific training. The approach treats pose estimation as an encoding-decoding process, where a Reference Object Pose Embedding (ROPE) is first obtained from a single reference view to encode the object's shape, orientation, and texture. A U-Net-based pose decoding module then uses this embedding to produce Reference Object Coordinates (ROC) for new views, facilitating fast and accurate pose estimation. This method allows for large-scale training and demonstrates scalability, generalizing well to novel objects and achieving state-of-the-art accuracy and robustness, even outperforming methods that require multi-view or CAD inputs.",
        "Tags": [
            "6D Object Pose Estimation",
            "RGB-D Image Processing",
            "Single-View Pose Estimation",
            "U-Net Architecture",
            "Scalable Training"
        ]
    },
    {
        "Title": "BOOTPLACE: Bootstrapped Object Placement with Detection Transformers",
        "Authors": "Hang Zhou \u00b7 Xinxin Zuo \u00b7 Rui Ma \u00b7 Li Cheng",
        "Abstract": "In this paper, we tackle the copy-paste image-to-image composition problem with a focus on object placement learning. Prior methods have leveraged generative models to minimize the need for dense supervision, which unfortunately may limit their ability to model complex data distributions. Alternatively, transformer networks with a sparse contrastive loss have been employed; yet their over-relaxed regularization often leads to imprecise placement. We propose BOOTPLACE, a novel paradigm that formulates object placement as a placement-by-detection problem. Our method first identifies regions of interest suitable for object placement by training a dedicated detection transformer on object-subtracted backgrounds with multi-object supervisions. It then associates each target compositing object with detected regions based on semantic complementary. Using a boostrapped training approach on randomly object-subtracted images, our model regularizes meaningful placements through richly paired data augmentation. Experimental results on standard benchmarks demonstrate BOOTPLACE's superior performance in object reposition, significantly outperforming state-fo-the-art baselines on Cityscapes and OPA datasets with notable improvements in IOU scores. Additional ablation studies further showcase the compositionality and generalizability of our approach, supported by user study evaluations.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces BOOTPLACE, a novel approach to the copy-paste image-to-image composition problem, focusing on object placement learning. Unlike previous methods that rely on generative models or transformer networks with sparse contrastive loss, BOOTPLACE formulates object placement as a placement-by-detection problem. It identifies suitable regions for object placement by training a detection transformer on object-subtracted backgrounds with multi-object supervision and associates target objects with detected regions based on semantic complementarity. The method employs a bootstrapped training approach on randomly object-subtracted images, enhancing placement precision through data augmentation. BOOTPLACE demonstrates superior performance in object reposition on standard benchmarks, outperforming state-of-the-art baselines on Cityscapes and OPA datasets, with significant improvements in IOU scores. The approach's compositionality and generalizability are further validated through ablation studies and user evaluations.",
        "Tags": [
            "Object Detection",
            "DETR (Detection Transformer)",
            "Image-to-Image Composition",
            "Semantic Complementarity",
            "Bootstrapped Training"
        ]
    },
    {
        "Title": "Advancing Adversarial Robustness in GNeRFs: The IL2-NeRF Attack",
        "Authors": "Nicole Meng \u00b7 Caleb Manicke \u00b7 Ronak Sahu \u00b7 Caiwen Ding \u00b7 Yingjie Lao",
        "Abstract": "Generalizable Neural Radiance Fields (GNeRF) are recognized as one of the most promising techniques for novel view synthesis and 3D model generation in real-world applications. However, like other generative models in computer vision, ensuring their adversarial robustness against various threat models is essential for practical use. The pioneering work in this area, NeRFool, introduced a state-of-the-art attack that targets GNeRFs by manipulating source views before feature extraction, successfully disrupting the color and density results of the constructed views. Building on this foundation, we propose IL2-NeRF(Iterative $L_2$ NeRF Attack), a novel adversarial attack method that explores a new threat model(in the $L_2$ domain) for attacking GNeRFs.  We evaluated IL2-NeRF against two standard GNeRF models across three benchmark datasets, demonstrating similar performance compared to NeRFool, based on the same evaluation metrics proposed by NeRFool. Our results establish IL2-NeRF as the first adversarial method for GNeRFs under the $L_2$ norm. We establish a foundational $L_2$ threat model for future research, enabling direct performance comparisons while introducing a smoother, image-wide perturbation approach in Adversarial 3D Reconstruction.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Generalizable Neural Radiance Fields (GNeRF) are a promising technique for novel view synthesis and 3D model generation, but their adversarial robustness remains a critical concern. This paper introduces IL2-NeRF, a novel adversarial attack method that operates in the $L_2$ domain, targeting GNeRFs by applying image-wide perturbations. IL2-NeRF demonstrates comparable performance to the state-of-the-art NeRFool attack across standard GNeRF models and benchmark datasets. The method establishes a foundational $L_2$ threat model for future research in adversarial 3D reconstruction, offering a smoother perturbation approach.",
        "Tags": [
            "NeRF (Neural Radiance Fields)",
            "Adversarial Robustness",
            "$L_2$ Threat Model",
            "Adversarial 3D Reconstruction",
            "Image-Wide Perturbation"
        ]
    },
    {
        "Title": "3D Occupancy Prediction with Low-Resolution Queries via Prototype-aware View Transformation",
        "Authors": "Gyeongrok Oh \u00b7 Sungjune Kim \u00b7 Heeju Ko \u00b7 Hyunggun Chi \u00b7 Jinkyu Kim \u00b7 Dongwook Lee \u00b7 Daehyun Ji \u00b7 Sungjoon Choi \u00b7 Sujin Jang \u00b7 Sangpil Kim",
        "Abstract": "The resolution of voxel queries significantly influences the quality of view transformation in camera-based 3D occupancy prediction. However, computational constraints and the practical necessity for real-time deployment require smaller query resolutions, which inevitably leads to an information loss. Therefore, it is essential to encode and preserve rich visual details within limited query sizes while ensuring a comprehensive representation of 3D occupancy. To this end, we introduce ProtoOcc, a novel occupancy network that leverages prototypes of clustered image segments in view transformation to enhance low-resolution context. In particular, the mapping of 2D prototypes onto 3D voxel queries encodes high-level visual geometries and complements the loss of spatial information from reduced query resolutions. Additionally, we design a multi-perspective decoding strategy to efficiently disentangle the densely compressed visual cues into a high-dimensional 3D occupancy scene. Experimental results on both Occ3D and SemanticKITTI benchmarks demonstrate the effectiveness of the proposed method, showing clear improvements over the baselines. More importantly, ProtoOcc achieves competitive performance against the baselines even with 75% reduced voxel resolution.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces ProtoOcc, a novel occupancy network designed to improve 3D occupancy prediction using low-resolution voxel queries. ProtoOcc leverages prototypes of clustered image segments during view transformation to encode high-level visual geometries, compensating for information loss due to reduced query resolutions. A multi-perspective decoding strategy is also proposed to efficiently disentangle compressed visual cues into a high-dimensional 3D occupancy scene. The method demonstrates superior performance on benchmarks like Occ3D and SemanticKITTI, achieving competitive results even with significantly reduced voxel resolution.",
        "Tags": [
            "3D Reconstruction",
            "3D Semantic Scene Completion",
            "Prototype-based Encoding",
            "Multi-Perspective Decoding",
            "Low-Resolution Queries"
        ]
    },
    {
        "Title": "ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices",
        "Authors": "Hao Yu \u00b7 Tangyu Jiang \u00b7 Shuning Jia \u00b7 Shannan Yan \u00b7 Shunning Liu \u00b7 Haolong Qian \u00b7 Guanghao Li \u00b7 Shuting Dong \u00b7 Chun Yuan",
        "Abstract": "The Transformer architecture has revolutionized various regions since it was proposed, and its effectiveness largely depends on the ability to encode positional information.Traditional position encoding methods exhibit significant limitations due to lack of robustness and flexibility of position.Therefore, Rotary Positional Encoding (RoPE) was proposed to alleviate these issues, which integrates positional information by rotating the embeddings in the attention mechanism.However, RoPE requires manually defined rotation matrices with limited transformation space, constraining the model's capacity.In this work, we propose ComRoPE, which generalizes RoPE by defining it in terms of trainable commuting angle matrices.Specifically, we demonstrate that pairwise commutativity of these matrices is essential for RoPE to achieve scalability and positional robustness. We formally define the RoPE Equation, which is an essential condition that ensures consistent performance with position offsets. Based on the theoretical analysis, we present two types of trainable commuting angle matrices as sufficient solutions to the RoPE equation,which significantly improve performance, surpassing the current state-of-the-art method by 1.6% at training resolution and 2.9% at higher resolution on the ImageNet-1K dataset.Furthermore, our framework shows versatility in generalizing to existing RoPE formulations and offering new insights for future positional encoding research. To ensure reproducibility, the source code and instructions are available at https://anonymous.4open.science/r/ComRoPE-08AA",
        "Link": "https://anonymous.4open.science/r/ComRoPE-08AA",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The Transformer architecture's effectiveness relies heavily on its ability to encode positional information. Traditional methods often lack robustness and flexibility. Rotary Positional Encoding (RoPE) was introduced to address these issues by rotating embeddings in the attention mechanism. However, RoPE's manually defined rotation matrices limit its transformation space. This paper introduces ComRoPE, which generalizes RoPE using trainable commuting angle matrices. The authors demonstrate that pairwise commutativity of these matrices is crucial for scalability and positional robustness. They formally define the RoPE Equation, ensuring consistent performance with position offsets. Two types of trainable commuting angle matrices are presented as solutions to the RoPE equation, significantly improving performance on the ImageNet-1K dataset. The framework also generalizes existing RoPE formulations and provides new insights for future research.",
        "Tags": [
            "Large Language Models (LLMs)",
            "Self-Supervised Learning",
            "Positional Encoding",
            "Attention Mechanism",
            "Trainable Matrices"
        ]
    },
    {
        "Title": "AnomalyNCD: Towards Novel Anomaly Class Discovery in Industrial Scenarios",
        "Authors": "Ziming Huang \u00b7 Xurui Li \u00b7 Haotian Liu \u00b7 Feng Xue \u00b7 Yuzhe Wang \u00b7 Yu Zhou",
        "Abstract": "Recently, multi-class anomaly classification has garnered increasing attention.Previous methods directly cluster anomalies but often struggle due to the lack of anomaly-prior knowledge.Acquiring this knowledge faces two issues: the non-prominent and weak-semantics anomalies.In this paper,we propose AnomalyNCD,a multi-class anomaly classification network compatible with different anomaly detection  methods.To address the non-prominence of anomalies,we design main element binarization (MEBin) to obtain anomaly-centered images,ensuring anomalies are learned while avoiding the impact of incorrect detections.Next, to learn anomalies with weak semantics,we design mask-guided representation learning,which focuses on isolated anomalies guided by masksand reduces confusion from erroneous inputs through re-corrected pseudo labels.Finally, to enable flexible classification at both region and image levels,we develop a region merging strategy that determines the overall image category based on the classified anomaly regions.Our method outperforms the state-of-the-art works on the MVTec AD and MTD datasets.Compared with the current methods,AnomalyNCD combined with zero-shot anomaly detection method achieves a 10.8\\% $F_1$ gain,8.8\\% NMI gain,and 9.5\\% ARI gain on MVTec AD,and 12.8\\% $F_1$ gain,5.7\\% NMI gain,and 10.8\\% ARI gain on MTD.The code will be publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces AnomalyNCD, a novel multi-class anomaly classification network designed for industrial scenarios. It addresses challenges in anomaly classification, such as the non-prominence and weak semantics of anomalies, through innovative techniques like main element binarization (MEBin) for anomaly-centered images and mask-guided representation learning for isolated anomalies. The method also includes a region merging strategy for flexible classification at both region and image levels. AnomalyNCD demonstrates superior performance over state-of-the-art methods on the MVTec AD and MTD datasets, particularly when combined with zero-shot anomaly detection methods, achieving significant improvements in F1 score, NMI, and ARI metrics.",
        "Tags": [
            "Anomaly Detection",
            "Zero-Shot Learning",
            "Mask-Guided Learning",
            "Region Merging Strategy",
            "Industrial Anomaly Classification"
        ]
    },
    {
        "Title": "GASP: Gaussian Avatars with Synthetic Priors",
        "Authors": "Jack Saunders \u00b7 Charlie Hewitt \u00b7 Yanan Jian \u00b7 Marek Kowalski \u00b7 Tadas Baltrusaitis \u00b7 Yiye Chen \u00b7 Darren Cosker \u00b7 Virginia Estellers \u00b7 Nicholas Gyd\u00e9 \u00b7 Vinay P. Namboodiri \u00b7 Benjamin E Lundell",
        "Abstract": "Gaussian Splatting has changed the game for real-time photo-realistic rendering. One of the most popular applications of Gaussian Splatting is to create animatable avatars, known as Gaussian Avatars. Recent works have pushed the boundaries of quality and rendering efficiency but suffer from two main limitations. Either they require expensive multi-camera rigs to produce avatars with free-view rendering, or they can be trained with a single camera but only rendered at high quality from this fixed viewpoint. An ideal model would be trained using a short monocular video or image from available hardware, such as a webcam, and rendered from any view. To this end, we propose $\\textbf{GASP: Gaussian Avatars with Synthetic Priors}$. To overcome the limitations of existing datasets, we exploit the pixel-perfect nature of synthetic data to train a Gaussian Avatar prior. By fitting this prior model to a single photo or videoand fine-tuning it, we get a high-quality Gaussian Avatar, which supports 360$^\\circ$ rendering. Our prior is only required for fitting, not inference, enabling real-time application. Through our method, we obtain high-quality, animatable Avatars from limited data which can be animated and rendered at 70fps on commercial hardware.",
        "Link": "https://aka.ms/GASP",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Gaussian Splatting has revolutionized real-time photo-realistic rendering, particularly in creating animatable avatars known as Gaussian Avatars. Existing methods either require costly multi-camera setups or are limited to high-quality rendering from a single viewpoint. To address these limitations, the authors propose GASP: Gaussian Avatars with Synthetic Priors. This approach leverages synthetic data to train a Gaussian Avatar prior, enabling the creation of high-quality avatars from a single photo or video. The method supports 360\u00b0 rendering and real-time application, achieving 70fps on commercial hardware without requiring the prior during inference.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Avatars",
            "Monocular Video Training",
            "Real-Time Rendering",
            "360\u00b0 Rendering"
        ]
    },
    {
        "Title": "GLUS: Global-Local Reasoning Unified into A Single Large Language Model for Video Segmentation",
        "Authors": "Lang Lin \u00b7 Xueyang Yu \u00b7 Ziqi Pang \u00b7 Yu-Xiong Wang",
        "Abstract": "This paper proposes a novel framework utilizing multi-modal large language models (MLLMs) for referring video object segmentation (RefVOS). Previous MLLM-based methods commonly struggle with the dilemma between \"Ref\" and \"VOS\": they either specialize in understanding a few key frames (global reasoning) or tracking objects on continuous frames (local reasoning), and rely on external VOS or frame selectors to mitigate the other end of the challenge. However, our framework GLUS shows that Global and Local consistency can be Unified into a single video Segmentation MLLM: a set of sparse \"context frames\" provides global information, while a stream of continuous \"query frames\" conducts local object tracking. This is further supported by jointly training the MLLM with a pre-trained VOS memory bank to simultaneously digest short-range and long-range temporal information. To improve the information efficiency within the limited context window of MLLMs, we introduce object contrastive learning to distinguish hard false-positive objects and a self-refined framework to identify crucial frames and perform propagation. By collectively integrating these insights, our GLUS delivers a simple yet effective baseline, achieving new state-of-the-art for MLLMs on the MeViS and Ref-Youtube-VOS benchmark.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces GLUS, a novel framework that unifies global and local reasoning within a single multimodal large language model (MLLM) for referring video object segmentation (RefVOS). Unlike previous methods that rely on external tools to balance global and local reasoning, GLUS integrates both aspects by using sparse 'context frames' for global information and continuous 'query frames' for local object tracking. The framework is enhanced by joint training with a pre-trained VOS memory bank to handle short-range and long-range temporal information. Additionally, object contrastive learning and a self-refined framework are introduced to improve information efficiency and identify crucial frames. GLUS achieves state-of-the-art performance on the MeViS and Ref-Youtube-VOS benchmarks.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Video Object Segmentation",
            "Object Contrastive Learning",
            "Self-Refined Framework",
            "Temporal Information Integration"
        ]
    },
    {
        "Title": "Flowing from Words to Pixels: A Framework for Cross-Modality Evolution",
        "Authors": "Qihao Liu \u00b7 Xi Yin \u00b7 Alan L. Yuille \u00b7 Andrew Brown \u00b7 Mannat Singh",
        "Abstract": "Diffusion models, and their generalization, flow matching, have had a remarkable impact on the field of media generation. Here, the conventional approach is to learn the complex mapping from a simple source distribution of Gaussian noise to the target media distribution. For cross-modal tasks such as text-to-image generation, this same mapping from noise to image is learnt whilst including a conditioning mechanism in the model. One key and thus far relatively unexplored feature of flow matching is that, unlike Diffusion models, they are not constrained for the source distribution to be noise. Hence, in this paper, we propose a paradigm shift, and ask the question of whether we can instead train flow matching models to learn a direct mapping from the distribution of one modality to the distribution of another, thus obviating the need for both the noise distribution and conditioning mechanism. We present a general and simple framework, CrossFlow, for cross-modal flow matching. We show the importance of applying Variational Encoders to the input data, and introduce a method to enable Classifier-free guidance. Surprisingly, for text-to-image, CrossFlow with a vanilla transformer without cross attention slightly outperforms standard flow matching, and we show that it scales better with training steps and model size, while also allowing for interesting latent arithmetic which results in semantically meaningful edits in the output space. To demonstrate the generalizability of our approach, we also show that CrossFlow is on par with or outperforms the state-of-the-art for various cross-modal / intra-modal mapping tasks, viz. image captioning, depth estimation, and image super-resolution. We hope this paper contributes to accelerating progress in cross-modal media generation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces CrossFlow, a novel framework for cross-modal flow matching that eliminates the need for noise distribution and conditioning mechanisms traditionally used in diffusion models. By leveraging Variational Encoders and Classifier-free guidance, CrossFlow demonstrates superior performance in text-to-image generation, outperforming standard flow matching techniques. The framework's effectiveness is further validated across various cross-modal and intra-modal tasks, including image captioning, depth estimation, and image super-resolution, showcasing its potential to advance cross-modal media generation.",
        "Tags": [
            "Diffusion Models",
            "Text-to-Image Generation",
            "Flow Matching",
            "Cross-Modal Learning",
            "Classifier-free Guidance"
        ]
    },
    {
        "Title": "M3amba: Memory Mamba is All You Need for Whole Slide Image Classification",
        "Authors": "Tingting Zheng \u00b7 Kui Jiang \u00b7 Yi Xiao \u00b7 Sicheng Zhao \u00b7 Hongxun Yao",
        "Abstract": "Multi-instance learning (MIL) has demonstrated impressive performance in whole slide image (WSI) analysis. However, existing approaches struggle with undesirable results and unbearable computational overhead due to the quadratic complexity of Transformers. Recently, Mamba has offered a feasible solution for modeling long-range dependencies with linear complexity. However, vanilla Mamba inherently suffers from contextual forgetting issues, making it ill-suited for capturing global dependencies across instances in large-scale WSIs. To address this, we propose a memory-driven Mamba network, dubbed M3amba, to fully explore the global latent relations among instances. Specifically, M3amba retains and iteratively updates historical information with a dynamic memory bank (DMB), thus overcoming the catastrophic forgetting defects of Mamba for long-term context representation. For better feature representation, M3amba involves an intra-group bidirectional Mamba (BiMamba) block to refine local interactions within groups. Meanwhile, we additionally perform cross-attention fusion to incorporate relevant historical information across groups, facilitating richer inter-group connections. The joint learning of inter- and intra-group representations with memory merits enables M3amba with a more powerful capability for achieving accurate and comprehensive WSI representation. Extensive experiments on four datasets demonstrate that M3amba outperforms the state-of-the-art by 6.2\\% and 7.0\\% in accuracy on the TCGA BRAC and TCGA Lung datasets while maintaining low computational costs.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces M3amba, a memory-driven Mamba network designed to address the limitations of existing multi-instance learning (MIL) approaches in whole slide image (WSI) classification. Traditional methods, particularly those based on Transformers, face challenges with computational overhead and contextual forgetting. M3amba overcomes these issues by incorporating a dynamic memory bank (DMB) to retain and update historical information, thus mitigating the catastrophic forgetting problem. Additionally, it employs an intra-group bidirectional Mamba (BiMamba) block for refining local interactions and cross-attention fusion for enhancing inter-group connections. This combination allows M3amba to achieve superior WSI representation with improved accuracy and computational efficiency, as demonstrated by its performance on the TCGA BRAC and TCGA Lung datasets.",
        "Tags": [
            "Mamba",
            "Medical Image Analysis",
            "Dynamic Memory Bank",
            "Bidirectional Mamba",
            "Cross-Attention Fusion"
        ]
    },
    {
        "Title": "RENO: Real-Time Neural Compression for 3D LiDAR Point Clouds",
        "Authors": "Kang You \u00b7 Tong Chen \u00b7 Dandan Ding \u00b7 M. Salman Asif \u00b7 Zhan Ma",
        "Abstract": "Despite the substantial advancements demonstrated by learning-based neural models in the LiDAR Point Cloud Compression (LPCC) task, realizing real-time compression\u2014an indispensable criterion for numerous industrial applications\u2014remains a formidable challenge. This paper proposes RENO, the first real-time neural codec for 3D LiDAR point clouds, achieving superior performance with a lightweight model. RENO skips the octree construction and directly builds upon the multiscale sparse tensor representation. Instead of the multi-stage inferring, RENO devises sparse occupancy codes, which exploit cross-scale correlation and derive voxels' occupancy in a one-shot manner, greatly saving processing time. Experimental results demonstrate that the proposed RENO achieves real-time coding speed, 10 fps at 14-bit depth on a desktop platform (e.g., one RTX 3090 GPU) for both encoding and decoding processes, while providing 12.25\\% and 48.34\\% bit-rate savings compared to G-PCCv23 and Draco, respectively, at a similar quality. RENO model size is merely 1MB, making it attractive for practical applications. The source code will be made publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces RENO, a real-time neural codec for 3D LiDAR point clouds, designed to address the challenge of real-time compression essential for industrial applications. RENO utilizes a lightweight model that bypasses traditional octree construction, instead employing a multiscale sparse tensor representation and sparse occupancy codes to efficiently determine voxel occupancy in a single step. This approach significantly reduces processing time, enabling RENO to achieve real-time coding speeds of 10 fps at 14-bit depth on a desktop platform with an RTX 3090 GPU. Compared to G-PCCv23 and Draco, RENO offers bit-rate savings of 12.25% and 48.34%, respectively, while maintaining similar quality. The model's compact size of 1MB further enhances its suitability for practical applications.",
        "Tags": [
            "3D Point Cloud",
            "Neural Architecture Search (NAS)",
            "Real-Time Processing",
            "Sparse Tensor Representation",
            "Lightweight Model"
        ]
    },
    {
        "Title": "S2Gaussian: Sparse-View Super-Resolution 3D Gaussian Splatting",
        "Authors": "Yecong Wan \u00b7 Mingwen Shao \u00b7 Yuanshuo Cheng \u00b7 Wangmeng Zuo",
        "Abstract": "In this paper, we aim ambitiously for a realistic yet challenging problem, namely, how to reconstruct high-quality 3D scenes from sparse low-resolution views that simultaneously suffer from deficient perspectives and contents. Whereas existing methods only deal with either sparse views or low-resolution observations, they fail to handle such hybrid and complicated scenarios. To this end, we propose a novel Sparse-view Super-resolution 3D Gaussian Splatting framework, dubbed S2Gaussian, that can reconstruct structure-accurate and detail-faithful 3D scenes with only sparse and low-resolution views. The S2Gaussian operates in a two-stage fashion. In the first stage, we initially optimize a low-resolution Gaussian representation with depth regularization and densify it to initialize the high-resolution Gaussians through a tailored Gaussian Shuffle operation. In the second stage, we refine the high-resolution Gaussians with the super-resolved images generated from both original sparse views and pseudo-views rendered by the low-resolution Gaussians. In which a customized blur-free inconsistency modeling scheme and a 3D robust optimization strategy are elaborately designed to mitigate multi-view inconsistency and eliminate erroneous updates caused by imperfect supervision. Extensive experiments demonstrate superior results and in particular establishing new state-of-the-art performances with more consistent geometry and finer details.",
        "Link": "https://jeasco.github.io/S2Gaussian/",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenging problem of reconstructing high-quality 3D scenes from sparse, low-resolution views, which existing methods fail to handle effectively. The authors propose S2Gaussian, a novel two-stage framework that first optimizes a low-resolution Gaussian representation with depth regularization and then refines high-resolution Gaussians using super-resolved images from original and pseudo-views. The framework incorporates a blur-free inconsistency modeling scheme and a 3D robust optimization strategy to mitigate multi-view inconsistencies and erroneous updates. The approach achieves state-of-the-art results, producing 3D scenes with consistent geometry and fine details.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "3D Reconstruction",
            "Sparse-View Reconstruction",
            "Super-Resolution",
            "Depth Regularization"
        ]
    },
    {
        "Title": "MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation",
        "Authors": "Zhenyu Wu \u00b7 Yuheng Zhou \u00b7 Xiuwei Xu \u00b7 Ziwei Wang \u00b7 Haibin Yan",
        "Abstract": "Mobile manipulation is the fundamental challenge for robotics to assist humans with diverse tasks and environments in everyday life. However, conventional mobile manipulation approaches often struggle to generalize across different tasks and environments because of the lack of large-scale training.In contrast, recent advances in vision-language-action (VLA) models have shown impressive generalization capabilities, but these foundation models are developed for fixed-base manipulation tasks.Therefore, we propose an efficient policy adaptation framework to transfer pre-trained VLA models of fix-base manipulation to mobile manipulation, so that high generalization ability across tasks and environments can be achieved in mobile manipulation policy.Specifically, we utilize pre-trained VLA models to generate waypoints of the end-effector with high generalization ability. We design motion planning objectives for the mobile base and the robot arm, which aim at maximizing the physical feasibility of the trajectory. Finally, we present an efficient bi-level objective optimization framework for trajectory generation, where the upper-level optimization predicts waypoints for base movement to enhance the manipulator policy space, and the lower-level optimization selects the optimal end-effector trajectory to complete the manipulation task. Extensive experimental results on OVMM and the real world demonstrate that our method achieves a 4.2\\% higher success rate than the state-of-the-art mobile manipulation, and only requires 50 training cost for real world deployment due to the strong generalization ability in the pre-trained VLA models.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces MoManipVLA, a framework designed to transfer pre-trained vision-language-action (VLA) models from fixed-base manipulation tasks to mobile manipulation tasks, enhancing generalization across diverse tasks and environments. The approach leverages pre-trained VLA models to generate generalized end-effector waypoints and incorporates a bi-level optimization framework for trajectory generation. This framework optimizes base movement and end-effector trajectory to maximize physical feasibility and task success. The method demonstrates superior performance with a 4.2% higher success rate compared to state-of-the-art mobile manipulation techniques and significantly reduces training costs for real-world deployment.",
        "Tags": [
            "Embodied AI",
            "Vision-Language Models (VLMs)",
            "Mobile Manipulation",
            "Bi-level Optimization",
            "Trajectory Generation"
        ]
    },
    {
        "Title": "SIDA: Social Media Image Deepfake Detection, Localization and Explanation with Large Multimodal Model",
        "Authors": "Zhenglin Huang \u00b7 Jinwei Hu \u00b7 Yiwei He \u00b7 Xiangtai Li \u00b7 Xiaowei Huang \u00b7 Bei Peng \u00b7 Xingyu Zhao \u00b7 Baoyuan Wu \u00b7 Guangliang Cheng",
        "Abstract": "The rapid advancement of generative models in creating highly realistic images poses substantial risks for misinformation dissemination. For instance, a synthetic image, when shared on social media, can mislead extensive audiences and erode trust in digital content, resulting in severe repercussions. Despite some progress, academia has not yet created a large and diversified deepfake detection dataset for social media, nor has it devised an effective solution to address this issue. In this paper, we introduce the $\\textbf{S}$ocial media $\\textbf{I}$mage $\\textbf{D}$etection data$\\textbf{Set}$ (SID-Set), which offers three key advantages: (1) $\\textbf{extensive volume}$, featuring 300K AI-generated/tampered and authentic images with comprehensive annotations, (2) $\\textbf{broad diversity}$, encompassing fully synthetic and tampered images across various classes, and (3) $\\textbf{elevated realism}$, with images that are predominantly indistinguishable from genuine ones through mere visual inspection. Furthermore, leveraging the exceptional capabilities of large multimodal models, we propose a new image deepfake detection, localization, and explanation framework, named SIDA ($\\textbf{S}$ocial media $\\textbf{I}$mage $\\textbf{D}$etection, localization, and explanation $\\textbf{A}$ssistant). SIDA not only discerns the authenticity of images, but also delineates tampered regions through mask prediction and provides textual explanations of the model's judgment criteria. Compared with state-of-the-art deepfake detection models on SID-Set and other benchmarks, extensive experiments demonstrate that SIDA achieves superior performance among diversified settings. The code, model, and dataset will be released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the growing challenge of deepfake images on social media by introducing SID-Set, a large and diverse dataset of 300K AI-generated/tampered and authentic images with comprehensive annotations. The dataset is characterized by its extensive volume, broad diversity, and elevated realism. Leveraging the capabilities of large multimodal models, the authors propose SIDA, a framework for deepfake detection, localization, and explanation. SIDA not only identifies image authenticity but also predicts tampered regions and provides textual explanations for its judgments. The framework demonstrates superior performance compared to state-of-the-art models on SID-Set and other benchmarks.",
        "Tags": [
            "Deepfake Detection",
            "Multimodal Large Language Models (MLLMs)",
            "Image Tampering Localization",
            "Textual Explanation",
            "Social Media Misinformation"
        ]
    },
    {
        "Title": "PSHuman: Photorealistic Single-image 3D Human Reconstruction using Cross-Scale Multiview Diffusion and Explicit Remeshing",
        "Authors": "Peng Li \u00b7 Wangguandong Zheng \u00b7 Yuan Liu \u00b7 Tao Yu \u00b7 Yangguang Li \u00b7 Xingqun Qi \u00b7 Xiaowei Chi \u00b7 Siyu Xia \u00b7 Yan-Pei Cao \u00b7 Wei Xue \u00b7 Wenhan Luo \u00b7 Yike Guo",
        "Abstract": "Photorealistic 3D human modeling is essential for various applications and has seen tremendous progress. However, existing methods for monocular full-body reconstruction, typically relying on front and/or predicted back view, still struggle with satisfactory performance due to the ill-posed nature of the problem and sophisticated self-occlusions. In this paper, we propose PSHuman, a novel framework that explicitly reconstructs human meshes utilizing priors from the multiview diffusion model.  It is found that directly applying multiview diffusion on single-view human images leads to severe geometric distortions, especially on generated faces. To address it, we propose a cross-scale diffusion that models the joint probability distribution of global full-body shape and local facial characteristics, enabling detailed and identity-preserved novel-view generation without any geometric distortion. Moreover, to enhance cross-view body shape consistency of varied human poses, we condition the generative model on parametric models like SMPL-X, which provide body priors and prevent unnatural views inconsistent with human anatomy. Leveraging the generated multiview normal and color images, we present SMPLX-initialized explicit human carving to recover realistic textured human meshes efficiently. Extensive experiments on CAPE and THuman2.1 datasets demonstrate PSHuman's superiority in geometry details, texture fidelity, and generalization capability.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces PSHuman, a novel framework for photorealistic 3D human reconstruction from a single image. It addresses the challenges of monocular full-body reconstruction by utilizing a cross-scale multiview diffusion model that jointly models the global full-body shape and local facial characteristics, ensuring detailed and identity-preserved novel-view generation without geometric distortion. The framework also incorporates parametric models like SMPL-X to enhance cross-view body shape consistency across varied human poses. The method efficiently recovers realistic textured human meshes using SMPLX-initialized explicit human carving, demonstrating superior performance in geometry details, texture fidelity, and generalization capability.",
        "Tags": [
            "3D Human Mesh Estimation",
            "3D Reconstruction",
            "Multiview Diffusion",
            "SMPL-X",
            "Explicit Remeshing"
        ]
    },
    {
        "Title": "Improving Semi-Supervised Semantic Segmentation with Sliced-Wasserstein Feature Alignment and Uniformity",
        "Authors": "Chen Yi Lu \u00b7 Kasra Derakhshandeh \u00b7 Somali Chaterji",
        "Abstract": "Semi-supervised semantic segmentation with consistencyregularization capitalizes on unlabeled images to enhancethe accuracy of pixel-level segmentation. Current consistencylearning methods primarily rely on the consistency loss be-tween pseudo-labels and unlabeled images, neglecting the in-formation within the feature representations of the backboneencoder. Preserving maximum information in feature embed-dings requires achieving the alignment and uniformity objec-tives, as widely studied. To address this, we present SWSEG,a semi-supervised semantic segmentation algorithm that opti-mizes alignment and uniformity using the Sliced-WassersteinDistance (SWD), and rigorously and empirically proves thisconnection. We further resolve the computational issues as-sociated with conventional Monte Carlo-based SWD by im-plementing a Gaussian-approximated variant, which not onlymaintains the alignment and uniformity objectives but alsoimproves training efficiency. We evaluate SWSEG on thePASCAL VOC 2012, Cityscapes, and ADE20K datasets, out-shining supervised baselines in mIoU by up to 11.8%, 8.9%,and 8.2%, respectively, given an equivalent number of labeledsamples. Further, SWSEG surpasses state-of-the-art methodsin multiple settings across these three datasets. Our extensiveablation studies confirm the optimization of the uniformityand alignment objectives of the feature representations.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces SWSEG, a semi-supervised semantic segmentation algorithm that enhances pixel-level segmentation accuracy by optimizing alignment and uniformity in feature representations using the Sliced-Wasserstein Distance (SWD). SWSEG addresses the computational challenges of traditional SWD methods by implementing a Gaussian-approximated variant, improving training efficiency while maintaining the objectives of alignment and uniformity. The algorithm demonstrates superior performance on the PASCAL VOC 2012, Cityscapes, and ADE20K datasets, outperforming supervised baselines and state-of-the-art methods in various settings. Ablation studies confirm the effectiveness of SWSEG in optimizing feature representation objectives.",
        "Tags": [
            "Semantic Segmentation",
            "Self-Supervised Learning",
            "Sliced-Wasserstein Distance",
            "Feature Alignment",
            "Uniformity Optimization"
        ]
    },
    {
        "Title": "GenAssets: Generating in-the-wild 3D Assets in Latent Space",
        "Authors": "Ze Yang \u00b7 Jingkang Wang \u00b7 Haowei Zhang \u00b7 Sivabalan Manivasagam \u00b7 Yun Chen \u00b7 Raquel Urtasun",
        "Abstract": "High-quality 3D assets for traffic participants such as vehicles and motorcycles is critical for multi-sensor simulation, which is required for the safe end-to-end development of autonomy. Building assets from in-the-wild real-world data is key for diversity and realism, but existing neural-rendering based reconstruction methods are slow and generate assets that can only render close to the original viewpoints of observed actors, restricting usage in simulation. Recent diffusion-based generative models build complete and diverse assets, but perform poorly on in-the-wild driving scenes, where observed actors are captured under sparse and limited fields of view, and are partially occluded. In this work, we propose a 3D latent diffusion model that learns on in-the-wild LiDAR and camera data captured by a sensor platform and generates high quality 3D assets with complete geometry and appearance. Key to our method is a ``reconstruct-then-generate'' approach that first leverages occlusion-aware neural rendering trained over multiple scenes to build a high-quality latent space for objects, and then trains a generative diffusion model that operates on the latent space. We show our method outperforms existing reconstruction and generative-based methods, unlocking diverse and scalable content creation for simulation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces GenAssets, a 3D latent diffusion model designed to generate high-quality 3D assets for traffic participants like vehicles and motorcycles from in-the-wild LiDAR and camera data. This method addresses the limitations of existing neural-rendering and diffusion-based models by employing a 'reconstruct-then-generate' approach. Initially, it uses occlusion-aware neural rendering to create a high-quality latent space for objects, followed by training a generative diffusion model on this latent space. The proposed method surpasses current reconstruction and generative techniques, enabling diverse and scalable content creation for simulation.",
        "Tags": [
            "3D Generation",
            "Diffusion Models",
            "3D Reconstruction",
            "Latent Space Modeling",
            "Occlusion-Aware Neural Rendering",
            "In-the-Wild Data Utilization"
        ]
    },
    {
        "Title": "Multi-party Collaborative Attention Control for Image Customization",
        "Authors": "Han Yang \u00b7 Chuanguang Yang \u00b7 Qiuli Wang \u00b7 Zhulin An \u00b7 Weilun Feng \u00b7 Libo Huang \u00b7 Yongjun Xu",
        "Abstract": "The rapid development of diffusion models has fueled a growing demand for customized image generation. However, current customization methods face several limitations: 1) typically accept either image or text conditions alone; 2) customization in complex visual scenarios often leads to subject leakage or confusion; 3) image-conditioned outputs tend to suffer from inconsistent backgrounds; and 4) high computational costs. To address these issues, this paper introduces Multi-party Collaborative Attention Control (MCA-Ctrl), a tuning-free approach that enables high-quality image customization under both text and complex visual conditions. Specifically, MCA-Ctrl leverages two key operations within the self-attention layer to coordinate multiple parallel diffusion processes and guide the target image generation. This approach allows MCA-Ctrl to capture the content and appearance of specific subjects while maintaining semantic consistency with the conditional input. Additionally, to mitigate subject leakage and confusion issues common in complex visual scenarios, we introduce a Subject Localization Module that extracts precise subject and editable image layers based on user instructions. Extensive quantitative and human evaluation experiments demonstrate that MCA-Ctrl outperforms previous methods in zero-shot image customization, effectively addressing the aforementioned issues.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Multi-party Collaborative Attention Control (MCA-Ctrl), a tuning-free approach for high-quality image customization under both text and complex visual conditions. MCA-Ctrl leverages key operations within the self-attention layer to coordinate multiple parallel diffusion processes, ensuring semantic consistency and precise subject localization. The proposed Subject Localization Module addresses issues of subject leakage and confusion, enabling effective customization in complex scenarios. MCA-Ctrl demonstrates superior performance in zero-shot image customization compared to existing methods.",
        "Tags": [
            "Diffusion Models",
            "Image Generation",
            "Self-Attention Mechanisms",
            "Subject Localization",
            "Zero-Shot Customization"
        ]
    },
    {
        "Title": "Consistent Normal Orientation for 3D Point Clouds via Least Squares on Delaunay Graph",
        "Authors": "Rao Fu \u00b7 Jianmin Zheng \u00b7 Liang Yu",
        "Abstract": "The orientation of surface normals in 3D point cloud is a fundamental problem in computer vision and graphics. Determining a globally consistent orientation solely from the point cloud is however challenging due to the global scope of the problem and the discrete nature of point cloud, particularly in the presence of noise, outliers, holes, thin structures, and complex topologies.This paper presents an efficient, robust, and global algorithm for generating consistent normal orientation of a dense 3D point cloud. The basic idea is to transform the original binary normal orientation problem to finding a relaxed sign field on a Delaunay graph, which can be achieved by solving a sparse linear system. The Delaunay graph is constructed by triangulating a level set of an implicit function defined from the input point cloud. The shape diameter function is estimated to serve as a prior for determining an appropriate level value such that the level set implicitly defines the inner and outer shells enclosing the input point clouds. As such, our algorithm leverages the strengths of the shape diameter function, Delaunay triangulation, and the least-square techniques, making the underlying processes take both geometry and topology into consideration, and thus provides an efficient and robust solution for handling point clouds with complicated geometry and topology. Extensive experiments on various shapes with noise and outliers confirm  the effectiveness and robustness of our algorithm.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of achieving globally consistent normal orientation for 3D point clouds, which is crucial in computer vision and graphics. The proposed algorithm transforms the binary normal orientation problem into finding a relaxed sign field on a Delaunay graph, solved via a sparse linear system. The Delaunay graph is constructed using a level set of an implicit function derived from the point cloud, with the shape diameter function providing a prior for determining the level value. This approach integrates shape diameter function, Delaunay triangulation, and least-square techniques, ensuring robustness and efficiency in handling complex geometries and topologies, even in the presence of noise and outliers.",
        "Tags": [
            "3D Point Cloud",
            "3D Reconstruction",
            "Delaunay Triangulation",
            "Least-Square Techniques",
            "Shape Diameter Function"
        ]
    },
    {
        "Title": "Adapter Merging with Centroid Prototype Mapping for Scalable Class-Incremental Learning",
        "Authors": "Takuma Fukuda \u00b7 Hiroshi Kera \u00b7 Kazuhiko Kawamoto",
        "Abstract": "We propose Adapter Merging with Centroid Prototype Mapping (ACMap), an exemplar-free framework for class-incremental learning (CIL) that addresses both catastrophic forgetting and scalability. While existing methods trade-off between inference time and accuracy, ACMap consolidates task-specific adapters into a single adapter, ensuring constant inference time across tasks without compromising accuracy.The framework employs adapter merging to build a shared subspace that aligns task representations and mitigates forgetting, while centroid prototype mapping maintains high accuracy through consistent adaptation in the shared subspace. To further improve scalability, an early stopping strategy limits adapter merging as tasks increase. Extensive experiments on five benchmark datasets demonstrate that ACMap matches state-of-the-art accuracy while maintaining inference speed comparable to the fastest existing methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Adapter Merging with Centroid Prototype Mapping (ACMap), a novel framework for class-incremental learning (CIL) that effectively addresses catastrophic forgetting and scalability. Unlike existing methods that compromise between inference time and accuracy, ACMap integrates task-specific adapters into a single adapter, ensuring consistent inference time without sacrificing accuracy. The framework utilizes adapter merging to create a shared subspace that aligns task representations and reduces forgetting, while centroid prototype mapping ensures high accuracy through consistent adaptation. An early stopping strategy is also implemented to enhance scalability as tasks increase. ACMap demonstrates state-of-the-art accuracy and maintains inference speed comparable to the fastest methods across five benchmark datasets.",
        "Tags": [
            "Class-Incremental Learning",
            "Adapter Merging",
            "Exemplar-Free Learning",
            "Scalability Enhancement",
            "Early Stopping Strategy"
        ]
    },
    {
        "Title": "Visual-Instructed Degradation Diffusion for All-in-One Image Restoration",
        "Authors": "Haina Qin \u00b7 Wenyang Luo \u00b7 Zewen Chen \u00b7 Yufan Liu \u00b7 Bing Li \u00b7 Weiming Hu \u00b7 libin wang \u00b7 DanDan Zheng \u00b7 Yuming Li",
        "Abstract": "Image restoration tasks, such as deblurring, denoising, and dehazing, typically require separate models for each degradation type, limiting their generalization in real-world scenarios where mixed or unknown degradations may occur. In this work, we propose \\textbf{Defusion}, a novel all-in-one image restoration framework that utilizes visual instruction-guided degradation diffusion. Unlike existing methods that rely on task-specific models or ambiguous text-based priors, Defusion constructs explicit \\textbf{visual instructions} that align with the visual degradation patterns. These instructions are grounded by applying degradations to standardized visual elements, capturing intrinsic degradation features while agnostic to image semantics. Defusion then uses these visual instructions to guide a diffusion-based model that operates directly in the degradation space, where it reconstructs high-quality images by denoising the degradation effects with enhanced stability and generalizability. Comprehensive experiments demonstrate that Defusion outperforms state-of-the-art methods across diverse image restoration tasks, including complex and real-world degradations.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Defusion, an all-in-one image restoration framework that leverages visual instruction-guided degradation diffusion to address multiple image restoration tasks such as deblurring, denoising, and dehazing. Unlike traditional methods that require separate models for each degradation type, Defusion uses explicit visual instructions derived from standardized visual elements to guide a diffusion-based model. This approach allows the model to operate directly in the degradation space, effectively reconstructing high-quality images by denoising degradation effects. The framework demonstrates superior performance across various image restoration tasks, including complex and real-world scenarios.",
        "Tags": [
            "Image Restoration",
            "Diffusion Models",
            "Visual Instruction Guidance",
            "Degradation Space Modeling",
            "All-in-One Restoration"
        ]
    },
    {
        "Title": "ZeroGrasp: Zero-Shot Shape Reconstruction Enabled Robotic Grasping",
        "Authors": "Shun Iwase \u00b7 Zubair Irshad \u00b7 Katherine Liu \u00b7 Vitor Guizilini \u00b7 Robert Lee \u00b7 Takuya Ikeda \u00b7 Ayako Amma \u00b7 Koichi Nishiwaki \u00b7 Kris Kitani \u00b7 Rares Andrei Ambrus \u00b7 Sergey Zakharov",
        "Abstract": "Robotic grasping is a cornerstone capability of embodied systems. Many methods directly output grasps from partial information without modeling the geometry of the scene, leading to suboptimal motion and even collisions. To address these issues, we introduce ZeroGrasp, a novel framework that simultaneously performs 3D reconstruction and grasp pose prediction in near real-time. A key insight of our method is that occlusion reasoning and modeling the spatial relationships between objects is beneficial for both accurate reconstruction and grasping. We couple our method with a novel large-scale synthetic dataset, which is an order of magnitude larger than existing datasets and comprises $1$M photo-realistic images, high-resolution 3D reconstructions and $8.9$B physically-valid grasp pose annotations for $12$K objects from the Objaverse-LVIS dataset. We evaluate ZeroGrasp on the GraspNet-1B benchmark as well as through real-world robot experiments. ZeroGrasp achieves state-of-the-art performance and generalizes to novel real-world objects even when trained only on synthetic data.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "ZeroGrasp introduces a novel framework for robotic grasping that simultaneously performs 3D reconstruction and grasp pose prediction in near real-time. The method leverages occlusion reasoning and spatial relationship modeling to improve both reconstruction accuracy and grasping effectiveness. It is trained on a large-scale synthetic dataset, significantly larger than existing datasets, and achieves state-of-the-art performance on the GraspNet-1B benchmark. ZeroGrasp also demonstrates strong generalization to novel real-world objects, even when trained solely on synthetic data.",
        "Tags": [
            "Embodied AI",
            "3D Reconstruction",
            "Zero-Shot Learning",
            "Occlusion Reasoning",
            "Spatial Relationship Modeling",
            "Synthetic Data Training"
        ]
    },
    {
        "Title": "Auto-Enocded Supervision for Perceptual Image Super-Resolution",
        "Authors": "MinKyu Lee \u00b7 Sangeek Hyun \u00b7 Woojin Jun \u00b7 Jae-Pil Heo",
        "Abstract": "This work tackles the fidelity objective in the perceptual super-resolution (SR).Specifically, we address the shortcomings of pixel-level $L_\\text{p}$ loss ($L_\\text{pix}$) in the GAN-based SR framework.Since $L_\\text{pix}$ is known to have a trade-off relationship against perceptual quality, prior methods often multiply a small scale factor or utilize low-pass filters.However, this work shows that these circumventions fail to address the fundamental factor that induces blurring.Accordingly, we focus on two points: 1) precisely discriminating the subcomponent of $L_\\text{pix}$ that contributes to blurring, and 2) only guiding based on the factor that is free from this trade-off relationship.We show that they can be achieved in a surprisingly simple manner, with an Auto-Encoder (AE) pretrained with $L_\\text{pix}$. Accordingly, we propose the Auto-Encoded Supervision for Optimal Penalization loss ($L_\\text{AESOP}$), a novel loss function that measures distance in the AE space, instead of the raw pixel space. (AE space indicates the space after the decoder, not the bottleneck.)By simply substituting the conventional $L_\\text{pix}$ with $L_\\text{AESOP}$, we can provide effective reconstruction guidance without compromising perceptual quality.Designed for simplicity, our method enables easy integration into existing SR frameworks. Experimental results verify the significance of our method in enhancing both fidelity and perceptual quality.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This work addresses the fidelity objective in perceptual super-resolution (SR) by tackling the limitations of pixel-level $L_\\text{p}$ loss ($L_\\text{pix}$) in GAN-based SR frameworks. The authors identify that prior methods, which use small scale factors or low-pass filters, fail to resolve the fundamental blurring issue caused by $L_\\text{pix}$. They propose a novel approach that discriminates the blur-inducing subcomponent of $L_\\text{pix}$ and guides the SR process using a factor free from the trade-off between fidelity and perceptual quality. This is achieved through an Auto-Encoder (AE) pretrained with $L_\\text{pix}$, leading to the introduction of the Auto-Encoded Supervision for Optimal Penalization loss ($L_\\text{AESOP}$). $L_\\text{AESOP}$ measures distance in the AE space rather than the raw pixel space, enabling effective reconstruction guidance without compromising perceptual quality. The method is simple and easily integrable into existing SR frameworks, with experimental results demonstrating its effectiveness in enhancing both fidelity and perceptual quality.",
        "Tags": [
            "Super-Resolution",
            "Generative Adversarial Networks (GANs)",
            "Auto-Encoder",
            "Loss Function Design",
            "Perceptual Quality"
        ]
    },
    {
        "Title": "Adapting Dense Matching for Homography Estimation with Grid-based Acceleration",
        "Authors": "Kaining Zhang \u00b7 Yuxin Deng \u00b7 Jiayi Ma \u00b7 Paolo Favaro",
        "Abstract": "Current deep homography estimation methods are constrained to processing low-resolution image pairs due to network architecture and computational limitations. For high-resolution images, downsampling is often required, which can greatly degrade estimation accuracy. In contrast, traditional methods, which match pixels and compute homography from correspondences, provide greater resolution flexibility. So in this work, we go back to the traditional ways for homography estimation. Specifically, we propose GFNet, a Grid Flow regression Network that adapts the high-accuracy dense matching framework for homography estimation and enhances efficiency with a grid-based strategy\u2014estimating flow only over a coarse grid by leveraging homography\u2019s global smoothness. We demonstrate the effectiveness of GFNet on a wide range of experiments on multiple datasets, including the common scene MSCOCO, multimodal datasets VIS-IR and GoogleMap, and the dynamic scene VIRAT. Specifically, on 448$\\times$448 GoogleMap, GFNet achieves an improvement of +9.9\\% in auc@3 while reducing MACs by $\\sim$47\\% compared to the SOTA dense matching method. Additionally, it shows a 1.7$\\times$ improvement in auc@3 over the SOTA deep homography method.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces GFNet, a Grid Flow regression Network designed to enhance homography estimation by adapting a dense matching framework and improving efficiency through a grid-based strategy. GFNet estimates flow over a coarse grid, leveraging the global smoothness of homography, which allows for high-resolution image processing without the need for downsampling. The method demonstrates superior performance across various datasets, including MSCOCO, VIS-IR, GoogleMap, and VIRAT, showing significant improvements in accuracy and computational efficiency over state-of-the-art methods.",
        "Tags": [
            "Homography Estimation",
            "Dense Matching",
            "Grid-based Acceleration",
            "High-Resolution Image Processing",
            "Flow Estimation",
            "Computational Efficiency"
        ]
    },
    {
        "Title": "Less Attention is More: Prompt Transformer for Generalized Category Discovery",
        "Authors": "Wei Zhang \u00b7 Baopeng Zhang \u00b7 Zhu Teng \u00b7 Wenxin Luo \u00b7 Junnan Zou \u00b7 Jianping Fan",
        "Abstract": "Generalized Category Discovery (GCD) typically relies on the pre-trained Vision Transformer (ViT) to extract features from a global receptive field, followed by contrastive learning to simultaneously classify unlabeled known classes and unknown classes without priors. Owing to the deficiency in the modeling capacity for inner-patch local information within ViT, current methods primarily focus on discriminative features at global level. This results in a model with more yet scattered attention, where neither excessive nor insufficient focus can grasp subtle differences to classify fine-grained unknown and known categories. To address this issue, we propose the AptGCD to deliver apt attention for GCD. It mimics the human brain how leveraging visual perception to refine local attention and comprehend global context by proposing a Meta Visual Prompt (MVP) and Prompt Transformer (PT). MVP is introduced into GCD for the first time, refining channel-level attention, while adaptively self-learning unique inner-patch features as prompts to achieve local visual modeling for our prompt transformer. Yet, relying solely on detailed features can lead to skewed judgments. Hence, PT harmonizes local and global representations, guiding the model's interpretation of features through broader contexts, thereby capturing more useful details with less attention. Extensive experiments on seven datasets demonstrate that AptGCD outperforms current methods, it achieves an average proportional 'New' accuracy improvement of approximately 9.2% over SOTA method on the all four fine-grained datasets, establishing a new standard in the field.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces AptGCD, a novel approach for Generalized Category Discovery (GCD) that addresses the limitations of current methods relying on Vision Transformers (ViTs) for global feature extraction. These methods often fail to capture fine-grained differences due to scattered attention. AptGCD proposes a Meta Visual Prompt (MVP) and Prompt Transformer (PT) to refine local attention and harmonize it with global context. MVP enhances channel-level attention and self-learns inner-patch features, while PT balances local and global representations to improve feature interpretation. The method demonstrates superior performance, achieving a 9.2% improvement in 'New' accuracy on fine-grained datasets compared to state-of-the-art methods.",
        "Tags": [
            "Vision Transformer (ViT)",
            "Generalized Category Discovery (GCD)",
            "Meta Visual Prompt (MVP)",
            "Prompt Transformer (PT)",
            "Fine-Grained Classification"
        ]
    },
    {
        "Title": "SaMam: Style-aware State Space Model for Arbitrary Image Style Transfer",
        "Authors": "Hongda Liu \u00b7 Longguang Wang \u00b7 Ye Zhang \u00b7 Ziru YU \u00b7 Yulan Guo",
        "Abstract": "Global effective receptive field plays a crucial role for image style transfer (ST) to obtain high-quality stylized results. However, existing ST backbones (e.g., CNNs and Transformers) suffer huge computational complexity to achieve global receptive fields. Recently, the State Space Model (SSM), especially the improved variant Mamba, has shown great potential for long-range dependency modeling with linear complexity, which offers a approach to resolve the above dilemma. In this paper, we develop a Mamba-based style transfer framework, termed SaMam. Specifically, a mamba encoder is designed to efficiently extract content and style information. In addition, a style-aware mamba decoder is developed to flexibly adapt to various styles. Moreover, to address the problems of local pixel forgetting, channel redundancy and spatial discontinuity of existing SSMs, we introduce both local enhancement and zigzag scan. Qualitative and quantitative results demonstrate that our SaMam outperforms state-of-the-art methods in terms of both accuracy and efficiency.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces SaMam, a Mamba-based framework for arbitrary image style transfer (ST) that addresses the computational complexity of achieving global receptive fields in existing ST backbones like CNNs and Transformers. Leveraging the State Space Model (SSM), particularly the Mamba variant, SaMam efficiently extracts content and style information using a Mamba encoder and adapts to various styles with a style-aware Mamba decoder. The framework incorporates local enhancement and zigzag scan techniques to mitigate issues such as local pixel forgetting, channel redundancy, and spatial discontinuity in SSMs. SaMam demonstrates superior performance in both accuracy and efficiency compared to state-of-the-art methods.",
        "Tags": [
            "Style Transfer",
            "Mamba",
            "State Space Model (SSM)",
            "Long-Range Dependency Modeling",
            "Linear Complexity",
            "Local Enhancement"
        ]
    },
    {
        "Title": "DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval",
        "Authors": "Leqi Shen \u00b7 Guoqiang Gong \u00b7 Tianxiang Hao \u00b7 Tao He \u00b7 Yifeng Zhang \u00b7 Pengzhang Liu \u00b7 Sicheng Zhao \u00b7 Jungong Han \u00b7 Guiguang Ding",
        "Abstract": "The parameter-efficient adaptation of the image-text pretraining model CLIP for video-text retrieval is a prominent area of research. While CLIP is focused on image-level vision-language matching, video-text retrieval demands comprehensive understanding at the video level. Three key discrepancies emerge in the transfer from image-level to video-level: vision, language, and alignment. However, existing methods mainly focus on vision while neglecting language and alignment. In this paper, we propose Discrepancy Reduction in Vision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all three discrepancies. Specifically, we introduce Image-Video Features Fusion to integrate image-level and video-level features, effectively tackling both vision and language discrepancies. Additionally, we generate pseudo image captions to learn fine-grained image-level alignment. To mitigate alignment discrepancies, we propose Image-to-Video Alignment Distillation, which leverages image-level alignment knowledge to enhance video-level alignment. Extensive experiments demonstrate the superiority of our DiscoVLA. In particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous methods by 2.2% R@1 and 7.5% R@sum. Our code will be made available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces DiscoVLA, a method designed to reduce discrepancies in vision, language, and alignment for parameter-efficient video-text retrieval. While CLIP models are traditionally used for image-text matching, video-text retrieval requires a deeper understanding at the video level. DiscoVLA addresses this by integrating image-level and video-level features through Image-Video Features Fusion, generating pseudo image captions for fine-grained alignment, and employing Image-to-Video Alignment Distillation to enhance video-level alignment using image-level knowledge. The method demonstrates significant improvements over existing approaches, particularly on the MSRVTT dataset with CLIP (ViT-B/16), showing a 2.2% increase in R@1 and a 7.5% increase in R@sum.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Video Understanding",
            "CLIP",
            "Parameter-Efficient Adaptation",
            "Image-to-Video Alignment",
            "Pseudo Image Captions"
        ]
    },
    {
        "Title": "Interactive Medical Image Segmentation: A Benchmark Dataset and Baseline",
        "Authors": "Junlong Cheng \u00b7 Bin Fu \u00b7 Jin Ye \u00b7 Guoan Wang \u00b7 Tianbin Tianbin \u00b7 Haoyu Wang \u00b7 Ruoyu Li \u00b7 He Yao \u00b7 Chen Junren \u00b7 Jingwen Li \u00b7 Yanzhou Su \u00b7 Min Zhu \u00b7 Junjun He",
        "Abstract": "Interactive Medical Image Segmentation (IMIS) has long been constrained by the limited availability of large-scale, diverse, and densely annotated datasets, which hinders model generalization and consistent evaluation across different models. In this paper, we introduce the IMed-361M benchmark dataset, a significant advancement in general IMIS research. First, we collect and standardize over 6.4 million medical images and their corresponding ground truth masks from multiple data sources. Then, leveraging the strong object recognition capabilities of a vision foundational model, we automatically generated dense interactive masks for each image and ensured their quality through rigorous quality control and granularity management. Unlike previous datasets, which are limited by specific modalities or sparse annotations, IMed-361M spans 14 modalities and 204 segmentation targets, totaling 361 million masks\u2014an average of 56 masks per image. Finally, we developed an IMIS baseline network on this dataset that supports high-quality mask generation through interactive inputs, including clicks, bounding boxes, text prompts, and their combinations. We evaluate its performance on medical image segmentation tasks from multiple perspectives, demonstrating superior accuracy and scalability compared to existing interactive segmentation models. To facilitate research on foundational models in medical computer vision, we release the IMed-361M and model at https://anonymous.4open.science/r/IMIS-Bench-FF8B.",
        "Link": "https://anonymous.4open.science/r/IMIS-Bench-FF8B",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces the IMed-361M benchmark dataset, a large-scale, diverse, and densely annotated dataset for Interactive Medical Image Segmentation (IMIS). The dataset includes over 6.4 million medical images with corresponding ground truth masks, spanning 14 modalities and 204 segmentation targets, totaling 361 million masks. A baseline network was developed to support high-quality mask generation through various interactive inputs, demonstrating superior accuracy and scalability compared to existing models.",
        "Tags": [
            "Medical Image Segmentation",
            "Datasets and Benchmarks",
            "Interactive Segmentation",
            "Vision Foundational Model",
            "Multi-Modal Dataset"
        ]
    },
    {
        "Title": "SAM2-LOVE: Segment Anything Model 2 in Language-aided Audio-Visual Scenes",
        "Authors": "Yuji Wang \u00b7 Haoran Xu \u00b7 Yong Liu \u00b7 Jiaze Li \u00b7 Yansong Tang",
        "Abstract": "Reference Audio-Visual Segmentation (Ref-AVS) aims to provide a pixel-wise scene understanding in Language-aided Audio-Visual Scenes (LAVS). This task requires the model to continuously segment objects referred to by text and audio from a video. Previous dual-modality methods always fail due to the lack of a third modality and the existing triple-modality method struggles with spatio-temporal consistency, leading to the target shift of different frames. In this work, we introduce a novel framework, termed SAM2-LOVE, which integrates textual, audio, and visual representations into a learnable token to prompt and align SAM2 for achieving Ref-AVS in the LAVS. Technically, our approach includes a multimodal fusion module aimed at improving multimodal understanding of SAM2, as well as token propagation and accumulation strategies designed to enhance spatio-temporal consistency without forgetting historical information. We conducted extensive experiments to demonstrate that SAM2-LOVE outperforms the SOTA by 8.5\\% in J&F on the Ref-AVS benchmark and showcase the simplicity and effectiveness of the components. Our code will be available soon.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces SAM2-LOVE, a novel framework for Reference Audio-Visual Segmentation (Ref-AVS) in Language-aided Audio-Visual Scenes (LAVS). The framework integrates textual, audio, and visual representations into a learnable token to prompt and align SAM2, addressing the challenges of spatio-temporal consistency and target shift across frames. It includes a multimodal fusion module for enhanced understanding and token propagation strategies to maintain historical information. SAM2-LOVE demonstrates superior performance, outperforming state-of-the-art methods by 8.5% in J&F on the Ref-AVS benchmark.",
        "Tags": [
            "Multimodal Learning",
            "Semantic Segmentation",
            "Multimodal Fusion",
            "Spatio-temporal Consistency",
            "Learnable Token"
        ]
    },
    {
        "Title": "AudCast: Audio-Driven Human Video Generation by Cascaded Diffusion Transformers",
        "Authors": "Jiazhi Guan \u00b7 Kaisiyuan Wang \u00b7 Zhiliang Xu \u00b7 Quanwei Yang \u00b7 Yasheng SUN \u00b7 Shengyi He \u00b7 Borong Liang \u00b7 Yukang Cao \u00b7 Yingying Li \u00b7 Haocheng Feng \u00b7 Errui Ding \u00b7 Jingdong Wang \u00b7 Youjian Zhao \u00b7 Hang Zhou \u00b7 Ziwei Liu",
        "Abstract": "Despite the recent progress of audio-driven video generation, existing methods mostly focus on driving facial movements, leading to non-coherent head and body dynamics. Moving forward, it is desirable yet challenging to generate holistic human videos with both accurate lip-sync and delicate co-speech gestures w.r.t. given audio. In this work, we propose AudCast, a generalized audio-driven human video generation framework adopting a cascade Diffusion-Transformers (DiTs) paradigm, which synthesizes holistic human videos based on a reference image and a given audio. 1) Firstly, an audio-conditioned Holistic Human DiT architecture is proposed to directly drive the movements of any human body with vivid gesture dynamics. 2) Then to enhance hand and face details that are well-knownly difficult to handle, a Regional Refinement DiT leverages regional 3D fitting as the bridge to reform the signals, producing the final results.  Extensive experiments demonstrate that our framework generates high-fidelity audio-driven holistic human videos with temporal coherence and fine facial and hand details.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "AudCast introduces a novel framework for generating holistic human videos driven by audio, addressing the limitations of existing methods that primarily focus on facial movements. The framework employs a cascade of Diffusion-Transformers (DiTs) to synthesize videos from a reference image and audio input. It features an audio-conditioned Holistic Human DiT for driving body movements and a Regional Refinement DiT for enhancing hand and face details. The approach successfully generates high-fidelity videos with coherent temporal dynamics and detailed facial and hand gestures.",
        "Tags": [
            "Audio-Driven Video Generation",
            "Diffusion Models",
            "Cascade Diffusion-Transformers",
            "Holistic Human Video Synthesis",
            "Regional 3D Fitting"
        ]
    },
    {
        "Title": "Toward Real-world BEV Perception: Depth Uncertainty Estimation via Gaussian Splatting",
        "Authors": "Shu-Wei Lu \u00b7 Yi-Hsuan Tsai \u00b7 Yi-Ting Chen",
        "Abstract": "Bird's-eye view (BEV) perception has gained significant attention because it provides a unified representation to fuse multiple view images and enables a wide range of downstream autonomous driving tasks, such as forecasting and planning. However, the task is challenging because it is inherently an ill-posed problem due to the lack of depth information. Moreover, fusing multi-view images into a unified representation without depth cues becomes more challenging.Recent grid-based methods formulate BEV perception as query learning to bypass explicit depth estimation. While we observe promising advancements in this paradigm, they still fall short of real-world applications because they lack uncertainty modeling and are computationally expensive.In this work, we revisit depth-based methods and endow them with uncertainty awareness. Specifically, we calculate the variance of the depth distribution to represent how objects are spatially dispersed around their mean depth. In addition, the proposed model learns a soft depth mean and implicitly captures the spatial extent of objects. We transform the depth distribution into 3D Gaussians and utilize the rasterization technique to form uncertainty-aware BEV features.We evaluate our method on the nuScenes dataset, achieving state-of-the-art performance compared to depth-based methods. Notably, our model provides significant advantages in speed\u2014running 2x faster\u2014and in memory efficiency, using 0.3x less memory compared to grid-sampling-based methods, while achieving competitive performance with only a 0.7% IoU difference.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Bird's-eye view (BEV) perception is crucial for autonomous driving tasks but faces challenges due to the lack of depth information and the difficulty of fusing multi-view images. Recent grid-based methods bypass explicit depth estimation but lack uncertainty modeling and are computationally expensive. This work revisits depth-based methods, introducing uncertainty awareness by calculating the variance of the depth distribution and learning a soft depth mean. The depth distribution is transformed into 3D Gaussians, and rasterization is used to form uncertainty-aware BEV features. The method achieves state-of-the-art performance on the nuScenes dataset, offering significant speed and memory efficiency improvements over grid-sampling-based methods.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Autonomous Driving",
            "Uncertainty Modeling",
            "Depth Estimation",
            "3D Gaussians"
        ]
    },
    {
        "Title": "Latent Drifting in Diffusion Models for Counterfactual Medical Image Synthesis",
        "Authors": "Yousef Yeganeh \u00b7 Ioannis Charisiadis \u00b7 Marta Hasny \u00b7 Martin Hartenberger \u00b7 Bj\u00f6rn Ommer \u00b7 Nassir Navab \u00b7 Azade Farshad \u00b7 Ehsan Adeli",
        "Abstract": "Scaling by training on large datasets has been shown to enhance the quality and fidelity of image generation and manipulation with diffusion models; however, such large datasets are not always accessible in medical imaging due to cost and privacy issues, which contradicts one of the main applications of such models to produce synthetic samples where real data is scarce. Also, finetuning on pre-trained general models has been a challenge due to the distribution shift between the medical domain and the pre-trained models. Here, we propose Latent Drift (LD) for diffusion models that can be adopted for any fine-tuning method to mitigate the issues faced by the distribution shift or employed in inference time as a condition. Latent Drifting enables diffusion models to be conditioned for medical images fitted for the complex task of counterfactual image generation, which is crucial to investigate how parameters such as gender, age, and adding or removing diseases in a patient would alter the medical images. We evaluate our method on three public longitudinal benchmark datasets of brain MRI and chest X-rays for counterfactual image generation. Our results demonstrate significant performance gains in various scenarios when combined with different fine-tuning schemes. The source code of this work will be publicly released upon its acceptance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Latent Drift (LD), a novel method for diffusion models designed to address challenges in medical image synthesis, particularly counterfactual image generation. LD mitigates issues arising from distribution shifts between general pre-trained models and the medical domain, enabling effective fine-tuning and inference-time conditioning. The method is evaluated on brain MRI and chest X-ray datasets, demonstrating significant performance improvements in generating counterfactual medical images, which are essential for exploring the impact of parameters like gender, age, and disease presence on medical imaging.",
        "Tags": [
            "Diffusion Models",
            "Medical Image Analysis",
            "Counterfactual Image Generation",
            "Latent Drift",
            "Medical Image Synthesis"
        ]
    },
    {
        "Title": "Nested Diffusion Models using Hierarchical Latent Priors",
        "Authors": "Xiao Zhang \u00b7 Ruoxi Jiang \u00b7 Rebecca Willett \u00b7 Michael Maire",
        "Abstract": "We introduce nested diffusion models, an efficient and powerful hierarchical generative framework that substantially enhances the generation quality of diffusion models, particularly for images of complex scenes. Our approach employs a series of diffusion models to progressively generate latent variables at different semantic levels. Each model in this series is conditioned on the output of the preceding higher-level models, culminating in image generation. Hierarchical latent variables guide the generation process along predefined semantic pathways, allowing our approach to capture intricate structural details while significantly improving image quality. To construct these latent variables, we leverage a pre-trained visual encoder, which learns strong semantic visual representations, and modulate its capacity via dimensionality reduction and noise injection. Across multiple datasets, our system demonstrates significant enhancements in image quality for both unconditional and class/text conditional generation. Moreover, our unconditional generation system substantially outperforms the baseline conditional system. These advancements incur minimal computational overhead as the more abstract levels of our hierarchy work with lower-dimensional representations.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces nested diffusion models, a hierarchical generative framework that enhances the quality of image generation, especially for complex scenes. The approach uses a series of diffusion models to generate latent variables at different semantic levels, conditioned on higher-level outputs, culminating in image generation. Hierarchical latent variables guide the process, capturing intricate structural details and improving image quality. A pre-trained visual encoder is used to construct latent variables, with dimensionality reduction and noise injection modulating its capacity. The system shows significant improvements in image quality across datasets for both unconditional and conditional generation, with minimal computational overhead.",
        "Tags": [
            "Diffusion Models",
            "Image Generation",
            "Hierarchical Latent Variables",
            "Semantic Pathways",
            "Dimensionality Reduction"
        ]
    },
    {
        "Title": "SSHNet: Unsupervised Cross-modal Homography Estimation via Problem Redefinition and Split Optimization",
        "Authors": "Junchen Yu \u00b7 Si-Yuan Cao \u00b7 Runmin Zhang \u00b7 Chenghao Zhang \u00b7 Zhu Yu \u00b7 Shujie Chen \u00b7 Bailin Yang \u00b7 Hui-Liang Shen",
        "Abstract": "We propose a novel unsupervised cross-modal homography estimation learning framework, named Split Supervised Homography estimation Network (SSHNet). SSHNet redefines the unsupervised cross-modal homography estimation into two supervised sub-problems, each addressed by its specialized network: a homography estimation network and a modality transfer network. To realize stable training, we introduce an effective split optimization strategy to train each network separately within its respective sub-problem. We also formulate an extra homography feature space supervision to enhance feature consistency, further boosting the estimation accuracy. Moreover, we employ a simple yet effective distillation training technique to reduce model parameters and improve cross-domain generalization ability while maintaining comparable performance. The training stability of SSHNet enables its cooperation with various homography estimation architectures. Experiments reveal that the SSHNet using IHN as homography estimation network, namely SSHNet-IHN, outperforms previous unsupervised approaches by a significant margin. Even compared to supervised approaches MHN and LocalTrans, SSHNet-IHN achieves 47.4\\% and 85.8\\% mean average corner errors (MACEs) reduction on the challenging OPT-SAR dataset. The source code is provided in the supplementary material.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces SSHNet, an unsupervised cross-modal homography estimation framework that redefines the problem into two supervised sub-problems: homography estimation and modality transfer. Each sub-problem is handled by a specialized network, trained using a split optimization strategy for stability. The framework incorporates homography feature space supervision to enhance feature consistency and employs distillation training to reduce model parameters and improve cross-domain generalization. SSHNet demonstrates superior performance over existing unsupervised and supervised methods, significantly reducing mean average corner errors on the OPT-SAR dataset.",
        "Tags": [
            "Homography Estimation",
            "Unsupervised Learning",
            "Cross-modal Learning",
            "Feature Space Supervision",
            "Distillation Training",
            "Cross-domain Generalization"
        ]
    },
    {
        "Title": "Towards All-in-One Medical Image Re-Identification",
        "Authors": "Yuan Tian \u00b7 Kaiyuan Ji \u00b7 Rongzhao Zhang \u00b7 Yankai Jiang \u00b7 Chunyi Li \u00b7 Xiaosong Wang \u00b7 Guangtao Zhai",
        "Abstract": "Medical image re-identification (MedReID) is under-explored so far, despite its critical applications in personalized healthcare and privacy protection.In this paper, we introduce a thorough benchmark and a unified model for this problem.First, to handle various medical modalities, we propose a novel Continuous Modality-based Parameter Adapter (ComPA). ComPA condenses medical content into a continuous modality representation and dynamically adjusts the modality-agnostic model with modality-specific parameters at runtime. This allows a single model to adaptively learn and process diverse modality data.Furthermore, we integrate medical priors into our model by aligning it with a bag of pre-trained medical foundation models, in terms of the differential features.Compared to single-image feature, modeling the inter-image difference better fits the re-identification problem, which involves discriminating multiple images.We evaluate the proposed model against 25 foundation models and 8 large multi-modal language models across 11 image datasets, demonstrating consistently superior performance.Additionally, we deploy the proposed MedReID technique to two real-world applications, i.e., history-augmented personalized diagnosis and medical privacy protection.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the under-explored area of medical image re-identification (MedReID), introducing a comprehensive benchmark and a unified model. The proposed model features a Continuous Modality-based Parameter Adapter (ComPA) that enables adaptive learning across diverse medical modalities by condensing medical content into a continuous modality representation and dynamically adjusting modality-specific parameters. Additionally, the model integrates medical priors by aligning with pre-trained medical foundation models, focusing on differential features to better suit the re-identification task. The model's effectiveness is demonstrated through superior performance across multiple datasets and real-world applications in personalized diagnosis and privacy protection.",
        "Tags": [
            "Medical Image Analysis",
            "ReID (Person Re-identification)",
            "Continuous Modality-based Parameter Adapter (ComPA)",
            "Medical Priors Integration",
            "Differential Features Modeling"
        ]
    },
    {
        "Title": "CXPMRG-Bench: Pre-training and Benchmarking for X-ray Medical Report Generation on CheXpert Plus Dataset",
        "Authors": "Xiao Wang \u00b7 Fuling Wang \u00b7 Yuehang Li \u00b7 Qingchuan Ma \u00b7 Shiao Wang \u00b7 Bo Jiang \u00b7 Jin Tang",
        "Abstract": "X-ray image-based medical report generation (MRG) is a pivotal area in artificial intelligence which can significantly reduce diagnostic burdens and patient wait times. Despite significant progress, we believe that the task has reached a bottleneck due to the limited benchmark datasets and the existing large models' insufficient capability enhancements in this specialized domain. Specifically, the recently released CheXpert Plus dataset lacks comparative evaluation algorithms and their results, providing only the dataset itself. This situation makes the training, evaluation, and comparison of subsequent algorithms challenging. Thus, we conduct a comprehensive benchmarking of existing mainstream X-ray report generation models and large language models (LLMs), on the CheXpert Plus dataset. We believe that the proposed benchmark can provide a solid comparative basis for subsequent algorithms and serve as a guide for researchers to quickly grasp the state-of-the-art models in this field. More importantly, we propose a large model for the X-ray image report generation using a multi-stage pre-training strategy, including self-supervised autoregressive generation and Xray-report contrastive learning, and supervised fine-tuning. Extensive experimental results indicate that the autoregressive pre-training based on Mamba effectively encodes X-ray images, and the image-text contrastive pre-training further aligns the feature spaces, achieving better experimental results. All the source codes will be released upon acceptance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "X-ray image-based medical report generation (MRG) is a critical area in AI, aimed at reducing diagnostic burdens and patient wait times. However, the field faces challenges due to limited benchmark datasets and insufficient enhancements in large models for this specialized domain. The CheXpert Plus dataset, while recently released, lacks comparative evaluation algorithms and results, complicating training, evaluation, and comparison of algorithms. This paper conducts a comprehensive benchmarking of existing X-ray report generation models and LLMs on the CheXpert Plus dataset, aiming to provide a comparative basis and guide for researchers. Additionally, a large model for X-ray image report generation is proposed, utilizing a multi-stage pre-training strategy that includes self-supervised autoregressive generation, X-ray-report contrastive learning, and supervised fine-tuning. The results show that autoregressive pre-training based on Mamba effectively encodes X-ray images, and image-text contrastive pre-training aligns feature spaces, leading to improved outcomes.",
        "Tags": [
            "Medical Image Analysis",
            "Large Language Models (LLMs)",
            "Autoregressive Pre-training",
            "Image-Text Contrastive Learning",
            "CheXpert Plus Dataset"
        ]
    },
    {
        "Title": "Question-Aware Gaussian Experts for Audio-Visual Question Answering",
        "Authors": "Hongyeob Kim \u00b7 Inyoung Jung \u00b7 Dayoon Suh \u00b7 Youjia Zhang \u00b7 Sangmin Lee \u00b7 Sungeun Hong",
        "Abstract": "Audio-Visual Question Answering (AVQA) requires not only question-based multimodal reasoning but also precise temporal grounding to capture subtle dynamics for accurate prediction. However, existing methods mainly use question information implicitly, limiting focus on question-specific details. Furthermore, most studies rely on uniform frame sampling, which can miss key question-relevant frames. Although recent Top-K frame selection methods aim to address this, their discrete nature still overlooks fine-grained temporal details. This paper proposes QA-TIGER, a novel framework that explicitly incorporates question information and models continuous temporal dynamics. Our key idea is to use Gaussian-based modeling to adaptively focus on both consecutive and non-consecutive frames based on the question, while explicitly injecting question information and applying progressive refinement. We leverage a Mixture of Experts (MoE) to flexibly implement multiple Gaussian models, activating temporal experts specifically tailored to the question. Extensive experiments on multiple AVQA benchmarks show that QA-TIGER consistently achieves state-of-the-art performance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces QA-TIGER, a novel framework for Audio-Visual Question Answering (AVQA) that explicitly incorporates question information and models continuous temporal dynamics. Unlike existing methods that rely on uniform frame sampling or discrete Top-K frame selection, QA-TIGER uses Gaussian-based modeling to adaptively focus on both consecutive and non-consecutive frames based on the question. The framework leverages a Mixture of Experts (MoE) to implement multiple Gaussian models, activating temporal experts tailored to the question. This approach enables precise temporal grounding and question-specific reasoning, leading to state-of-the-art performance on multiple AVQA benchmarks.",
        "Tags": [
            "Audio-Visual Question Answering",
            "Multimodal Learning",
            "Gaussian-based Modeling",
            "Temporal Grounding",
            "Mixture of Experts (MoE)",
            "Progressive Refinement",
            "Question-Specific Temporal Modeling"
        ]
    },
    {
        "Title": "3D Gaussian Head Avatars with Expressive Dynamic Appearances by Compact Tensorial Representations",
        "Authors": "yating wang \u00b7 Xuan Wang \u00b7 Ran Yi \u00b7 Yanbo Fan \u00b7 Jichen Hu \u00b7 Jingcheng Zhu \u00b7 Lizhuang Ma",
        "Abstract": "Recent studies have combined 3D Gaussian and 3D Morphable Models (3DMM) to construct high-quality 3D head avatars. In this line of research, existing methods either fail to capture the dynamic textures or incur significant overhead in terms of runtime speed or storage space. To this end, we propose a novel method that addresses all the aforementioned demands.  In specific, we introduce an expressive and compact representation that encodes texture-related attributes of the 3D Gaussians in the tensorial format. We store appearance of neutral expression in static tri-planes, and represents dynamic texture details for different expressions using lightweight 1D feature lines, which are then decoded into opacity offset relative to the neutral face. We further propose adaptive truncated opacity penalty and class-balanced sampling to improve generalization across different expressions. Experiments show this design enables accurate face dynamic details capturing while maintains real-time rendering and significantly reduces storage costs, thus broadening the applicability to more scenarios.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a novel method for constructing high-quality 3D head avatars using a combination of 3D Gaussian and 3D Morphable Models (3DMM). The proposed approach addresses the limitations of existing methods, which either fail to capture dynamic textures or require significant runtime and storage overhead. The method employs an expressive and compact tensorial representation to encode texture-related attributes of 3D Gaussians. Static tri-planes store the appearance of neutral expressions, while lightweight 1D feature lines represent dynamic texture details for different expressions, decoded into opacity offsets relative to the neutral face. Adaptive truncated opacity penalty and class-balanced sampling are introduced to enhance generalization across expressions. The design enables accurate capture of dynamic facial details, maintains real-time rendering, and significantly reduces storage costs, making it applicable to a broader range of scenarios.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Avatars",
            "Tensorial Representation",
            "Dynamic Texture Encoding",
            "Real-Time Rendering"
        ]
    },
    {
        "Title": "Variance-Based Membership Inference Attacks Against Large-Scale Image Captioning Models",
        "Authors": "Daniel Samira \u00b7 Edan Habler \u00b7 Yuval Elovici \u00b7 Asaf Shabtai",
        "Abstract": "The proliferation of multi-modal generative models has introduced new privacy and security challenges, especially due to the risks of memorization and unintentional disclosure of sensitive information. This paper focuses on the vulnerability of multi-modal image captioning models to membership inference attacks (MIAs). These models, which synthesize textual descriptions from visual content, could inadvertently reveal personal or proprietary data embedded in their training datasets. We explore the feasibility of MIAs in the context of such models. Specifically, our approach leverages a variance-based strategy tailored for image captioning models, utilizing only image data without knowing the corresponding caption. We introduce the means-of-variance threshold attack (MVTA) and confidence-based weakly supervised attack (C-WSA) based on the metric, means-of-variance (MV), to assess variability among vector embeddings. Our experiments demonstrate that these models are susceptible to MIAs, indicating substantial privacy risks. The effectiveness of our methods is validated through rigorous evaluations on these real-world models, confirming the practical implications of our findings.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper investigates the vulnerability of multi-modal image captioning models to membership inference attacks (MIAs), which pose significant privacy risks by potentially revealing sensitive information from training datasets. The authors propose a variance-based strategy, including the means-of-variance threshold attack (MVTA) and confidence-based weakly supervised attack (C-WSA), to assess the variability among vector embeddings using only image data. The findings demonstrate that these models are susceptible to MIAs, highlighting substantial privacy concerns.",
        "Tags": [
            "Image Captioning",
            "Membership Inference Attacks",
            "Privacy Risks",
            "Variance-Based Strategy",
            "Multi-Modal Models"
        ]
    },
    {
        "Title": "Blurred LiDAR for Sharper 3D: Robust Handheld 3D Scanning with Diffuse LiDAR and RGB",
        "Authors": "Nikhil Behari \u00b7 Aaron Young \u00b7 Siddharth Somasundaram \u00b7 Tzofi Klinghoffer \u00b7 Akshat Dave \u00b7 Ramesh Raskar",
        "Abstract": "3D surface reconstruction is essential across applications of virtual reality, robotics, and mobile scanning. However, RGB-based reconstruction often fails in low-texture, low-light, and low-albedo scenes. Handheld LiDARs, now common on mobile devices, aim to address these challenges by capturing depth information from time-of-flight measurements of a coarse grid of projected dots. Yet, these sparse LiDARs struggle with scene coverage on limited input views, leaving large gaps in depth information. In this work, we propose using an alternative class of \"blurred\" LiDAR that emits a diffuse flash, greatly improving scene coverage but introducing spatial ambiguity from mixed time-of-flight measurements across a wide field of view. To handle these ambiguities, we propose leveraging the complementary strengths of diffuse LiDAR with RGB. We introduce a Gaussian surfel-based rendering framework with a scene-adaptive loss function that dynamically balances RGB and diffuse LiDAR signals. We demonstrate that, surprisingly, diffuse LiDAR can outperform traditional sparse LiDAR, enabling robust 3D scanning with accurate color and geometry estimation in challenging environments.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the limitations of RGB-based 3D surface reconstruction in challenging environments such as low-texture, low-light, and low-albedo scenes. Traditional handheld LiDARs, while useful, often leave gaps in depth information due to sparse scene coverage. The authors propose using a 'blurred' LiDAR that emits a diffuse flash, which improves scene coverage but introduces spatial ambiguity. To resolve these ambiguities, they combine diffuse LiDAR with RGB data using a Gaussian surfel-based rendering framework and a scene-adaptive loss function. This approach dynamically balances RGB and LiDAR signals, resulting in robust 3D scanning with accurate color and geometry estimation, even outperforming traditional sparse LiDAR in challenging environments.",
        "Tags": [
            "3D Reconstruction",
            "Depth Estimation",
            "Diffuse LiDAR",
            "Gaussian Surfels",
            "Scene-Adaptive Loss Function"
        ]
    },
    {
        "Title": "Dataset Distillation with Neural Characteristic Function: A Minmax Perspective",
        "Authors": "Shaobo Wang \u00b7 Yicun Yang \u00b7 Zhiyuan Liu \u00b7 Chenghao Sun \u00b7 Xuming Hu \u00b7 Conghui He \u00b7 Linfeng Zhang",
        "Abstract": "Dataset distillation has emerged as a powerful approach for reducing data requirements in deep learning. Among various methods, distribution matching-based approaches stand out for their balance of computational efficiency and strong performance. However, existing distance metrics used in distribution matching often fail to accurately capture distributional differences, leading to unreliable measures of discrepancy. In this paper, we reformulate dataset distillation as a minmax optimization problem and introduce Neural Characteristic Function Discrepancy (NCFD), a comprehensive and theoretically grounded metric for measuring distributional differences. NCFD leverages the Characteristic Function (CF) to encapsulate full distributional information, employing a neural network to optimize the sampling strategy for the CF's frequency arguments, thereby maximizing the discrepancy to enhance distance estimation. Simultaneously, we minimize the difference between real and synthetic data under this optimized NCFD measure. Our approach, termed Neural Characteristic Function Matching (NCFM), inherently aligns the phase and amplitude of neural features in the complex plane for both real and synthetic data, achieving a balance between realism and diversity in synthetic samples. Experiments demonstrate that our method achieves significant performance gains over state-of-the-art methods on both low- and high-resolution datasets. Notably, we achieve a 20.5\\% accuracy boost on ImageSquawk. Our method also reduces GPU memory usage by over 300$\\times$ and achieves 20$\\times$ faster processing speeds compared to state-of-the-art methods. To the best of our knowledge, this is the first work to achieve lossless compression of CIFAR-100 on a single NVIDIA 2080 Ti GPU using only 2.3 GB of memory. *Code is provided in the supplementary material.*",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Dataset distillation is a key technique for reducing data requirements in deep learning, with distribution matching-based methods being particularly effective due to their computational efficiency and performance. However, existing metrics often inadequately capture distributional differences. This paper introduces Neural Characteristic Function Discrepancy (NCFD), a novel metric that reformulates dataset distillation as a minmax optimization problem. NCFD uses the Characteristic Function to fully represent distributional information and optimizes the sampling strategy for frequency arguments via a neural network, enhancing discrepancy measurement. The proposed Neural Characteristic Function Matching (NCFM) method aligns the phase and amplitude of neural features in the complex plane for both real and synthetic data, balancing realism and diversity. The method significantly outperforms state-of-the-art techniques on various datasets, notably improving accuracy on ImageSquawk by 20.5%, reducing GPU memory usage by over 300 times, and speeding up processing by 20 times. It also achieves lossless compression of CIFAR-100 on a single GPU with minimal memory usage.",
        "Tags": [
            "Data Augmentation",
            "Knowledge Distillation",
            "Neural Characteristic Function Discrepancy",
            "Minmax Optimization",
            "Distribution Matching"
        ]
    },
    {
        "Title": "SpiritSight Agent: Advanced GUI Agent with One Look",
        "Authors": "Zhiyuan Huang \u00b7 Ziming Cheng \u00b7 Junting Pan \u00b7 Zhaohui Hou \u00b7 Mingjie Zhan",
        "Abstract": "Graphical User Interface (GUI) agents show amazing abilities in assisting human-computer interaction, automating human user's navigation on digital devices. An ideal GUI agent is expected to achieve high accuracy, low latency, and compatibility for different GUI platforms. Recent vision-based approaches have shown promise by leveraging advanced Vision Language Models (VLMs). While they generally meet the requirements of compatibility and low latency, these vision-based GUI agents tend to have low accuracy due to their limitations in element grounding. To address this issue, we propose $\\textbf{SpiritSight}$, a vision-based, end-to-end GUI agent that excels in GUI navigation tasks across various GUI platforms. First, we create a multi-level, large-scale, high-quality GUI dataset called $\\textbf{GUI-Lasagne}$ using scalable methods, empowering SpiritSight with robust GUI understanding and grounding capabilities. Second, we introduce the $\\textbf{Universal Block Parsing (UBP)}$ method to resolve the ambiguity problem in dynamic high-resolution of visual inputs, further enhancing SpiritSight's ability to ground GUI objects. Through these efforts, SpiritSight agent outperforms other advanced methods on diverse GUI benchmarks, demonstrating its superior capability and compatibility in GUI navigation tasks. The models and code will be made available upon publications.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces SpiritSight, a vision-based, end-to-end GUI agent designed to improve GUI navigation tasks across various platforms. To address the low accuracy of existing vision-based GUI agents, SpiritSight utilizes a multi-level, large-scale, high-quality GUI dataset called GUI-Lasagne, developed using scalable methods. Additionally, the Universal Block Parsing (UBP) method is introduced to resolve ambiguity in dynamic high-resolution visual inputs, enhancing the agent's ability to ground GUI objects. SpiritSight demonstrates superior performance and compatibility on diverse GUI benchmarks compared to other advanced methods.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Graphical User Interface (GUI)",
            "Universal Block Parsing (UBP)",
            "GUI-Lasagne Dataset",
            "Element Grounding"
        ]
    },
    {
        "Title": "Self-Cross Diffusion Guidance for Text-to-Image Synthesis of Similar Subjects",
        "Authors": "Weimin Qiu \u00b7 Jieke Wang \u00b7 Meng Tang",
        "Abstract": "Diffusion models have achieved unprecedented fidelity and diversity for synthesizing image, video, 3D assets, etc. However, subject mixing is a known and unresolved issue for diffusion-based image synthesis, particularly for synthesizing multiple similar-looking subjects. We propose Self-Cross diffusion guidance to penalize the overlap between cross-attention maps and aggregated self-attention maps. Compared to previous methods based on self-attention or cross-attention alone, our self-cross guidance is more effective in eliminating subject mixing. What's more, our guidance addresses mixing for all relevant patches of a subject beyond the most discriminant one, e.g., beak of a bird. We aggregate self-attention maps of automatically selected patches for a subject to form a region that the whole subject attends to. Our method is training-free and can boost the performance of any transformer-based diffusion model such as Stable Diffusion.% for synthesizing similar subjects. We also release a more challenging benchmark with many text prompts of similar-looking subjects and utilize GPT-4o for automatic and reliable evaluation. Extensive qualitative and quantitative results demonstrate the effectiveness of our Self-Cross guidance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Self-Cross diffusion guidance, a novel approach to address the issue of subject mixing in diffusion-based image synthesis, particularly for generating multiple similar-looking subjects. The method penalizes overlap between cross-attention maps and aggregated self-attention maps, effectively eliminating subject mixing. Unlike previous methods that rely solely on self-attention or cross-attention, Self-Cross guidance considers all relevant patches of a subject, forming a region that the entire subject attends to. The approach is training-free and compatible with transformer-based diffusion models like Stable Diffusion. A new benchmark with challenging text prompts for similar subjects is introduced, and GPT-4o is used for reliable evaluation. Qualitative and quantitative results validate the effectiveness of the proposed method.",
        "Tags": [
            "Diffusion Models",
            "Text-to-Image Generation",
            "Attention Mechanisms",
            "Subject Mixing",
            "Training-Free Methods"
        ]
    },
    {
        "Title": "The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition",
        "Authors": "Otto Brookes \u00b7 Maksim Kukushkin \u00b7 Majid Mirmehdi \u00b7 Colleen Stephens \u00b7 Paula Dieguez \u00b7 Thurston Cleveland Hicks \u00b7 Sorrel CZ Jones \u00b7 Kevin C. Lee \u00b7 Maureen S. McCarthy \u00b7 Amelia C. Meier \u00b7 NORMAND E. \u00b7 Erin G. Wessling \u00b7 Roman M. Wittig \u00b7 Kevin Langergraber \u00b7 Klaus Zuberb\u00fchler \u00b7 Lukas Boesch \u00b7 Thomas Schmid \u00b7 Mimi Arandjelovic \u00b7 Hjalmar S. K\u00fchl \u00b7 Tilo Burghardt",
        "Abstract": "Computer vision analysis of camera trap video footage is essential for wildlife conservation, as captured behaviours offer some of the earliest indicators of changes in population health. Recently, several high-impact animal behaviour datasets and methods have been introduced to encourage their use; however, the role of behaviour-correlated background information and its significant effect on out-of-distribution generalisation remain unexplored. In response, we present the PanAf-FGBG dataset, featuring 20 hours of wild chimpanzee behaviours, recorded at over 350 individual camera locations. Uniquely, it pairs every video with a chimpanzee (referred to as a foreground video) with a corresponding background video (with no chimpanzee) from the same camera location. We present two views of the dataset: one with overlapping camera locations and one with disjoint locations. This setup enables, for the first time, direct evaluation of in-distribution and out-of-distribution conditions, and for the impact of backgrounds on behaviour recognition models to be quantified. All clips come with rich behavioural annotations and metadata including unique camera IDs and detailed textual scene descriptions. Additionally, we establish several baselines and present a highly effective latent-space normalisation technique that boosts out-of-distribution performance by +5.42\\% mAP for convolutional and +3.75\\% mAP for transformer-based models. Finally, we provide an in-depth analysis on the role of backgrounds in out-of-distribution behaviour recognition, including the so far unexplored impact of background durations (i.e., the count of background frames within foreground videos). The full dataset, baseline models, and weights will be  available at `anonymous'.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The PanAf-FGBG dataset is introduced to explore the impact of background information on wildlife behaviour recognition, particularly in out-of-distribution scenarios. It includes 20 hours of wild chimpanzee behaviour videos from over 350 camera locations, each paired with a corresponding background video from the same location. The dataset supports both overlapping and disjoint camera location views, facilitating the evaluation of in-distribution and out-of-distribution conditions. It is richly annotated with behavioural data and metadata, including camera IDs and scene descriptions. The study also presents a latent-space normalisation technique that significantly improves out-of-distribution performance for both convolutional and transformer-based models. The dataset and related resources will be made available publicly.",
        "Tags": [
            "Wildlife Behaviour Recognition",
            "Out-of-Distribution Generalisation",
            "Latent-Space Normalisation",
            "Behaviour-Correlated Backgrounds",
            "Camera Trap Video Analysis"
        ]
    },
    {
        "Title": "Real-time High-fidelity Gaussian Human Avatars with Position-based Interpolation of Spatially Distributed MLPs",
        "Authors": "Youyi Zhan \u00b7 Tianjia Shao \u00b7 Yin Yang \u00b7 Kun Zhou",
        "Abstract": "Many works have succeeded in reconstructing Gaussian human avatars from multi-view videos. However, they either struggle to capture pose-dependent appearance details with a single MLP, or rely on a computationally intensive neural network to reconstruct high-fidelity appearance but with rendering performance degraded to non-real-time. We propose a novel Gaussian human avatar representation that can reconstruct high-fidelity pose-dependence appearance with details and meanwhile can be rendered in real time. Our Gaussian avatar is empowered by spatially distributed MLPs which are explicitly located on different positions on human body. The parameters stored in each Gaussian are obtained by interpolating from the outputs of its nearby MLPs based on their distances. To avoid undesired smooth Gaussian property changing during interpolation, for each Gaussian we define a set of Gaussian offset basis, and a linear combination of basis represents the Gaussian property offsets relative to the neutral properties. Then we propose to let the MLPs output a set of coefficients corresponding to the basis. In this way, although Gaussian coefficients are derived from interpolation and change smoothly, the Gaussian offset basis is learned freely without constraints. The smoothly varying coefficients combined with freely learned basis can still produce distinctly different Gaussian property offsets, allowing the ability to learn high-frequency spatial signals. We further use control points to constrain the Gaussians distributed on a surface layer rather than allowing them to be irregularly distributed inside the body, to help the human avatar generalize better when animated under novel poses. Compared to the state-of-the-art method, our method achieves better appearance quality with finer details while the rendering speed is significantly faster under novel views and novel poses.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a novel Gaussian human avatar representation capable of reconstructing high-fidelity, pose-dependent appearances in real-time. The method utilizes spatially distributed MLPs positioned on the human body, with Gaussian properties interpolated from nearby MLPs based on distance. To maintain detail fidelity, Gaussian offset basis and coefficients are introduced, allowing for high-frequency spatial signal learning. Control points ensure Gaussians are distributed on a surface layer, enhancing generalization under novel poses. The approach outperforms state-of-the-art methods in appearance quality and rendering speed.",
        "Tags": [
            "Avatars",
            "3DGS (Gaussian Splatting)",
            "Real-time Rendering",
            "Pose-dependent Appearance",
            "Spatially Distributed MLPs"
        ]
    },
    {
        "Title": "FRESA: Feedforward Reconstruction of Personalized Skinned Avatars from Few Images",
        "Authors": "Rong Wang \u00b7 Fabian Prada \u00b7 Ziyan Wang \u00b7 Zhongshi Jiang \u00b7 Chengxiang Yin \u00b7 Junxuan Li \u00b7 Shunsuke Saito \u00b7 Igor Santesteban \u00b7 Javier Romero \u00b7 Rohan Joshi \u00b7 Hongdong Li \u00b7 Jason Saragih \u00b7 Yaser Sheikh",
        "Abstract": "We present a novel method for reconstructing personalized 3D human avatars with realistic animation from only a few images. Due to the large variations in body shapes, poses, and cloth types, existing methods mostly require hours of per-subject optimization during inference, which limits their practical applications. In contrast, we learn a universal prior from over a thousand clothed humans to achieve instant feedforward generation and zero-shot generalization. Specifically, instead of rigging the avatar with shared skinning weights, we jointly infer personalized avatar shape, skinning weights, and pose-dependent deformations, which effectively improves overall geometric fidelity and reduces deformation artifacts. Moreover, to normalize pose variations and resolve coupled ambiguity between canonical shapes and skinning weights, we design a 3D canonicalization process to produce pixel-aligned initial conditions, which helps to reconstruct fine-grained geometric details. We then propose a multi-frame feature aggregation to robustly reduce artifacts introduced in canonicalization and fuse a plausible avatar preserving person-specific identities. Finally, we train the model in an end-to-end framework on a large-scale capture dataset, which contains diverse human subjects paired with high-quality 3D scans. Extensive experiments show that our method generates more authentic reconstruction and animation than state-of-the-arts, and can be directly generalized to inputs from casually taken phone photos.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces FRESA, a novel method for reconstructing personalized 3D human avatars from a few images, capable of realistic animation. Unlike existing methods that require extensive per-subject optimization, FRESA leverages a universal prior learned from over a thousand clothed humans to enable instant feedforward generation and zero-shot generalization. The method jointly infers personalized avatar shape, skinning weights, and pose-dependent deformations, enhancing geometric fidelity and reducing deformation artifacts. A 3D canonicalization process is designed to normalize pose variations and resolve ambiguities between canonical shapes and skinning weights, facilitating the reconstruction of fine-grained details. Additionally, a multi-frame feature aggregation technique is proposed to minimize artifacts and preserve person-specific identities. Trained on a large-scale dataset of diverse human subjects with high-quality 3D scans, FRESA demonstrates superior reconstruction and animation quality, generalizing effectively to casually taken phone photos.",
        "Tags": [
            "Avatars",
            "3D Reconstruction",
            "Feedforward Generation",
            "Zero-Shot Generalization",
            "Pose-Dependent Deformations"
        ]
    },
    {
        "Title": "Stereo Anywhere: Robust Zero-Shot Deep Stereo Matching Even Where Either Stereo or Mono Fail",
        "Authors": "Luca Bartolomei \u00b7 Fabio Tosi \u00b7 Matteo Poggi \u00b7 Stefano Mattoccia",
        "Abstract": "We introduce Stereo Anywhere, a novel stereo-matching framework that combines geometric constraints with robust priors from monocular depth Vision Foundation Models (VFMs). By elegantly coupling these complementary worlds through a dual-branch architecture, we seamlessly integrate stereo matching with learned contextual cues. Following this design, our framework introduces novel cost volume fusion mechanisms that effectively handle critical challenges such as textureless regions, occlusions, and non-Lambertian surfaces. Through our novel optical illusion dataset, MonoTrap, and extensive evaluation across multiple benchmarks, we demonstrate that our synthetic-only trained model achieves state-of-the-art results in zero-shot generalization, significantly outperforming existing solutions while showing remarkable robustness to challenging cases such as mirrors and transparencies.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Stereo Anywhere is a novel stereo-matching framework that integrates geometric constraints with robust priors from monocular depth Vision Foundation Models (VFMs). Using a dual-branch architecture, it combines stereo matching with learned contextual cues, addressing challenges like textureless regions, occlusions, and non-Lambertian surfaces. The framework introduces innovative cost volume fusion mechanisms and demonstrates state-of-the-art zero-shot generalization performance, particularly in handling challenging cases such as mirrors and transparencies.",
        "Tags": [
            "Stereo Matching",
            "Zero-Shot Learning",
            "Cost Volume Fusion",
            "Dual-Branch Architecture",
            "MonoTrap Dataset"
        ]
    },
    {
        "Title": "DropGaussian: Structural Regularization for Sparse-view Gaussian Splatting",
        "Authors": "Hyunwoo Park \u00b7 Gun Ryu \u00b7 Wonjun Kim",
        "Abstract": "Recently, 3D Gaussian splatting (3DGS) has gained considerable attentions in the field of novel view synthesis due to its fast performance while yielding the excellent image quality. However, 3DGS in sparse-view settings (e.g., three-view inputs) often faces with the problem of overfitting to training views, which significantly drops the visual quality of novel view images. Many existing approaches have tackled this issue by using strong priors, such as 2D generative contextual information and external depth signals. In contrast, this paper introduces a prior-free method, so-called DropGaussian, with simple changes in 3D Gaussian splatting. Specifically, we randomly remove Gaussians during the training process in a similar way of dropout, which allows non-excluded Gaussians to have larger gradients while improving their visibility. This makes the remaining Gaussians to contribute more to the optimization process for rendering with sparse input views. Such simple operation effectively alleviates the overfitting problem and enhances the quality of novel view synthesis. By simply applying DropGaussian to the original 3DGS framework, we can achieve the competitive performance with existing prior-based 3DGS methods in sparse-view settings of benchmark datasets without any additional complexity.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "3D Gaussian splatting (3DGS) is effective for novel view synthesis but struggles with overfitting in sparse-view settings, leading to poor visual quality. This paper introduces DropGaussian, a prior-free method that randomly removes Gaussians during training, akin to dropout. This approach enhances the visibility and contribution of remaining Gaussians, mitigating overfitting and improving novel view synthesis quality. DropGaussian achieves competitive performance with prior-based methods on benchmark datasets without added complexity.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Novel View Synthesis",
            "Dropout Regularization",
            "Sparse-view Optimization",
            "Gaussian Visibility Enhancement"
        ]
    },
    {
        "Title": "UltraFusion: Ultra High Dynamic Imaging using Exposure Fusion",
        "Authors": "Zixuan Chen \u00b7 Yujin Wang \u00b7 Xin Cai \u00b7 Zhiyuan You \u00b7 Zhe-Ming Lu \u00b7 Zhang Zhang \u00b7 Shi Guo \u00b7 Tianfan Xue",
        "Abstract": "Capturing high dynamic range (HDR) scenes is one of the most important issues in camera design. Majority of cameras use exposure fusion technique, which fuses images captured by different exposure levels, to increase dynamic range. However, this approach can only handle images with limited exposure difference, normally 3-4 stops. When applying to very high dynamic scenes where a large exposure difference is required, this approach often fails due to incorrect alignment or inconsistent lighting between inputs, or tone mapping artifacts. In this work, we propose UltraFusion, the first exposure fusion technique that can merge input with 9 stops differences. The key idea is that we model the exposure fusion as a guided inpainting problem, where the under-exposed image is used as a guidance to fill the missing information of over-exposed highlight in the over-exposed region. Using under-exposed image as a soft guidance, instead of a hard constrain, our model is robust to potential alignment issue or lighting variations. Moreover, utilizing the image prior of the generative model, our model also generates natural tone mapping, even for very high-dynamic range scene. Our approach outperforms HDR-Transformer on latest HDR benchmarks. Moreover, to test its performance in ultra high dynamic range scene, we capture a new real-world exposure fusion benchmark, UltraFusion dataset, with exposure difference up to 9 stops, and experiments show that \\model~can generate beautiful and high-quality fusion results under various scenarios.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "UltraFusion introduces a novel exposure fusion technique capable of merging images with up to 9 stops of exposure difference, significantly surpassing the limitations of traditional methods that handle only 3-4 stops. By modeling exposure fusion as a guided inpainting problem, where the under-exposed image guides the filling of over-exposed regions, UltraFusion addresses common issues like incorrect alignment, inconsistent lighting, and tone mapping artifacts. This approach leverages a generative model's image prior to produce natural tone mapping, even in very high-dynamic range scenes. UltraFusion outperforms existing methods like HDR-Transformer on standard benchmarks and demonstrates superior performance on a newly introduced UltraFusion dataset, designed for ultra high dynamic range scenarios.",
        "Tags": [
            "Exposure Fusion",
            "High Dynamic Range (HDR)",
            "Guided Inpainting",
            "Tone Mapping",
            "Ultra High Dynamic Range Imaging"
        ]
    },
    {
        "Title": "Advancing Semantic Future Prediction through Multimodal Visual Sequence Transformers",
        "Authors": "Efstathios Karypidis \u00b7 Ioannis Kakogeorgiou \u00b7 Spyros Gidaris \u00b7 Nikos Komodakis",
        "Abstract": "Semantic future prediction is important for autonomous systems navigating dynamic environments. This paper introduces FUTURIST, a method for multimodal future semantic prediction that uses a unified and efficient visual sequence transformer architecture. Our approach incorporates a multimodal masked visual modeling objective and a novel masking mechanism designed for multimodal training. This allows the model to effectively integrate visible information from various modalities, improving prediction accuracy. Additionally, we propose a VAE-free hierarchical tokenization process, which reduces computational complexity, streamlines the training pipeline, and enables end-to-end training with high-resolution, multimodal inputs. We validate FUTURIST on the Cityscapes dataset, demonstrating state-of-the-art performance in future semantic segmentation for both short- and mid-term forecasting.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces FUTURIST, a method for multimodal future semantic prediction using a unified visual sequence transformer architecture. The approach integrates a multimodal masked visual modeling objective and a novel masking mechanism for effective multimodal training. A VAE-free hierarchical tokenization process is proposed to reduce computational complexity and enable end-to-end training with high-resolution inputs. FUTURIST achieves state-of-the-art performance in future semantic segmentation on the Cityscapes dataset for short- and mid-term forecasting.",
        "Tags": [
            "Semantic Segmentation",
            "Multimodal Learning",
            "Visual Sequence Transformer",
            "Multimodal Masked Visual Modeling",
            "VAE-free Tokenization"
        ]
    },
    {
        "Title": "DiSciPLE: Learning Interpretable Programs for Scientific Visual Discovery",
        "Authors": "Utkarsh Mall \u00b7 Cheng Perng Phoo \u00b7 Mia Chiquier \u00b7 Bharath Hariharan \u00b7 Kavita Bala \u00b7 Carl Vondrick",
        "Abstract": "Visual data is used in numerous different scientific workflows ranging from remote sensing to ecology. As the amount of observation data increases, the challenge is not just to make accurate predictions but also to understand the underlying mechanisms for those predictions. Good interpretation is important in scientific workflows, as it allows for better decision-making by providing insights into the data. This paper introduces an automatic way of obtaining such interpretable-by-design models, by learning programs that interleave neural networks. We propose DiSciPLE (Discovering Scientific Programs using LLMs and Evolution) an evolutionary algorithm that leverages common sense and prior knowledge of large language models (LLMs) to create Python programs explaining visual data. Additionally, we propose two improvements: a program critic and a program simplifier to improve our method further to synthesize good programs. On three different real world problems, DiSciPLE learns state-of-the-art programs on novel tasks with no prior literature. For example, we can learn programs with 35% lower error than the closest non-interpretable baseline for population density estimation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces DiSciPLE, a method for learning interpretable programs that interleave neural networks to explain visual data in scientific workflows. DiSciPLE uses an evolutionary algorithm enhanced by large language models (LLMs) to generate Python programs that provide insights into the data. The method includes a program critic and a program simplifier to refine the programs further. Applied to three real-world problems, DiSciPLE achieves state-of-the-art performance on novel tasks, demonstrating significant improvements over non-interpretable baselines, such as a 35% reduction in error for population density estimation.",
        "Tags": [
            "Large Language Models (LLMs)",
            "Interpretable Models",
            "Evolutionary Algorithm",
            "Program Synthesis",
            "Scientific Data Interpretation"
        ]
    },
    {
        "Title": "MaSS13K: A Matting-level Semantic Segmentation Benchmark",
        "Authors": "Chenxi Xie \u00b7 Minghan LI \u00b7 Hui Zeng \u00b7 Jun Luo \u00b7 Lei Zhang",
        "Abstract": "High-resolution semantic segmentation is essential for applications like image editing, bokeh imaging,  and AR/VR, etc. Unfortunately, existing datasets often have limited resolution and lack precise mask details and boundaries. In this work, we build a large-scale, matting-level semantic segmentation dataset, named MaSS13K, which consists of 13,348 real-world images, all at 4K resolution. MaSS13K provides high-quality mask annotations of a number of objects, which are categorized into seven categories: human, vegetation, ground, sky, water, building, and others. MaSS13K features with precise masks, with an average mask complexity 20-50 times higher than existing semantic segmentation datasets. We consequently present a method specifically designed for high-resolution semantic segmentation, namely MaSSFormer, which employs an efficient pixel decoder that aggregates high-level semantic features and low-level texture features across three stages, aiming to produce high-resolution masks with minimal computational cost. Finally, we propose a new learning paradigm, which integrates the high-quality masks of the seven given categories with pseudo labels form new classes, enabling MaSSFormer to transfer its accurate segmentation capability to other classes of objects. Our proposed MaSSFormer is comprehensively evaluated on the MaSS13K benchmark together with 14 representative segmentation models.  We expect that our meticulously annotated MaSS13K dataset and the MaSSFormer model can facilitate the research of high-resolution and high-quality semantic segmentation. Datasets and codes will be released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces MaSS13K, a large-scale, high-resolution semantic segmentation dataset comprising 13,348 real-world images at 4K resolution. MaSS13K provides precise mask annotations for seven categories of objects, significantly surpassing existing datasets in mask complexity. The authors also propose MaSSFormer, a method designed for high-resolution semantic segmentation, which efficiently aggregates semantic and texture features across three stages to produce high-quality masks with minimal computational cost. Additionally, a novel learning paradigm is introduced to enhance the model's segmentation capability across new classes. The effectiveness of MaSSFormer is validated through comprehensive evaluations on the MaSS13K benchmark against 14 representative segmentation models. The dataset and model aim to advance research in high-resolution and high-quality semantic segmentation.",
        "Tags": [
            "Semantic Segmentation",
            "Datasets and Benchmarks",
            "High-Resolution Segmentation",
            "Mask Complexity",
            "Learning Paradigm"
        ]
    },
    {
        "Title": "SinGS: Animatable Single-Image Human Gaussian Splats with Kinematic Priors",
        "Authors": "Yufan Wu \u00b7 Xuanhong Chen \u00b7 Wen Li \u00b7 Shunran Jia \u00b7 Hualiang Wei \u00b7 Kairui Feng \u00b7 Jialiang CHEN \u00b7 Yuhan Li \u00b7 Ang He \u00b7 Weimin Zhang \u00b7 Bingbing Ni \u00b7 Wenjun Zhang",
        "Abstract": "Despite significant advances in accurately estimating geometry in contemporary single-image 3D human reconstruction, creating a high-quality, efficient, and animatable 3D avatar remains an open challenge. Two key obstacles persist: incomplete observation and inconsistent 3D priors.To address these challenges, we propose \\textbf{SinG}, aiming to achieve high-quality and efficient animatable 3D avatar reconstruction.At the heart of SinG are two key components: Kinematic Human Diffusion (KHD) and compact 3D distillation. The former is a foundational human model that samples within pose space to generate a highly 3D-consistent and high-quality sequence of human images, inferring unseen viewpoints and providing kinematic priors. The latter is a system that reconstructs a compact, high-quality 3D avatar even under imperfect priors, achieved through a novel regional Laplacian that enables precise and compact assembly of 3D primitives.Extensive experiments demonstrate that SinG enables lifelike, animatable human reconstructions, maintaining both high quality and inference efficiency.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces SinGS, a method for creating high-quality, efficient, and animatable 3D avatars from single images. It addresses challenges of incomplete observation and inconsistent 3D priors through two main components: Kinematic Human Diffusion (KHD) and compact 3D distillation. KHD generates a sequence of human images with consistent 3D views and kinematic priors, while compact 3D distillation reconstructs a high-quality 3D avatar using a novel regional Laplacian for precise assembly of 3D primitives. The approach results in lifelike, animatable human reconstructions with high quality and efficient inference.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Avatars",
            "Kinematic Priors",
            "Regional Laplacian",
            "Single-Image 3D Reconstruction"
        ]
    },
    {
        "Title": "DELT: A Simple Diversity-driven EarlyLate Training for Dataset Distillation",
        "Authors": "Zhiqiang Shen \u00b7 Ammar Sherif \u00b7 Zeyuan Yin \u00b7 Shitong Shao",
        "Abstract": "Recent advances in dataset distillation have led to solutions in two main directions. The conventional batch-to-batch matching mechanism is ideal for small-scale datasets and includes bi-level optimization methods on models and syntheses, such as FRePo, RCIG, and RaT-BPTT, as well as other methods like distribution matching, gradient matching, and weight trajectory matching. Conversely, batch-to-global matching typifies decoupled methods, which are particularly advantageous for large-scale datasets. This approach has garnered substantial interest within the community, as seen in SRe$^2$L, G-VBSM, WMDD, and CDA. A primary challenge with the second approach is the lack of diversity among syntheses within each class since samples are optimized independently and the same global supervision signals are reused across different synthetic images. In this study, we propose a new Diversity-driven EarlyLate Training (DELT) scheme to enhance the diversity of images in batch-to-global matching with less computation. Our approach is conceptually simple yet effective, it partitions predefined IPC samples into smaller subtasks and employs local optimizations to distill each subset into distributions from distinct phases, reducing the uniformity induced by the unified optimization process. These distilled images from the subtasks demonstrate effective generalization when applied to the entire task. We conduct extensive experiments on CIFAR, Tiny-ImageNet, ImageNet-1K, and its sub-datasets. Our approach outperforms the previous state-of-the-art by 2$\\sim$5\\% on average across different datasets and IPCs (images per class), increasing diversity per class by more than 5\\% while reducing synthesis time by up to 39.3\\% for enhancing the training efficiency.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces DELT, a Diversity-driven EarlyLate Training scheme designed to improve the diversity of images in batch-to-global matching for dataset distillation. The method partitions samples into smaller subtasks and uses local optimizations to distill each subset into distributions from distinct phases, thereby reducing uniformity and enhancing diversity. DELT demonstrates superior performance across various datasets and IPCs, increasing diversity by over 5% and reducing synthesis time by up to 39.3%, while outperforming previous state-of-the-art methods by 2~5% on average.",
        "Tags": [
            "Dataset Distillation",
            "Data Augmentation",
            "Batch-to-Global Matching",
            "Local Optimization",
            "Training Efficiency"
        ]
    },
    {
        "Title": "HUNet: Homotopy Unfolding Network for Image Compressive Sensing",
        "Authors": "Feiyang Shen \u00b7 Hongping Gan",
        "Abstract": "Deep Unfolding Networks (DUNs) have risen to prominence due to their interpretability and superior performance for image Compressive Sensing (CS). However, existing DUNs still face significant issues, such as the insufficient representation capability of single-scale image information during the iterative reconstruction phase and loss of feature information, which fundamentally limit the further enhancement of image CS performance. In this paper, we propose Homotopy Unfolding Network (HUNet) for image CS, which enables phase-by-phase reconstruction of images along homotopy path. Specifically, each iteration step of the traditional homotopy algorithm is mapped to a Multi-scale Homotopy Iterative Module (MHIM), which includes U-shaped stacked Window-based Transformer Blocks capable of efficient feature extraction. Within the MHIM, we design the Deep Homotopy Continuation Strategy to ensure the interpretability of the homotopy algorithm and facilitate feature learning. Additionally, we introduce a Dual-path Feature Fusion Module to mitigate the loss of high-dimensional feature information during the transmission between iterative phases, thereby maximizing the preservation of details in the reconstructed image. Extensive experiments indicate that HUNet achieves superior image reconstruction results compared to existing state-of-the-art methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces the Homotopy Unfolding Network (HUNet) for image Compressive Sensing (CS), addressing limitations in existing Deep Unfolding Networks (DUNs) such as insufficient representation capability and feature information loss during iterative reconstruction. HUNet employs a Multi-scale Homotopy Iterative Module (MHIM) with U-shaped stacked Window-based Transformer Blocks for efficient feature extraction, a Deep Homotopy Continuation Strategy for interpretability and feature learning, and a Dual-path Feature Fusion Module to minimize high-dimensional feature loss between iterative phases. The proposed method demonstrates superior image reconstruction performance over current state-of-the-art techniques.",
        "Tags": [
            "Compressive Sensing",
            "Deep Unfolding Networks",
            "Image Reconstruction",
            "Multi-scale Feature Extraction",
            "Window-based Transformer Blocks",
            "Dual-path Feature Fusion"
        ]
    },
    {
        "Title": "MERGE: Multi-faceted Hierarchical Graph-based GNN for Gene Expression Prediction from Whole Slide Histopathology Images",
        "Authors": "Aniruddha Ganguly \u00b7 Debolina Chatterjee \u00b7 Wentao Huang \u00b7 Jie Zhang \u00b7 Alisa Yurovsky \u00b7 Travis Steele Johnson \u00b7 Chao Chen",
        "Abstract": "Recent advances in Spatial Transcriptomics (ST) pair histology images with spatially resolved gene expression profiles, enabling predictions of gene expression across different tissue locations based on image patches. This opens up new possibilities for enhancing whole slide image (WSI) prediction tasks with localized gene expression. However, existing methods fail to fully leverage the interactions between different tissue locations, which are crucial for accurate joint prediction. To address this, we introduce MERGE (Multi-faceted hiErarchical gRaph for Gene Expressions), which combines a multi-faceted hierarchical graph construction strategy with graph neural networks (GNN) to improve gene expression predictions from WSIs. By clustering tissue image patches based on both spatial and morphological features, and incorporating intra- and inter-cluster edges, our approach fosters interactions between distant tissue locations during GNN learning. As an additional contribution, we evaluate different data smoothing techniques that are necessary to mitigate artifacts in ST data, often caused by technical imperfections. We advocate for adopting gene-aware smoothing methods that are more biologically justified. Experimental results on gene expression prediction show that our GNN method outperforms state-of-the-art techniques across multiple metrics.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces MERGE, a novel approach for predicting gene expression from whole slide histopathology images using a multi-faceted hierarchical graph construction strategy combined with graph neural networks (GNNs). MERGE enhances gene expression predictions by clustering tissue image patches based on spatial and morphological features and incorporating intra- and inter-cluster edges to facilitate interactions between distant tissue locations. The method also evaluates data smoothing techniques to address artifacts in Spatial Transcriptomics (ST) data, advocating for gene-aware smoothing methods. MERGE demonstrates superior performance over state-of-the-art techniques in gene expression prediction.",
        "Tags": [
            "Graph Neural Networks (GNNs)",
            "Medical Image Analysis",
            "Spatial Transcriptomics",
            "Gene Expression Prediction",
            "Hierarchical Graph Construction"
        ]
    },
    {
        "Title": "Logits DeConfusion with CLIP for Few-Shot Learning",
        "Authors": "Shuo Li \u00b7 Fang Liu \u00b7 Zehua Hao \u00b7 Xinyi Wang \u00b7 Lingling Li \u00b7 Xu Liu \u00b7 Puhua Chen \u00b7 Wenping Ma",
        "Abstract": "With its powerful visual-language alignment capability, CLIP performs well in zero-shot and few-shot learning tasks. However, we found in experiments that CLIP's logits suffer from serious inter-class confusion problems in downstream tasks, and the ambiguity between categories seriously affects the accuracy. To address this challenge, we propose a novel method called Logits DeConfusion, which effectively learns and eliminates inter-class confusion in logits by combining our Multi-level Adapter Fusion (MAF) module with our Inter-Class Deconfusion (ICD) module. First, MAF extracts features from different levels of the CLIP image encoder and fuses them uniformly to enhance feature representation. Second, ICD learnably eliminates inter-class confusion in logits with a residual structure. Experimental results on multiple benchmarks show that our method can significantly improve the classification performance and alleviate the category confusion problem.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "CLIP demonstrates strong visual-language alignment capabilities, excelling in zero-shot and few-shot learning tasks. However, its logits often suffer from inter-class confusion, which negatively impacts accuracy. To address this, the authors propose Logits DeConfusion, a method combining the Multi-level Adapter Fusion (MAF) module and the Inter-Class Deconfusion (ICD) module. MAF enhances feature representation by extracting and fusing features from different levels of the CLIP image encoder, while ICD uses a residual structure to eliminate inter-class confusion in logits. The method significantly improves classification performance and reduces category confusion across multiple benchmarks.",
        "Tags": [
            "CLIP",
            "Few-Shot Learning",
            "Multi-level Adapter Fusion",
            "Inter-Class Deconfusion",
            "Residual Structure"
        ]
    },
    {
        "Title": "HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation",
        "Authors": "Yiming Liang \u00b7 Tianhan Xu \u00b7 Yuta Kikuchi",
        "Abstract": "We present Hierarchical Motion Representation (HiMoR), a novel deformation representation for 3D Gaussian primitives that is capable of achieving high-quality monocular dynamic 3D reconstruction. HiMoR's foundation is the insight that motions in everyday scenes can be decomposed into coarser motion, which forms the basis for finer details. Using a tree structure, HiMoR's nodes represent different levels of motion detail, with shallower nodes depicting coarse motion for temporal smoothness and deeper nodes denoting finer motion. Additionally, our model uses a few shared motion bases to represent motions of different set of nodes, aligning with the assumption of motion being generally low-rank. This motion representation design provides Gaussians with a more rational deformation, maximizing the use of temporal relationships to tackle the challenging task of monocular dynamic 3D reconstruction. We also propose using a more reliable perceptual metric as a substitute, given that pixel-level metrics for evaluating monocular dynamic 3D reconstruction can sometimes fail to effectively reflect true reconstruction quality. Extensive experiments demonstrate our method's efficacy in achieving superior novel view synthesis from challenging monocular videos with complex motions.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Hierarchical Motion Representation (HiMoR), a novel deformation representation for 3D Gaussian primitives designed for high-quality monocular dynamic 3D reconstruction. HiMoR decomposes motions into coarser and finer details using a tree structure, where shallower nodes represent coarse motion for temporal smoothness and deeper nodes denote finer motion. The model employs shared motion bases to represent motions of different node sets, leveraging the low-rank nature of motion. This approach enhances the deformation of Gaussians by maximizing temporal relationships, addressing the complexities of monocular dynamic 3D reconstruction. Additionally, the paper proposes a more reliable perceptual metric to evaluate reconstruction quality, as traditional pixel-level metrics may not effectively reflect true quality. The method demonstrates superior performance in novel view synthesis from challenging monocular videos with complex motions.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "3D Reconstruction",
            "Hierarchical Motion Representation",
            "Monocular Dynamic 3D Reconstruction",
            "Perceptual Metric"
        ]
    },
    {
        "Title": "Foveated Instance Segmentation",
        "Authors": "Hongyi Zeng \u00b7 Wenxuan Liu \u00b7 Tianhua Xia \u00b7 Jinhui Chen \u00b7 Ziyun Li \u00b7 Sai Qian Zhang",
        "Abstract": "Instance segmentation is essential for augmented reality and virtual reality (AR/VR) as it enables precise object recognition and interaction, enhancing the integration of virtual and real-world elements for an immersive experience. However, the high computational overhead of segmentation limits its application on resource-constrained AR/VR devices, causing large processing latency and degrading user experience. In contrast to conventional scenarios, AR/VR users typically focus on only a few regions within their field of view before shifting perspective, allowing segmentation to be concentrated on gaze-specific areas. This insight drives the need for efficient segmentation methods that prioritize processing the instance of interest (IOI), reducing computational load and enhancing real-time performance.In this paper, we present a~\\textit{foveated instance segmentation} (FovealSeg) framework that leverages real-time user gaze data to perform instance segmentation exclusively on instance of interest, resulting in substantial computational savings. Evaluation results show that FSNet achieves an IoU of 0.52 on CityScapes and 0.43 on ADE20K, notably outperforming the baseline.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces FovealSeg, a foveated instance segmentation framework designed for AR/VR applications. It utilizes real-time user gaze data to focus segmentation on the instance of interest, significantly reducing computational overhead and improving real-time performance. The method demonstrates superior performance on the CityScapes and ADE20K datasets compared to baseline approaches.",
        "Tags": [
            "Instance Segmentation",
            "Autonomous Driving",
            "Real-Time Processing",
            "Gaze Data Utilization",
            "Computational Efficiency",
            "AR/VR Applications"
        ]
    },
    {
        "Title": "Generalized Diffusion Detector: Mining Robust Features from Diffusion Models for Domain-Generalized Detection",
        "Authors": "Boyong He \u00b7 Yuxiang Ji \u00b7 Qianwen Ye \u00b7 Zhuoyue Tan \u00b7 Liaoni Wu",
        "Abstract": "Domain generalization (DG) for object detection aims to enhance detectors' performance in unseen scenarios. This task remains challenging due to complex variations in real-world applications. Recently, diffusion models have demonstrated remarkable capabilities in diverse scene generation, which inspires us to explore their potential for improving DG tasks. Instead of generating images, our method extracts multi-step intermediate features during the diffusion process to obtain domain-invariant features for generalized detection. Furthermore, we propose an efficient knowledge transfer framework that enables detectors to inherit the generalization capabilities of diffusion models through feature and object-level alignment, without increasing inference time. We conduct extensive experiments on six challenging DG benchmarks. The results demonstrate that our method achieves substantial improvements of 14.0\\% mAP over existing DG approaches across different domains and corruption types. Notably, our method even outperforms most domain adaptation methods without accessing any target domain data. Moreover, the diffusion-guided detectors show consistent improvements of 15.9\\% mAP on average compared to the baseline. Our work aims to present an effective approach for domain-generalized detection and provide potential insights for robust visual recognition in real-world scenarios.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a novel approach to domain generalization (DG) for object detection by leveraging diffusion models. Instead of generating images, the method extracts multi-step intermediate features during the diffusion process to obtain domain-invariant features, enhancing the detector's performance in unseen scenarios. An efficient knowledge transfer framework is proposed, enabling detectors to inherit the generalization capabilities of diffusion models through feature and object-level alignment without increasing inference time. The approach achieves significant improvements over existing DG methods, outperforming most domain adaptation methods without requiring target domain data. The results demonstrate consistent performance gains, highlighting the potential of diffusion models for robust visual recognition in real-world applications.",
        "Tags": [
            "Object Detection",
            "Domain Generalization",
            "Diffusion Models",
            "Feature Extraction",
            "Knowledge Transfer",
            "Domain-Invariant Features"
        ]
    },
    {
        "Title": "DefectFill: Realistic Defect Generation with Inpainting Diffusion Model for Visual Inspection",
        "Authors": "Jaewoo Song \u00b7 Daemin Park \u00b7 Kanghyun Baek \u00b7 Sangyub Lee \u00b7 Jooyoung Choi \u00b7 Eunji Kim \u00b7 Sungroh Yoon",
        "Abstract": "Developing effective visual inspection models remains challenging due to the scarcity of defect data, especially in new or low-defect-rate manufacturing processes. While recent approaches have attempted to generate defect images using image generation models, producing highly realistic defects remains difficult. In this paper, we propose DefectFill, a novel method for realistic defect generation that requires only a few reference defect images. DefectFill leverages a fine-tuned inpainting diffusion model, optimized with our custom loss functions that incorporate defect, object, and cross-attention terms. This approach enables the inpainting diffusion model to precisely capture detailed, localized defect features and seamlessly blend them into defect-free objects. Additionally, we introduce the Low-Fidelity Selection method to further enhance the quality of the generated defect samples. Experiments demonstrate that DefectFill can generate high-quality defect images, and visual inspection models trained on these images achieve state-of-the-art performance on the MVTec AD dataset.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "DefectFill addresses the challenge of generating realistic defect images for visual inspection models, which often suffer from a lack of defect data, particularly in new or low-defect-rate manufacturing processes. The proposed method utilizes a fine-tuned inpainting diffusion model enhanced with custom loss functions that include defect, object, and cross-attention terms. This allows for the precise capture of detailed, localized defect features and their seamless integration into defect-free objects. Additionally, the Low-Fidelity Selection method is introduced to improve the quality of generated defect samples. The effectiveness of DefectFill is demonstrated through its ability to produce high-quality defect images, leading to state-of-the-art performance in visual inspection models on the MVTec AD dataset.",
        "Tags": [
            "Image Inpainting",
            "Data Augmentation",
            "Inpainting Diffusion Model",
            "Custom Loss Functions",
            "Low-Fidelity Selection"
        ]
    },
    {
        "Title": "CLIP-driven Coarse-to-fine Semantic Guidance for Fine-grained Open-set Semi-supervised Learning",
        "Authors": "Xiaokun Li \u00b7 Yaping Huang \u00b7 Qingji Guan",
        "Abstract": "Fine-grained open-set semi-supervised learning (OSSL) investigates a practical scenario where unlabeled data may contain fine-grained out-of-distribution (OOD) samples. Due to the subtle visual differences among in-distribution (ID) samples, as well as between ID and OOD samples, it is extremely challenging to separate ID and OOD samples. Recent Vision-Language Models, such as CLIP, have shown excellent generalization capabilities. However, it tends to focus on general attributes, and thus is insufficient to distinguish the fine-grained details. To tackle the issues, in this paper, we propose a novel CLIP-driven coarse-to-fine semantic-guided framework, named CFSG-CLIP, by progressively filtering and focusing the distinctive fine-grained clues. Specifically, CFSG-CLIP comprises a coarse-guidance module and a fine-guidance module derived from the pre-trained CLIP model. In the coarse-guidance module, we design a semantic filtering strategy to initially filter out local visual features guided by cross-modality guidance. Then, in the fine-guidance module, we further design a visual-semantic injection strategy, which embeds category-related visual cues into the visual encoder to further refine the local visual features. By the designed dual-guidance framework, the local subtle cues are progressively discovered to distinct the subtle difference between ID and OOD samples. Extensive experiments demonstrates that CFSG-CLIP is able to not only improve the reliability of the fine-grained semi-supervised learning training process, but also achieves a competitive performance on multiple fine-grained datasets.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of fine-grained open-set semi-supervised learning (OSSL), where unlabeled data may include fine-grained out-of-distribution (OOD) samples. The subtle visual differences between in-distribution (ID) and OOD samples make separation difficult. Leveraging the generalization capabilities of Vision-Language Models like CLIP, the authors propose a novel framework, CFSG-CLIP, which employs a coarse-to-fine semantic-guided approach. The framework consists of a coarse-guidance module that filters local visual features using cross-modality guidance and a fine-guidance module that refines these features by embedding category-related visual cues. This dual-guidance framework progressively uncovers subtle local cues to distinguish between ID and OOD samples, enhancing the reliability of fine-grained semi-supervised learning and achieving competitive performance on multiple fine-grained datasets.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Fine-Grained Learning",
            "Cross-Modality Guidance",
            "Semantic Filtering",
            "Visual-Semantic Injection"
        ]
    },
    {
        "Title": "DiffCAM: Data-Driven Saliency Maps by Capturing Feature Differences",
        "Authors": "Xingjian Li \u00b7 Qiming Zhao \u00b7 Neelesh Bisht \u00b7 Mostofa Uddin Uddin \u00b7 Jin Yu Kim \u00b7 Bryan Zhang \u00b7 Min Xu",
        "Abstract": "In recent years, the interpretability of Deep Neural Networks (DNNs) has garnered significant attention, particularly due to their widespread deployment in critical domains like healthcare, finance, and autonomous systems. To address the challenge of understanding how DNNs make decisions, Explainable AI (XAI) methods, such as saliency maps, have been developed to provide insights into the inner workings of these models. This paper introduces DiffCAM, a novel XAI method designed to overcome limitations in existing Class Activation Map (CAM)-based techniques, which often rely on decision boundary gradients to estimate feature importance. DiffCAM differentiates itself by considering the actual data distribution of the reference class, identifying feature importance based on how a target example differs from reference examples. This approach captures the most discriminative features without relying on decision boundaries or prediction results, making DiffCAM applicable to a broader range of models, including foundation models. Through extensive experiments, we demonstrate the superior performance and flexibility of DiffCAM in providing meaningful explanations across diverse datasets and scenarios.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces DiffCAM, a novel Explainable AI (XAI) method designed to improve the interpretability of Deep Neural Networks (DNNs) by generating saliency maps. Unlike traditional Class Activation Map (CAM)-based techniques that rely on decision boundary gradients, DiffCAM identifies feature importance by analyzing how a target example differs from reference examples, leveraging the actual data distribution of the reference class. This approach enables DiffCAM to capture discriminative features without depending on decision boundaries or prediction results, making it applicable to a wide range of models, including foundation models. The method demonstrates superior performance and flexibility in providing meaningful explanations across diverse datasets and scenarios.",
        "Tags": [
            "Explainable AI (XAI)",
            "Saliency Maps",
            "Feature Importance Analysis",
            "Data-Driven Interpretability",
            "Foundation Model Compatibility"
        ]
    },
    {
        "Title": "RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics",
        "Authors": "Chan Hee Song \u00b7 Valts Blukis \u00b7 Jonathan Tremblay \u00b7 Stephen Tyree \u00b7 Yu Su \u00b7 Stan Birchfield",
        "Abstract": "Spatial understanding is a crucial capability for robots to make grounded decisions based on their environment. This foundational skill enables robots not only to perceive their surroundings but also to reason about and interact meaningfully within the world. In modern robotics, these capabilities are taken on by visual language models, and they face significant challenges when applied to spatial reasoning context due to their training data sources. These sources utilize general-purpose image datasets, and they often lack sophisticated spatial scene understanding capabilities. For example, the datasets do not address reference frame comprehension \u2014 spatial relationships require clear contextual understanding, whether from a ego-centric, object-centric, or world-centric perspective, which allow for effective real-world interaction. To address this issue, we introduce RoboSpatial, a large-scale spatial understanding dataset consisting of real indoor and tabletop scenes captured as 3D scans and ego-centric images, annotated with rich spatial information relevant to robotics. The dataset includes 1M images, 5K 3D scans, and 3M annotated spatial relationships, with paired 2D egocentric images and 3D scans to make it both 2D and 3D ready. Our experiments show that models trained with RoboSpatial outperform baselines on downstream tasks such as spatial affordance prediction, spatial relationship prediction, and robotics manipulation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Spatial understanding is essential for robots to interact effectively with their environment. Current visual language models, trained on general-purpose image datasets, often lack sophisticated spatial reasoning capabilities. To address this, the authors introduce RoboSpatial, a large-scale dataset comprising 1M images, 5K 3D scans, and 3M annotated spatial relationships, designed to enhance spatial understanding in both 2D and 3D contexts. The dataset includes real indoor and tabletop scenes, annotated with rich spatial information relevant to robotics. Models trained on RoboSpatial demonstrate superior performance in tasks such as spatial affordance prediction, spatial relationship prediction, and robotics manipulation compared to baseline models.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "3D Point Cloud",
            "Robotics",
            "Spatial Reasoning",
            "Ego-centric Perception",
            "Robotics Manipulation"
        ]
    },
    {
        "Title": "ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models",
        "Authors": "Ozgur Kara \u00b7 Krishna Kumar Singh \u00b7 Feng Liu \u00b7 Duygu Ceylan \u00b7 James Rehg \u00b7 Tobias Hinz",
        "Abstract": "Current diffusion-based text-to-video methods are limited to producing short video clips of a single shot and lack the capability to generate multi-shot videos with discrete transitions where the same character performs distinct activities across the same or different backgrounds. To address this limitation we propose a framework that includes a dataset collection pipeline and architectural extensions to video diffusion models to enable text-to-multi-shot video generation. Our approach enables generation of multi-shot videos as a single video with full attention across all frames of all shots, ensuring character and background consistency, and allows users to control the number, duration, and content of shots through shot-specific conditioning. This is achieved by incorporating a transition token into the text-to-video model to control at which frames a new shot begins and a local attention masking strategy which controls the transition token's effect and allows shot-specific prompting. To obtain training data we propose a novel data collection pipeline to construct a multi-shot video dataset from existing single-shot video datasets. Extensive experiments demonstrate that fine-tuning a pre-trained text-to-video model for a few thousand iterations is enough for the model to subsequently be able to generate multi-shot videos with shot-specific control, outperforming the baselines.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces ShotAdapter, a framework designed to overcome the limitations of current diffusion-based text-to-video methods, which are restricted to generating short, single-shot video clips. ShotAdapter enables the generation of multi-shot videos with discrete transitions, maintaining character and background consistency across shots. The framework includes a novel data collection pipeline for constructing a multi-shot video dataset from existing single-shot video datasets and architectural extensions to video diffusion models. These extensions incorporate a transition token for controlling the start of new shots and a local attention masking strategy for shot-specific prompting. The approach allows users to control the number, duration, and content of shots through shot-specific conditioning. Fine-tuning a pre-trained text-to-video model with the proposed framework enables the generation of multi-shot videos with shot-specific control, demonstrating superior performance over baseline methods.",
        "Tags": [
            "Diffusion Models",
            "Video Generation",
            "Multi-Shot Video Generation",
            "Transition Token",
            "Local Attention Masking"
        ]
    },
    {
        "Title": "Foley-Flow: Coordinated Video-to-Audio Generation with Masked Audio-Visual Alignment and Dynamic Conditional Flows",
        "Authors": "Shentong Mo \u00b7 Yibing Song",
        "Abstract": "Coordinated audio generation based on video inputs typically requires a strict audio-visual (AV) alignment, where both semantics and rhythmics of the generated audio segments shall correspond to those in the video frames. Previous studies leverage a two-stage design where the AV encoders are firstly aligned via contrastive learning, then the encoded video representations guide the audio generation process. We observe that both contrastive learning and global video guidance are effective in aligning overall AV semantics while limiting temporally rhythmic synchronization. In this work, we propose Foley-Flow to first align unimodal AV encoders via masked modeling training, where the masked audio segments are recovered under the guidance of the corresponding video segments. After training, the AV encoders which are separately pretrained using only unimodal data are aligned with semantic and rhythmic consistency. Then, we develop a dynamic conditional flow for the final audio generation. Built upon the efficient velocity flow generation framework, our dynamic conditional flow utilizes temporally varying video features as the dynamic condition to guide corresponding audio segment generations. To this end, we extract coherent semantic and rhythmic representations during masked AV alignment, and use this representation of video segments to guide audio generation temporally. Our audio results are evaluated on the standard benchmarks and largely surpass existing results under several metrics. The superior performance indicates that Foley-Flow is effective in generating coordinated audios that are both semantically and rhythmically coherent to various video sequences.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Foley-Flow introduces a novel approach for coordinated video-to-audio generation by addressing the limitations of previous methods in achieving both semantic and rhythmic alignment between video and audio. The proposed method first aligns unimodal audio-visual (AV) encoders through masked modeling training, where masked audio segments are reconstructed using corresponding video segments. This ensures semantic and rhythmic consistency between the AV encoders. Subsequently, a dynamic conditional flow is developed for audio generation, leveraging temporally varying video features to guide the generation of corresponding audio segments. This approach allows for coherent semantic and rhythmic representations during the AV alignment process, which are then used to temporally guide audio generation. Evaluations on standard benchmarks demonstrate that Foley-Flow significantly outperforms existing methods in generating semantically and rhythmically coherent audio for various video sequences.",
        "Tags": [
            "Video-to-Audio Generation",
            "Masked Audio-Visual Alignment",
            "Dynamic Conditional Flows",
            "Masked Modeling Training",
            "Velocity Flow Generation",
            "Temporally Varying Video Features"
        ]
    },
    {
        "Title": "Doppelg\u00e4ngers and Adversarial Vulnerability",
        "Authors": "George Kamberov",
        "Abstract": "Many machine learning (ML) classifiers are claimed to outperform humans, but they still make mistakes that humans do not. The most notorious examples of such mistakes are adversarial visual metamers. This paper aims to define and investigate the phenomenon of adversarial Doppelg\u00e4ngers (AD), which includes adversarial visual metamers, and to compare the performance and robustness of ML classifiers to human performance.We find that AD are inputs that are close to each other with respect to a perceptual metric defined in this paper, and show that AD are qualitatively different from the usual adversarial examples. The vast majority of classifiers are vulnerable to AD and robustness-accuracy trade-offs may not improve them. Some classification problems may not admit any AD robust classifiers because the underlying classes are ambiguous.  We provide criteria that can be used to determine whether a classification problem is well defined or not; describe of an AD robust classifiers\u2019 structure and attributes; introduce and explore the notions of conceptual entropy and regions of conceptual ambiguity for classifiers that are vulnerable to AD attacks, along with methods to bound the AD fooling rate of an attack. We define the notion of classifiers that exhibit hyper-sensitive behavior, that is, classifiers whose only mistakes are adversarial Doppelg\u00e4ngers. Improving the AD robustness of hyper-sensitive classifiers proving accuracy.  We identify conditions guaranteeing that all classifiers with sufficiently high accuracy are hyper-sensitive.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper investigates the phenomenon of adversarial Doppelg\u00e4ngers (AD), which are inputs close to each other under a perceptual metric and differ from typical adversarial examples. The study reveals that most classifiers are vulnerable to AD, and robustness-accuracy trade-offs may not mitigate this vulnerability. The authors propose criteria to determine if a classification problem is well-defined, describe the structure of AD-robust classifiers, and introduce concepts like conceptual entropy and regions of conceptual ambiguity. They also define hyper-sensitive classifiers, whose only errors are AD, and establish conditions under which high-accuracy classifiers are hyper-sensitive.",
        "Tags": [
            "Adversarial Vulnerability",
            "Robustness-Accuracy Trade-offs",
            "Conceptual Entropy",
            "Hyper-Sensitive Classifiers",
            "Perceptual Metric"
        ]
    },
    {
        "Title": "Zero-Shot Image Restoration Using Few-Step Guidance of Consistency Models (and Beyond)",
        "Authors": "Tomer Garber \u00b7 Tom Tirer",
        "Abstract": "In recent years, it has become popular to tackle image restoration tasks with a single pretrained diffusion model (DM) and data-fidelity guidance, instead of training a dedicated deep neural network per task. However, such ``zero-shot'' restoration schemes currently require many Neural Function Evaluations (NFEs) for performing well, which may be attributed to the many NFEs needed in the original generative functionality of the DMs. Recently, faster variants of DMs have been explored for image generation. These include Consistency Models (CMs), which can generate samples via a couple of NFEs. However, existing works that use guided CMs for restoration still require tens of NFEs or fine-tuning of the model per task that leads to performance drop if the assumptions during the fine-tuning are not accurate. In this paper, we propose a zero-shot restoration scheme that uses CMs and operates well with as little as 4 NFEs. It is based on a wise combination of several ingredients: better initialization, back-projection guidance, and above all a novel noise injection mechanism. We demonstrate the advantages of our approach for image super-resolution and inpainting. Interestingly, we show that the usefulness of our noise injection technique goes beyond CMs: it can also mitigate the performance degradation of existing guided DM methods when reducing their NFE count.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a zero-shot image restoration method using Consistency Models (CMs) that achieves high performance with as few as 4 Neural Function Evaluations (NFEs). Unlike existing approaches that require many NFEs or task-specific fine-tuning, the proposed method leverages better initialization, back-projection guidance, and a novel noise injection mechanism. The technique is demonstrated to be effective for tasks like image super-resolution and inpainting. Additionally, the noise injection mechanism is shown to enhance the performance of existing guided diffusion models when reducing their NFE count.",
        "Tags": [
            "Zero-Shot Learning",
            "Image Restoration",
            "Diffusion Models",
            "Consistency Models",
            "Noise Injection Mechanism",
            "Back-Projection Guidance",
            "Few-Step Guidance"
        ]
    },
    {
        "Title": "Floxels: Fast Unsupervised Voxel Based Scene Flow Estimation",
        "Authors": "David T. Hoffmann \u00b7 Syed Haseeb Raza \u00b7 Hanqiu Jiang \u00b7 Steffen Klingenhoefer \u00b7 Denis Tananaev \u00b7 Martin Meinke",
        "Abstract": "Scene flow estimation is a foundational task for many robotics applications, ranging from robust dynamic object detection to automatic labeling and sensor synchronization. Two distinct approaches to the problem have evolved: 1) Supervised and 2) optimization-based methods. While supervised methods are fast during inference and achieve high-quality results, they are limited by the need for large amounts of labeled training data and are susceptible to domain gaps. In contrast, unsupervised test-time optimization methods do not face the problem of domain gaps but usually suffer from substantial runtime or fail to converge to the right solution. Current optimization-based approaches often perform poorly on dynamic objects and mainly predict ego-motion. In this work, we mitigate several limitations of existing optimization-based methods. To this end, we 1) introduce a simple voxel grid-based model that exhibits advantageous characteristics compared to the standard MLP-based formulation and 2) introduce a new multi-frame loss formulation. We combine both contributions in our new method, termed Floxels. On our ego-motion compensated benchmark, based on nuScenes and Argoverse, Floxels achieves state of the art (SOTA) results and performs on par with a recently proposed SOTA supervised method. At the same time compute costs scale significantly more gracefully with point cloud size for Floxels, making it 3x faster than the previous fastest optimization based method for mid-sized point clouds.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Scene flow estimation is crucial for robotics applications, but existing methods face challenges such as the need for labeled data in supervised approaches and runtime inefficiencies in unsupervised optimization-based methods. This paper introduces Floxels, a novel voxel grid-based model combined with a multi-frame loss formulation, addressing these limitations. Floxels achieves state-of-the-art results on ego-motion compensated benchmarks (nuScenes and Argoverse) and matches the performance of a leading supervised method while being significantly faster for mid-sized point clouds.",
        "Tags": [
            "3D Point Cloud",
            "Scene Flow Estimation",
            "Voxel Grid",
            "Unsupervised Learning",
            "Ego-Motion Compensation"
        ]
    },
    {
        "Title": "HOIGen-1M: A Large-scale Dataset for Human-Object Interaction Video Generation",
        "Authors": "Kun Liu \u00b7 Qi Liu \u00b7 Xinchen Liu \u00b7 Jie Li \u00b7 Yongdong Zhang \u00b7 Jiebo Luo \u00b7 Xiaodong He \u00b7 Wu Liu",
        "Abstract": "Text-to-video (T2V) generation has made tremendous progress in generating complicated scenes based on texts. However, human-object interaction (HOI) often cannot be precisely generated by current T2V models due to the lack of large-scale videos with accurate captions for HOI. To address this issue, we introduce HOIGen-1M, the first large-scale dataset for HOI Generation, consisting of over one million high-quality videos collected from diverse sources. In particular, to guarantee the high quality of videos, we first design an efficient framework to automatically curate HOI videos using the powerful multimodal large language models (MLLMs), and then the videos are further cleaned by human annotators. Moreover, to obtain accurate textual captions for HOI videos, we design a novel video description method based on a Mixture-of-Multimodal-Experts (MoME) strategy that not only generates expressive captions but also eliminates the hallucination by individual MLLM. Furthermore, due to the lack of an evaluation framework for generated HOI videos, we propose two new metrics to assess the quality of generated videos in a coarse-to-fine manner. Extensive experiments reveal that current T2V models struggle to generate high-quality HOI videos and confirm that our HOIGen-1M dataset is instrumental for improving HOI video generation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces HOIGen-1M, a large-scale dataset designed to improve human-object interaction (HOI) video generation, addressing the limitations of current text-to-video (T2V) models. The dataset comprises over one million high-quality videos, curated using a framework that leverages multimodal large language models (MLLMs) and human annotators. A novel Mixture-of-Multimodal-Experts (MoME) strategy is employed to generate accurate and expressive captions for the videos, mitigating issues like hallucination. Additionally, the authors propose two new metrics to evaluate the quality of generated HOI videos in a coarse-to-fine manner. Experiments demonstrate that existing T2V models struggle with HOI video generation, highlighting the significance of the HOIGen-1M dataset in advancing this field.",
        "Tags": [
            "Video Generation",
            "Multimodal Large Language Models (MLLMs)",
            "Mixture-of-Multimodal-Experts (MoME)",
            "Human-Object Interaction (HOI)",
            "Video Quality Metrics"
        ]
    },
    {
        "Title": "Compass Control: Multi-Object Orientation Control for Text-to-Image Generation",
        "Authors": "Rishubh Parihar \u00b7 Vaibhav Agrawal \u00b7 Sachidanand VS \u00b7 R. Venkatesh Babu",
        "Abstract": "Existing approaches for controlling text-to-image diffusion models, while powerful, do not allow for explicit 3D object-centric control, such as precise control of object orientation. In this work, we address the problem of multi-object orientation control in text-to-image diffusion models. This enables the generation of diverse multi-object scenes with precise orientation control for each object. The key idea is to condition the diffusion model with a set of orientation-aware \\textbf{compass} tokens, one for each object, along with text tokens. A light-weight encoder network predicts these compass tokens taking object orientation as the input. The model is trained on a synthetic dataset of procedurally generated scenes, each containing one or two 3D-assets on a plain background. However, direct training this framework results in poor orientation control as well as leads to entanglement among objects. To mitigate this, we intervene in the generation process and constrain the cross-attention maps of each compass token to its corresponding object regions. The trained model is able to achieve precise orientation control for a) complex objects not seen during training, and b) multi-object scenes with more than two objects, indicating strong generalization capabilities. Further, when combined with personalization methods, our method precisely controls the orientation of the new object in diverse contexts. Our method achieves state-of-the-art orientation control and text alignment quantified with extensive evaluations and a user study.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a novel approach for multi-object orientation control in text-to-image diffusion models, enabling precise control over object orientation in generated scenes. The method employs orientation-aware compass tokens, predicted by a light-weight encoder network, to condition the diffusion model alongside text tokens. To address challenges such as poor orientation control and object entanglement, the authors intervene in the generation process by constraining cross-attention maps of compass tokens to their corresponding object regions. The model demonstrates strong generalization capabilities, achieving precise orientation control for complex objects and multi-object scenes beyond those seen during training. When integrated with personalization methods, it effectively controls the orientation of new objects in diverse contexts. The approach sets a new standard for orientation control and text alignment, as validated by extensive evaluations and a user study.",
        "Tags": [
            "Text-to-Image Generation",
            "Diffusion Models",
            "3D Object Control",
            "Cross-Attention Mechanism",
            "Synthetic Dataset Training",
            "Object-Centric Control"
        ]
    },
    {
        "Title": "MaIR: A Locality- and Continuity-Preserving Mamba for Image Restoration",
        "Authors": "Boyun Li \u00b7 Haiyu Zhao \u00b7 Wenxin Wang \u00b7 Peng Hu \u00b7 Yuanbiao Gou \u00b7 Xi Peng",
        "Abstract": "Recent advancements in Mamba have shown promising results in image restoration. These methods typically flatten 2D images into multiple distinct 1D sequences along rows and columns, process each sequence independently using selective scan operation, and recombine them to form the outputs. However, such a paradigm overlooks two vital aspects: i) the local relationships and spatial continuity inherent in natural images, and ii) the discrepancies among sequences unfolded through totally different ways. To overcome the drawbacks, we explore two problems in Mamba-based restoration methods: i) how to design a scanning strategy preserving both locality and continuity while facilitating restoration, and ii) how to aggregate the distinct sequences unfolded in totally different ways. To address these problems, we propose a novel Mamba-based Image Restoration model (MaIR), which consists of Nested S-shaped Scanning strategy (NSS) and Sequence Shuffle Attention block (SSA). Specifically, NSS preserves locality and continuity of the input images through the stripe-based scanning region and the S-shaped scanning path, respectively. SSA aggregates sequences through calculating attention weights within the corresponding channels of different sequences. Thanks to NSS and SSA, MaIR surpasses 40 baselines across 14 challenging datasets, achieving state-of-the-art performance on the tasks of image super-resolution, denoising, deblurring and dehazing. Our codes will be available after acceptance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces MaIR, a novel Mamba-based Image Restoration model designed to address limitations in existing methods that flatten 2D images into 1D sequences, neglecting local relationships and spatial continuity. MaIR incorporates a Nested S-shaped Scanning strategy (NSS) to preserve locality and continuity and a Sequence Shuffle Attention block (SSA) to aggregate sequences effectively. The model demonstrates superior performance across 14 datasets, excelling in tasks such as image super-resolution, denoising, deblurring, and dehazing, outperforming 40 baselines.",
        "Tags": [
            "Mamba",
            "Image Restoration",
            "Nested S-shaped Scanning",
            "Sequence Shuffle Attention",
            "Image Super-Resolution"
        ]
    },
    {
        "Title": "Let's Verify and Reinforce Image Generation Step by Step",
        "Authors": "Renrui Zhang \u00b7 Chengzhuo Tong \u00b7 Zhizheng Zhao \u00b7 Ziyu Guo \u00b7 Haoquan Zhang \u00b7 Manyuan Zhang \u00b7 Jiaming Liu \u00b7 Peng Gao \u00b7 Hongsheng Li",
        "Abstract": "Chain-of-Thought (CoT) reasoning has been extensively explored in large models to tackle complex understanding tasks. However, it still remains an open question whether such strategies can be applied to verifying and reinforcing image generation scenarios. In this paper, we provide the first comprehensive investigatation in the potential of CoT reasoning to enhance autoregressive image generation. We focus on three techniques: scaling test-time computation for verification, aligning model preferences with Direct Preference Optimization (DPO), and integrating these techniques for complementary effects. Our results demonstrate that these approaches can be effectively adapted and combined to significantly improve image generation performance. Furthermore, given the pivotal role of reward models in our findings, we propose the Potential Assessment Reward Model (PARM) specialized for autoregressive image generation. PARM adaptively assesses each generation step through a potential assessment mechanism, merging the strengths of existing reward models. Using our investigated reasoning strategies, we enhance a baseline model, Show-o, to achieve superior results, with a significant +24% improvement on the GenEval benchmark, surpassing Stable Diffusion 3 by +15%. We hope our study provides unique insights and paves a new path for integrating CoT reasoning with autoregressive image generation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper explores the application of Chain-of-Thought (CoT) reasoning to verify and reinforce autoregressive image generation. The study introduces three key techniques: scaling test-time computation for verification, aligning model preferences using Direct Preference Optimization (DPO), and integrating these methods for complementary effects. A novel Potential Assessment Reward Model (PARM) is proposed to adaptively assess each generation step, combining the strengths of existing reward models. The enhanced baseline model, Show-o, achieves a +24% improvement on the GenEval benchmark, outperforming Stable Diffusion 3 by +15%. The findings highlight the potential of CoT reasoning in improving image generation performance and offer new insights for future research.",
        "Tags": [
            "Image Generation",
            "Chain-of-Thought (CoT)",
            "Autoregressive Models",
            "Reward Models",
            "Direct Preference Optimization (DPO)"
        ]
    },
    {
        "Title": "World-consistent Video Diffusion with Explicit 3D Modeling",
        "Authors": "Qihang Zhang \u00b7 Shuangfei Zhai \u00b7 Miguel \u00c1ngel Bautista \u00b7 Kevin Miao \u00b7 Alexander Toshev \u00b7 Joshua Susskind \u00b7 Jiatao Gu",
        "Abstract": "Recent advancements in diffusion models have set new benchmarks in image and video generation, enabling realistic visual synthesis across single- and multi-frame contexts. However, these models still struggle with efficiently and explicitly generating 3D-consistent content. To address this, we propose World-consistent Video Diffusion (WVD), a novel framework that incorporates explicit 3D supervision using XYZ images, which encode global 3D coordinates for each image pixel. More specifically, we train a diffusion transformer to learn the joint distribution of RGB and XYZ frames. This approach supports multi-task adaptability via a flexible inpainting strategy. For example, WVD can estimate XYZ frames from ground-truth RGB or generate novel RGB frames using XYZ projections along a specified camera trajectory. In doing so, WVD unifies tasks like single-image-to-3D generation, multi-view stereo, and camera-controlled video generation.Our approach demonstrates competitive performance across multiple benchmarks, providing a scalable solution for 3D-consistent video and image generation with a single pretrained model.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces World-consistent Video Diffusion (WVD), a novel framework that enhances video and image generation by incorporating explicit 3D supervision through XYZ images, which encode global 3D coordinates for each pixel. WVD trains a diffusion transformer to learn the joint distribution of RGB and XYZ frames, enabling tasks such as single-image-to-3D generation, multi-view stereo, and camera-controlled video generation. This approach offers a scalable solution for generating 3D-consistent content using a single pretrained model.",
        "Tags": [
            "Diffusion Models",
            "3D Generation",
            "XYZ Images",
            "Diffusion Transformer",
            "Camera-Controlled Video Generation"
        ]
    },
    {
        "Title": "Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent Diffusion Models",
        "Authors": "Jinjin Zhang \u00b7 qiuyu Huang \u00b7 Junjie Liu \u00b7 Xiefan Guo \u00b7 Di Huang",
        "Abstract": "In this paper, we present Diffusion-4K, a novel framework for direct ultra-high-resolution image synthesis using text-to-image diffusion models.The core advancements include:(1) Aesthetic-4K Benchmark: addressing the absence of a publicly available 4K image synthesis dataset, we construct Aesthetic-4K, a comprehensive benchmark for ultra-high-resolution image generation. We curated a high-quality 4K dataset with carefully selected images and captions generated by GPT-4o.Additionally, we introduce GLCM Score and compression ratio metrics to evaluate fine details, combined with holistic measures such as FID, Aesthetics and CLIPScore for a comprehensive assessment of ultra-high-resolution images.(2) Wavelet-based Fine-tuning: we propose a wavelet-based fine-tuning approach for direct training with photorealistic 4K images, applicable to various latent diffusion models, demonstrating its effectiveness in synthesizing highly detailed 4K images.Consequently, Diffusion-4K achieves impressive performance in high-quality image synthesis and text prompt adherence, especially when powered by modern large-scale diffusion models (e.g., SD3-2B and Flux-12B).Extensive experimental results from our benchmark demonstrate the superiority of Diffusion-4K in ultra-high-resolution image synthesis.The code and dataset will be made publicly available soon.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Diffusion-4K, a framework for ultra-high-resolution image synthesis using text-to-image diffusion models. Key contributions include the creation of the Aesthetic-4K Benchmark, a dataset for 4K image synthesis with images and captions generated by GPT-4o, and the introduction of new metrics for evaluating fine details and overall image quality. Additionally, a wavelet-based fine-tuning approach is proposed for training with 4K images, enhancing the synthesis of detailed images. The framework demonstrates superior performance in generating high-quality, ultra-high-resolution images that adhere closely to text prompts.",
        "Tags": [
            "Diffusion Models",
            "Text-to-Image Generation",
            "Ultra-High-Resolution Image Synthesis",
            "Wavelet-based Fine-tuning",
            "Aesthetic-4K Benchmark"
        ]
    },
    {
        "Title": "GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D Object Affordance Grounding",
        "Authors": "Yawen Shao \u00b7 Wei Zhai \u00b7 Yuhang Yang \u00b7 Hongchen Luo \u00b7 Yang Cao \u00b7 Zheng-Jun Zha",
        "Abstract": "Open-Vocabulary 3D object affordance grounding aims to anticipate ``action possibilities'' regions on 3D objects with arbitrary instructions, which is crucial for robots to generically perceive real scenarios and respond to operational changes. Existing methods focus on combining images or languages that depict interactions with 3D geometries to introduce external interaction priors. However, they are still vulnerable to a limited semantic space by failing to leverage implied invariant geometries and potential interaction intentions. Normally, humans address complex tasks through multi-step reasoning and respond to diverse situations by leveraging associative and analogical thinking. In light of this, we propose GREAT (Geometry-Intention Collaborative Inference) for Open-Vocabulary 3D Object Affordance Grounding, a novel framework that mines the object invariant geometry attributes and performs analogically reason in potential interaction scenarios to form affordance knowledge, fully combining the knowledge with both geometries and visual contents to ground 3D object affordance. Besides, we introduce the Point Image Affordance Dataset v2 (PIADv2), the largest 3D object affordance dataset at present to support the task. Extensive experiments demonstrate the effectiveness and superiority of GREAT. The code and dataset will be released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces GREAT, a novel framework for Open-Vocabulary 3D Object Affordance Grounding, which aims to identify regions on 3D objects that suggest possible actions based on arbitrary instructions. This capability is essential for robots to adaptively perceive and interact with their environments. GREAT distinguishes itself by leveraging invariant geometry attributes and potential interaction intentions through analogical reasoning, integrating these with visual content to enhance affordance knowledge. The framework is supported by the newly introduced Point Image Affordance Dataset v2 (PIADv2), currently the largest dataset of its kind. The effectiveness of GREAT is validated through extensive experiments.",
        "Tags": [
            "3D Object Detection",
            "Open-Vocabulary Learning",
            "Affordance Grounding",
            "Analogical Reasoning",
            "Invariant Geometry"
        ]
    },
    {
        "Title": "LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity",
        "Authors": "Hongjie Wang \u00b7 Chih-Yao Ma \u00b7 Yen-Cheng Liu \u00b7 Ji Hou \u00b7 Tao Xu \u00b7 Jialiang Wang \u00b7 Felix Juefei-Xu \u00b7 Yaqiao Luo \u00b7 Peizhao Zhang \u00b7 Tingbo Hou \u00b7 Peter Vajda \u00b7 Niraj Jha \u00b7 Xiaoliang Dai",
        "Abstract": "Text-to-video generation enhances content creation but is highly computationally intensive: The computational cost of Diffusion Transformers (DiTs) scales quadratically in the number of pixels.  This makes minute-length video generation extremely expensive, limiting most existing models to generating videos of only 10-20 seconds length. We propose a Linear-complexity text-to-video Generation (LinGen) framework whose cost scales linearly in the number of pixels. For the first time, LinGen enables high-resolution minute-length video generation on a single GPU without compromising quality. It replaces the computationally-dominant and quadratic-complexity block, self-attention, with a linear-complexity block called MATE, which consists of an MA-branch and a TE-branch. The MA-branch targets short-to-long-range correlations, combining a bidirectional Mamba2 block with our token rearrangement method, Rotary Major Scan, and our review tokens developed for long video generation. The TE-branch is a novel TEmporal Swin Attention block that focuses on temporal correlations between adjacent tokens and medium-range tokens. The MATE block addresses the adjacency preservation issue of Mamba and improves the consistency of generated videos significantly. Experimental results show that  LinGen outperforms DiT (with a 75.6\\% win rate) in video quality with up to 15$\\times$ (11.5$\\times$) FLOPs (latency) reduction.  Furthermore, both automatic metrics and human evaluation demonstrate our LinGen-4B yields comparable video quality to state-of-the-art models (with a 50.5\\%, 52.1\\%, 49.1\\% win rate with respect to Gen-3, LumaLabs, and Kling, respectively). This paves the way to hour-length movie generation and real-time interactive video generation.  We provide 68s video generation results and more examples in the supplementary section.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces LinGen, a Linear-complexity text-to-video Generation framework designed to address the computational inefficiency of existing text-to-video models, particularly Diffusion Transformers (DiTs), which scale quadratically with the number of pixels. LinGen enables high-resolution, minute-length video generation on a single GPU by replacing the quadratic-complexity self-attention mechanism with a linear-complexity MATE block. This block consists of an MA-branch for short-to-long-range correlations and a TE-branch for temporal correlations, significantly improving video consistency and quality. LinGen outperforms DiT in video quality with substantial reductions in computational cost and latency, paving the way for longer video generation and real-time applications.",
        "Tags": [
            "Video Generation",
            "Diffusion Models",
            "Linear Computational Complexity",
            "MATE Block",
            "Temporal Swin Attention"
        ]
    },
    {
        "Title": "Efficient Motion-Aware Video MLLM",
        "Authors": "Zijia Zhao \u00b7 Yuqi Huo \u00b7 Tongtian Yue \u00b7 Longteng Guo \u00b7 Haoyu Lu \u00b7 Bingning Wang \u00b7 Weipeng Chen \u00b7 Jing Liu",
        "Abstract": "Most current video MLLMs rely on uniform frame sampling and image-level encoders, resulting in inefficient data processing and limited motion awareness. To address these challenges, we introduce EMA, an Efficient Motion-Aware video MLLM that utilizes compressed video structures as inputs. We propose a motion-aware GOP (Group of Pictures) encoder that fuses spatial and motion information within a GOP unit in the compressed video stream, generating compact, informative visual tokens. By integrating fewer but denser RGB frames with more but sparser motion vectors in this native slow-fast input architecture, our approach reduces redundancy and enhances motion representation. Additionally, we introduce MotionBench, a benchmark for evaluating motion understanding across four motion types: linear, curved, rotational, and contact-based. Experimental results show that EMA achieves state-of-the-art performance on both MotionBench and popular video question answering benchmarks, while reducing inference costs. Moreover, EMA demonstrates strong scalability, as evidenced by its competitive performance on long video understanding benchmarks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces EMA, an Efficient Motion-Aware video MLLM designed to address inefficiencies in current video MLLMs, which often rely on uniform frame sampling and image-level encoders. EMA utilizes compressed video structures and a motion-aware GOP encoder to fuse spatial and motion information, generating compact visual tokens. This approach reduces redundancy and enhances motion representation by integrating fewer but denser RGB frames with more but sparser motion vectors. The authors also introduce MotionBench, a benchmark for evaluating motion understanding across four motion types. EMA achieves state-of-the-art performance on MotionBench and video question answering benchmarks while reducing inference costs and demonstrating strong scalability on long video understanding tasks.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Video Understanding",
            "Motion-Aware Encoding",
            "Compressed Video Processing",
            "Slow-Fast Input Architecture"
        ]
    },
    {
        "Title": "SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model",
        "Authors": "Chunlin Yu \u00b7 Hanqing Wang \u00b7 Ye Shi \u00b7 Haoyang Luo \u00b7 Sibei Yang \u00b7 Jingyi Yu \u00b7 Jingya Wang",
        "Abstract": "3D affordance segmentation aims to link human instructions to touchable regions of 3D objects for embodied manipulations. Existing efforts typically adhere to single-object, single-affordance paradigms, where each affordance type or explicit instruction strictly corresponds to a specific affordance region and are unable to handle long-horizon tasks. Such a paradigm cannot actively reason about complex user intentions that often imply sequential affordances. In this paper, we introduce the Sequential 3D Affordance Reasoning task, which extends the traditional paradigm by reasoning from cumbersome user intentions and then decomposing them into a series of segmentation maps. Toward this, we construct the first instruction-based affordance segmentation benchmark that includes reasoning over both single and sequential affordances, comprising 180K instruction-point cloud pairs. Based on the benchmark, we propose our model, SeqAfford, to unlock the 3D multi-modal large language model with additional affordance segmentation abilities, which ensures reasoning with world knowledge and fine-grained affordance grounding in a cohesive framework. We further introduce a multi-granular language-point integration module to endow 3D dense prediction. Extensive experimental evaluations show that our model excels over well-established methods and exhibits open-world generalization with sequential reasoning abilities.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces the Sequential 3D Affordance Reasoning task, which extends traditional 3D affordance segmentation by enabling reasoning from complex user intentions and decomposing them into sequential segmentation maps. A new benchmark with 180K instruction-point cloud pairs is constructed to support this task. The proposed model, SeqAfford, integrates a 3D multimodal large language model with affordance segmentation capabilities, facilitating reasoning with world knowledge and fine-grained affordance grounding. A multi-granular language-point integration module is introduced to enhance 3D dense prediction. The model demonstrates superior performance over existing methods and exhibits open-world generalization with sequential reasoning abilities.",
        "Tags": [
            "3D Point Cloud",
            "Multimodal Large Language Models (MLLMs)",
            "Sequential Affordance Reasoning",
            "Instruction-Based Segmentation",
            "World Knowledge Integration"
        ]
    },
    {
        "Title": "Improving Visual and Downstream Performance of Low-Light Enhancer with Vision Foundation Models Collaboration",
        "Authors": "yuxuan Gu \u00b7 Huaian Chen \u00b7 Yi Jin \u00b7 Haoxuan Wang \u00b7 Pengyang Ling \u00b7 ZHIXIANG WEI \u00b7 Enhong Chen",
        "Abstract": "In this paper, we observe that the collaboration of various foundation models can perceive semantic and degraded information within images, thereby guiding the low-light enhancement process. Specifically, we propose a self-supervised low-light enhancement framework based on the multiple foundation models collaboration (dubbed FoCo), aimed at improving both the visual quality of enhanced images and the performance in high-level applications. At the feature level, FoCo leverages the rich features from various foundation models to enhance the model's semantic perception during training, thereby reducing the gap between enhanced results and high-quality images from a high-level perspective. At the task level, we exploit the robustness-gap between strong foundation models and weak models, applying high-level task guidance to the low-light enhancement training process. Through the collaboration of multiple foundation models, the proposed framework shows better enhancement performance and adapts better to high-level tasks. Extensive experiments across various enhancement and application benchmarks demonstrate the qualitative and quantitative superiority of the proposed method over numerous state-of-the-art techniques.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a self-supervised low-light enhancement framework, FoCo, which leverages the collaboration of multiple foundation models to improve both the visual quality of enhanced images and their performance in high-level applications. By utilizing rich features from various foundation models, FoCo enhances semantic perception during training, narrowing the gap between enhanced results and high-quality images. The framework also exploits the robustness-gap between strong and weak foundation models to guide the low-light enhancement process. The proposed method demonstrates superior enhancement performance and better adaptability to high-level tasks compared to state-of-the-art techniques.",
        "Tags": [
            "Low-Light Image Enhancement",
            "Vision-Language Models (VLMs)",
            "Self-Supervised Learning",
            "Semantic Perception",
            "Robustness-Gap Exploitation"
        ]
    },
    {
        "Title": "SocialMOIF\uff1aMulti-Order Intention Fusion for Pedestrain Trajectory Prediction",
        "Authors": "Kai Chen \u00b7 Xiaodong Zhao \u00b7 Yujie Huang \u00b7 GuoyuFang \u00b7 Xiao Song \u00b7 Ruiping Wang \u00b7 Ziyuan Wang",
        "Abstract": "The analysis and prediction of agent trajectories are crucial for decision-making processes in intelligent systems, with precise short-term trajectory forecasting being highly significant across a range of applications. Agents and their social interactions have been quantified and modeled by researchers from various perspectives; however, substantial limitations exist in the current work due to the inherent high uncertainty of agent intentions and the complex higher-order influences among neighboring groups. SocialMOIF is proposed to tackle these challenges, concentrating on the higher-order intention interactions among neighboring groups while reinforcing the primary role of first-order intention interactions between neighbors and the target agent. This method develops a multi-order intention fusion model to achieve a more comprehensive understanding of both direct and indirect intention information. Within SocialMOIF, a trajectory distribution approximator is designed to guide the trajectories toward values that align more closely with the actual data, thereby enhancing model interpretability. Furthermore, a global trajectory optimizer is introduced to enable more accurate and efficient parallel predictions. By incorporating a novel loss function that accounts for distance and direction during training, experimental results demonstrate that the model outperforms previous state-of-the-art baselines across multiple metrics in both dynamic and static datasets.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "SocialMOIF addresses the challenges of predicting pedestrian trajectories by focusing on higher-order intention interactions among neighboring groups while emphasizing first-order intention interactions between neighbors and the target agent. The proposed method introduces a multi-order intention fusion model to better capture direct and indirect intention information. It includes a trajectory distribution approximator to align predictions with actual data and a global trajectory optimizer for efficient parallel predictions. A novel loss function considering distance and direction during training improves performance, with experimental results showing superior performance over state-of-the-art baselines in dynamic and static datasets.",
        "Tags": [
            "Visual Tracking",
            "Autonomous Driving",
            "Trajectory Prediction",
            "Intention Fusion",
            "Social Interaction Modeling"
        ]
    },
    {
        "Title": "Graph-Embedded Structure-Aware Perceptual Hashing for Neural Network Protection and Piracy Detection",
        "Authors": "Ruiheng Liu \u00b7 Haozhe Chen \u00b7 Boyao Zhao \u00b7 Kejiang Chen \u00b7 Weiming Zhang",
        "Abstract": "The advancement of AI technology has significantly influenced production activities, increasing the focus on copyright protection for AI models. Model perceptual hashing offers an efficient solution for retrieving the pirated models. Existing methods, such as handcrafted feature-based and dual-branch network-based perceptual hashing, have proven effective in detecting pirated models. However, these approaches often struggle to differentiate non-pirated models, leading to frequent false positives in model authentication and protection. To address this challenge, this paper proposes a structurally-aware perceptual model hashing technique that achieved reduced false positives while maintaining high true positive rates. Specifically, we introduce a method for converting the diverse neural network structures into graph structures suitable for DNN processing, then utilize a graph neural network to learn their structural features representation. Our approach integrates perceptual parameter-based model hashing, achieving robust performance with higher detection accuracy and fewer false positives. Experimental results show that the proposed method has only 3\\% false positive rate when detecting the non-pirated model, and the detection accuracy of pirated model reaches more than 98\\%.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of copyright protection for AI models by proposing a structurally-aware perceptual hashing technique. The method converts neural network structures into graph structures suitable for deep neural network processing and employs a graph neural network to learn their structural features. This approach integrates perceptual parameter-based model hashing, achieving robust performance with higher detection accuracy and fewer false positives. The proposed method demonstrates a false positive rate of only 3% when detecting non-pirated models and a detection accuracy of over 98% for pirated models.",
        "Tags": [
            "Graph Neural Networks (GNNs)",
            "Model Pruning",
            "Perceptual Hashing",
            "Copyright Protection",
            "Neural Network Structures"
        ]
    },
    {
        "Title": "Any6DPose: Model-free 6D Pose Estimation of Novel Objects",
        "Authors": "Taeyeop Lee \u00b7 Korea Advanced Institute of Science and Technology \u00b7 NVIDIA \u00b7 Korea Advanced Institute of Science and Technology \u00b7 Korea Advanced Institute of Science &amp; Technology \u00b7 Korea Advanced Institute of Science and Technology",
        "Abstract": "We introduce Any6D, a model-free framework for 6D object pose estimation that requires only a single RGB-D anchor image to estimate both the 6D pose and size of unknown objects in novel scenes. Unlike existing methods that rely on textured 3D models or multiple viewpoints, Any6D leverages a joint object alignment process to enhance 2D-3D alignment and metric size estimation for improved pose accuracy. Our approach integrates a render-and-compare strategy to generate and refine pose hypotheses, enabling robust performance in scenarios with occlusions, non-overlapping views, diverse lighting conditions, and large cross-environment variations. We evaluate our method on four challenging datasets: REAL275, Toyota-Light, HO3D, and YCBINEOAT, demonstrating its effectiveness in significantly outperforming state-of-the-art methods for novel object pose estimation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Any6D is a model-free framework for 6D object pose estimation that uses a single RGB-D anchor image to estimate the 6D pose and size of unknown objects in novel scenes. It employs a joint object alignment process to improve 2D-3D alignment and metric size estimation, enhancing pose accuracy. The framework utilizes a render-and-compare strategy to generate and refine pose hypotheses, achieving robust performance under challenging conditions such as occlusions, non-overlapping views, diverse lighting, and cross-environment variations. Evaluations on four datasets (REAL275, Toyota-Light, HO3D, YCBINEOAT) demonstrate its superior performance over state-of-the-art methods.",
        "Tags": [
            "6D Object Pose Estimation",
            "3D Reconstruction",
            "Single RGB-D Anchor Image",
            "Render-and-Compare Strategy",
            "Cross-Environment Robustness"
        ]
    },
    {
        "Title": "Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent Spaces",
        "Authors": "Souhail Hadgi \u00b7 Luca Moschella \u00b7 Andrea Santilli \u00b7 Diego Gomez \u00b7 Qixing Huang \u00b7 Emanuele Rodol\u00e0 \u00b7 Simone Melzi \u00b7 Maks Ovsjanikov",
        "Abstract": "Recent works have shown that, when trained at scale, uni-modal 2D vision and text encoders converge to learned features that share remarkable structural properties, despite arising from different representations. However, the role of 3D encoders with respect to other modalities remains unexplored. Furthermore, existing 3D foundation models that leverage large datasets are typically trained with explicit alignment objectives with respect to frozen encoders from other representations. In this work, we investigate the possibility of a posteriori alignment of representations obtained from uni-modal 3D encoders compared to text-based feature spaces. We show that naive post-training feature alignment of uni-modal text and 3D encoders results in limited performance. We then focus on extracting subspaces of the corresponding feature spaces and discover that by projecting learned representations onto well-chosen lower-dimensional subspaces the quality of alignment becomes significantly higher, leading to improved accuracy on matching and retrieval tasks. Our analysis further sheds light on the nature of these shared subspaces, which roughly separate between semantic and geometric data representations. Overall, ours is the first work that helps to establish a baseline for post-training alignment of 3D uni-modal and text feature spaces, and helps to highlight both the shared and unique properties of 3D data compared to other representations.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper explores the alignment of 3D and text latent spaces, focusing on the post-training alignment of uni-modal 3D encoders with text-based feature spaces. The study reveals that naive alignment methods yield limited performance, but projecting learned representations onto lower-dimensional subspaces significantly enhances alignment quality, improving accuracy in matching and retrieval tasks. The analysis also identifies shared subspaces that separate semantic and geometric data representations, providing insights into the unique properties of 3D data compared to other modalities.",
        "Tags": [
            "3D Point Cloud",
            "Multimodal Learning",
            "Feature Space Alignment",
            "Subspace Projection",
            "Semantic-Geometric Separation"
        ]
    },
    {
        "Title": "RoboPEPP: Vision-Based Robot Pose and Joint Angle Estimation through Embedding Predictive Pre-Training",
        "Authors": "Raktim Gautam Goswami \u00b7 Prashanth Krishnamurthy \u00b7 Yann LeCun \u00b7 Farshad Khorrami",
        "Abstract": "Vision-based pose estimation of articulated robots with unknown joint angles has applications in collaborative robotics and human-robot interaction tasks. Current frameworks use neural network encoders to extract image features and downstream layers to predict joint angles and robot pose. While images of robots inherently contain rich information about the robot\u2019s physical structures, existing methods often fail to leverage it fully; therefore, limiting performance under occlusions and truncations. To address this, we introduce RoboPEPP, a method that fuses information about the robot\u2019s physical model into the encoder using a masking-based self-supervised embedding-predictive architecture. Specifically, we mask the robot\u2019s joints and pre-train an encoder-predictor model to infer the joints\u2019 embeddings from surrounding unmasked regions, enhancing the encoder\u2019s understanding of the robot\u2019s physical model. The pre-trained encoder-predictor pair, along with joint angle and keypoint prediction networks, is then fine-tuned for pose and joint angle estimation. Random masking of input during fine-tuning and keypoint filtering during evaluation further improves robustness. Our method, evaluated on several datasets, achieves the best results in robot pose and joint angle estimation while being the least sensitive to occlusions and requiring the lowest execution time.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "RoboPEPP introduces a vision-based method for estimating robot pose and joint angles, addressing limitations in current frameworks that fail to fully utilize the rich information in robot images. By incorporating the robot's physical model into the encoder through a masking-based self-supervised embedding-predictive architecture, RoboPEPP enhances the encoder's understanding of the robot's structure. The method involves pre-training an encoder-predictor model to infer joint embeddings from unmasked regions, followed by fine-tuning for pose and joint angle estimation. This approach improves robustness to occlusions and truncations, achieving superior performance in pose and joint angle estimation with reduced execution time.",
        "Tags": [
            "3D Human Pose Estimation",
            "Self-Supervised Learning",
            "Masking-Based Pre-Training",
            "Keypoint Filtering",
            "Embedding-Predictive Architecture"
        ]
    },
    {
        "Title": "FIFA: Fine-grained Inter-frame Attention for Driver's Video Gaze Estimation",
        "Authors": "Daosong Hu \u00b7 Mingyue Cui \u00b7 Kai Huang",
        "Abstract": "Gaze direction serves as a pivotal indicator for assessing the level of driver attention. While image-based gaze estimation has been extensively researched, there has been a recent shift towards capturing gaze direction from video sequences. This approach encounters notable challenges, including the comprehension of the dynamic pupil evolution across frames and the extraction of head pose information from a relatively static background. To surmount these challenges, we introduce a dual-stream deep learning framework that explicitly models the displacement changes of the pupil through a fine-grained inter-frame attention mechanism and generates weights to adjust gaze embeddings. This technique transforms the face into a set of distinct patches and employs cross-attention to ascertain the correlation between pixel displacements in various patches and adjacent frames, thereby tracking spatial dynamics within the sequence. Our method is validated using two publicly available driver gaze datasets, and the results indicate that it achieves state-of-the-art performance or is on par with the best outcomes while reducing the parameters.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Gaze direction is a critical measure of driver attention, and while image-based gaze estimation has been widely studied, video-based approaches face challenges such as understanding dynamic pupil changes and extracting head pose information from static backgrounds. To address these issues, the authors propose a dual-stream deep learning framework that uses a fine-grained inter-frame attention mechanism to model pupil displacement changes and adjust gaze embeddings. This method divides the face into patches and applies cross-attention to track spatial dynamics across frames. The approach is validated on two driver gaze datasets, demonstrating state-of-the-art performance with fewer parameters.",
        "Tags": [
            "Visual Tracking",
            "Autonomous Driving",
            "Dual-stream Framework",
            "Cross-attention Mechanism",
            "Pupil Displacement Tracking"
        ]
    },
    {
        "Title": "Prototype-Based Image Prompting for Weakly Supervised Histopathological Image Segmentation",
        "Authors": "Qingchen Tang \u00b7 Lei Fan \u00b7 Maurice Pagnucco \u00b7 Yang Song",
        "Abstract": "Weakly supervised image segmentation with image-level labels has drawn attention due to the high cost of pixel-level annotations. Traditional methods using Class Activation Maps (CAMs) often highlight only the most discriminative regions, leading to incomplete masks. Recent approaches that introduce textual information struggle with histopathological images due to inter-class homogeneity and intra-class heterogeneity. In this paper, we propose a prototype-based image prompting framework for histopathological image segmentation. It constructs an image bank from the training set using clustering, extracting multiple prototype features per class to capture intra-class heterogeneity. By designing a matching loss between input features and class-specific prototypes using contrastive learning, our method addresses inter-class homogeneity and guides the model to generate more accurate CAMs. Experiments on four datasets (LUAD-HistoSeg, BCSS-WSSS, GCSS, and BCSS) show that our method outperforms existing weakly supervised segmentation approaches, setting new benchmarks in histopathological image segmentation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a prototype-based image prompting framework for weakly supervised histopathological image segmentation, addressing the limitations of traditional methods like Class Activation Maps (CAMs) that often produce incomplete masks. By constructing an image bank from the training set through clustering and extracting multiple prototype features per class, the method captures intra-class heterogeneity. A matching loss, designed using contrastive learning, aligns input features with class-specific prototypes, mitigating inter-class homogeneity and improving CAM accuracy. The framework demonstrates superior performance on four datasets, setting new benchmarks in histopathological image segmentation.",
        "Tags": [
            "Medical Image Segmentation",
            "Weakly Supervised Learning",
            "Contrastive Learning",
            "Prototype-Based Learning",
            "Prototype-Based Learning",
            "Contrastive Learning",
            "Histopathological Image Analysis"
        ]
    },
    {
        "Title": "Task-Aware Clustering for Prompting Vision-Language Models",
        "Authors": "Fusheng Hao \u00b7 Fengxiang He \u00b7 Fuxiang Wu \u00b7 Tichao Wang \u00b7 Chengqun Song \u00b7 Jun Cheng",
        "Abstract": "Prompt learning has attracted widespread attention in adapting vision-language models to downstream tasks. Existing methods largely rely on optimization strategies to ensure the task-awareness of learnable prompts. Due to the scarcity of task-specific data, overfitting is prone to occur. The resulting prompts often do not generalize well or exhibit limited task-awareness. To address this issue, we propose a novel Task-Aware Clustering (TAC) framework for prompting vision-language models, which increases the task-awareness of learnable prompts by introducing task-aware pre-context. The key ingredients are as follows: (a) generating task-aware pre-context based on task-aware clustering that can preserve the backbone structure of a downstream task with only a few clustering centers, (b) enhancing the task-awareness of learnable prompts by enabling them to interact with task-aware pre-context via the well-pretrained encoders, and (c) preventing the visual task-aware pre-context from interfering the interaction between patch embeddings by masked attention mechanism. Extensive experiments are conducted on benchmark datasets, covering the base-to-novel, domain generalization, and cross-dataset transfer settings. Ablation studies validate the effectiveness of key ingredients. Comparative results show the superiority of our TAC over competitive counterparts. The code will be made publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a Task-Aware Clustering (TAC) framework to enhance the task-awareness of learnable prompts in vision-language models. The proposed method addresses the issue of overfitting due to limited task-specific data by generating task-aware pre-context through clustering, which preserves the backbone structure of downstream tasks with minimal clustering centers. The framework improves task-awareness by enabling interaction between learnable prompts and task-aware pre-context using pre-trained encoders and employs a masked attention mechanism to prevent interference between patch embeddings and visual task-aware pre-context. The effectiveness of the TAC framework is validated through extensive experiments and ablation studies, demonstrating its superiority over existing methods.",
        "Tags": [
            "Vision-Language Models",
            "Prompt Learning",
            "Task-Aware Clustering",
            "Masked Attention Mechanism",
            "Pre-trained Encoders"
        ]
    },
    {
        "Title": "Sample- and Parameter-Efficient Auto-Regressive Image Models",
        "Authors": "Elad Amrani \u00b7 Leonid Karlinsky \u00b7 Alex M. Bronstein",
        "Abstract": "We introduce $\\textbf{XTRA}$, a vision model pre-trained with a novel auto-regressive objective that significantly enhances both sample and parameter efficiency compared to previous auto-regressive image models. Unlike contrastive or masked image modeling methods, which have not been demonstrated as having consistent scaling behavior on unbalanced internet data,auto-regressive vision models exhibit scalable and promising performance as model and dataset size increase. In contrast to standard auto-regressive models, XTRA employs a Block Causal Mask, where each Block represents $k \\times k$ tokens rather than relying on a standard causal mask. By reconstructing pixel values block by block, XTRA captures higher-level structural patterns over larger image regions. Predicting on blocks allows the model to learn relationships across broader areas of pixels, enabling more abstract and semantically meaningful representations than traditional next-token prediction.This simple modification yields two key results. First, $\\textbf{XTRA is sample-efficient}$. Despite being trained on 152$\\times$ fewer samples (13.1M vs. 2B), XTRA ViT-H/14 surpasses the top-1 average accuracy of the previous state-of-the-art auto-regressive model across 15 diverse image recognition benchmarks. Second, $\\textbf{XTRA is parameter-efficient}$. Compared to auto-regressive models trained on ImageNet-1k, XTRA ViT-B/16 outperforms in linear and attentive probing tasks, using 7\u201316$\\times$ fewer parameters (85M vs. 1.36B/0.63B).",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces $\textbf{XTRA}$, a vision model pre-trained with a novel auto-regressive objective that enhances both sample and parameter efficiency. Unlike contrastive or masked image modeling methods, XTRA employs a Block Causal Mask, where each block represents $k \times k$ tokens, enabling the model to capture higher-level structural patterns over larger image regions. This approach allows XTRA to learn more abstract and semantically meaningful representations than traditional next-token prediction. Key results include XTRA's sample efficiency, achieving higher accuracy with significantly fewer samples, and its parameter efficiency, outperforming other models with fewer parameters.",
        "Tags": [
            "Self-Supervised Learning",
            "Vision Transformer (ViT)",
            "Auto-Regressive Models",
            "Block Causal Mask",
            "Parameter Efficiency"
        ]
    },
    {
        "Title": "Image-Referenced Sketch Colorization Based on Animation Creation Workflow",
        "Authors": "Dingkun Yan \u00b7 Xinrui Wang \u00b7 Zhuoru Li \u00b7 Suguru Saito \u00b7 Yusuke Iwasawa \u00b7 Yutaka Matsuo \u00b7 Jiaxian Guo",
        "Abstract": "Sketch colorization plays an important role in animation and digital illustration production tasks. However, existing methods still meet problems in that text-guided methods fail to provide accurate color and style reference, hint-guided methods still involve manual operation, and image-guided methods are prone to cause artifacts. To address these limitations, we propose a diffusion-based framework inspired by real-world animation production workflows. Our approach leverages the sketch as the spatial reference and an RGB image as the color guidance, and separately extracts foreground and background information from the reference image with spatial masks. Particularly, we introduce a split cross-attention mechanism with LoRA (Low-Rank Adaptation) modules for foreground and background separately trained to control the corresponding embeddings for keys and values in cross-attention. This design allows the diffusion model to integrate information from foreground and background independently, preventing interference and eliminating the need to fine-tune model parameters. During inference, we design switchable inference modes for diverse use scenarios by changing modules activated in the framework. Extensive qualitative and quantitative experiments, along with user studies, demonstrate our advantages over existing methods in generating high-qualigy artifact-free results with geometric mismatched references. Ablation studies further confirm the effectiveness of each component. Codes and trained models will be made publicly available upon paper acceptance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a diffusion-based framework for sketch colorization, inspired by real-world animation production workflows. The method uses a sketch as spatial reference and an RGB image for color guidance, employing spatial masks to separately extract foreground and background information. A novel split cross-attention mechanism with LoRA modules is introduced, allowing independent control of foreground and background embeddings in cross-attention, which eliminates the need for model fine-tuning and prevents interference. The framework supports switchable inference modes for diverse scenarios, demonstrating superior performance in generating high-quality, artifact-free results with geometrically mismatched references. The effectiveness of each component is validated through ablation studies.",
        "Tags": [
            "Image Editing",
            "Diffusion Models",
            "Animation Creation",
            "LoRA Modules",
            "Split Cross-Attention Mechanism",
            "Switchable Inference Modes"
        ]
    },
    {
        "Title": "Automated Proof of Polynomial Inequalities via Reinforcement Learning",
        "Authors": "Banglong Liu \u00b7 Niuniu Qi \u00b7 Xia Zeng \u00b7 Lydia Dehbi \u00b7 Zhengfeng Yang",
        "Abstract": "Polynomial inequality proving is fundamental to many mathematical disciplines and finds wide applications in diverse fields. Current traditional algebraic methods are based on searching for a polynomial positive definite representation over a set of basis. However, these methods are limited by truncation degree.To address this issue, this paper proposes an approach based on reinforcement learning to find a Krivine-basis representation for proving polynomial inequalities.Specifically, we formulate the inequality proving problem as a linear programming (LP) problem and encode it as a basis selection problem using reinforcement learning (RL), achieving a non-negative Krivine basis. Moreover, a fast multivariate polynomial multiplication method based on Fast Fourier Transform (FFT) is deployed to enhance the efficiency of action space search. Furthermore, we have implemented a tool called APPIRL (Automated Proof of Polynomial Inequalities via Reinforcement Learning).Experimental evaluation on benchmark problems demonstrates the feasibility and effectiveness of our approach. In addition, APPIRL can successfully be applied to attack the maximum stable set problem.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a novel approach to proving polynomial inequalities using reinforcement learning (RL), addressing limitations of traditional algebraic methods that rely on polynomial positive definite representations. The method formulates the inequality proving problem as a linear programming (LP) problem and encodes it as a basis selection problem using RL, achieving a non-negative Krivine basis. To improve efficiency, a fast multivariate polynomial multiplication method based on Fast Fourier Transform (FFT) is employed. The authors also developed a tool called APPIRL (Automated Proof of Polynomial Inequalities via Reinforcement Learning), which demonstrates feasibility and effectiveness on benchmark problems and successfully tackles the maximum stable set problem.",
        "Tags": [
            "Reinforcement Learning",
            "Polynomial Inequalities",
            "Krivine-basis Representation",
            "Fast Fourier Transform (FFT)",
            "Linear Programming (LP)"
        ]
    },
    {
        "Title": "ReNeg: Learning Negative Embedding with Reward Guidance",
        "Authors": "Xiaomin Li \u00b7 yixuan liu \u00b7 Takashi Isobe \u00b7 Xu Jia \u00b7 Qinpeng Cui \u00b7 Dong Zhou \u00b7 Dong Li \u00b7 You He \u00b7 Huchuan Lu \u00b7 Zhongdao Wang \u00b7 Emad Barsoum",
        "Abstract": "In text-to-image (T2I) generation applications, negative embeddings have proven to be a simple yet effective approach for enhancing generation quality. Typically, these negative embeddings are derived from user-defined negative prompts, which, while being functional, are not necessarily optimal.In this paper, we introduce ReNeg, an end-to-end method designed to learn improved Negative embeddings guided by a Reward model. We employ a reward feedback learning framework and integrate classifier-free guidance (CFG) into the training process, which was previously utilized only during inference, thus enabling the effective learning of negative embeddings.We also propose two strategies for learning both global and per-sample negative embeddings. Extensive experiments show that the learned negative embedding significantly outperforms null-text and handcrafted counterparts, achieving substantial improvements in human preference alignment. Additionally, the negative embedding learned within the same text embedding space exhibits strong generalization capabilities.For example, using the same CLIP text encoder, the negative embedding learned on SD1.5 can be seamlessly transferred to text-to-image or even text-to-video models such as ControlNet, ZeroScope, and VideCrafter2, resulting in consistent performance improvements across the board. Code and learned negative embeddings will be released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces ReNeg, an end-to-end method for learning improved negative embeddings in text-to-image (T2I) generation, guided by a reward model. ReNeg integrates classifier-free guidance (CFG) into the training process, enabling effective learning of negative embeddings. The method proposes two strategies for learning global and per-sample negative embeddings, which significantly outperform null-text and handcrafted counterparts in human preference alignment. The learned embeddings also demonstrate strong generalization capabilities, allowing seamless transfer across different models like ControlNet, ZeroScope, and VideCrafter2, leading to consistent performance improvements.",
        "Tags": [
            "Text-to-Image Generation",
            "Diffusion Models",
            "Reward Feedback Learning",
            "Classifier-Free Guidance",
            "Generalization in Embeddings"
        ]
    },
    {
        "Title": "PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies",
        "Authors": "Mojtaba Nafez \u00b7 Amirhossein Koochakian \u00b7 Arad Maleki \u00b7 Jafar Habibi \u00b7 Mohammad Rohban",
        "Abstract": "Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields that demand high reliability, such as medical imaging and industrial monitoring. However, current AD and AL approaches are often susceptible to adversarial attacks due to limitations in training data, which typically include only normal, unlabeled samples. This study introduces PatchGuard, an adversarially robust AD and AL method that incorporates pseudo anomalies with localization masks within a Vision Transformer (ViT)-based architecture to address these vulnerabilities.We begin by examining the essential properties of pseudo anomalies, and follow it by providing theoretical insights into the attention mechanisms required to enhance the adversarial robustness of AD and AL systems. We then present our approach, which leverages Foreground-Aware Pseudo-Anomalies to overcome the deficiencies of previous anomaly-aware methods. Our method incorporates these crafted pseudo-anomaly samples into a ViT-based framework, with adversarial training guided by a novel loss function designed to improve model robustness, as supported by our theoretical analysis.Experimental results on well-established industrial and medical datasets demonstrate that PatchGuard significantly outperforms previous methods in adversarial settings, achieving performance gains of $53.2\\%$ in AD and $68.5\\%$ in AL, while also maintaining competitive accuracy in non-adversarial settings.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "PatchGuard introduces an adversarially robust method for Anomaly Detection (AD) and Anomaly Localization (AL) using Vision Transformers (ViTs) and pseudo anomalies. The approach leverages Foreground-Aware Pseudo-Anomalies within a ViT-based framework, guided by a novel loss function to enhance robustness against adversarial attacks. Theoretical insights into attention mechanisms are provided, and the method demonstrates significant performance improvements in adversarial settings, outperforming previous methods by 53.2% in AD and 68.5% in AL, while maintaining competitive accuracy in non-adversarial scenarios.",
        "Tags": [
            "Anomaly Detection",
            "Vision Transformer (ViT)",
            "Adversarial Robustness",
            "Pseudo Anomalies",
            "Foreground-Aware Training"
        ]
    },
    {
        "Title": "Motions as Queries: One-Stage Multi-Person Holistic Human Motion Capture",
        "Authors": "Kenkun Liu \u00b7 Yurong Fu \u00b7 Weihao Yuan \u00b7 Jing Lin \u00b7 Peihao Li \u00b7 Xiaodong Gu \u00b7 Lingteng Qiu \u00b7 Haoqian Wang \u00b7 Zilong Dong \u00b7 Xiaoguang Han",
        "Abstract": "Existing methods for capturing multi-person holistic human motions from a monocular video usually involve integrating the detector, the tracker, and the human pose & shape estimator into a cascaded system. Differently, we develop a one-stage multi-person holistic human motion capture system, which 1) employs only one network, enabling significant benefits from the end-to-end training on a large-scale dataset; 2) enables performance improving of the tracking module during training, avoiding being limited by a pre-trained tracker; 3) captures the motions of all individuals within a single shot, rather than tracking and estimating each person sequentially. In this system, each query within a temporal cross-attention module is responsible for the long motion of a specific individual, implicitly aggregating individual-specific information throughout the entire video. To further boost the proposed system from end-to-end training, we also construct a synthetic human video dataset, with multi-person and whole-body annotations. Extensive experiments across different datasets demonstrate both the efficacy and the efficiency of both the proposed method and the dataset. The code of our method will be made publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a one-stage multi-person holistic human motion capture system that simplifies the traditional cascaded approach by employing a single network. This system benefits from end-to-end training on a large-scale dataset, improves tracking module performance during training, and captures all individuals' motions in a single shot. A temporal cross-attention module assigns each query to a specific individual's long motion, aggregating individual-specific information across the video. The system's effectiveness is enhanced by a newly constructed synthetic human video dataset with multi-person and whole-body annotations. The method demonstrates efficacy and efficiency across various datasets.",
        "Tags": [
            "3D Human Pose Estimation",
            "3D Human Mesh Estimation",
            "Temporal Cross-Attention",
            "Synthetic Dataset",
            "End-to-End Training"
        ]
    },
    {
        "Title": "BIP3D: Bridging 2D Images and 3D Perception for Embodied Intelligence",
        "Authors": "Xuewu Lin \u00b7 Tianwei Lin \u00b7 Alan Huang \u00b7 HONGYU XIE \u00b7 Zhizhong Su",
        "Abstract": "In embodied intelligence systems, a key component is 3D perception algorithm, which enables agents to understand their surrounding environments.Previous algorithms primarily rely on point cloud, which, despite offering precise geometric information, still constrain perception performance due to inherent sparsity, noise, and data scarcity.In this work, we introduce a novel image-centric 3D perception model, BIP3D, which leverages expressive image features with explicit 3D position encoding to overcome the limitations of point-centric methods.Specifically, we leverage pre-trained 2D vision foundation models to enhance semantic understanding, and introduce a spatial enhancer module to improve spatial understanding. Together, these modules enable BIP3D to achieve multi-view, multi-modal feature fusion and end-to-end 3D perception.In our experiments, BIP3D outperforms current state-of-the-art results on the EmbodiedScan benchmark, achieving improvements of 5.69\\% in the 3D detection task and 15.25\\% in the 3D visual grounding task.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces BIP3D, a novel image-centric 3D perception model designed to enhance embodied intelligence systems by overcoming the limitations of point-centric methods. BIP3D leverages pre-trained 2D vision foundation models for improved semantic understanding and incorporates a spatial enhancer module to enhance spatial comprehension. The model achieves multi-view, multi-modal feature fusion and end-to-end 3D perception, demonstrating superior performance on the EmbodiedScan benchmark with significant improvements in 3D detection and visual grounding tasks.",
        "Tags": [
            "Embodied AI",
            "3D Object Detection",
            "Multi-modal Feature Fusion",
            "Spatial Enhancer Module",
            "Image-Centric 3D Perception"
        ]
    },
    {
        "Title": "KeyFace: Expressive Audio-Driven Facial Animation for Long Sequences via KeyFrame Interpolation",
        "Authors": "Antoni Bigata Casademunt \u00b7 Micha\u0142 Stypu\u0142kowski \u00b7 Rodrigo Mira \u00b7 Stella Bounareli \u00b7 Konstantinos Vougioukas \u00b7 Zoe Landgraf \u00b7 Nikita Drobyshev \u00b7 Maciej Zieba \u00b7 Stavros Petridis \u00b7 Maja Pantic",
        "Abstract": "Current audio-driven facial animation methods achieve impressive results for short videos but suffer from error accumulation and identity drift when extended to longer durations. Existing methods attempt to mitigate this through external spatial control, increasing long-term consistency but compromising the naturalness of motion. We propose \\textbf{KeyFace}, a novel two-stage diffusion-based framework, to address these issues. In the first stage, keyframes are generated at a low frame rate, conditioned on audio input and an identity frame, to capture essential facial expressions and movements over extended periods of time. In the second stage, an interpolation model fills in the gaps between keyframes, ensuring smooth transitions and temporal coherence. To further enhance realism, we incorporate continuous emotion representations and handle a wide range of non-speech vocalizations (NSVs), such as laughter and sighs. We also introduce two new evaluation metrics for assessing lip synchronization and NSV generation. Experimental results show that KeyFace outperforms state-of-the-art methods in generating natural, coherent facial animations over extended durations, successfully encompassing NSVs and continuous emotions.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "KeyFace introduces a two-stage diffusion-based framework for generating expressive audio-driven facial animations over long sequences. The first stage generates keyframes at a low frame rate, capturing essential facial expressions and movements, while the second stage interpolates between keyframes to ensure smooth transitions and temporal coherence. The framework incorporates continuous emotion representations and handles non-speech vocalizations (NSVs) like laughter and sighs. KeyFace outperforms existing methods in generating natural, coherent facial animations over extended durations, with new metrics introduced for evaluating lip synchronization and NSV generation.",
        "Tags": [
            "Avatars",
            "Diffusion Models",
            "Facial Animation",
            "Non-Speech Vocalizations",
            "Temporal Coherence"
        ]
    },
    {
        "Title": "GenDeg: Diffusion-Based Degradation Synthesis for Generalizable All-in-One Image Restoration",
        "Authors": "Sudarshan Rajagopalan \u00b7 Nithin Gopalakrishnan Nair \u00b7 Jay Paranjape \u00b7 Vishal M. Patel",
        "Abstract": "Deep learning\u2013based models for All-In-One image Restoration (AIOR) have achieved significant advancements in recent years. However, their practical applicability is limited by poor generalization to samples outside the training distribution. This limitation arises primarily from insufficient diversity in degradation variations and scenes within existing datasets, resulting in inadequate representations of real-world scenarios. Additionally, capturing large-scale real-world paired data for degradations such as haze, low-light, and raindrops is often cumbersome and sometimes infeasible. In this paper, we leverage the generative capabilities of latent diffusion models to synthesize high-quality degraded images from their clean counterparts. Specifically, we introduce GenDeg, a degradation and intensity-aware conditional diffusion model, capable of producing diverse degradation patterns on clean images. Using GenDeg, we synthesize over $550$k samples across six degradation types: haze, rain, snow, motion blur, low-light, and raindrops. These generated samples are integrated with existing datasets to form the GenDS dataset, comprising over $750$k samples. Our experiments reveal that image restoration models trained on GenDS dataset exhibit significant improvements in out-of-distribution performance as compared to when trained solely on existing datasets. Furthermore, we provide comprehensive analyses on implications of diffusion model-based synthetic degradations for AIOR. The code will be made publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Deep learning-based models for All-In-One Image Restoration (AIOR) often struggle with generalization due to limited diversity in degradation variations and scenes in existing datasets. This paper introduces GenDeg, a degradation and intensity-aware conditional diffusion model, which synthesizes high-quality degraded images from clean counterparts. GenDeg produces diverse degradation patterns across six types: haze, rain, snow, motion blur, low-light, and raindrops. Over 550k synthetic samples are integrated with existing datasets to form the GenDS dataset, totaling over 750k samples. Experiments show that models trained on GenDS exhibit improved out-of-distribution performance compared to those trained on existing datasets. The paper also analyzes the implications of diffusion model-based synthetic degradations for AIOR.",
        "Tags": [
            "Diffusion Models",
            "Image Restoration",
            "Degradation Synthesis",
            "Conditional Diffusion Models",
            "Out-of-Distribution Generalization"
        ]
    },
    {
        "Title": "BiLoRA: Almost-Orthogonal Parameter Spaces for Continual Learning",
        "Authors": "Hao Zhu \u00b7 Yifei Zhang \u00b7 Junhao Dong \u00b7 Piotr Koniusz",
        "Abstract": "Continual learning requires models to learn tasks sequentially while maintaining a delicate balance between stability (retaining knowledge of previous tasks) and plasticity (adapting to new tasks). A key challenge is preventing interference between tasks - where learning new tasks degrades performance on previously learned ones. Recent approaches have leveraged parameter-efficient fine-tuning (PEFT) methods, which adapt pre-trained models by injecting a small number of learnable parameters. However, existing PEFT-based continual learning methods like InfLoRA face fundamental limitations: they rely on complex optimization procedures to learn orthogonal task-specific spaces, and finding such spaces becomes increasingly difficult as tasks accumulate. We propose a novel bilinear reformulation that fundamentally reimagines task separation through fixed orthogonal bases. Our key insight is that by expanding the parameter space quadratically through two fixed bases, we can achieve \"almost orthogonal\" task subspaces probabilistically, eliminating the need for explicit interference elimination procedures. We provide theoretical guarantees that this approach reduces the probability of task interference from $\\bigO((k/d)^2)$ to $\\bigO((k/d^2)^2)$, ensuring reliable task separation without complex optimization. Through extensive experiments on ImageNet-R, CIFAR100, and DomainNet, we validate our theoretical bounds and demonstrate state-of-the-art performance with reduced parameter count.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Continual learning aims to enable models to learn tasks sequentially while balancing stability and plasticity. A major challenge is preventing task interference, where learning new tasks degrades performance on previously learned ones. Existing methods like InfLoRA rely on complex optimization to achieve orthogonal task-specific spaces, which becomes increasingly difficult as tasks accumulate. This paper introduces BiLoRA, a novel bilinear reformulation that uses fixed orthogonal bases to create 'almost orthogonal' task subspaces probabilistically. This approach eliminates the need for explicit interference elimination and provides theoretical guarantees of reduced task interference. The method is validated through experiments on ImageNet-R, CIFAR100, and DomainNet, demonstrating state-of-the-art performance with fewer parameters.",
        "Tags": [
            "Continual Learning",
            "Parameter-efficient Fine-tuning (PEFT)",
            "Bilinear Reformulation",
            "Task Interference Reduction",
            "Fixed Orthogonal Bases"
        ]
    },
    {
        "Title": "HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian Splatting",
        "Authors": "Jingyu Lin \u00b7 Jiaqi Gu \u00b7 Lubin Fan \u00b7 Bojian Wu \u00b7 Yujing Lou \u00b7 Renjie Chen \u00b7 Ligang Liu \u00b7 Jieping Ye",
        "Abstract": "Generating high-quality novel view renderings of 3D Gaussian Splatting (3DGS) in scenes featuring transient objects is challenging. We propose a novel hybrid representation, termed as HybridGS, using 2D Gaussians for transient objects per image and maintaining traditional 3D Gaussians for the whole static scenes. Note that, the 3DGS itself is better suited for modeling static scenes that assume multi-view consistency, but the transient objects appear occasionally and do not adhere to the assumption, thus we model them as planar objects from a single view, represented with 2D Gaussians. Our novel representation decomposes the scene from the perspective of fundamental viewpoint consistency, making it more reasonable. Additionally, we present a novel multi-view regulated supervision method for 3DGS that leverages information from co-visible regions, further enhancing the distinctions between the transients and statics. Then, we propose a straightforward yet effective multi-stage training strategy to ensure robust training and high-quality view synthesis across various settings. Experiments on benchmark datasets show our state-of-the-art performance of novel view synthesis in both indoor and outdoor scenes, even in the presence of distracting elements.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces HybridGS, a novel hybrid representation for 3D Gaussian Splatting (3DGS) that decouples transient and static objects in scenes. Transient objects, which appear occasionally and do not adhere to multi-view consistency, are modeled using 2D Gaussians per image, while static scenes are represented with traditional 3D Gaussians. The approach leverages a multi-view regulated supervision method to enhance the distinction between transient and static elements and employs a multi-stage training strategy for robust training and high-quality view synthesis. The method achieves state-of-the-art performance in novel view synthesis across indoor and outdoor scenes, even with distracting elements.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Novel View Synthesis",
            "2D Gaussians",
            "Multi-View Consistency",
            "Scene Decomposition"
        ]
    },
    {
        "Title": "Balanced Direction from Multifarious Choices: Arithmetic Meta-Learning for Domain Generalization",
        "Authors": "Xiran Wang \u00b7 Jian Zhang \u00b7 Lei Qi \u00b7 Yinghuan Shi",
        "Abstract": "Domain generalization is proposed to address distribution shift, arising from statistical disparities between training source and unseen target domains. The widely used first-order meta-learning algorithms demonstrate strong performance for domain generalization by leveraging the gradient matching theory, which aims to establish balanced parameters across source domains to reduce overfitting to any particular domain. However, our analysis reveals that there are actually numerous directions to achieve gradient matching, with current methods representing just one possible path. These methods actually overlook another critical factor that the balanced parameters should be close to the centroid of optimal parameters of each source domain. To address this, we propose a simple yet effective arithmetic meta-learning with arithmetic-weighted gradients. This approach, while adhering to the principles of gradient matching, promotes a more precise balance by estimating the centroid between domain-specific optimal parameters. Experimental results conducted on ten datasets validate the effectiveness of our strategy. Our code is available in the supplementary material.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Domain generalization aims to address distribution shifts between training and unseen target domains. While first-order meta-learning algorithms perform well by leveraging gradient matching to balance parameters across source domains, they often overlook the importance of aligning balanced parameters with the centroid of optimal parameters for each domain. This paper introduces an arithmetic meta-learning approach with arithmetic-weighted gradients, which enhances gradient matching by estimating the centroid of domain-specific optimal parameters. The proposed method demonstrates effectiveness across ten datasets.",
        "Tags": [
            "Domain Generalization",
            "Meta-Learning",
            "Gradient Matching",
            "Arithmetic-Weighted Gradients",
            "Centroid Estimation"
        ]
    },
    {
        "Title": "SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step Diffusion",
        "Authors": "Trong-Tung Nguyen \u00b7 Quang Nguyen \u00b7 Khoi Nguyen \u00b7 Anh Tran \u00b7 Cuong Pham",
        "Abstract": "Recent advances in text-guided image editing enable users to perform image edits through simple text inputs, leveraging the extensive priors of multi-step diffusion-based text-to-image models. However, these methods often fall short of the speed demands required for real-world and on-device applications due to the costly multi-step inversion and sampling process involved. In response to this, we introduce SwiftEdit, a simple yet highly efficient editing tool that achieve instant text-guided image editing (in 0.23s). The advancement of SwiftEdit lies in its two novel contributions: a one-step inversion framework that enables one-step image reconstruction via inversion and a mask-guided editing technique with our proposed attention rescaling mechanism to perform localized image editing. Extensive experiments are provided to demonstrate the effectiveness and efficiency of SwiftEdit. In particular, SwiftEdit enables instant text-guided image editing, which is extremely faster than previous multi-step methods (at least 50 times faster) while maintain a competitive performance in editing results.",
        "Link": "https://swift-edit.github.io/",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "SwiftEdit introduces a highly efficient text-guided image editing tool that achieves instant editing in 0.23 seconds, addressing the speed limitations of existing multi-step diffusion-based methods. The tool features a one-step inversion framework for image reconstruction and a mask-guided editing technique with an attention rescaling mechanism for localized edits. SwiftEdit significantly outperforms previous methods in speed, being at least 50 times faster, while maintaining competitive editing quality.",
        "Tags": [
            "Image Editing",
            "Diffusion Models",
            "One-Step Inversion",
            "Attention Rescaling",
            "Real-Time Editing"
        ]
    },
    {
        "Title": "SnowMaster: Comprehensive Real-world Image Desnowing via MLLM with Multi-Model Feedback Optimization",
        "Authors": "Jianyu LAI \u00b7 Sixiang Chen \u00b7 Yunlong Lin \u00b7 Tian Ye \u00b7 Yun Liu \u00b7 Song Fei \u00b7 Zhaohu Xing \u00b7 Hongtao Wu \u00b7 Weiming Wang \u00b7 Lei Zhu",
        "Abstract": "Snowfall poses a significant challenge to visual data processing, requiring specialized desnowing algorithms. However, current models often struggle with generalization due to their reliance on synthetic datasets, creating a domain gap. Evaluating real snowfall images is difficult due to the lack of ground truth. To tackle these issues, we introduce a large-scale, high-quality dataset of 10,000 annotated real snow scenes, develop a dataset with 36k preference pairs based on human expert rankings, enhance multimodal large language models' perception of snowfall images using direct preference optimization (DPO), and refine desnowing models through a mean teacher semi-supervised framework with high-quality pseudo-label screening. This Framework substantially improves the generalization and performance of desnowing models on real snowfall images.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "SnowMaster addresses the challenge of image desnowing by introducing a large-scale dataset of 10,000 annotated real snow scenes and 36k preference pairs based on human expert rankings. The approach enhances multimodal large language models' perception of snowfall images using direct preference optimization (DPO) and refines desnowing models through a mean teacher semi-supervised framework with high-quality pseudo-label screening. This framework significantly improves the generalization and performance of desnowing models on real snowfall images.",
        "Tags": [
            "Low-Level Vision",
            "Multimodal Large Language Models (MLLMs)",
            "Direct Preference Optimization (DPO)",
            "Mean Teacher Semi-Supervised Learning",
            "Real-world Image Desnowing"
        ]
    },
    {
        "Title": "CrossSDF: 3D Reconstruction of Thin Structures From Cross-Sections",
        "Authors": "Thomas Walker \u00b7 Salvatore Esposito \u00b7 Daniel Rebain \u00b7 Amir Vaxman \u00b7 Arno Onken \u00b7 Changjian Li \u00b7 Oisin Mac Aodha",
        "Abstract": "Reconstructing complex structures from planar cross-sections is a challenging problem, with wide-reaching applications in medical imaging, manufacturing, and topography. Out-of-the-box point cloud reconstruction methods can often fail due to the data sparsity between slicing planes, while current bespoke methods struggle to reconstruct thin geometric structures and preserve topological continuity. This is important for medical applications where thin vessel structures are present in CT and MRI scans. This paper introduces CrossSDF, a novel approach for extracting a 3D signed distance field from 2D signed distances generated from planar contours. Our approach makes the training of neural SDFs contour-aware by using losses designed for the case where geometry is known within 2D slices. Our results demonstrate a significant improvement over existing methods, effectively reconstructing  thin structures and producing accurate 3D models without the interpolation artifacts or over-smoothing of prior approaches.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "CrossSDF introduces a novel approach for 3D reconstruction of thin structures from 2D cross-sections, addressing challenges in medical imaging, manufacturing, and topography. Unlike existing methods that struggle with data sparsity and topological continuity, CrossSDF leverages 2D signed distances from planar contours to train neural signed distance fields (SDFs) with contour-aware losses. This method significantly improves the reconstruction of thin geometric structures, avoiding interpolation artifacts and over-smoothing, and producing accurate 3D models.",
        "Tags": [
            "3D Reconstruction",
            "Medical Image Analysis",
            "Signed Distance Fields",
            "Thin Structure Reconstruction",
            "Topological Continuity"
        ]
    },
    {
        "Title": "EdgeMovingNet: Edge-preserving Point Cloud Reconstruction via Joint Geometry Features",
        "Authors": "Xinran Yang \u00b7 Donghao Ji \u00b7 Yuanqi Li \u00b7 Junyuan Xie \u00b7 Jie Guo \u00b7 Yanwen Guo",
        "Abstract": "Point cloud reconstruction is a critical process in 3D representation and reverse engineering. When it comes to CAD models, edges are significant features that play a crucial role in characterizing the geometry of 3D shapes. However, few points are exactly sampled on edges during acquisition, resulting in apparent artifacts for the reconstruction task. Upsampling point cloud is a direct technical route, but there is a main challenge that the upsampled points may not align with the model edge accurately. To overcome this, we develop an integrated framework to estimate edges by joint regression of three geometry features\u2014point-to-edge direction, point-to-edge distance and point normal. Benefiting these features, we implement a novel refinement process to move and produce more points which lie accurately on edges of the model, allowing for high-quality edge-preserving reconstruction. Experiments and comparisons against previous methods demonstrate our method's effectiveness and superiority.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Point cloud reconstruction is essential for 3D representation and reverse engineering, particularly for CAD models where edges are critical geometric features. However, edge sampling during acquisition is often sparse, leading to reconstruction artifacts. This paper introduces EdgeMovingNet, an integrated framework that addresses this issue by jointly regressing three geometry features: point-to-edge direction, point-to-edge distance, and point normal. These features enable a novel refinement process that accurately moves and generates points on model edges, achieving high-quality edge-preserving reconstruction. The method demonstrates superior performance compared to existing approaches.",
        "Tags": [
            "3D Point Cloud",
            "3D Reconstruction",
            "Edge-preserving Reconstruction",
            "Geometry Feature Regression",
            "Point Cloud Upsampling"
        ]
    },
    {
        "Title": "CASP: Compression of Large Multimodal Models Based on Attention Sparsity",
        "Authors": "Mohsen Gholami \u00b7 Mohammad Akbari \u00b7 Kevin Cannons \u00b7 Yong Zhang",
        "Abstract": "In this work, we propose an extreme compression technique for Large Multimodal Models (LMMs). While previous studies have explored quantization as an efficient post-training compression method for Large Language Models (LLMs), low-bit compression for multimodal models remains under-explored. The redundant nature of inputs in multimodal models results in a highly sparse attention matrix. We theoretically and experimentally demonstrate that the attention matrix's sparsity bounds the compression error of the Query and Key weight matrices. Based on this, we introduce CASP, a model compression technique for LMMs. Our approach performs a data-aware low-rank decomposition on the Query and Key weight matrix, followed by quantization across all layers based on an optimal bit allocation process. CASP is compatible with any quantization technique and enhances state-of-the-art 2-bit quantization methods (AQLM and QuIP#) by an average of 21% on image- and video-language benchmarks. The code is provided in the supplementary materials.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces CASP, an extreme compression technique for Large Multimodal Models (LMMs) that leverages the sparsity of attention matrices. Unlike previous methods focused on quantization for Large Language Models (LLMs), CASP addresses the under-explored area of low-bit compression for multimodal models. The technique involves a data-aware low-rank decomposition of the Query and Key weight matrices, followed by quantization across all layers using an optimal bit allocation process. CASP is compatible with any quantization method and improves state-of-the-art 2-bit quantization techniques (AQLM and QuIP#) by an average of 21% on image- and video-language benchmarks.",
        "Tags": [
            "Large Language Models (LLMs)",
            "Multimodal Large Language Models (MLLMs)",
            "Attention Sparsity",
            "Low-Rank Decomposition",
            "Optimal Bit Allocation"
        ]
    },
    {
        "Title": "TSP-Mamba: The Travelling Salesman Problem Meets Mamba for Image Super-resolution and Beyond",
        "Authors": "Kun Zhou \u00b7 Xinyu Lin \u00b7 Jiangbo Lu",
        "Abstract": "Recently, Mamba-based frameworks have achieved substantial advancements across diverse computer vision and NLP tasks, particularly in their capacity for reasoning over long-range information with linear complexity. However, the fixed 2D-to-1D scanning pattern overlooks the local structures of an image, limiting its effectiveness in aggregating 2D spatial information. While stacking additional Mamba layers can partially address this issue, it increases parameter intensity and constrains real-time application. In this work, we reconsider the local optimal scanning path in Mamba, enhancing the rigid and uniform 1D scan through the local shortest path theory, thus creating a structure-aware Mamba suited for lightweight single-image super-resolution. Specifically, we draw inspiration from the Traveling Salesman Problem (TSP) to establish a local optimal scanning path for improved structural 2D information utilization. Here, local patch aggregation occurs in a content-adaptive manner with minimal propagation cost. TSP-Mamba demonstrates substantial improvements over existing Mamba-based and Transformer-based architectures. For example, TSP-Mamba surpasses MambaIR by up to 0.7dB in lightweight SISR, with comparable parameters and very slightly extra computational demands (1-2 GFlops for 720P images).",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces TSP-Mamba, a novel approach that integrates the Traveling Salesman Problem (TSP) with Mamba to enhance single-image super-resolution (SISR). Traditional Mamba frameworks, while effective in long-range information reasoning, struggle with local image structures due to their fixed 2D-to-1D scanning patterns. TSP-Mamba addresses this by employing a local optimal scanning path inspired by TSP, which improves 2D spatial information aggregation in a content-adaptive manner with minimal computational overhead. The proposed method outperforms existing Mamba-based and Transformer-based architectures in lightweight SISR tasks, achieving up to 0.7dB improvement over MambaIR with comparable parameters and only a slight increase in computational demand.",
        "Tags": [
            "Mamba",
            "Super-Resolution",
            "Traveling Salesman Problem",
            "Content-Adaptive Scanning",
            "Lightweight Architecture"
        ]
    },
    {
        "Title": "Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis",
        "Authors": "Bingda Tang \u00b7 Sayak Paul \u00b7 Boyang Zheng \u00b7 Saining Xie",
        "Abstract": "Recent advances in text-to-image synthesis have delivered impressive results, yet existing approaches still struggle to align with complex prompts. While decoder-only Large Language Models (LLMs) excel at handling such intricate texts, their integration with text-to-image generative models remains unsatisfactory. The rise of Diffusion Transformers (DiTs) presents a promising path forward via the deep fusion with LLMs. In this work, we explore this deep fusion for text-to-image synthesis by replacing the text stream Transformer in the MM-DiT model with an LLM, establishing shared self-attention between the LLM and DiT models. This design better aligns with the training objective and inference nature of both autoregressive and diffusion models, brigding the gap between the two paradigms. We empirically examine the design spaces of this approach and demonstrate its effectiveness through extensive experiments. We hope the positive evidence will kindle interest in this approach and inspire reflection on the pursuit of utilizing LLMs for text-to-image synthesis.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper explores the deep fusion of Large Language Models (LLMs) and Diffusion Transformers (DiTs) for text-to-image synthesis. Existing methods often struggle with complex prompts, but LLMs excel in handling intricate texts. The authors propose replacing the text stream Transformer in the MM-DiT model with an LLM, enabling shared self-attention between the LLM and DiT. This approach aligns better with the training and inference objectives of both autoregressive and diffusion models, bridging the gap between the two paradigms. The paper examines the design spaces of this approach and demonstrates its effectiveness, aiming to inspire further research into utilizing LLMs for text-to-image synthesis.",
        "Tags": [
            "Large Language Models (LLMs)",
            "Diffusion Models",
            "Text-to-Image Generation",
            "Self-Attention Mechanism",
            "Autoregressive Models",
            "Model Fusion"
        ]
    },
    {
        "Title": "Learning to Normalize on the SPD Manifold under Bures-Wasserstein geometry",
        "Authors": "Rui Wang \u00b7 Shaocheng Jin \u00b7 Ziheng Chen \u00b7 Xiaoqing Luo \u00b7 Xiaojun Wu",
        "Abstract": "Covariance matrices have proven highly effective across many scientific fields. Since these matrices lie within the Symmetric Positive Definite (SPD) manifold\u2014a Riemannian space with intrinsic non-Euclidean geometry, the primary challenge in representation learning is to respect this underlying geometric structure. Drawing inspiration from the success of Euclidean deep learning, researchers have developed neural networks on the SPD manifolds for more faithful covariance embedding learning. A notable advancement in this area is the implementation of Riemannian batch normalization (RBN), which has been shown to improve the performance of SPD network models. Nonetheless, the Riemannian metric beneath the existing RBN might fail to effectively deal with the ill-conditioned SPD matrices (ICSM), undermining the effectiveness of RBN. In contrast, the Bures-Wasserstein metric (BWM) demonstrates superior performance for ill-conditioning. In addition, the recently introduced Generalized BWM (GBWM) parameterizes the vanilla BWM via an SPD matrix, allowing for a more nuanced representation of vibrant geometries of the SPD manifold. Therefore, we propose a novel RBN algorithm based on the GBW geometry, incorporating a learnable metric parameter. Moreover, the deformation of GBWM by matrix power is also introduced to further enhance the representational capacity of GBWM-based RBN. Experimental results on different datasets validate the effectiveness of our proposed method.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Covariance matrices, residing in the Symmetric Positive Definite (SPD) manifold, are crucial in various scientific domains. The challenge in representation learning is to respect the non-Euclidean geometry of the SPD manifold. While Riemannian batch normalization (RBN) has improved SPD network models, it struggles with ill-conditioned SPD matrices (ICSM). The Bures-Wasserstein metric (BWM) outperforms in handling ICSM, and its generalized version (GBWM) allows for more nuanced SPD manifold representations. This paper introduces a novel RBN algorithm based on GBWM, featuring a learnable metric parameter and matrix power deformation to enhance representation. The method's effectiveness is validated across different datasets.",
        "Tags": [
            "SPD Manifold",
            "Riemannian Batch Normalization",
            "Bures-Wasserstein Metric",
            "Generalized Bures-Wasserstein Metric",
            "Learnable Metric Parameter"
        ]
    },
    {
        "Title": "Audio-Visual Instance Segmentation",
        "Authors": "Ruohao Guo \u00b7 Xianghua Ying \u00b7 Yaru Chen \u00b7 Dantong Niu \u00b7 Guangyao Li \u00b7 Liao Qu \u00b7 Yanyu Qi \u00b7 Jinxing Zhou \u00b7 Bowei Xing \u00b7 Wenzhen Yue \u00b7 Ji Shi \u00b7 Qixun Wang \u00b7 Peiliang Zhang \u00b7 Buwen Liang",
        "Abstract": "In this paper, we propose a new multi-modal task, termed audio-visual instance segmentation (AVIS), which aims to simultaneously identify, segment and track individual sounding object instances in audible videos. To facilitate this research, we introduce a high-quality benchmark named AVISeg, containing over 90K instance masks from 26 semantic categories in 926 long videos. Additionally, we propose a strong baseline model for this task. Our model first localizes sound source within each frame, and condenses object-specific contexts into concise tokens. Then it builds long-range audio-visual dependencies between these tokens using window-based attention, and tracks sounding objects among the entire video sequences. Extensive experiments reveal that our method performs best on AVISeg, surpassing the existing methods from related tasks. We further conduct the evaluation on several multi-modal large models; however, they exhibits subpar performance on instance-level sound source localization and temporal perception. We expect that AVIS will inspire the community towards a more comprehensive multi-modal understanding.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces a novel multi-modal task called audio-visual instance segmentation (AVIS), which focuses on identifying, segmenting, and tracking individual sounding objects in audible videos. A high-quality benchmark, AVISeg, is presented, containing over 90K instance masks from 26 semantic categories across 926 long videos. A baseline model is proposed, which localizes sound sources, condenses object-specific contexts into tokens, and builds long-range audio-visual dependencies using window-based attention. The model demonstrates superior performance on AVISeg compared to existing methods, though multi-modal large models show limitations in instance-level sound source localization and temporal perception. The work aims to advance multi-modal understanding in the research community.",
        "Tags": [
            "Instance Segmentation",
            "Multimodal Learning",
            "Audio-Visual Dependencies",
            "Sound Source Localization",
            "Temporal Perception"
        ]
    },
    {
        "Title": "SDGOCC: Semantic and Depth-Guided Bird\u2019s-Eye View Transformation for 3D Multimodal Occupancy Prediction",
        "Authors": "ZaiPeng Duan \u00b7 Xuzhong Hu \u00b7 Pei An \u00b7 Junfeng Ding \u00b7 Jie Zhan \u00b7 Chenxu Dang \u00b7 Yunbiao Xu \u00b7 Jie Ma",
        "Abstract": "Multimodal 3D occupancy prediction has garnered significant attention for its potential in autonomous driving. However, most existing approaches are single-modality: camera-based methods lack depth information, while LiDAR-based methods struggle with occlusions. Current lightweight methods primarily rely on the Lift-Splat-Shoot (LSS) pipeline, which suffers from inaccurate depth estimation and fails to fully exploit the geometric and semantic information of 3D LiDAR points. Therefore, we propose a novel multimodal occupancy prediction network called SDG-OCC, which incorporates a joint semantic and depth-guided view transformation coupled with a fusion-to-occupancy-driven active distillation. The enhanced view transformation constructs accurate depth distributions by integrating pixel semantics and co-point depth through diffusion and bilinear discretization. The fusion-to-occupancy-driven active distillation extracts rich semantic information from multimodal data and selectively transfers knowledge to image features based on LiDAR-identified regions. Finally, for optimal performance, we introduce SDG-Fusion, which uses fusion alone, and SDG-KL, which integrates both fusion and distillation for faster inference. Our method achieves state-of-the-art (SOTA) performance with real-time processing on the Occ3D-nuScenes dataset and shows comparable performance on the more challenging SurroundOcc-nuScenes dataset, demonstrating its effectiveness and robustness. The code will be released at https://github.com/",
        "Link": "https://github.com/",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces SDG-OCC, a novel multimodal occupancy prediction network designed for autonomous driving. It addresses the limitations of single-modality approaches by integrating semantic and depth-guided view transformation with fusion-to-occupancy-driven active distillation. This method enhances depth estimation accuracy and leverages geometric and semantic information from 3D LiDAR points. Two variants, SDG-Fusion and SDG-KL, are proposed for optimal performance, achieving state-of-the-art results on the Occ3D-nuScenes dataset and demonstrating robustness on the SurroundOcc-nuScenes dataset.",
        "Tags": [
            "Autonomous Driving",
            "3D Semantic Segmentation",
            "Multimodal Learning",
            "Depth Estimation",
            "Knowledge Distillation",
            "Real-Time Processing"
        ]
    },
    {
        "Title": "A Lightweight UDF Learning Framework for 3D Reconstruction Based on Local Shape Functions",
        "Authors": "Jiangbei Hu \u00b7 Yanggeng Li \u00b7 Fei Hou \u00b7 Junhui Hou \u00b7 Zhebin Zhang \u00b7 Shengfa Wang \u00b7 Na Lei \u00b7 Ying He",
        "Abstract": "Unsigned distance fields (UDFs) provide a versatile framework for representing a diverse array of 3D shapes, encompassing both watertight and non-watertight geometries. Traditional UDF learning methods typically require extensive training on large 3D shape datasets, which is costly and necessitates re-training for new datasets. This paper presents a novel neural framework, LoSF-UDF, for reconstructing surfaces from 3D point clouds by leveraging local shape functions to learn UDFs. We observe that 3D shapes manifest simple patterns in localized regions, prompting us to develop a training dataset of point cloud patches characterized by mathematical functions that represent a continuum from smooth surfaces to sharp edges and corners. Our approach learns features within a specific radius around each query point and utilizes an attention mechanism to focus on the crucial features for UDF estimation. Despite being highly lightweight, with only 653 KB of trainable parameters and a modest-sized training dataset with 0.5 GB storage, our method enables efficient and robust surface reconstruction from point clouds without requiring for shape-specific training. Furthermore, our method exhibits enhanced resilience to noise and outliers in point clouds compared to existing methods. We conduct comprehensive experiments and comparisons across various datasets, including synthetic and real-scanned point clouds, to validate our method's efficacy. Notably, our lightweight framework offers rapid and reliable initialization for other unsupervised iterative approaches, improving both the efficiency and accuracy of their reconstructions.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces LoSF-UDF, a lightweight neural framework for 3D surface reconstruction from point clouds using local shape functions to learn unsigned distance fields (UDFs). The method leverages localized patterns in 3D shapes, creating a training dataset of point cloud patches represented by mathematical functions that capture smooth surfaces, sharp edges, and corners. The framework employs an attention mechanism to focus on key features for UDF estimation within a specific radius around query points. Despite its minimal trainable parameters (653 KB) and small training dataset (0.5 GB), LoSF-UDF achieves efficient and robust surface reconstruction without shape-specific training. It demonstrates superior resilience to noise and outliers compared to existing methods and serves as a rapid, reliable initialization for unsupervised iterative approaches, enhancing their efficiency and accuracy.",
        "Tags": [
            "3D Reconstruction",
            "Unsigned Distance Fields (UDFs)",
            "Local Shape Functions",
            "Attention Mechanism",
            "Lightweight Neural Framework",
            "Noise Resilience"
        ]
    },
    {
        "Title": "LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping",
        "Authors": "Pascal Chang \u00b7 Sergio Sancho \u00b7 Jingwei Tang \u00b7 Markus Gross \u00b7 Vinicius C. Azevedo",
        "Abstract": "Anamorphosis refers to a category of images that are intentionally distorted, making them unrecognizable when viewed directly. Their true form only reveals itself when seen from a specific viewpoint, which can be through some catadioptric device like a mirror or a lens. While the construction of these mathematical devices can be traced back to as early as the 17th century, they are only interpretable when viewed from a specific vantage point and tend to lose meaning when seen normally. In this paper, we revisit these famous optical illusions with a generative twist. With the help of latent rectified flow models, we propose a method to create anamorphic images that still retain a valid interpretation when viewed directly. To this end, we introduce Laplacian Pyramid Warping, a frequency-aware image warping technique key to generating high-quality visuals. Our work extends Visual Anagrams [Geng et al. 2024] to latent space models and to a wider range of spatial transforms, enabling the creation of novel generative perceptual illusions.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces a novel method for creating anamorphic images that remain interpretable when viewed directly, using latent rectified flow models and Laplacian Pyramid Warping. This technique extends previous work by enabling a wider range of spatial transforms and generating high-quality visuals, thus advancing the creation of generative perceptual illusions.",
        "Tags": [
            "Image Editing",
            "Image Generation",
            "Laplacian Pyramid Warping",
            "Latent Rectified Flow Models",
            "Generative Perceptual Illusions"
        ]
    },
    {
        "Title": "CARL: A Framework for  Equivariant Image Registration",
        "Authors": "Hastings Greer \u00b7 Lin Tian \u00b7 Fran\u00e7ois-Xavier Vialard \u00b7 Roland Kwitt \u00b7 Ra\u00fal San Jos\u00e9 Est\u00e9par \u00b7 Marc Niethammer",
        "Abstract": "Image registration estimates spatial correspondences between image pairs. These estimates are typically obtained via numerical optimization or regression by a deep network. A desirable property is that a correspondence estimate (e.g., the true oracle correspondence) for an image pair is maintained under deformations of the input images. Formally, the estimator should be equivariant to a desired class of image transformations. In this work, we present careful analyses of equivariance properties in the context of multi-step deep registration networks. Based on these analyses we 1) introduce the notions of $[U,U]$ equivariance (network equivariance to the same deformations of the input images) and $[W,U]$ equivariance (where input images can undergo different deformations); we 2) show that in a suitable multi-step registration setup it is sufficient for overall $[W,U]$ equivariance if the first step has $[W,U]$ equivariance and all others have $[U,U]$ equivariance; we 3) show that common displacement-predicting networks only exhibit $[U,U]$ equivariance to translations instead of the more powerful $[W,U]$ equivariance; and we 4) show how to achieve multi-step $[W,U]$ equivariance via a coordinate-attention mechanism combined with displacement-predicting networks. Our approach obtains excellent practical performance for 3D abdomen, lung, and brain medical image registration. We match or outperform state-of-the-art (SOTA) registration approaches on all the datasets with a particularly strong performance for the challening abdomen registration.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces CARL, a framework for equivariant image registration, which ensures that correspondence estimates between image pairs are maintained under deformations of the input images. The authors analyze equivariance properties in multi-step deep registration networks, introducing two types of equivariance: $[U,U]$ and $[W,U]$. They demonstrate that achieving overall $[W,U]$ equivariance requires only the first step to have $[W,U]$ equivariance, with subsequent steps needing $[U,U]$ equivariance. The paper also shows that common displacement-predicting networks only exhibit $[U,U]$ equivariance to translations and proposes a method to achieve multi-step $[W,U]$ equivariance using a coordinate-attention mechanism. The approach achieves state-of-the-art performance in 3D medical image registration for abdomen, lung, and brain datasets.",
        "Tags": [
            "3D Registration",
            "Medical Image Analysis",
            "Equivariance",
            "Coordinate-Attention Mechanism",
            "Displacement-Predicting Networks"
        ]
    },
    {
        "Title": "Mitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation",
        "Authors": "Jiaming Zhou \u00b7 Teli Ma \u00b7 Kun-Yu Lin \u00b7 Zifan Wang \u00b7 Ronghe Qiu \u00b7 Junwei Liang",
        "Abstract": "Learning generalizable visual representations across different embodied environments is essential for effective robotic manipulation in real-world scenarios. However, the limited scale and diversity of robot demonstration data pose a significant challenge. Recent research has explored leveraging large-scale human activity data for pre-training, but the substantial morphological differences between humans and robots introduce a significant human-robot domain discrepancy, hindering the generalization of these models to downstream manipulation tasks.To overcome this, we propose a novel adaptation paradigm that leverages readily available paired human-robot video data to bridge the domain gap. Our method employs a human-robot contrastive alignment loss to align the semantics of human and robot videos, adapting pre-trained models to the robot domain in a parameter-efficient manner.Experiments on 20 simulated tasks across two different benchmarks and five real-world tasks demonstrate significant improvements.These results span both single-task and language-conditioned multi-task settings, evaluated using two different pre-trained models.Compared to existing pre-trained models, our adaptation method improves the average success rate by over 7% across multiple tasks on both simulated benchmarks and real-world evaluations.We will release the code and models.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of generalizing visual representations for robotic manipulation by mitigating the human-robot domain discrepancy. The authors propose a novel adaptation paradigm that uses paired human-robot video data to align the semantics of human and robot videos through a contrastive alignment loss. This method adapts pre-trained models to the robot domain efficiently, improving performance on both simulated and real-world tasks. The approach demonstrates significant improvements, increasing the average success rate by over 7% across multiple tasks compared to existing pre-trained models.",
        "Tags": [
            "Embodied AI",
            "Self-Supervised Learning",
            "Contrastive Learning",
            "Domain Adaptation",
            "Robotic Manipulation"
        ]
    },
    {
        "Title": "Argus: A Compact and Versatile Foundation Model for Vision",
        "Authors": "Weiming Zhuang \u00b7 Chen Chen \u00b7 Zhizhong Li \u00b7 Sina Sajadmanesh \u00b7 Jingtao Li \u00b7 Jiabo Huang \u00b7 Vikash Sehwag \u00b7 Vivek Sharma \u00b7 Hirotaka Shinozaki \u00b7 Felan Carlo Garcia \u00b7 Yihao Zhan \u00b7 Naohiro Adachi \u00b7 Ryoji Eki \u00b7 Michael Spranger \u00b7 Peter Stone \u00b7 Lingjuan Lyu",
        "Abstract": "While existing vision and multi-modal foundation models can handle multiple computer vision tasks, they often suffer from significant limitations, including huge demand for data and computational resources during training and inconsistent performance across vision tasks at deployment time. To address these challenges, we introduce Argus (The name comes from Argus Panoptes--a hundred-eyed giant with ''all-seeing'' capability in Greek mythology), a compact and versatile vision foundation model designed to support a wide range of vision tasks through a unified multitask architecture. Argus employs a two-stage training strategy: (i) multitask pretraining over core vision tasks with a shared backbone that includes a lightweight adapter to inject task-specific inductive biases, and (ii) scalable and efficient adaptation to new tasks by fine-tuning only the task-specific decoders. Extensive evaluations demonstrate that Argus, despite its relatively compact and training-efficient design of merely 100M backbone parameters (only 13.6\\% of which are trained using 1.6M images), competes with and even surpasses much larger models. Compared to state-of-the-art foundation models, Argus not only covers a broader set of vision tasks but also matches or outperforms the models with similar sizes on 12 tasks. We expect that Argus will accelerate the real-world adoption of vision foundation models in resource-constrained scenarios.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Argus is a compact and versatile vision foundation model designed to address the limitations of existing models, such as high data and computational demands, and inconsistent performance across tasks. It employs a two-stage training strategy: multitask pretraining with a shared backbone and lightweight task-specific adapters, followed by efficient adaptation to new tasks by fine-tuning only task-specific decoders. Despite its compact design with 100M backbone parameters, Argus competes with or surpasses larger models on 12 vision tasks, demonstrating its potential for real-world adoption in resource-constrained scenarios.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Self-Supervised Learning",
            "Multitask Learning",
            "Lightweight Adapters",
            "Resource-Efficient Training"
        ]
    },
    {
        "Title": "DViN: Dynamic Visual Routing Network for Weakly Supervised Referring Expression Comprehension",
        "Authors": "Xiaofu Chen \u00b7 Yaxin Luo \u00b7 Gen Luo \u00b7 Jiayi Ji \u00b7 Henghui Ding \u00b7 Yiyi Zhou",
        "Abstract": "In this paper, we focus on weakly supervised referring expression comprehension (REC), and identify that the lack of fine-grained visual capability greatly limits the upper performance bound of existing methods. To address this issue, we propose a novel framework for weakly supervised REC, namely Dynamic Visual routing Network (DViN), which overcomes the visual shortcomings from the perspective of feature combination and alignment. In particular, DViN is equipped with a novel sparse routing mechanism to efficiently combine features of multiple visual encoders in a dynamic manner, thus improving the visual descriptive power. Besides, we further propose an innovative weakly supervised objective, namely Routing-based Feature Alignment (RFA), which facilitates the visual understanding of routed features through the intra-modal and inter-modal alignment. To validate DViN, we conduct extensive experiments on four REC benchmark datasets. Experiments demonstrate that DViN achieves state-of-the-art results on four benchmarks while maintaining competitive inference efficiency. Besides, the strong generalization ability of DViN is also validated on weakly supervised referring expression segmentation. Source codes are anonymously released at: https://anonymous.4open.science/r/DViN-7736.",
        "Link": "https://anonymous.4open.science/r/DViN-7736",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces the Dynamic Visual Routing Network (DViN) for weakly supervised referring expression comprehension (REC), addressing the limitation of fine-grained visual capabilities in existing methods. DViN incorporates a sparse routing mechanism to dynamically combine features from multiple visual encoders, enhancing visual descriptive power. Additionally, it introduces a Routing-based Feature Alignment (RFA) objective to improve visual understanding through intra-modal and inter-modal alignment. DViN demonstrates state-of-the-art performance on four REC benchmarks and shows strong generalization in weakly supervised referring expression segmentation.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Weakly Supervised Learning",
            "Sparse Routing Mechanism",
            "Routing-based Feature Alignment",
            "Weakly Supervised REC"
        ]
    },
    {
        "Title": "Multi-View Pose-Agnostic Change Localization with Zero Labels",
        "Authors": "Chamuditha Jayanga Galappaththige \u00b7 Jason Lai \u00b7 Lloyd Windrim \u00b7 Donald G. Dansereau \u00b7 Niko Suenderhauf \u00b7 Dimity Miller",
        "Abstract": "Autonomous agents often require accurate methods for detecting and localizing changes in their environment, particularly when observations are captured from unconstrained and inconsistent viewpoints. We propose a novel label-free, pose-agnostic change detection method that integrates information from multiple viewpoints to construct a change-aware 3D Gaussian Splatting (3DGS) representation of the scene. With as few as 5 images of the post-change scene, our approach can learn additional change channels in a 3DGS and produce change masks that outperform single-view techniques. Our change-aware 3D scene representation additionally enables the generation of accurate change masks for unseen viewpoints. Experimental results demonstrate state-of-the-art performance in complex multi-object scenes, achieving a 1.7$\\times$ and 1.6$\\times$ improvement in Mean Intersection Over Union and F1 score respectively over other baselines. We also contribute a new real-world dataset to benchmark change detection in diverse challenging scenes in the presence of lighting variations. Our code and dataset will be made publicly available upon acceptance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces a novel label-free, pose-agnostic change detection method for autonomous agents, utilizing multiple viewpoints to create a change-aware 3D Gaussian Splatting (3DGS) representation. This approach requires only a few images of the post-change scene to learn additional change channels and generate superior change masks compared to single-view techniques. It also allows for the creation of accurate change masks for previously unseen viewpoints. The method demonstrates significant improvements in performance metrics over existing baselines and introduces a new real-world dataset to benchmark change detection in challenging environments with varying lighting conditions.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Object Detection",
            "Autonomous Driving",
            "Change Detection",
            "Pose-Agnostic",
            "Multi-View Integration"
        ]
    },
    {
        "Title": "Learned Binocular-Encoding Optics for RGBD Imaging Using Joint Stereo and Focus Cues",
        "Authors": "Yuhui Liu \u00b7 Liangxun Ou \u00b7 Qiang Fu \u00b7 Hadi Amata \u00b7 Wolfgang Heidrich \u00b7 YIFAN PENG",
        "Abstract": "Extracting high-fidelity RGBD information from two-dimensional (2D) images is essential for various visual computing applications. Stereo imaging, as a reliable passive imaging technique for obtaining three-dimensional (3D) scene information, has benefited greatly from deep learning advancements. However, existing stereo depth estimation algorithms struggle to perceive high-frequency information and resolve high-resolution depth maps in realistic camera settings with large depth variations. These algorithms commonly neglect the hardware parameter configuration, limiting the potential for achieving optimal solutions solely through software-based design strategies.This work presents a hardware-software co-designed RGBD imaging framework that leverages both stereo and focus cues to reconstruct texture-rich color images along with detailed depth maps over a wide depth range. A pair of rank-2 parameterized diffractive optical elements (DOEs) is employed to encode perpendicular complementary information optically during stereo acquisitions. Additionally, we employ an IGEV-UNet-fused neural network tailored to the proposed rank-2 encoding for stereo matching and image reconstruction. Through prototyping a stereo camera with customized DOEs, our deep stereo imaging paradigm has demonstrated superior performance over existing monocular and stereo imaging systems in both image PSNR by 2.96 dB gain and depth accuracy in high-frequency details across distances from 0.67 to 8 meters.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a hardware-software co-designed RGBD imaging framework that utilizes stereo and focus cues to reconstruct detailed color images and depth maps across a wide depth range. The framework employs a pair of rank-2 parameterized diffractive optical elements (DOEs) to encode perpendicular complementary information during stereo acquisitions and an IGEV-UNet-fused neural network for stereo matching and image reconstruction. The proposed system outperforms existing monocular and stereo imaging systems in image quality and depth accuracy, particularly in high-frequency details.",
        "Tags": [
            "Stereo Matching",
            "Depth Estimation",
            "Hardware-Software Co-Design",
            "Diffractive Optical Elements",
            "IGEV-UNet Neural Network"
        ]
    },
    {
        "Title": "Realistic Test-Time Adaptation of Vision-Language Models",
        "Authors": "Maxime Zanella \u00b7 Cl\u00e9ment Fuchs \u00b7 Christophe De Vleeschouwer \u00b7 Ismail Ben Ayed",
        "Abstract": "The zero-shot capabilities of Vision-Language Models (VLMs) have been widely leveraged to improve predictive performance. However, previous works on transductive or test-time adaptation (TTA) often make strong assumptions about the data distribution, such as the presence of all classes. Our work challenges these favorable deployment scenarios, and introduces a more realistic evaluation framework, including: (i) a variable number of effective classes for adaptation within a single batch, and (ii) non-i.i.d. batches of test samples in online adaptation settings. We provide comprehensive evaluations, comparisons, and ablation studies that demonstrate how current transductive or TTA methods for VLMs systematically compromise the models\u2019 initial zero-shot robustness across various realistic scenarios, favoring performance gains under advantageous assumptions about the test samples' distributions. Furthermore, we introduce Stat${\\cal A}$, a versatile method that could handle a wide range of deployment scenarios, including those with a variable number of effective classes at test time. Our approach incorporates a novel regularization term designed specifically for VLMs, which acts as a statistical anchor preserving the initial text-encoder knowledge, particularly in low-data regimes. Code will be made available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the limitations of current transductive or test-time adaptation (TTA) methods for Vision-Language Models (VLMs), which often rely on unrealistic assumptions about data distribution, such as the presence of all classes. The authors propose a more realistic evaluation framework that includes a variable number of effective classes within a single batch and non-i.i.d. batches in online adaptation settings. They demonstrate that existing TTA methods compromise the zero-shot robustness of VLMs under these realistic scenarios. To address this, they introduce Stat${\\cal A}$, a versatile method that incorporates a novel regularization term to preserve the initial text-encoder knowledge, especially in low-data regimes. The approach is shown to handle a wide range of deployment scenarios effectively.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Test-Time Adaptation (TTA)",
            "Zero-Shot Learning",
            "Regularization Techniques",
            "Non-i.i.d. Data Adaptation"
        ]
    },
    {
        "Title": "AnyMap: Learning a General Camera Model for Structure-from-Motion with Unknown Distortion in Dynamic Scenes",
        "Authors": "Andrea Porfiri Dal Cin \u00b7 Georgi Dikov \u00b7 Jihong Ju \u00b7 Mohsen Ghafoorian",
        "Abstract": "Current learning-based Structure-from-Motion (SfM) methods struggle with videos of dynamic scenes from wide-angle cameras. We present AnyMap, a differentiable SfM framework that jointly addresses image distortion and motion estimation. By learning a general implicit camera model without predefined parameters, AnyMap effectively handles lens distortion, estimating multi-view consistent 3D geometry, camera poses, and (un)projection functions. To resolve the ambiguity where motion estimation can compensate for undistortion errors and vice versa, we introduce a low-dimensional motion representation consisting of a set of learnable basis trajectories, interpolated to produce regularized motion estimates. Experimental results show that our method produces accurate camera poses, excels in camera calibration and image rectification, and enables high-quality novel view synthesis. Our low-dimensional motion representation effectively disentangles undistortion with motion estimation, outperforming existing methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces AnyMap, a differentiable Structure-from-Motion (SfM) framework designed to handle videos from wide-angle cameras in dynamic scenes. AnyMap learns a general implicit camera model to address image distortion and motion estimation simultaneously, enabling accurate 3D geometry, camera poses, and (un)projection functions. A low-dimensional motion representation, based on learnable basis trajectories, is introduced to disentangle undistortion from motion estimation, improving performance over existing methods. The framework demonstrates strong results in camera calibration, image rectification, and novel view synthesis.",
        "Tags": [
            "3D Reconstruction",
            "Camera Calibration",
            "Implicit Neural Representations",
            "Novel View Synthesis",
            "Dynamic Scenes",
            "Wide-Angle Cameras",
            "Differentiable SfM"
        ]
    },
    {
        "Title": "Can Large Vision-Language Models Correct Grounding Errors By Themselves?",
        "Authors": "Yuan-Hong Liao \u00b7 Rafid Mahmood \u00b7 Sanja Fidler \u00b7 David Acuna",
        "Abstract": "Improving semantic grounding in Vision-Language Models (VLMs) often involves collecting domain-specific training data, refining the network architectures, or modifying the training recipes. In this work, we venture into an orthogonal direction and explore self-correction in VLMs focusing on semantic grounding. We find that VLMs can correct their own semantic grounding mistakes when properly prompted and framed for the task, without any fine-tuning or even access to oracle feedback. We also introduce a self-correction framework in an iterative setting which consistently improves performance across all models investigated. Overall, we show that iterative self-correction consistently improves VLM performance in semantic grounding by up to 8.4 accuracy points across all models investigated, without requiring fine-tuning, additional architectural changes, or external data. Our exploration of self-correction also reveals that, even after several rounds of feedback, strong models like GPT-4V and GPT-4o  retain limited capability in leveraging oracle feedback, suggesting promising directions for further research.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This study explores self-correction in Vision-Language Models (VLMs) for improving semantic grounding without fine-tuning or external feedback. The authors introduce a self-correction framework that iteratively enhances VLM performance, achieving up to 8.4 accuracy points improvement across models. The research also highlights the limitations of strong models like GPT-4V and GPT-4o in leveraging oracle feedback, indicating potential areas for future investigation.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Self-Supervised Learning",
            "Self-Correction Framework",
            "Iterative Improvement",
            "Semantic Grounding"
        ]
    },
    {
        "Title": "LAL: Enhancing 3D Human Motion Prediction with Latency-aware Auxiliary Learning",
        "Authors": "Xiaoning Sun \u00b7 Dong Wei \u00b7 Huaijiang Sun \u00b7 Shengxiang Hu",
        "Abstract": "Making accurate prediction of human motions based on the historical observation is a crucial technology for robots to collaborate with humans. Existing human motion prediction methods are all built under an ideal assumption that robots can instantaneously react, which ignores the time delay introduced during data processing & analysis and future reaction planning -- jointly known as \"response latency''. Consequently, the predictions made within this latency period become meaningless for practical use, as part of the time has passed and the corresponding real motions have already occurred before robot deliver its reaction. In this paper, we argue that the seemingly meaningless prediction period, however, can be leveraged to enhance prediction accuracy significantly. We propose LAL, a Latency-aware Auxiliary Learning framework, which shifts the existing \"reaction instantaneous'' convention into a new motion prediction paradigm with both latency compatibility and utility. The framework consists of two branches handling different tasks: the primary branch learns to directly predict the valid target (excluding the beginning latency period) based on observation; while the auxiliary branch learns the same target, but based on the reformed observation with additional latency data incorporated. A direct and effective way of auxiliary feature sharing is forced by our tailored consistency loss, to gradually integrate auxiliary latency insights into the primary prediction branch. Estimated feature statistics-based alignment method is presented as optional step for primary branch refinement. Experiments show that LAL achieves significant improvement on prediction accuracy, without additional time consumption during testing.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces LAL, a Latency-aware Auxiliary Learning framework designed to enhance 3D human motion prediction by addressing the overlooked issue of response latency in existing methods. LAL operates through two branches: a primary branch that predicts future motions excluding the latency period, and an auxiliary branch that incorporates latency data to refine these predictions. The framework employs a consistency loss to facilitate feature sharing between the branches, improving prediction accuracy without increasing testing time. This approach shifts the conventional instantaneous reaction paradigm to one that is latency-compatible and more practical for real-world applications.",
        "Tags": [
            "3D Human Pose Estimation",
            "Human Action Prediction",
            "Latency-aware Learning",
            "Auxiliary Feature Sharing",
            "Consistency Loss"
        ]
    },
    {
        "Title": "Finer-CAM: Spotting the Difference Reveals Finer Details for Visual Explanation",
        "Authors": "Ziheng Zhang \u00b7 Jianyang Gu \u00b7 Arpita Chowdhury \u00b7 Zheda Mai \u00b7 David Carlyn \u00b7 Tanya Berger-Wolf \u00b7 Yu Su \u00b7 Wei-Lun Chao",
        "Abstract": "Class activation map (CAM) has been broadly investigated to highlight image regions that contribute to class predictions. Despite its simplicity to implement and computational efficiency, CAM often struggles to identify discriminative regions in fine-grained classification tasks. Previous efforts address this by introducing more sophisticated explanation processes on the model prediction, at a cost of extra complexity. In this paper, we propose Finer-CAM, which retains CAM's efficiency while achieving precise localization of discriminative regions. Our insight is that the deficiency of CAM is not about how it explains but what it explains. Previous methods independently look at all the possible hints that explain the target class prediction, which inevitably also activates regions predictive of similar classes. By explicitly comparing and spotting the difference between the target class and similar classes, Finer-CAM suppresses features shared with other classes, and emphasizes the unique key details of the target class. The method allows adjustable comparing strength, enabling Finer-CAM to focus coarsely on general object contours or discriminatively on fine-grained details. The method is compatible with various CAM methods, and can be extended to multi-modal models to accurately activate specific concepts. Quantitatively, we demonstrate that masking out the top 5% activated pixels by Finer-CAM leads to a larger relative confidence drop compared with baselines. The source code is attached in the supplementary material.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Finer-CAM improves upon traditional Class Activation Map (CAM) methods by enhancing the localization of discriminative regions in fine-grained classification tasks. Unlike previous approaches that independently analyze all possible hints for class prediction, Finer-CAM explicitly compares the target class with similar classes to suppress shared features and emphasize unique details. This method maintains CAM's efficiency and can adjust the focus from general object contours to fine-grained details. It is compatible with various CAM methods and can be extended to multi-modal models for precise concept activation. Quantitative results show that Finer-CAM achieves a larger relative confidence drop when masking the top 5% activated pixels compared to baseline methods.",
        "Tags": [
            "Class Activation Map (CAM)",
            "Fine-Grained Classification",
            "Discriminative Region Localization",
            "Multi-Modal Model Compatibility",
            "Adjustable Feature Comparison"
        ]
    },
    {
        "Title": "Linguistics-aware Masked Image Modeling for Self-supervised Scene Text Recognition",
        "Authors": "Yifei Zhang \u00b7 Chang Liu \u00b7 Jin Wei \u00b7 Xiaomeng Yang \u00b7 Yu ZHOU \u00b7 Can Ma \u00b7 Xiangyang Ji",
        "Abstract": "Text images are unique in their dual nature, encompassing both visual and linguistic information. The visual component encompasses structural and appearance-based features, while the linguistic dimension incorporates contextual and semantic elements. In scenarios with degraded visual quality, linguistic patterns serve as crucial supplementary for comprehension, highlighting the necessity of integrating both aspects for robust scene text recognition (STR). Contemporary STR approaches often use language models or semantic reasoning modules to capture linguistic features, typically requiring large-scale annotated datasets. Self-supervised learning, which lacks annotations, presents challenges in disentangling linguistic features related to the global context. Typically, sequence contrastive learning emphasizes the alignment of local features, while masked image modeling (MIM) tends to exploit local structures to reconstruct visual patterns, resulting in limited linguistic knowledge. In this paper, we propose a Linguistics-aware Masked Image Modeling (LMIM) approach, which channels the linguistic information into the decoding process of MIM through a separate branch. Specifically, we design a linguistics alignment module to extract vision-independent features as linguistic guidance using inputs with different visual appearances. As features extend beyond mere visual structures, LMIM must consider the global context to achieve reconstruction. Extensive experiments on various benchmarks quantitatively demonstrate our state-of-the-art performance, and attention visualizations qualitatively show the simultaneous capture of both visual and linguistic information. Our code will be made available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Linguistics-aware Masked Image Modeling (LMIM), a novel approach for self-supervised scene text recognition (STR) that integrates both visual and linguistic information. Unlike traditional methods that rely on large annotated datasets or focus solely on local feature alignment, LMIM incorporates a separate branch in the decoding process of masked image modeling to channel linguistic information. A linguistics alignment module is designed to extract vision-independent features, enabling the model to consider global context for reconstruction. The approach demonstrates state-of-the-art performance across various benchmarks, effectively capturing both visual and linguistic aspects of text images.",
        "Tags": [
            "Self-Supervised Learning",
            "Optical Character Recognition (OCR)",
            "Linguistics Alignment",
            "Masked Image Modeling",
            "Global Context Reconstruction"
        ]
    },
    {
        "Title": "EVPGS: Enhanced View Prior Guidance for Splatting-based Extrapolated View Synthesis",
        "Authors": "Jiahe Li \u00b7 Feiyu Wang \u00b7 Xiaochao Qu \u00b7 WU CHENGJING \u00b7 Xiaochao Qu \u00b7 Ting Liu",
        "Abstract": "Gaussian Splatting (GS)-based methods rely on sufficient training view coverage and perform synthesis on interpolated views. In this work, we tackle the more challenging and underexplored Extrapolated View Synthesis (EVS) task. Here we enable GS-based models trained with limited view coverage to generalize well to extrapolated views. To achieve our goal, we propose a view augmentation framework to guide training through a coarse-to-fine process. At the coarse stage, we reduce rendering artifacts due to insufficient view coverage by introducing a regularization strategy at both appearance and geometry levels. At the fine stage, we generate reliable view priors to provide further training guidance. To this end, we incorporate an occlusion awareness into the view prior generation process, and refine the view priors with the aid of coarse stage output. We call our framework Enhanced View Prior Guidance for Splatting (EVPGS). To comprehensively evaluate EVPGS on the EVS task, we collect a real-world dataset called Merchandise3D dedicated to the EVS scenario. Experiments on three datasets including both real and synthetic demonstrate EVPGS achieves state-of-the-art performance, while improving synthesis quality at extrapolated views for GS-based methods both qualitatively and quantitatively. We will make our code, dataset, and models public.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the Extrapolated View Synthesis (EVS) task, which is more challenging than traditional interpolated view synthesis, especially for Gaussian Splatting (GS)-based methods that typically require extensive training view coverage. The authors propose Enhanced View Prior Guidance for Splatting (EVPGS), a framework that enhances GS-based models to generalize effectively to extrapolated views with limited training views. EVPGS employs a two-stage coarse-to-fine process: the coarse stage reduces rendering artifacts through regularization at both appearance and geometry levels, while the fine stage generates and refines reliable view priors, incorporating occlusion awareness and leveraging outputs from the coarse stage. The effectiveness of EVPGS is demonstrated through state-of-the-art performance on a newly collected real-world dataset, Merchandise3D, and other datasets, showing significant improvements in synthesis quality at extrapolated views.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Novel View Synthesis",
            "View Augmentation",
            "Occlusion Awareness",
            "Real-world Dataset"
        ]
    },
    {
        "Title": "SoftVQ-VAE: Efficient 1-Dimensional Continuous Tokenizer",
        "Authors": "Hao Chen \u00b7 Ze Wang \u00b7 Xiang Li \u00b7 Ximeng Sun \u00b7 Fangyi Chen \u00b7 Jiang Liu \u00b7 Jindong Wang \u00b7 Bhiksha Raj \u00b7 Zicheng Liu \u00b7 Emad Barsoum",
        "Abstract": "Efficient image tokenization with high compression ratios remains a critical challenge for training generative models.We present SoftVQ-VAE, a continuous image tokenizer that leverages soft categorical posteriors to aggregate multiple codewords into each latent token, substantially increasing the representation capacity of the latent space. When applied to Transformer-based architectures, our approach compresses 256$\\times$256 and 512$\\times$512 images using only 32 or 64 1-dimensional tokens.Not only does SoftVQ-VAE show consistent and high-quality reconstruction, more importantly, it also achieves state-of-the-art and significantly faster image generation results across different denoising-based generative models. Remarkably, SoftVQ-VAE improves inference throughput by up to 18x for generating 256$\\times$256 images and 55x for 512$\\times$512 images while achieving competitive FID scores of 1.78 and 2.21 for SiT-XL.It also improves the training efficiency of the generative models by reducing the number of training iterations by 2.3x while maintaining comparable performance. With its fully-differentiable design and semantic-rich latent space, our experiment demonstrates that SoftVQ-VQE achieves efficient tokenization without compromising generation quality, paving the way for more efficient generative models.Code and model will be released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "SoftVQ-VAE introduces a continuous image tokenizer that uses soft categorical posteriors to aggregate multiple codewords into each latent token, enhancing the latent space's representation capacity. This method compresses high-resolution images into fewer 1-dimensional tokens, achieving state-of-the-art image generation results with significantly faster inference speeds and improved training efficiency. The approach maintains high-quality reconstructions and competitive FID scores, demonstrating its effectiveness in efficient tokenization without sacrificing generation quality.",
        "Tags": [
            "Image Generation",
            "Diffusion Models",
            "Continuous Tokenization",
            "Latent Space Optimization",
            "High Compression Ratios"
        ]
    },
    {
        "Title": "OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows",
        "Authors": "Shufan Li \u00b7 Konstantinos Kallidromitis \u00b7 Akash Gokul \u00b7 Zichun Liao \u00b7 Yusuke Kato \u00b7 Kazuki Kozuka \u00b7 Aditya Grover",
        "Abstract": "We introduce OminiFlow, a novel generative model designed for any-to-any generation tasks such as text-to-image, text-to-audio, and audio-to-image synthesis. OminiFlow  advances the rectified flow (RF) framework used in text-to-image models to handle the joint distribution of multiple modalities. It outperforms previous any-to-any models on a wide range of tasks, such as text-to-image and text-to-audio synthesis. Our work offers three key contributions: First, we extend RF to a multi-modal setting and introduce a novel guidance mechanism, enabling users to flexibly control the alignment between different modalities in the generated outputs. Second, we propose a novel architecture that extends the text-to-image MMDiT architecture of Stable Diffusion 3 and enables audio and text generation. The extended modules can be efficiently pretrained individually and merged with the vanilla text-to-image MMDiT for fine-tuning. Lastly, we conduct a comprehensive study on the design choices of rectified flow transformers for large-scale audio and text generation, providing valuable insights into optimizing performance across diverse modalities.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "OmniFlow is a novel generative model designed for any-to-any generation tasks, such as text-to-image, text-to-audio, and audio-to-image synthesis. It extends the rectified flow (RF) framework to handle joint distributions of multiple modalities, outperforming previous models in tasks like text-to-image and text-to-audio synthesis. Key contributions include: (1) extending RF to a multi-modal setting with a novel guidance mechanism for flexible control over modality alignment, (2) proposing an architecture that extends the text-to-image MMDiT of Stable Diffusion 3 to support audio and text generation, enabling efficient pretraining and fine-tuning, and (3) conducting a comprehensive study on rectified flow transformers for large-scale audio and text generation, offering insights into optimizing performance across modalities.",
        "Tags": [
            "Multimodal Learning",
            "Text-to-Image Generation",
            "Rectified Flow Framework",
            "Multi-Modal Guidance Mechanism",
            "MMDiT Architecture"
        ]
    },
    {
        "Title": "Towards Universal Soccer Video Understanding",
        "Authors": "Jiayuan Rao \u00b7 Haoning Wu \u00b7 Hao Jiang \u00b7 Ya Zhang \u00b7 Yanfeng Wang \u00b7 Weidi Xie",
        "Abstract": "As a globally celebrated sport, soccer has attracted widespread interest from fans over the world. This paper aims to develop a comprehensive multi-modal framework for soccer video understanding.Specifically, we make the following contributions in this paper:(i) we introduce SoccerReplay-1988, the largest multi-modal soccer dataset to date, featuring videos and detailed annotations from 1,988 complete matches, with an automated annotation pipeline;(ii) we present the first visual-language foundation model in the soccer domain, MatchVision, which leverages spatiotemporal information across soccer videos and excels in various downstream tasks;(iii) we conduct extensive experiments and ablation studies on action classification, commentary generation, and multi-view foul recognition,and demonstrate state-of-the-art performance on all of them, substantially outperforming existing models, which has demonstrated the superiority of our proposed data and model. We believe that this work will offer a standard paradigm for sports understanding research. The code and model will be publicly available for reproduction.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a comprehensive multi-modal framework for soccer video understanding, featuring three key contributions: (i) the creation of SoccerReplay-1988, the largest multi-modal soccer dataset with 1,988 complete matches and an automated annotation pipeline; (ii) the development of MatchVision, the first visual-language foundation model for soccer that leverages spatiotemporal information and excels in various downstream tasks; and (iii) extensive experiments demonstrating state-of-the-art performance in action classification, commentary generation, and multi-view foul recognition. The proposed framework sets a new standard for sports understanding research.",
        "Tags": [
            "Video Understanding",
            "Multimodal Learning",
            "Spatiotemporal Analysis",
            "Automated Annotation",
            "Visual-Language Foundation Model"
        ]
    },
    {
        "Title": "Show and Segment: Universal Medical Image Segmentation via In-Context Learning",
        "Authors": "Yunhe Gao \u00b7 Di Liu \u00b7 Jack Li \u00b7 Yunsheng Li \u00b7 Dongdong Chen \u00b7 Mu Zhou \u00b7 Dimitris N. Metaxas",
        "Abstract": "Medical image segmentation remains challenging due to the vast diversity of anatomical structures, imaging modalities, and segmentation tasks. While deep learning has made significant advances, current approaches struggle to generalize as they require task-specific training or fine-tuning on unseen classes. We present \\textbf{Iris}, a novel In-context Reference Image guided Segmentation framework that enables flexible adaptation to novel tasks through the use of reference examples without fine-tuning. At its core, Iris features a lightweight context task encoding module that distills task-specific information from reference context image-label pairs. This rich context embedding information is used to guide the segmentation of target objects. Given a decoupled architecture on 3D data processing, Iris supports diverse inference strategies including one-shot inference, context example ensemble, object-level context example retrieval, and in-context tuning. Through comprehensive evaluation across twelve datasets, we demonstrate that Iris performs strongly compared to specialized supervised models on in-distribution tasks. On seven held-out dataset, Iris shows superior generalization to out-of-distribution data and unseen classes. Further, Iris's task encoding module can automatically discover anatomical relationships across datasets and modalities, offering insights into cross-modality medical objects without explicit anatomical supervision.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Medical image segmentation faces challenges due to the diversity of anatomical structures, imaging modalities, and tasks. Current deep learning methods often require task-specific training or fine-tuning, limiting their generalization. This paper introduces Iris, an In-context Reference Image guided Segmentation framework that adapts to novel tasks using reference examples without fine-tuning. Iris employs a lightweight context task encoding module to extract task-specific information from reference image-label pairs, guiding the segmentation of target objects. The framework supports diverse inference strategies, including one-shot inference and in-context tuning. Evaluations across twelve datasets show Iris performs comparably to specialized supervised models on in-distribution tasks and generalizes well to out-of-distribution data and unseen classes. Additionally, Iris's task encoding module can uncover anatomical relationships across datasets and modalities, providing insights without explicit anatomical supervision.",
        "Tags": [
            "Medical Image Segmentation",
            "In-Context Learning",
            "3D Data Processing",
            "Cross-Modality Analysis",
            "Task Encoding Module"
        ]
    },
    {
        "Title": "Benchmarking Object Detectors under Real-World Distribution Shifts in Satellite Imagery",
        "Authors": "Sara Al-Emadi \u00b7 Yin Yang \u00b7 Ferda Ofli",
        "Abstract": "Object detectors have achieved remarkable performance in many applications; however, these deep learning models are typically designed under the i.i.d. assumption, meaning they are trained and evaluated on data sampled from the same (source) distribution. In real-world deployment, however, target distributions often differ from source data, leading to substantial performance degradation. Domain Generalisation (DG) seeks to bridge this gap by enabling models to generalise to Out-Of-Distribution (OOD) data without access to target distributions during training, enhancing robustness to unseen conditions. In this work, we examine the generalisability and robustness of state-of-the-art object detectors under real-world distribution shifts, focusing particularly on spatial domain shifts. Despite the need, a standardised benchmark dataset specifically designed for assessing object detection under realistic DG scenarios is currently lacking. To address this, we introduce a suite RWDS consisting of three novel DG benchmarking datasets\\footnote{The datasets are available in the Supplementary Materials} that focus on humanitarian and climate change applications. These datasets allow investigation of domain shifts across (i) climate zones and (ii) various disasters and geographic regions. To our knowledge, these are the first DG benchmarking datasets tailored for object detection in real-world, high-impact contexts. We aim for these datasets to serve as valuable resources for evaluating the robustness and generalisation of future object detection models.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of object detectors' performance degradation under real-world distribution shifts, particularly in satellite imagery. It highlights the lack of standardized benchmark datasets for evaluating object detection in Domain Generalisation (DG) scenarios. To fill this gap, the authors introduce RWDS, a suite of three novel DG benchmarking datasets designed for humanitarian and climate change applications. These datasets enable the study of domain shifts across different climate zones, disasters, and geographic regions, aiming to enhance the robustness and generalization of future object detection models.",
        "Tags": [
            "Object Detection",
            "Domain Generalisation (DG)",
            "Satellite Imagery",
            "Distribution Shifts",
            "Climate Change Applications"
        ]
    },
    {
        "Title": "FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D Gaussian Splatting",
        "Authors": "Hengyu Liu \u00b7 Yuehao Wang \u00b7 Chenxin Li \u00b7 Ruisi Cai \u00b7 Kevin Wang \u00b7 Wuyang Li \u00b7 Pavlo Molchanov \u00b7 Peihao Wang \u00b7 Zhangyang Wang",
        "Abstract": "3D Gaussian splatting (3DGS) has enabled various applications in 3D scene representation and novel view synthesis due to its efficient rendering capabilities. However, 3DGS demands significant GPU memory, limiting its use on devices with restricted computational resources. Previous approaches have focused on pruning less important Gaussians, effectively compressing 3DGS but often requiring a fine-tuning stage and lacking adaptability for the specific memory needs of different devices. In this work, we present an elastic inference method for 3DGS. Given an input for the desired model size, our method selects and transforms a subset of Gaussians, achieving substantial rendering performance without additional fine-tuning. We introduce a tiny learnable module that controls Gaussian selection based on the input percentage, along with a transformation module that adjusts the selected Gaussians to complement the performance of the reduced model. Comprehensive experiments on ZipNeRF, MipNeRF and Tanks\\&Temples scenes demonstrate the effectiveness of our approach. Code will be publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces FlexGS, an elastic inference method for 3D Gaussian splatting (3DGS) that addresses the high GPU memory demands of traditional 3DGS, which limits its use on devices with restricted computational resources. Unlike previous methods that require fine-tuning and lack adaptability, FlexGS selects and transforms a subset of Gaussians based on the desired model size, enabling efficient rendering without additional fine-tuning. The approach includes a learnable module for Gaussian selection and a transformation module to adjust the selected Gaussians, ensuring robust performance across various devices. The method's effectiveness is validated through experiments on multiple datasets.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "NeRF (Neural Radiance Fields)",
            "Elastic Inference",
            "Memory Efficiency",
            "Device Adaptability"
        ]
    },
    {
        "Title": "MoVE-KD: Knowledge Distillation for VLMs with Mixture of Visual Encoders",
        "Authors": "jiajun cao \u00b7 Yuan Zhang \u00b7 Tao Huang \u00b7 Ming Lu \u00b7 Qizhe Zhang \u00b7 Ruichuan An \u00b7 Ningning Ma \u00b7 Shanghang Zhang",
        "Abstract": "Visual encoders are fundamental components in vision-language models (VLMs), each showcasing unique strengths derived from various pre-trained visual foundation models. To leverage the various capabilities of these encoders, recent studies incorporate multiple encoders within a single VLM, leading to a considerable increase in computational cost. In this paper, we present Mixture-of-Visual-Encoder Knowledge Distillation (MoVE-KD), a novel framework that distills the unique proficiencies of multiple vision encoders into a single, efficient encoder model. Specifically, to mitigate conflicts and retain the unique characteristics of each teacher encoder, we employ low-rank adaptation (LoRA) and mixture-of-experts (MoEs) to selectively activate specialized knowledge based on input features, enhancing both adaptability and efficiency. To regularize the KD process and enhance performance, we propose an attention-based distillation strategy that adaptively weighs the different visual encoders and emphasizes valuable visual tokens, reducing the burden of replicating comprehensive but distinct features from multiple teachers. Comprehensive experiments on popular VLMs, such as LLaVA and LLaVA-NeXT, validate the effectiveness of our method. The code will be released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Mixture-of-Visual-Encoder Knowledge Distillation (MoVE-KD), a framework designed to distill the unique capabilities of multiple visual encoders into a single, efficient encoder model within vision-language models (VLMs). By employing low-rank adaptation (LoRA) and mixture-of-experts (MoEs), the method selectively activates specialized knowledge based on input features, enhancing adaptability and efficiency. An attention-based distillation strategy is also proposed to adaptively weigh different visual encoders and emphasize valuable visual tokens, improving performance while reducing computational costs. The effectiveness of MoVE-KD is validated through experiments on popular VLMs like LLaVA and LLaVA-NeXT.",
        "Tags": [
            "Knowledge Distillation",
            "Vision-Language Models (VLMs)",
            "Low-Rank Adaptation (LoRA)",
            "Mixture-of-Experts (MoEs)",
            "Attention-Based Distillation"
        ]
    },
    {
        "Title": "SPC-GS: Gaussian Splatting with Semantic-Prompt Consistency for Indoor Open-World Free-view Synthesis from Sparse Inputs",
        "Authors": "Guibiao Liao \u00b7 Qing Li \u00b7 Zhenyu Bao \u00b7 Guoping Qiu \u00b7 KANGLIN LIU",
        "Abstract": "3D Gaussian Splatting-based indoor open-world free-view synthesis approaches have shown significant performance with dense input images. However, they exhibit poor performance when confronted with sparse inputs, primarily due to the sparse distribution of Gaussian points and insufficient view supervision. To relieve these challenges, we propose SPC-GS, leveraging Scene-layout-based Gaussian Initialization (SGI) and Semantic-Prompt Consistency (SPC) Regularization for open-world free view synthesis with sparse inputs. Specifically, SGI provides a dense, scene-layout-based Gaussian distribution by utilizing view-changed images generated from the video generation model and view-constraint Gaussian points densification. Additionally, SPC mitigates limited view supervision by employing semantic-prompt-based consistency constraints developed by SAM2. This approach leverages available semantics from training views, serving as instructive prompts, to optimize visually overlapping regions in novel views with 2D and 3D consistency constraints. Extensive experiments demonstrate the superior performance of SPC-GS across Replica and ScanNet benchmarks. Notably, our SPC-GS achieves a 3.06 dB gain in PSNR for reconstruction quality and a 7.3\\% improvement in mIoU for open-world semantic segmentation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces SPC-GS, a novel approach for indoor open-world free-view synthesis from sparse inputs using 3D Gaussian Splatting. Traditional methods struggle with sparse inputs due to inadequate Gaussian point distribution and limited view supervision. SPC-GS addresses these issues through Scene-layout-based Gaussian Initialization (SGI) and Semantic-Prompt Consistency (SPC) Regularization. SGI enhances Gaussian point distribution using view-changed images and densification techniques, while SPC employs semantic-prompt-based consistency constraints to optimize novel views with 2D and 3D consistency. The method demonstrates significant improvements in reconstruction quality and semantic segmentation accuracy on Replica and ScanNet benchmarks.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Novel View Synthesis",
            "Scene-layout-based Gaussian Initialization",
            "Semantic-Prompt Consistency",
            "Sparse Inputs"
        ]
    },
    {
        "Title": "DEAL: Data-Efficient Adversarial Learning for High-Quality Infrared Imaging",
        "Authors": "Zhu Liu \u00b7 Zijun Wang \u00b7 Jinyuan Liu \u00b7 Fanqi Meng \u00b7 Long Ma \u00b7 Risheng Liu",
        "Abstract": "Thermal imaging is often compromised by dynamic, complex degradations caused by hardware limitations and unpredictable environmental factors. The scarcity of high-quality infrared data, coupled with the challenges of dynamic, intricate degradations, makes it difficult to recover   details using existing methods. In this paper, we introduce thermal degradation simulation integrated into the training process via a mini-max optimization, by modeling these degraded factors as adversarial attacks on thermal images. The simulation is dynamic to maximize objective functions, thus capturing a broad spectrum of degraded data distributions. This approach enables training with limited data, thereby improving model performance.Additionally, we introduce a dual-interaction network that combines the benefits of spiking neural networks with scale transformation to capture degraded features with sharp spike signal intensities. This architecture ensures compact model parameters while preserving efficient feature representation. Extensive experiments demonstrate that our method not only achieves superior visual quality under diverse single and composited degradation, but also delivers a significant reduction in processing when trained on only fifty clear images, outperforming existing techniques in efficiency and accuracy.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the challenge of high-quality infrared imaging, which is often degraded by hardware limitations and environmental factors. To overcome the scarcity of high-quality data, the authors propose a data-efficient adversarial learning framework that integrates thermal degradation simulation into the training process. This simulation is modeled as adversarial attacks on thermal images, enabling dynamic optimization to capture a wide range of degraded data distributions. The approach is complemented by a dual-interaction network that leverages spiking neural networks and scale transformation to efficiently capture degraded features. The method achieves superior visual quality and processing efficiency, even when trained on a limited dataset of fifty clear images, outperforming existing techniques in both accuracy and efficiency.",
        "Tags": [
            "Low-Level Vision",
            "Data Augmentation",
            "Adversarial Learning",
            "Spiking Neural Networks",
            "Infrared Imaging"
        ]
    },
    {
        "Title": "PersonaBooth: Personalized Text-to-Motion Generation",
        "Authors": "Boeun Kim \u00b7 Hea In Jeong \u00b7 JungHoon Sung \u00b7 Yihua Cheng \u00b7 Jeongmin Lee \u00b7 Ju Yong Chang \u00b7 Sang-Il Choi \u00b7 YOUNGGEUN CHOI \u00b7 Saim Shin \u00b7 Jungho Kim \u00b7 Hyung Jin Chang",
        "Abstract": "This paper introduces Motion Personalization, a new task that generates personalized motions aligned with text descriptions using several basic motions containing Persona. To support this novel task, we introduce a new large-scale motion dataset called PerMo (PersonaMotion), which captures the unique personas of multiple actors. We also propose a multi-modal finetuning method of a pretrained motion diffusion model called PersonaBooth. PersonaBooth addresses two main challenges: i) A significant distribution gap between the persona-focused PerMo dataset and the pretraining datasets, which lack persona-specific data, and ii) the difficulty of capturing a consistent persona from the motions vary in content (action type). To tackle the dataset distribution gap, we introduce a persona token to accept new persona features and perform multi-modal adaptation for both text and visuals during finetuning. To capture a consistent persona, we incorporate a contrastive learning technique to enhance intra-cohesion among samples with the same persona. Furthermore, we introduce a context-aware fusion mechanism to maximize the integration of persona cues from multiple input motions. PersonaBooth outperforms state-of-the-art motion style transfer methods, establishing a new benchmark for motion personalization.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper presents Motion Personalization, a novel task aimed at generating personalized motions aligned with text descriptions using a set of basic motions that encapsulate Persona. To facilitate this task, the authors introduce PerMo (PersonaMotion), a large-scale motion dataset that captures the unique personas of multiple actors. They also propose PersonaBooth, a multi-modal finetuning method for a pretrained motion diffusion model. PersonaBooth addresses two primary challenges: the distribution gap between the persona-focused PerMo dataset and pretraining datasets lacking persona-specific data, and the difficulty of maintaining a consistent persona across varying motion content. The solution involves introducing a persona token for new persona features and employing multi-modal adaptation for text and visuals during finetuning. Additionally, a contrastive learning technique is used to enhance intra-cohesion among samples with the same persona, and a context-aware fusion mechanism is introduced to better integrate persona cues from multiple input motions. PersonaBooth sets a new benchmark in motion personalization, outperforming existing motion style transfer methods.",
        "Tags": [
            "Text-to-Image Generation",
            "Diffusion Models",
            "Motion Personalization",
            "Contrastive Learning",
            "Context-Aware Fusion"
        ]
    },
    {
        "Title": "DEFOM-Stereo: Depth Foundation Model Based Stereo Matching",
        "Authors": "Hualie Jiang \u00b7 Zhiqiang Lou \u00b7 Laiyan Ding \u00b7 Rui Xu \u00b7 Minglang Tan \u00b7 jerett \u00b7 Rui Huang",
        "Abstract": "Stereo matching is a key technique for metric depth estimation in computer vision and robotics. Real-world challenges like occlusion and non-texture hinder accurate disparity estimation from binocular matching cues. Recently, monocular relative depth estimation has shown remarkable generalization using vision foundation models. Thus, to facilitate robust stereo matching with monocular depth cues, we incorporate a robust monocular relative depth model into the recurrent stereo-matching framework, building a new framework for depth foundation model-based stereo-matching, DEFOM-Stereo. In the feature extraction stage, we construct the combined context and matching feature encoder by integrating features from conventional CNNs and DEFOM. In the update stage, we use the depth predicted by DEFOM to initialize the recurrent disparity and introduce a scale update module to refine the disparity at the correct scale.  DEFOM-Stereo is verified to have comparable performance on the Scene Flow dataset with state-of-the-art (SOTA) methods and notably shows much stronger zero-shot generalization. Moreover, DEFOM-Stereo achieves SOTA performance on the KITTI 2012, KITTI 2015, Middlebury, and ETH3D leaderboards, ranking $1^{st}$ on many metrics. The code and models will be made publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Stereo matching is crucial for metric depth estimation but faces challenges like occlusion and non-texture regions. To address these, DEFOM-Stereo integrates a robust monocular relative depth model into a recurrent stereo-matching framework. This approach combines features from conventional CNNs and the depth foundation model (DEFOM) to enhance feature extraction and uses DEFOM's depth predictions to initialize and refine disparity at the correct scale. DEFOM-Stereo demonstrates strong performance on the Scene Flow dataset, achieving state-of-the-art results on KITTI 2012, KITTI 2015, Middlebury, and ETH3D benchmarks, with notable zero-shot generalization capabilities.",
        "Tags": [
            "Stereo Matching",
            "Depth Estimation",
            "Zero-Shot Learning",
            "Recurrent Framework",
            "Monocular Depth Cues",
            "Scale Update Module"
        ]
    },
    {
        "Title": "HumanDreamer: Generating Controllable Human-Motion Videos via Decoupled Generation",
        "Authors": "Boyuan Wang \u00b7 Xiaofeng Wang \u00b7 Chaojun Ni \u00b7 Guosheng Zhao \u00b7 Zhiqin Yang \u00b7 Zheng Zhu \u00b7 Muyang Zhang \u00b7 YuKun Zhou \u00b7 xinze chen \u00b7 Guan Huang \u00b7 lihong liu \u00b7 Xingang Wang",
        "Abstract": "Human-motion video generation has been a challenging task, primarily due to the difficulty inherent in learning human body movements. While some approaches have attempted to drive human-centric video generation explicitly through pose control, these methods typically rely on poses derived from existing videos, thereby lacking flexibility. To address this, we propose HumanDreamer, a decoupled human video generation framework that first generates diverse poses from text prompts and then leverages these poses to generate human-motion videos. Specifically, we propose MotionVid, the largest dataset for human-motion pose generation. Based on the dataset, we present MotionDiT, which is trained to generate structured human-motion poses from text prompts. Besides, a novel LAMA loss is introduced, which together contribute to a significant improvement in FID by 62.4\\%, along with respective enhancements in R-precision for top1, top2, and top3 by 41.8\\%, 26.3\\%, and 18.3\\%, thereby advancing both the Text-to-Pose control accuracy and FID metrics. Our experiments across various Pose-to-Video baselines demonstrate that the poses generated by our method can produce diverse and high-quality human-motion videos. Furthermore, our model can facilitate other downstream tasks, such as pose sequence prediction and 2D-3D motion lifting.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "HumanDreamer introduces a decoupled framework for generating controllable human-motion videos by first generating diverse poses from text prompts and then using these poses to create videos. The framework leverages MotionVid, the largest dataset for human-motion pose generation, and MotionDiT, a model trained to generate structured poses from text. A novel LAMA loss significantly improves FID and R-precision metrics, enhancing text-to-pose control accuracy. The generated poses enable diverse and high-quality video generation and support downstream tasks like pose sequence prediction and 2D-3D motion lifting.",
        "Tags": [
            "Video Generation",
            "Text-to-Image Generation",
            "Pose Generation",
            "LAMA Loss",
            "Text-to-Pose Control"
        ]
    },
    {
        "Title": "SpatialCLIP: Learning 3D-aware Image Representations from Spatially Discriminative Language",
        "Authors": "zehan wang \u00b7 Sashuai zhou \u00b7 Shaoxuan He \u00b7 Haifeng Huang \u00b7 Lihe Yang \u00b7 Ziang Zhang \u00b7 Xize Cheng \u00b7 Shengpeng Ji \u00b7 Tao Jin \u00b7 Hengshuang Zhao \u00b7 Zhou Zhao",
        "Abstract": "Contrastive Language-Image Pre-training (CLIP) learns robust visual models through language supervision, making it a crucial visual encoding technique for various applications. However, CLIP struggles with comprehending spatial concepts in images, potentially restricting the spatial intelligence of CLIP-based AI systems. In this work, we propose SpatialCLIP, an enhanced version of CLIP with better spatial understanding capabilities. To capture the intricate 3D spatial relationships in images, we improve both \"visual model\" and \"language supervision\" of CLIP. Specifically, we design 3D-inspired ViT to replace the standard ViT in CLIP. By lifting 2D image tokens into 3D space and incorporating design insights from point cloud networks, our visual model gains greater potential for spatial perception. Meanwhile, captions with accurate and detailed spatial information are very rare. To explore better language supervision for spatial understanding, we re-caption images and perturb their spatial phrases as negative descriptions, which compels the visual model to seek spatial cues to distinguish these hard negative captions. With the enhanced visual model, we introduce SpatialLLaVA, following the same LLaVA-1.5 training protocol, to investigate the importance of visual representations for MLLM's spatial intelligence. Furthermore, we create SpatialBench, a benchmark specifically designed to evaluate CLIP and MLLM in spatial reasoning. SpatialCLIP and SpatialLLaVA achieve substantial performance improvements, demonstrating stronger capabilities in spatial perception and reasoning, while maintaining comparable results on general-purpose benchmarks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces SpatialCLIP, an enhanced version of CLIP designed to improve spatial understanding in visual models. SpatialCLIP incorporates a 3D-inspired Vision Transformer (ViT) that lifts 2D image tokens into 3D space, leveraging insights from point cloud networks to enhance spatial perception. Additionally, the authors address the scarcity of detailed spatial information in captions by re-captioning images and perturbing spatial phrases to create hard negative descriptions, forcing the model to focus on spatial cues. The paper also presents SpatialLLaVA, a model trained using the LLaVA-1.5 protocol, to explore the role of visual representations in multimodal large language models (MLLMs) for spatial intelligence. A new benchmark, SpatialBench, is introduced to evaluate spatial reasoning in CLIP and MLLMs. SpatialCLIP and SpatialLLaVA demonstrate significant improvements in spatial perception and reasoning while maintaining performance on general-purpose benchmarks.",
        "Tags": [
            "CLIP",
            "Vision-Language Models (VLMs)",
            "3D-aware Image Representations",
            "Spatial Reasoning",
            "Multimodal Learning"
        ]
    },
    {
        "Title": "ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning",
        "Authors": "Kailin Li \u00b7 Puhao Li \u00b7 Tengyu Liu \u00b7 Yuyang Li \u00b7 Siyuan Huang",
        "Abstract": "Human hands are central to environmental interactions, motivating increasing research on dexterous robotic manipulation. Data-driven embodied AI algorithms demand precise, large-scale, human-like manipulation sequences, which are challenging to obtain with conventional reinforcement learning or real-world teleoperation. This paper introduces ManipTrans, a novel two-stage method for efficiently transferring human bimanual skills to dexterous robotic hands in simulation. ManipTrans first pre-trains a generalist trajectory imitator for mimicking hand motion, then fine-tunes a specialist residual module for each skill under physics-based interaction constraints. This approach decouples motion imitation from physical effects, enabling efficient learning and accurate execution of complex bimanual tasks.Experiments show ManipTrans surpasses state-of-the-art methods in success rate, fidelity, and efficiency. Using ManipTrans, we transfer multiple hand-object datasets to robotic hands, creating DexManipNet, a large-scale dataset featuring previously unexplored tasks like pen capping and bottle unscrewing. DexManipNet includes 3.3K episodes of robotic manipulation and supports easy expansion with our framework, facilitating further policy training for dexterous hands and enabling real-world applications. Both code and dataset will be publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper presents ManipTrans, a two-stage method for transferring human bimanual manipulation skills to dexterous robotic hands in simulation. The approach involves pre-training a generalist trajectory imitator for hand motion and fine-tuning a specialist residual module for each skill under physics-based constraints. This decoupling allows for efficient learning and accurate execution of complex tasks. The method outperforms existing techniques in success rate, fidelity, and efficiency. Additionally, the paper introduces DexManipNet, a large-scale dataset of robotic manipulation tasks, facilitating further research and real-world applications.",
        "Tags": [
            "Embodied AI",
            "3D Reconstruction",
            "Residual Learning",
            "Bimanual Manipulation",
            "Skill Transfer"
        ]
    },
    {
        "Title": "UniK3D: Universal Camera Monocular 3D Estimation",
        "Authors": "Luigi Piccinelli \u00b7 Christos Sakaridis \u00b7 Mattia Segu \u00b7 Yung-Hsu Yang \u00b7 Siyuan Li \u00b7 Wim Abbeloos \u00b7 Luc Van Gool",
        "Abstract": "Monocular 3D estimation is crucial for visual perception.However, current methods fall short by relying on oversimplified assumptions, such as pinhole camera models or rectified images.These limitations severely restrict their general applicability, causing poor performance in real-world scenarios with fisheye or panoramic images and resulting in substantial context loss.To address this, we present UniK3D, the first generalizable method for monocular 3D estimation able to model any camera. Our method introduces a spherical 3D representation which allows for better disentanglement of camera and scene geometry and enables accurate metric 3D reconstruction for unconstrained camera models.Our camera component features a novel, model-independent representation of the pencil of rays, achieved through a learned superposition of spherical harmonics.We also introduce an angular loss, which, together with the camera module design, prevents the contraction of the 3D outputs for wide-view cameras.A comprehensive zero-shot evaluation on 13 diverse datasets demonstrates the state-of-the-art performance of UniK3D across 3D, depth, and camera metrics, with substantial gains in challenging large-field-of-view and panoramic settings, while maintaining top accuracy in conventional pinhole small-field-of-view domains. Code and models will be made public.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "UniK3D introduces a universal method for monocular 3D estimation that overcomes the limitations of existing approaches by accommodating any camera model, including fisheye and panoramic cameras. The method employs a spherical 3D representation to separate camera and scene geometry effectively, facilitating accurate metric 3D reconstruction. A novel model-independent representation of the pencil of rays using spherical harmonics is proposed, alongside an angular loss to prevent 3D output contraction in wide-view cameras. UniK3D demonstrates superior performance in zero-shot evaluations across 13 diverse datasets, excelling in both large-field-of-view and conventional pinhole camera settings.",
        "Tags": [
            "3D Reconstruction",
            "Depth Estimation",
            "Spherical 3D Representation",
            "Angular Loss",
            "Model-Independent Camera Representation"
        ]
    },
    {
        "Title": "DocSAM: Unified Document Image Segmentation via Query Decomposition and Heterogeneous Mixed Learning",
        "Authors": "Xiao-Hui Li \u00b7 Fei Yin \u00b7 Cheng-Lin Liu",
        "Abstract": "Document image segmentation is crucial in document analysis and recognition but remains challenging due to the heterogeneity of document formats and diverse segmentation tasks. Existing methods often treat these tasks separately, leading to limited generalization and resource wastage.This paper introduces DocSAM, a transformer-based unified framework for various document image segmentation tasks, including document layout analysis, multi-granularity text segmentation, and table structure recognition by modelling these tasks as a combination of instance and semantic segmentation.Specifically, DocSAM uses a Sentence BERT to map category names from each dataset into semantic queries of the same dimension as instance queries. These queries interact through attention mechanisms and are cross-attended with image features to predict instance and semantic segmentation masks. To predict instance categories, instance queries are dot-producted with semantic queries, and scores are normalized using softmax.As a result, DocSAM can be jointly trained on heterogeneous datasets, enhancing robustness and generalization while reducing computing and storage resources. Comprehensive evaluations show that DocSAM outperforms existing methods in accuracy, efficiency, and adaptability, highlighting its potential for advancing document image understanding and segmentation in various applications.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "DocSAM is a transformer-based unified framework designed for various document image segmentation tasks, including document layout analysis, text segmentation, and table structure recognition. It models these tasks as a combination of instance and semantic segmentation, using Sentence BERT to map category names into semantic queries that interact with instance queries through attention mechanisms. This approach allows DocSAM to be jointly trained on heterogeneous datasets, improving robustness and generalization while reducing resource usage. Evaluations demonstrate that DocSAM outperforms existing methods in accuracy, efficiency, and adaptability, making it a promising solution for advancing document image understanding.",
        "Tags": [
            "Semantic Segmentation",
            "Instance Segmentation",
            "Document Layout Analysis",
            "Text Segmentation",
            "Table Structure Recognition",
            "Transformer-based Framework",
            "Query Decomposition",
            "Heterogeneous Mixed Learning"
        ]
    },
    {
        "Title": "Spectral State Space Model for Rotation-Invariant Visual Representation Learning",
        "Authors": "Sahar Dastani \u00b7 Ali Bahri \u00b7 Moslem Yazdanpanah \u00b7 Mehrdad Noori \u00b7 David OSOWIECHI \u00b7 Gustavo Vargas Hakim \u00b7 Farzad Beizaee \u00b7 Milad Cheraghalikhani \u00b7 Arnab Mondal \u00b7 Herve Lombaert \u00b7 Christian Desrosiers",
        "Abstract": "State Space Models (SSMs) have recently emerged as an alternative to Vision Transformers (ViTs) due to their unique ability of modeling global relationships with linear complexity. SSMs are specifically designed to capture spatially proximate relationships of image patches. However, they fail to identify relationships between conceptually related yet not adjacent patches. This limitation arises from the non-causal nature of image data, which lacks inherent directional relationships. Additionally, current vision-based SSMs are highly sensitive to transformations such as rotation. Their predefined scanning directions depend on the original image orientation, which can cause the model to produce inconsistent patch-processing sequences after rotation.To address these limitations, we introduce Spectral VMamba, a novel approach that effectively captures the global structure within an image by leveraging spectral information derived from the graph Laplacian of image patches. Through spectral decomposition, our approach encodes patch relationships independently of image orientation, achieving rotation invariance with the aid of our Rotational Feature Normalizer (RFN) module. Our experiments on classification tasks show that Spectral VMamba outperforms the leading SSM models in vision, such as VMamba, while maintaining invariance to rotations and a providing a similar runtime efficiency.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "State Space Models (SSMs) have gained attention as an alternative to Vision Transformers (ViTs) for their ability to model global relationships with linear complexity. However, SSMs struggle with identifying relationships between conceptually related but non-adjacent image patches and are sensitive to transformations like rotation due to their predefined scanning directions. To overcome these limitations, the authors propose Spectral VMamba, a novel approach that leverages spectral information from the graph Laplacian of image patches to capture global structures independently of image orientation. This method achieves rotation invariance through a Rotational Feature Normalizer (RFN) module. Experimental results on classification tasks demonstrate that Spectral VMamba outperforms leading SSM models like VMamba while maintaining rotation invariance and runtime efficiency.",
        "Tags": [
            "State Space Models (SSMs)",
            "Vision Transformer (ViT)",
            "Rotation-Invariant Learning",
            "Graph Laplacian",
            "Spectral Decomposition",
            "Rotational Feature Normalizer (RFN)"
        ]
    },
    {
        "Title": "Flash-Split: 2D Reflection Removal with Flash Cues and Latent Separation",
        "Authors": "Tianfu Wang \u00b7 Mingyang Xie \u00b7 Haoming Cai \u00b7 Sachin Shah \u00b7 Christopher Metzler",
        "Abstract": "Transparent surfaces, such as glass, create complex reflections that obscure images, posing challenges for various computer vision applications. We present Flash-Split, a robust, two-stage flash-based approach for separating transmitted and reflected light using a single pair of flash/no-flash images, even if they are misaligned. Stage 1 performs separation in latent space via a dual-branch diffusion model, reducing the need for alignment by encoding both transmission and reflection with cross-attention. Stage 2 enhances image sharpness through conditional decoding, blending separated low-frequency and original high-frequency information. Flash-Split achieves state-of-the-art results in single-view reflection separation, validated on synthetic and real-world data, and outperforms existing methods in challenging scenarios.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Flash-Split introduces a two-stage flash-based method for separating transmitted and reflected light from a single pair of flash/no-flash images, even when misaligned. The first stage uses a dual-branch diffusion model in latent space to encode transmission and reflection with cross-attention, reducing alignment requirements. The second stage enhances image sharpness by blending separated low-frequency and original high-frequency information through conditional decoding. The method achieves state-of-the-art performance in single-view reflection separation, validated on synthetic and real-world data, and outperforms existing approaches in challenging scenarios.",
        "Tags": [
            "Image Editing",
            "Low-Level Vision",
            "Reflection Removal",
            "Diffusion Models",
            "Cross-Attention"
        ]
    },
    {
        "Title": "Towards More General Video-based Deepfake Detection through Facial Feature Guided Adaptation for Foundation Model",
        "Authors": "Yue-Hua Han \u00b7 Tai-Ming Huang \u00b7 Kailung Hua \u00b7 Jun-Cheng Chen",
        "Abstract": "Generative models have enabled the creation of highly realistic facial-synthetic images, raising significant concerns due to their potential for misuse. While research in Deepfake detection has advanced rapidly, many methods still struggle to generalize to unseen Deepfakes generated by novel synthesis techniques. To address this challenge, we propose a novel side-network-based decoder that extracts spatial and temporal cues based on the CLIP image encoder for generalized video-based Deepfake detection. Additionally, we introduce the Facial Component Guidance (FCG) to enhance the spatial learning generalizability by encouraging the model to focus on key facial regions. The cross-dataset evaluation demonstrates the superior performance of our approach, surpassing state-of-the-art methods on challenging datasets. Extensive experiments further validate the effectiveness of the proposed method in terms of data efficiency, parameter efficiency and model robustness.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the challenge of generalizing Deepfake detection to unseen synthetic techniques by proposing a novel side-network-based decoder that leverages the CLIP image encoder to extract spatial and temporal cues. The method incorporates Facial Component Guidance (FCG) to improve spatial learning by focusing on key facial regions. Cross-dataset evaluations show that the approach outperforms state-of-the-art methods, demonstrating its effectiveness in data efficiency, parameter efficiency, and robustness.",
        "Tags": [
            "Deepfake Detection",
            "CLIP",
            "Facial Component Guidance",
            "Spatial-Temporal Cues",
            "Cross-Dataset Generalization"
        ]
    },
    {
        "Title": "PACT: Pruning and Clustering-Based Token Reduction for Faster Visual Language Models",
        "Authors": "Dhouib Mohamed \u00b7 Davide Buscaldi \u00b7 Vanier Sonia \u00b7 Aymen Shabou",
        "Abstract": "Visual Language Models require substantial computational resources for inference due to the additional input tokens needed to represent visual information. However, these visual tokens often contain redundant and unimportant information, resulting in an unnecessarily high number of tokens. To address this, we introduce PACT, a method that reduces inference time and memory usage by pruning irrelevant tokens and merging visually redundant ones at an early layer of the language model. Our approach uses a novel importance metric to identify unimportant tokens without relying on attention scores, making it compatible with FlashAttention. We also propose a novel clustering algorithm, called Distance Bounded Density Peak Clustering, which efficiently clusters visual tokens while constraining the distances between elements within a cluster by a predefined threshold. We demonstrate the effectiveness of PACT through extensive experiments.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Visual Language Models (VLMs) often require significant computational resources due to the large number of input tokens representing visual information, many of which are redundant or unimportant. To address this, the authors introduce PACT, a method that reduces inference time and memory usage by pruning irrelevant tokens and merging visually redundant ones early in the model. PACT employs a novel importance metric to identify unimportant tokens without relying on attention scores, ensuring compatibility with FlashAttention. Additionally, a new clustering algorithm, Distance Bounded Density Peak Clustering, is proposed to efficiently cluster visual tokens while maintaining a predefined distance threshold within clusters. The effectiveness of PACT is validated through extensive experiments.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Model Pruning",
            "Token Reduction",
            "Clustering Algorithm",
            "FlashAttention Compatibility"
        ]
    },
    {
        "Title": "SimLTD: Simple Semi-Supervised Long-Tailed Object Detection",
        "Authors": "Phi Vu Tran",
        "Abstract": "Recent years have witnessed tremendous advances on modern visual recognition systems. Despite such progress, many vision models still struggle with the open problem of learning from few exemplars. This paper focuses on the task of object detection in the setting where object classes follow a natural long-tailed distribution. Existing approaches to long-tailed detection resort to external ImageNet labels to augment the low-shot training instances. However, such dependency on a large labeled database is impractical and has limited utility in realistic scenarios. We propose a more versatile approach to leverage optional unlabeled images, which are easy to collect without the burden of human annotations. Our framework is straightforward and intuitive, and consists of three simple steps: (1) pre-training on abundant head classes; (2) transfer learning on scarce tail classes; and (3) fine-tuning on a sampled set of both head and tail classes. Our approach can be viewed as an improved head-to-tail model transfer paradigm without the added complexities of meta-learning or knowledge distillation, as was required in past research. By harnessing supplementary unlabeled images, without the need for extra image labels, SimLTD establishes new record results on the challenging LVIS v1 benchmark across both supervised and semi-supervised settings.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of object detection in scenarios where object classes follow a long-tailed distribution, a common issue in real-world datasets. Traditional methods rely on external labeled datasets like ImageNet to augment training data for rare classes, which is impractical. The authors propose SimLTD, a semi-supervised approach that utilizes unlabeled images to improve detection performance without additional labeling. The method involves pre-training on abundant head classes, transferring learning to scarce tail classes, and fine-tuning on a balanced set of head and tail classes. This straightforward approach avoids the complexities of meta-learning or knowledge distillation and achieves state-of-the-art results on the LVIS v1 benchmark in both supervised and semi-supervised settings.",
        "Tags": [
            "Long-Tail Learning",
            "Object Detection",
            "Semi-Supervised Learning",
            "Transfer Learning",
            "Fine-Tuning"
        ]
    },
    {
        "Title": "Image Quality Assessment: Investigating Causal Perceptual Effects with Abductive Counterfactual Inference",
        "Authors": "Wenhao Shen \u00b7 Mingliang Zhou \u00b7 Yu Chen \u00b7 Xuekai WEI \u00b7 Yong Feng \u00b7 Huayan Pu \u00b7 Weijia Jia",
        "Abstract": "Existing full-reference image quality assessment (FR-IQA) methods often fail to capture the complex causal mechanisms that underlie human perceptual responses to image distortions, limiting their ability to generalize across diverse scenarios. In this paper, we propose an FR-IQA method based on abductive counterfactual inference to investigate the causal relationships between deep network features and perceptual distortions. First, we explore the causal effects of deep features on perception and integrate causal reasoning with feature comparison, constructing a model that effectively handles complex distortion types across different IQA scenarios. Second, the analysis of the perceptual causal correlations of our proposed method is independent of the backbone architecture and thus can be applied to a variety of deep networks. Through abductive counterfactual experiments, we validate the proposed causal relationships, confirming the model's superior perceptual relevance and interpretability of quality scores. The experimental results demonstrate the robustness and effectiveness of the method, providing competitive quality predictions across multiple benchmarks. The source code is available at https://anonymous.4open.science/r/DeepCausalQuality-25BC.",
        "Link": "https://anonymous.4open.science/r/DeepCausalQuality-25BC",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a novel full-reference image quality assessment (FR-IQA) method that leverages abductive counterfactual inference to explore causal relationships between deep network features and perceptual distortions. The proposed approach integrates causal reasoning with feature comparison, enabling effective handling of complex distortion types across various IQA scenarios. The method's analysis of perceptual causal correlations is independent of the backbone architecture, making it adaptable to different deep networks. Abductive counterfactual experiments validate the causal relationships, demonstrating the model's superior perceptual relevance and interpretability of quality scores. The method shows robustness and effectiveness, providing competitive quality predictions across multiple benchmarks.",
        "Tags": [
            "Image Quality Assessment",
            "Causal Inference",
            "Abductive Counterfactual Inference",
            "Perceptual Distortions",
            "Deep Network Features"
        ]
    },
    {
        "Title": "GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs",
        "Authors": "Yi Fang \u00b7 Bowen Jin \u00b7 Jiacheng Shen \u00b7 Sirui Ding \u00b7 Qiaoyu Tan \u00b7 Jiawei Han",
        "Abstract": "The rapid development of Multimodal Large Language Models (MLLMs) has enabled the integration of multiple modalities, including texts and images, within the large language model (LLM) framework.However, texts and images are usually interconnected, forming a multimodal attributed graph (MMAG).It is underexplored how MLLMs can incorporate the relational information (i.e., graph structure) and semantic information (i.e., texts and images) on such graphs for multimodal comprehension and generation.In this paper, we propose GraphGPT-o, which supports omni-multimodal understanding and creation on MMAGs.We first comprehensively study linearization variants to transform semantic and structural information as input for MLLMs.Then, we propose a hierarchical aligner that enables deep graph encoding, bridging the gap between MMAGs and MLLMs.Finally, we explore the inference choices, adapting MLLM to interleaved text and image generation in graph scenarios. Extensive experiments on three datasets from different domains demonstrate the effectiveness of our proposed method.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces GraphGPT-o, a novel approach for omni-multimodal understanding and generation on multimodal attributed graphs (MMAGs). It addresses the challenge of integrating relational (graph structure) and semantic (texts and images) information within Multimodal Large Language Models (MLLMs). The method involves studying linearization variants for input transformation, proposing a hierarchical aligner for deep graph encoding, and adapting MLLMs for interleaved text and image generation in graph scenarios. The effectiveness of GraphGPT-o is demonstrated through extensive experiments across various domains.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Graph Neural Networks (GNNs)",
            "Multimodal Attributed Graphs (MMAGs)",
            "Hierarchical Aligner",
            "Interleaved Text and Image Generation"
        ]
    },
    {
        "Title": "CALICO: Multi-Image Pixel-Grounded Object Comparison by Parts with Large Language Models",
        "Authors": "Kiet A. Nguyen \u00b7 Adheesh Juvekar \u00b7 Tianjiao Yu \u00b7 Muntasir Wahed \u00b7 Ismini Lourentzou",
        "Abstract": "Recent advances in Large Vision-Language Models (LVLMs) advances have sparked significant progress in general-purpose vision tasks through visual instruction tuning. While some works have demonstrated the capability of LVLMs to generate segmentation masks that align phrases with natural language descriptions in a single image, they struggle with segmentation-grounded comparisons across multiple images, particularly at finer granularities such as object parts. In this paper, we introduce the new task of *part-focused semantic co-segmentation*, which seeks to identify and segment common and unique objects and parts across multiple images. To address this task, we present CALICO, the first LVLM that can segment and reason over multiple masks across images, enabling object comparison based on their constituent parts. CALICO features two novel components, a Correspondence Extraction Module, which captures semantic-rich information to identify part-level correspondences between objects, and a Correspondence Adaptation Module, which embeds this information into the LLM and facilitates multi-image understanding in a parameter-efficient manner. To support training and evaluation, we curate MIXEDPARTS, a comprehensive multi-image segmentation dataset containing $\\sim$2.4M samples across $\\sim$44K images with diverse object and part categories. Experimental results show CALICO, finetuned on only 0.3\\% of its architecture, achieves robust performance in part-focused semantic co-segmentation. Code, models, and data are available at anon.link.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces CALICO, a Large Vision-Language Model (LVLM) designed for part-focused semantic co-segmentation, a novel task that identifies and segments common and unique objects and their parts across multiple images. CALICO incorporates two innovative modules: a Correspondence Extraction Module for identifying part-level correspondences and a Correspondence Adaptation Module for embedding this information into the model efficiently. The model is supported by MIXEDPARTS, a new dataset with ~2.4M samples across ~44K images. CALICO demonstrates robust performance in part-focused semantic co-segmentation, even when fine-tuned on a minimal portion of its architecture.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Semantic Segmentation",
            "Large Language Models (LLMs)",
            "Part-Level Segmentation",
            "Multi-Image Understanding",
            "Parameter-Efficient Learning"
        ]
    },
    {
        "Title": "VidBot: Learning Generalizable 3D Actions from In-the-Wild 2D Human Videos for Zero-Shot Robotic Manipulation",
        "Authors": "Hanzhi Chen \u00b7 Boyang Sun \u00b7 Anran Zhang \u00b7 Marc Pollefeys \u00b7 Stefan Leutenegger",
        "Abstract": "Future robots are envisioned as versatile systems capable of performing a variety of household tasks. The big question remains, how can we bridge the embodiment gap while minimizing physical robot learning, which fundamentally does not scale well. We argue that learning from in-the-wild human videos offers a promising solution for robotic manipulation tasks, as vast amounts of relevant data already exist on the internet. In this work, we present VidBot, a framework enabling zero-shot robotic manipulation using learned 3D affordance from in-the-wild monocular RGB-only human videos. VidBot leverages a pipeline to extract explicit representations from them, namely 3D hand trajectories from videos, combining a depth foundation model with structure-from-motion techniques to reconstruct temporally consistent, metric-scale 3D affordance representations agnostic to embodiments. We introduce a coarse-to-fine affordance learning model that first identifies coarse actions from the pixel space and then generates fine-grained interaction trajectories with a diffusion model, conditioned on coarse actions and guided by test-time constraints for context-aware interaction planning, enabling substantial generalization to novel scenes and embodiments. Extensive experiments demonstrate the efficacy of VidBot, which significantly outperforms counterparts across 13 manipulation tasks in zero-shot settings and can be seamlessly deployed across robot systems in real-world environments. VidBot paves the way for leveraging everyday human videos to make robot learning more scalable. Our project will be open-sourced upon acceptance.",
        "Link": "https://hanzhic.github.io/vidbot-project/",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "VidBot is a framework designed to enable zero-shot robotic manipulation by learning 3D affordances from in-the-wild monocular RGB human videos. The system extracts 3D hand trajectories using a depth foundation model combined with structure-from-motion techniques, creating metric-scale 3D affordance representations. VidBot employs a coarse-to-fine affordance learning model that identifies coarse actions from pixel space and generates fine-grained interaction trajectories using a diffusion model, guided by test-time constraints for context-aware planning. This approach allows for substantial generalization to novel scenes and embodiments, demonstrating superior performance across 13 manipulation tasks in zero-shot settings. VidBot highlights the potential of leveraging everyday human videos to scale robot learning effectively.",
        "Tags": [
            "Embodied AI",
            "Zero-Shot Learning",
            "3D Hand Trajectories",
            "Diffusion Models",
            "Context-Aware Planning"
        ]
    },
    {
        "Title": "GFlowVLM: Enhancing Multi-step Reasoning in Vision-Language Models with Generative Flow Networks",
        "Authors": "Haoqiang Kang \u00b7 Enna Sachdeva \u00b7 Piyush Gupta \u00b7 Sangjae Bae \u00b7 Kwonjoon Lee",
        "Abstract": "Vision-Language Models (VLMs) have recently shown promising advancements in sequential decision-making tasks through task-specific fine-tuning. However, common fine-tuning methods, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) techniques like Proximal Policy Optimization (PPO), present notable limitations: SFT assumes Independent and Identically Distributed (IID) data, while PPO focuses on maximizing cumulative rewards. These limitations often restrict solution diversity and hinder generalization in multi-step reasoning tasks. To address these challenges, we introduce a novel framework, GFlowVLM, a framework that fine-tune VLMs using Generative Flow Networks (GFlowNets) to promote generation of diverse solutions for complex reasoning tasks. GFlowVLM models the environment as a non-Markovian decision process, allowing it to capture long-term dependencies essential for real-world applications. It takes observations and task descriptions as inputs to prompt chain-of-thought (CoT) reasoning which subsequently guides action selection. We use task based rewards to fine-tune VLM with GFlowNets. This approach enables VLMs to outperform prior fine-tuning methods, including SFT and RL. Empirical results demonstrate the effectiveness of GFlowVLM on complex tasks such as card games (NumberLine, BlackJack) and embodied planning tasks (ALFWorld), showing enhanced training efficiency, solution diversity, and stronger generalization capabilities across both in-distribution and out-of-distribution scenarios.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces GFlowVLM, a novel framework that fine-tunes Vision-Language Models (VLMs) using Generative Flow Networks (GFlowNets) to enhance multi-step reasoning tasks. Unlike traditional fine-tuning methods such as Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) techniques like Proximal Policy Optimization (PPO), GFlowVLM addresses limitations in solution diversity and generalization by modeling the environment as a non-Markovian decision process. This approach captures long-term dependencies and uses task-based rewards to guide chain-of-thought (CoT) reasoning and action selection. GFlowVLM demonstrates superior performance in complex tasks like card games and embodied planning, achieving improved training efficiency, solution diversity, and generalization across in-distribution and out-of-distribution scenarios.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Generative Flow Networks",
            "Non-Markovian Decision Process",
            "Chain-of-Thought Reasoning",
            "Task-Based Rewards"
        ]
    },
    {
        "Title": "Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction",
        "Authors": "Rui Qian \u00b7 Shuangrui Ding \u00b7 Xiaoyi Dong \u00b7 Pan Zhang \u00b7 Yuhang Zang \u00b7 Yuhang Cao \u00b7 Dahua Lin \u00b7 Jiaqi Wang",
        "Abstract": "Active Real-time interaction with video LLMs introduces a new paradigm for human-computer interaction, where the model not only understands user intent but also responds while continuously processing streaming video on the fly. Unlike offline video LLMs, which analyze the entire video before answering questions, active real-time interaction requires three capabilities: 1) Perception: real-time video monitoring and interaction capturing. 2) Decision: raising proactive interaction in proper situations, 3) Reaction: continuous interaction with users. However, inherent conflicts exist among the desired capabilities. The Decision and Reaction require a contrary Perception scale and grain, and the autoregressive decoding blocks the real-time Perception and Decision during the Reaction. To unify the conflicted capabilities within a harmonious system, we present Dispider, a solution built on a Disentangled Perception, Decision, and Reaction framework. Dispider features a lightweight Proactive Streaming Video Processing module that tracks the video stream and identifies optimal moments for interaction. Once the interaction is triggered, an asynchronous Precise Interaction module provides detailed responses, while the processing module continues to monitor the video in the meantime. Our disentangled and asynchronous design ensures timely, contextually accurate, and computationally efficient responses, making Dispider ideal for active real-time interaction for long-duration video streams. Experiments prove that Dispider outperforms existing methods not only in its superior understanding of video content in conventional video QA settings, but also in proactive response capability and temporal awareness under the streaming setting.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Dispider introduces a novel framework for active real-time interaction with video LLMs, enabling models to understand user intent and respond while processing streaming video. The framework disentangles three key capabilities: Perception (real-time video monitoring), Decision (proactive interaction), and Reaction (continuous user interaction). Dispider employs a lightweight Proactive Streaming Video Processing module to track video streams and identify optimal interaction moments, alongside an asynchronous Precise Interaction module for detailed responses. This design ensures timely, accurate, and efficient responses, making Dispider suitable for long-duration video streams. The system outperforms existing methods in video content understanding, proactive response capability, and temporal awareness.",
        "Tags": [
            "Video Understanding",
            "Large Language Models (LLMs)",
            "Multimodal Learning",
            "Real-Time Interaction",
            "Asynchronous Processing",
            "Streaming Video Analysis"
        ]
    },
    {
        "Title": "Detecting Out-of-Distribution through the Lens of Neural Collapse",
        "Authors": "Litian Liu \u00b7 Yao Qin",
        "Abstract": "Out-of-Distribution (OOD) detection is critical for safe deployment; however, existing detectors often struggle to generalize across datasets of varying scales and model architectures, and some can incur high computational costs in real-world applications. Inspired by the phenomenon of Neural Collapse, we propose a versatile and efficient OOD detection method. Specifically, we re-characterize prior observations that in-distribution (ID) samples form clusters, demonstrating that, with appropriate centering, these clusters align closely with model weight vectors. Additionally, we reveal that ID features tend to expand into a simplex Equiangular Tight Frame, explaining the common observation that ID features are situated farther from the origin than OOD features. Incorporating both insights from Neural Collapse, our OOD detector leverages feature proximity to weight vectors and complements this approach by using feature norms to effectively filter out OOD samples. Extensive experiments on off-the-shelf models demonstrate the robustness of our OOD detector across diverse scenarios, mitigating generalization discrepancies and enhancing overall performance, with inference latency comparable to that of the basic softmax-confidence detector.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Out-of-Distribution (OOD) detection is essential for safe deployment, but existing methods often fail to generalize across datasets and architectures, and some are computationally expensive. Inspired by Neural Collapse, this paper proposes an efficient and versatile OOD detection method. The authors re-characterize in-distribution (ID) samples as forming clusters that align closely with model weight vectors when appropriately centered. They also show that ID features expand into a simplex Equiangular Tight Frame, explaining why ID features are farther from the origin than OOD features. The proposed OOD detector uses feature proximity to weight vectors and feature norms to effectively filter out OOD samples. The method demonstrates robustness across diverse scenarios, mitigates generalization discrepancies, and achieves inference latency comparable to basic softmax-confidence detectors.",
        "Tags": [
            "Out-of-Distribution (OOD) Detection",
            "Neural Collapse",
            "Feature Proximity",
            "Simplex Equiangular Tight Frame",
            "Inference Efficiency"
        ]
    },
    {
        "Title": "FALCON: Fairness Learning via Contrastive Attention Approach to Continual Semantic Scene Understanding",
        "Authors": "Thanh-Dat Truong \u00b7 Utsav Prabhu \u00b7 Bhiksha Raj \u00b7 Jackson Cothren \u00b7 Khoa Luu",
        "Abstract": "Continual Learning in semantic scene segmentation aims to continually learn new unseen classes in dynamic environments while maintaining previously learned knowledge. Prior studies focused on modeling the catastrophic forgetting and background shift challenges in continual learning. However, fairness, another major challenge that causes unfair predictions leading to low performance among major and minor classes, still needs to be well addressed. In addition, prior methods have yet to model the unknown classes well, thus resulting in producing non-discriminative features among unknown classes. This work presents a novel Fairness Learning via Contrastive Attention Approach to continual learning in semantic scene understanding. In particular, we first introduce a new Fairness Contrastive Clustering loss to address the problems of catastrophic forgetting and fairness. Then, we propose an attention-based visual grammar approach to effectively model the background shift problem and unknown classes, producing better feature representations for different unknown classes. Through our experiments, our proposed approach achieves State-of-the-Art (SoTA) performance on different continual learning benchmarks, i.e., ADE20K, Cityscapes, and Pascal VOC. It promotes the fairness of the continual semantic segmentation model.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces FALCON, a novel approach to continual learning in semantic scene understanding that addresses fairness, catastrophic forgetting, and background shift challenges. The method employs a Fairness Contrastive Clustering loss to mitigate catastrophic forgetting and improve fairness among classes. Additionally, an attention-based visual grammar approach is proposed to better model background shifts and unknown classes, enhancing feature representation. The approach achieves state-of-the-art performance on benchmarks like ADE20K, Cityscapes, and Pascal VOC, promoting fairness in continual semantic segmentation.",
        "Tags": [
            "Semantic Segmentation",
            "Continual Learning",
            "Fairness Learning",
            "Contrastive Attention",
            "Visual Grammar"
        ]
    },
    {
        "Title": "CheXwhatsApp: A Dataset for Exploring Challenges in the Diagnosis of Chest X-rays through Mobile Devices",
        "Authors": "Mariamma Antony \u00b7 Rajiv Porana \u00b7 Sahil M. Lathiya \u00b7 Siva Teja Kakileti \u00b7 Chiranjib Bhattacharyya",
        "Abstract": "Mobile health (mHealth) has emerged as a transformative solution to enhance healthcare accessibility and affordability, particularly in resource-constrained regions and low-to-middle-income countries.mHealth leverages mobile platforms to improve healthcare accessibility, addressing radiologist shortages in low-resource settings by enabling remote diagnosis and consultation through mobile devices. Mobile phones allow healthcare workers to transmit radiographic images, such as chest X-rays (CXR), to specialists or AI-driven models for interpretation. However, AI-based diagnosis using CXR images shared via apps like WhatsApp suffers from reduced predictability and explainability due to compression artifacts, and there is a lack of datasets to systematically study these challenges. To address this, we introduce CheXwhatsApp, a dataset of 175,029 paired original and WhatsApp-compressed CXR images. We present a benchmarking study which shows the dataset improves prediction stability and explainability of state-of-the-art models by up to 80%, while also enhancing localization performance. CheXwhatsApp is open-sourced to support advancements in mHealth applications for CXR analysis.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces CheXwhatsApp, a dataset of 175,029 paired original and WhatsApp-compressed chest X-ray (CXR) images, aimed at addressing challenges in AI-based diagnosis using mobile devices. The dataset improves prediction stability and explainability of state-of-the-art models by up to 80%, enhancing localization performance. It supports advancements in mobile health (mHealth) applications for CXR analysis, particularly in resource-constrained regions.",
        "Tags": [
            "Medical Image Analysis",
            "Datasets and Benchmarks",
            "Image Compression",
            "Explainable AI",
            "Remote Diagnosis"
        ]
    },
    {
        "Title": "From Faces to Voices: Learning Hierarchical Representations for High-quality Video-to-Speech",
        "Authors": "Jihoon Kim \u00b7 Jeongsoo Choi \u00b7 Jaehun Kim \u00b7 Chaeyoung Jung \u00b7 Joon Chung",
        "Abstract": "The objective of this study is to generate high-quality speech from silent talking face videos, a task also known as video-to-speech synthesis.A significant challenge in video-to-speech synthesis lies in the substantial modality gap between silent video and multi-faceted speech. In this paper, we propose a novel video-to-speech system that effectively bridges this modality gap, significantly enhancing the quality of synthesized speech.This is achieved by learning of hierarchical representations from video to speech.Specifically, we gradually transform silent video into acoustic feature spaces through three sequential stages--content, timbre, and prosody modeling.In each stage, we align visual factors---lip movements, face identity, and facial expressions---with corresponding acoustic counterparts to ensure the seamless transformation.Additionally, to generate realistic and coherent speech from the visual representations, we employ a flow matching model that estimates direct trajectories from a simple prior distribution to the target speech distribution.Extensive experiments demonstrate that our method achieves exceptional generation quality comparable to real utterances, outperforming existing methods by a significant margin.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This study focuses on generating high-quality speech from silent talking face videos, addressing the challenge of bridging the modality gap between visual and acoustic data. The proposed system learns hierarchical representations through three stages\u2014content, timbre, and prosody modeling\u2014aligning visual factors (lip movements, face identity, and facial expressions) with corresponding acoustic features. A flow matching model is employed to generate realistic speech by estimating trajectories from a prior distribution to the target speech distribution. The method achieves exceptional speech generation quality, surpassing existing approaches.",
        "Tags": [
            "Video-to-Speech Synthesis",
            "Multimodal Learning",
            "Hierarchical Representation Learning",
            "Flow Matching Model",
            "Acoustic Feature Alignment"
        ]
    },
    {
        "Title": "Optimizing for the Shortest Path in Denoising Diffusion Model",
        "Authors": "Ping Chen \u00b7 Xingpeng Zhang \u00b7 Zhaoxiang Liu \u00b7 Huan Hu \u00b7 Xiang Liu \u00b7 Kai Wang \u00b7 Min Wang \u00b7 Yanlin Qian \u00b7 Shiguo Lian",
        "Abstract": "In this research, we propose a novel denoising diffusion model based on shortest-path modeling that optimizes residual propagation to enhance both denoising efficiency and quality. Drawing on Denoising Diffusion Implicit Models (DDIM) and insights from graph theory, our model, termed the Shortest Path Diffusion Model (ShortDF), treats the denoising process as a shortest-path problem aimed at minimizing reconstruction error. By optimizing the initial residuals, we improve the efficiency of the reverse diffusion process and the quality of the generated samples. Extensive experiments on multiple standard benchmarks demonstrate that ShortDF significantly reduces diffusion time (or steps) while enhancing the visual fidelity of generated samples compared to prior methods. This work, we suppose, paves the way for interactive diffusion-based applications and establishes a foundation for rapid data generation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This research introduces the Shortest Path Diffusion Model (ShortDF), a novel denoising diffusion model that optimizes residual propagation to improve denoising efficiency and quality. By framing the denoising process as a shortest-path problem, ShortDF minimizes reconstruction error and enhances the reverse diffusion process. The model significantly reduces diffusion time while improving the visual fidelity of generated samples, offering potential for interactive diffusion-based applications and rapid data generation.",
        "Tags": [
            "Diffusion Models",
            "Denoising",
            "Shortest-Path Optimization",
            "Residual Propagation",
            "Reconstruction Error Minimization"
        ]
    },
    {
        "Title": "Do ImageNet-trained models learn shortcuts? The impact of frequency shortcuts on generalization",
        "Authors": "Shunxin Wang \u00b7 Raymond Veldhuis \u00b7 Nicola Strisciuglio",
        "Abstract": "Frequency shortcuts refer to specific frequency patterns that models heavily rely on for correct classification. Previous studies have shown that models trained on small image datasets often exploit such shortcuts, potentially impairing their generalization performance. However,  existing methods for identifying frequency shortcuts require  expensive computations and become impractical for analyzing models trained on large datasets.  In this work, we propose the first approach to more efficiently analyze frequency shortcuts at a larger scale.  We show that both CNN and transformer models learn frequency shortcuts on ImageNet. We also expose that frequency shortcut solutions can yield good performance on out-of-distribution (OOD) test sets which largely retain texture information. However, these shortcuts, mostly aligned with texture patterns, hinder model generalization on rendition-based OOD test sets. These observations suggest that current OOD evaluations often overlook the impact of frequency shortcuts on model generalization. Future benchmarks could thus benefit from explicitly assessing and accounting for these shortcuts to build models that generalize across a broader range of OOD scenarios.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This study investigates the phenomenon of frequency shortcuts, where models rely on specific frequency patterns for classification, potentially harming generalization. The authors introduce a novel method to efficiently analyze these shortcuts in models trained on large datasets like ImageNet. They demonstrate that both CNN and transformer models learn frequency shortcuts, which perform well on out-of-distribution (OOD) test sets retaining texture information but hinder generalization on rendition-based OOD sets. The findings suggest that current OOD evaluations often neglect the impact of frequency shortcuts, advocating for future benchmarks to explicitly assess these shortcuts to improve model generalization across diverse OOD scenarios.",
        "Tags": [
            "Frequency Shortcuts",
            "Generalization",
            "ImageNet",
            "CNN",
            "Transformer",
            "Texture Patterns",
            "Out-of-Distribution Generalization",
            "Model Evaluation"
        ]
    },
    {
        "Title": "Uni-Renderer: Unifying Rendering and Inverse Rendering Via Dual Stream Diffusion",
        "Authors": "ZhiFei Chen \u00b7 Tianshuo Xu \u00b7 Wenhang Ge \u00b7 Leyi Wu \u00b7 Dongyu Yan \u00b7 Jing He \u00b7 Luozhou Wang \u00b7 Lu Zeng \u00b7 Shunsi Zhang \u00b7 Ying-Cong Chen",
        "Abstract": "Rendering and inverse rendering are pivotal tasks in both computer vision and graphics. The rendering equation is the core of the two tasks, as an ideal conditional distribution transfer function from intrinsic properties to RGB images. Despite achieving promising results of existing rendering methods, they merely approximate the ideal estimation for a specific scene and come with a high computational cost. Additionally, the inverse conditional distribution transfer is intractable due to the inherent ambiguity. To address these challenges, we propose a data-driven method that jointly models rendering and inverse rendering as two conditional generation tasks within a single diffusion framework. Inspired by UniDiffuser, we utilize two distinct time schedules to model both tasks, and with a tailored dual streaming module, we achieve cross-conditioning of two pre-trained diffusion models. This unified approach, named Uni-Renderer, allows the two processes to facilitate each other through a cycle-consistent constrain, mitigating ambiguity by enforcing consistency between intrinsic properties and rendered images.Combined with a meticulously prepared dataset, our method effectively decomposition of intrinsic properties and demonstrating a strong capability to recognize changes during rendering. We will open-source our training and inference code to the public, fostering further research and development in this area.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Rendering and inverse rendering are critical tasks in computer vision and graphics, traditionally handled separately with high computational costs and inherent ambiguities. This paper introduces Uni-Renderer, a novel data-driven method that unifies rendering and inverse rendering within a single diffusion framework. By employing two distinct time schedules and a dual streaming module, Uni-Renderer facilitates cross-conditioning between pre-trained diffusion models, enhancing consistency between intrinsic properties and rendered images through a cycle-consistent constraint. This approach not only mitigates the ambiguity in inverse rendering but also improves the decomposition of intrinsic properties, demonstrating robust capabilities in recognizing changes during rendering.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Diffusion Models",
            "Cycle-Consistent Constraint",
            "Dual Streaming Module",
            "Unified Rendering Framework"
        ]
    },
    {
        "Title": "One-way ticket: Time-Independent Unified Encoder for Distilling Text-to-Image Diffusion Models",
        "Authors": "Senmao Li \u00b7 Lei Wang \u00b7 Kai Wang \u00b7 Tao Liu \u00b7 Jiehang Xie \u00b7 Joost van de Weijer \u00b7 Fahad Shahbaz Khan \u00b7 Shiqi Yang \u00b7 Yaxing Wang \u00b7 Jian Yang",
        "Abstract": "Text-to-Image (T2I) diffusion models have made remarkable advancements in generative modeling; however, they face a trade-off between inference speed and image quality, posing challenges for efficient deployment. Existing distilled T2I models can generate high-fidelity images with fewer sampling steps, but often struggle with diversity and quality, especially in one-step models. From our analysis, we observe redundant computations in the UNet encoders. Our findings suggest that, for T2I diffusion models, decoders are more adept at capturing richer and more explicit semantic information, while encoders can be effectively shared across decoders from diverse time steps.Based on these observations, we introduce the first Time-independent Unified Encoder (TiUE) for the student model UNet architecture, which is a loop-free image generation approach for distilling T2I diffusion models. Using a one-pass scheme, TiUE shares encoder features across multiple decoder time steps, enabling parallel sampling and significantly reducing inference time complexity. In addition, we incorporate a KL divergence term to regularize noise prediction, which enhances the perceptual realism and diversity of the generated images. Experimental results demonstrate that TiUE outperforms state-of-the-art methods, including LCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results while maintaining the computational efficiency.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Text-to-Image (T2I) diffusion models face a trade-off between inference speed and image quality, with existing distilled models often struggling with diversity and quality. This paper identifies redundant computations in UNet encoders and proposes a Time-independent Unified Encoder (TiUE) for the student model UNet architecture. TiUE shares encoder features across multiple decoder time steps, enabling parallel sampling and reducing inference time complexity. A KL divergence term is incorporated to regularize noise prediction, enhancing perceptual realism and diversity. TiUE outperforms state-of-the-art methods, producing more diverse and realistic results while maintaining computational efficiency.",
        "Tags": [
            "Diffusion Models",
            "Text-to-Image Generation",
            "UNet Architecture",
            "KL Divergence Regularization",
            "Parallel Sampling"
        ]
    },
    {
        "Title": "TIMotion: Temporal and Interactive Framework for Efficient Human-Human Motion Generation",
        "Authors": "Yabiao Wang \u00b7 Shuo Wang \u00b7 Jiangning Zhang \u00b7 Ke Fan \u00b7 Jiafu Wu \u00b7 Xuezhucun \u00b7 Yong Liu",
        "Abstract": "Human-human motion generation is essential for understanding humans as social beings. Current methods fall into two main categories: single-person-based methods and separate modeling-based methods. To delve into this field, we abstract the overall generation process into a general framework MetaMotion, which consists of two phases: temporal modeling and interaction mixing. For temporal modeling, the single-person-based methods concatenate two people into a single one directly, while the separate modeling-based methods skip the modeling of interaction sequences. The inadequate modeling described above resulted in sub-optimal performance and redundant model parameters. In this paper, we introduce TIMotion (Temporal and Interactive Modeling), an efficient and effective framework for human-human motion generation. Specifically, we first propose Causal Interactive Injection to model two separate sequences as a causal sequence leveraging the temporal and causal properties. Then we present Role-Evolving Scanning to adjust to the change in the active and passive roles throughout the interaction. Finally, to generate smoother and more rational motion, we design Localized Pattern Amplification to capture short-term motion patterns.Extensive experiments on InterHuman and InterX demonstrate that our method achieves superior performance. The project code will be released upon acceptance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces TIMotion, a novel framework for human-human motion generation that addresses the limitations of existing methods, which either concatenate individuals into a single entity or neglect interaction modeling. TIMotion integrates temporal and interactive modeling through three key components: Causal Interactive Injection for causal sequence modeling, Role-Evolving Scanning to adapt to role changes during interactions, and Localized Pattern Amplification for capturing short-term motion patterns. This approach results in smoother and more rational motion generation, as demonstrated by superior performance on the InterHuman and InterX datasets.",
        "Tags": [
            "Human Action Prediction",
            "Multimodal Learning",
            "Causal Interactive Injection",
            "Role-Evolving Scanning",
            "Localized Pattern Amplification"
        ]
    },
    {
        "Title": "GroundingFace: Fine-grained Face Understanding via Pixel Grounding Multimodal Large Language Model",
        "Authors": "Yue Han \u00b7 Jiangning Zhang \u00b7 Junwei Zhu \u00b7 Runze Hou \u00b7 Xiaozhong Ji \u00b7 Chuming Lin \u00b7 Xiaobin Hu \u00b7 Xuezhucun \u00b7 Yong Liu",
        "Abstract": "Multimodal Language Learning Models (MLLMs) have shown remarkable performance in image understanding, generation, and editing, with recent advancements achieving pixel-level grounding with reasoning. However, these models for common objects struggle with fine-grained face understanding. In this work, we introduce the \\textbf{\\textit{FacePlayGround-240K}} dataset, the first pioneering large-scale, pixel-grounded face caption and question-answer (QA) dataset, meticulously curated for alignment pretraining and instruction-tuning. We present the \\textbf{\\textit{GroundingFace}} framework, specifically designed to enhance fine-grained face understanding. This framework significantly augments the capabilities of existing grounding models in face part segmentation, face attribute comprehension, while preserving general scene understanding. Comprehensive experiments validate that our approach surpasses current state-of-the-art models in pixel-grounded face captioning/QA and various downstream tasks, including face captioning, referring segmentation, and zero-shot face attribute recognition.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of fine-grained face understanding using Multimodal Large Language Models (MLLMs), which excel in image tasks but struggle with detailed facial analysis. The authors introduce the FacePlayGround-240K dataset, a large-scale, pixel-grounded dataset for face captioning and question-answering, designed for alignment pretraining and instruction-tuning. They also propose the GroundingFace framework, which enhances fine-grained face understanding, improving face part segmentation and attribute comprehension while maintaining general scene understanding. The framework outperforms state-of-the-art models in tasks like face captioning, referring segmentation, and zero-shot face attribute recognition.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Face Attribute Comprehension",
            "Pixel Grounding",
            "Face Part Segmentation",
            "Zero-Shot Face Attribute Recognition",
            "Referring Segmentation"
        ]
    },
    {
        "Title": "MOS: Modeling Object-Scene Associations in Generalized Category Discovery",
        "Authors": "Zhengyuan Peng \u00b7 Jinpeng Ma \u00b7 Zhimin Sun \u00b7 Ran Yi \u00b7 Haichuan Song \u00b7 Xin Tan \u00b7 Lizhuang Ma",
        "Abstract": "Generalized Category Discovery (GCD) is a classification task that aims to classify both base and novel classes in unlabeled images, using knowledge from a labeled dataset. In GCD, previous research typically treats scene information as noise and minimizes its influence during model training. However, in this paper, we argue that scene information should not be treated as noise, but rather recognized as a strong prior for inferring novel classes. We attribute the misinterpretation of scene information to a key factor: the Ambiguity Challenge inherent in GCD. Specifically, novel objects in base scenes might be wrongly classified into base categories, while base objects in novel scenes might be mistakenly recognized as novel categories. Once the ambiguity challenge is addressed, scene information can reach its full potential, significantly enhancing the performance of GCD models. To more effectively leverage scene information, we propose the Modeling Object-Scene Associations (MOS) framework, which utilizes a simple MLP-based scene-awareness module to enhance GCD performance. It achieves an exceptional average accuracy of  4\\% improvement on the challenging fine-grained datasets compared to state-of-the-art methods, emphasizing its superior performance in GCD tasks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Generalized Category Discovery (GCD) involves classifying both base and novel classes in unlabeled images using knowledge from labeled datasets. Contrary to prior approaches that treat scene information as noise, this paper argues that scene information is a valuable prior for inferring novel classes. The authors identify the Ambiguity Challenge in GCD, where novel objects in base scenes may be misclassified as base categories, and base objects in novel scenes may be misclassified as novel categories. To address this, they propose the Modeling Object-Scene Associations (MOS) framework, which incorporates a scene-awareness module to enhance GCD performance. The framework demonstrates a 4% improvement in average accuracy on fine-grained datasets compared to state-of-the-art methods.",
        "Tags": [
            "Generalized Category Discovery (GCD)",
            "Scene-Awareness",
            "Ambiguity Challenge",
            "MLP-based Scene-Awareness",
            "Fine-Grained Classification"
        ]
    },
    {
        "Title": "Dr. Splat: Directly Referring 3D Gaussian Splatting via Direct Language Embedding Registration",
        "Authors": "JUNSEONG KIM \u00b7 GeonU Kim \u00b7 Kim Yu-Ji \u00b7 Yu-Chiang Frank Wang \u00b7 Jaesung Choe \u00b7 Tae-Hyun Oh",
        "Abstract": "We introduce Dr. Splat, a novel approach for open-vocabulary 3D scene understanding leveraging 3D Gaussian Splatting. Unlike existing language-embedded 3DGS methods, which rely on a rendering process, our method directly associates language-aligned CLIP embeddings with 3D Gaussians for holistic 3D scene understanding. The key of our method is a language feature registration technique where CLIP embeddings are assigned to the dominant Gaussians intersected by each pixel-ray.  Moreover, we integrate Product Quantization (PQ) trained on general large scale image data to compactly represent embeddings without per-scene optimization. Experiments demonstrate that our approach significantly outperforms existing approaches in 3D perception benchmarks, such as open-vocabulary 3D semantic segmentation, 3D object localization, and 3D object selection tasks. Code will be publicly available if accepted.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Dr. Splat introduces a novel approach for open-vocabulary 3D scene understanding using 3D Gaussian Splatting. Unlike existing methods that rely on rendering, Dr. Splat directly associates language-aligned CLIP embeddings with 3D Gaussians, enabling holistic 3D scene understanding. The method employs a language feature registration technique, assigning CLIP embeddings to dominant Gaussians intersected by pixel-rays. Additionally, Product Quantization (PQ) is integrated to compactly represent embeddings without per-scene optimization. The approach demonstrates superior performance in 3D perception benchmarks, including open-vocabulary 3D semantic segmentation, 3D object localization, and 3D object selection tasks.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "CLIP",
            "3D Semantic Segmentation",
            "Open-Vocabulary 3D Scene Understanding",
            "Product Quantization",
            "Language Feature Registration"
        ]
    },
    {
        "Title": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation via Feature Caching",
        "Authors": "Emanuele Aiello \u00b7 Umberto Michieli \u00b7 Diego Valsesia \u00b7 Mete Ozay \u00b7 Enrico Magli",
        "Abstract": "Personalized image generation requires text-to-image generative models that capture the core features of a reference subject to allow for controlled generation across different contexts. Existing methods face challenges due to complex training requirements, high inference costs, limited flexibility, or a combination of these issues. In this paper, we introduce DreamCache, a scalable approach for efficient and high-quality personalized image generation. By caching a small number of reference image features from a subset of layers and a single timestep of the pretrained diffusion denoiser, DreamCache enables dynamic modulation of the generated image features through lightweight, trained conditioning adapters. DreamCache achieves state-of-the-art image and text alignment, utilizing an order of magnitude fewer extra parameters, and is both more computationally effective and versatile than existing models.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "DreamCache introduces a scalable and efficient approach for personalized image generation without the need for finetuning. By caching a small number of reference image features from specific layers and a single timestep of a pretrained diffusion denoiser, DreamCache dynamically modulates generated image features using lightweight conditioning adapters. This method achieves state-of-the-art alignment between images and text, uses significantly fewer parameters, and is computationally efficient and versatile compared to existing models.",
        "Tags": [
            "Diffusion Models",
            "Image Generation",
            "Feature Caching",
            "Lightweight Conditioning",
            "Dynamic Modulation"
        ]
    },
    {
        "Title": "Heterogeneous Skeleton-Based Action Representation Learning",
        "Authors": "Xiaoyan Ma \u00b7 jidong kuang \u00b7 Hongsong Wang \u00b7 Jie Gui",
        "Abstract": "Skeleton-based human action recognition has received widespread attention in recent years due to its diverse range of application scenarios. Due to the different sources of human skeletons, skeleton data naturally exhibit heterogeneity. The previous works, however, overlook the heterogeneity of human skeletons and solely construct models tailored for homogeneous skeletons. This work addresses the challenge of heterogeneous skeleton-based action representation learning, specifically focusing on processing skeleton data that varies in joint dimensions and topological structures. The proposed framework comprises two primary components: heterogeneous skeleton processing and unified representation learning. The former first converts two-dimensional skeleton data into three-dimensional skeleton via an auxiliary network, and then constructs a prompted unified skeleton using skeleton-specific prompts. We also design an additional modality named semantic motion encoding to harness the semantic information within skeletons. The latter module learns a unified action representation using a shared backbone network that processes different heterogeneous skeletons, which have been processed by the former module. Extensive experiments on the NTU-60, NTU-120, and PKU-MMD II datasets demonstrate the effectiveness of our method in various tasks, such as action recognition, action retrieval and semi-supervised action recognition.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a novel framework for heterogeneous skeleton-based action representation learning, addressing the challenge of processing skeleton data with varying joint dimensions and topological structures. The framework consists of two main components: heterogeneous skeleton processing and unified representation learning. The first component transforms two-dimensional skeleton data into three-dimensional skeletons using an auxiliary network and constructs a unified skeleton with skeleton-specific prompts. It also incorporates semantic motion encoding to utilize semantic information within skeletons. The second component employs a shared backbone network to learn a unified action representation from the processed heterogeneous skeletons. The method's effectiveness is demonstrated through tasks such as action recognition, action retrieval, and semi-supervised action recognition on multiple datasets.",
        "Tags": [
            "3D Human Pose Estimation",
            "Action Detection",
            "Heterogeneous Data Processing",
            "Semantic Motion Encoding",
            "Unified Representation Learning"
        ]
    },
    {
        "Title": "Self-Expansion of Pre-trained Models with Mixture of Adapters for Continual Learning",
        "Authors": "Huiyi Wang \u00b7 Haodong Lu \u00b7 Lina Yao \u00b7 Dong Gong",
        "Abstract": "Continual learning (CL) aims to continually accumulate knowledge from a non-stationary data stream without catastrophic forgetting of learned knowledge, requiring a balance between stability and adaptability. Relying on the generalizable representation in pre-trained models (PTMs), PTM-based CL methods perform effective continual adaptation on downstream tasks by adding learnable adapters or prompts upon the frozen PTMs. However, many existing PTM-based CL methods use restricted adaptation on a fixed set of these modules to avoid forgetting, suffering from limited CL ability. Periodically adding task-specific modules results in linear model growth rate and impaired knowledge reuse. We propose Self-Expansion of pre-trained models with Modularized Adaptation (SEMA), a novel approach to enhance the control of stability-plasticity balance in PTM-based CL. SEMA automatically decides to reuse or add adapter modules on demand in CL, depending on whether significant distribution shift that cannot be handled is detected at different representation levels. We design modular adapter consisting of a functional adapter and a representation descriptor. The representation descriptors are trained as a distribution shift indicator and used to trigger self-expansion signals. For better composing the adapters, an expandable weighting router is learned jointly for mixture of adapter outputs. SEMA enables better knowledge reuse and sub-linear expansion rate. Extensive experiments demonstrate the effectiveness of the proposed self-expansion method, achieving state-of-the-art performance compared to PTM-based CL methods without memory rehearsal.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Continual learning (CL) aims to accumulate knowledge from non-stationary data streams without forgetting previously learned information, balancing stability and adaptability. Pre-trained model (PTM)-based CL methods leverage frozen PTMs with learnable adapters or prompts for downstream tasks but often face limitations due to restricted adaptation and linear model growth. This paper introduces Self-Expansion of pre-trained models with Modularized Adaptation (SEMA), a novel approach that dynamically reuses or adds adapter modules based on detected distribution shifts at different representation levels. SEMA employs modular adapters with functional adapters and representation descriptors, trained to indicate distribution shifts and trigger self-expansion. An expandable weighting router is learned to mix adapter outputs, enabling better knowledge reuse and sub-linear expansion. The method achieves state-of-the-art performance in PTM-based CL without memory rehearsal.",
        "Tags": [
            "Continual Learning",
            "Pre-trained Models",
            "Modularized Adaptation",
            "Distribution Shift Detection",
            "Sub-linear Expansion"
        ]
    },
    {
        "Title": "MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through Disentangled Spatial-Temporal Representations",
        "Authors": "Kyungho Bae \u00b7 Jinhyung Kim \u00b7 Sihaeng Lee \u00b7 Soonyoung Lee \u00b7 Gunhee Lee \u00b7 Jinwoo Choi",
        "Abstract": "In this work, we tackle action-scene hallucination in Video Large Language Models (Video-LLMs), where models incorrectly predict actions based on the scene context or scenes based on observed actions. We observe that existing Video-LLMs often suffer from action-scene hallucination due to two main factors. First, existing Video-LLMs intermingle spatial and temporal features by applying an attention operation across all tokens. Second, they use the standard Rotary Position Embedding (RoPE), which causes the text tokens to overemphasize certain types of tokens depending on their sequential orders. To address these issues, we introduce MASH-VLM, Mitigating Action-Scene Hallucination in Video-LLMs through disentangled spatial-temporal representations. Our approach includes two key innovations: (1) DST-attention, a novel attention mechanism that disentangles the spatial and temporal tokens within the LLM by using masked attention to restrict direct interactions between the spatial and temporal tokens; (2) Harmonic-RoPE, which extends the dimensionality of the positional IDs, allowing the spatial and temporal tokens to maintain balanced positions relative to the text tokens. To evaluate the action-scene hallucination in Video-LLMs, we introduce the UNSCENE benchmark with 1,320 videos and 4,078 QA pairs. Extensive experiments demonstrate that MASH-VLM achieves state-of-the-art results on the UNSCENE benchmark, as well as on existing video understanding benchmarks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the issue of action-scene hallucination in Video Large Language Models (Video-LLMs), where models incorrectly predict actions based on scene context or scenes based on observed actions. The authors identify two main causes: the intermingling of spatial and temporal features through attention operations and the overemphasis of certain text tokens due to standard Rotary Position Embedding (RoPE). To mitigate these issues, they propose MASH-VLM, which introduces two key innovations: DST-attention, a novel attention mechanism that disentangles spatial and temporal tokens using masked attention, and Harmonic-RoPE, which extends positional IDs to balance spatial and temporal tokens relative to text tokens. The approach is evaluated using the UNSCENE benchmark, demonstrating state-of-the-art performance on both UNSCENE and existing video understanding benchmarks.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Video Understanding",
            "Disentangled Representations",
            "Attention Mechanisms",
            "Positional Encoding"
        ]
    },
    {
        "Title": "Physical Plausibility-aware Trajectory Prediction via Locomotion Embodiment",
        "Authors": "Hiromu Taketsugu \u00b7 Takeru Oba \u00b7 Takahiro Maeda \u00b7 Shohei Nobuhara \u00b7 Norimichi Ukita",
        "Abstract": "Humans can predict future human trajectories even from momentary observations by using human pose-related cues. However, previous Human Trajectory Prediction (HTP) methods leverage the pose cues implicitly, resulting in implausible predictions. To address this, we propose Locomotion Embodiment, a framework that explicitly evaluates the physical plausibility of the predicted trajectory by locomotion generation under the laws of physics.While the plausibility of locomotion is learned with an indifferentiable physics simulator, it is replaced by our differentiable Locomotion Value function to train an HTP network in a data-driven manner. In particular, our proposed Embodied Locomotion loss is beneficial for efficiently training a stochastic HTP network using multiple heads.Furthermore, the Locomotion Value filter is proposed to filter out implausible trajectories at inference. Experiments demonstrate that our method further enhances even the state-of-the-art HTP methods across diverse datasets and problem settings. Our code will be publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Locomotion Embodiment, a framework for Human Trajectory Prediction (HTP) that explicitly evaluates the physical plausibility of predicted trajectories using locomotion generation under physical laws. The framework employs a differentiable Locomotion Value function to train an HTP network, replacing an indifferentiable physics simulator. It introduces an Embodied Locomotion loss for efficient training of stochastic HTP networks and a Locomotion Value filter to eliminate implausible trajectories during inference. The method enhances state-of-the-art HTP methods across various datasets and settings.",
        "Tags": [
            "Embodied AI",
            "Human Action Prediction",
            "Differentiable Physics Simulation",
            "Stochastic Network Training",
            "Trajectory Filtering"
        ]
    },
    {
        "Title": "Toward Robust Neural Reconstruction from Sparse Point Sets",
        "Authors": "Amine Ouasfi \u00b7 Shubhendu Jena \u00b7 Eric Marchand \u00b7 Adnane Boukhayma",
        "Abstract": "We consider the challenging problem of  learning Signed Distance Functions (SDF) from sparse and noisy 3D point clouds. In contrast to recent methods that depend on smoothness priors, our method, rooted in a distributionally robust optimization (DRO) framework, incorporates a regularization term that leverages samples from the uncertainty regions of the model to improve the learned SDFs. Thanks to tractable dual formulations, we show that this framework enables a stable and efficient optimization of SDFs in the absence of ground truth supervision. Using a variety of synthetic and real data evaluations from different modalities, we show  that of our DRO based learning framework can improve  SDF learning with respect to baselines and the state-of-the-art.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of learning Signed Distance Functions (SDF) from sparse and noisy 3D point clouds. The proposed method employs a distributionally robust optimization (DRO) framework, which includes a regularization term that utilizes samples from model uncertainty regions to enhance SDF learning. The framework allows for stable and efficient optimization of SDFs without ground truth supervision. Evaluations on synthetic and real data demonstrate that the DRO-based approach outperforms existing baselines and state-of-the-art methods in SDF learning.",
        "Tags": [
            "3D Reconstruction",
            "3D Point Cloud",
            "Signed Distance Functions",
            "Distributionally Robust Optimization",
            "Uncertainty Regularization"
        ]
    },
    {
        "Title": "Ev-3DOD: Pushing the Temporal Boundaries of 3D Object Detection with Event Cameras",
        "Authors": "Hoonhee Cho \u00b7 Jae-Young Kang \u00b7 Youngho Kim \u00b7 Kuk-Jin Yoon",
        "Abstract": "Detecting 3D objects in point clouds plays a crucial role in autonomous driving systems. Recently, advanced multi-modal methods incorporating camera information have achieved notable performance. For a safe and effective autonomous driving system, algorithms that excel not only in accuracy but also in speed and low latency are essential. However, existing algorithms fail to meet these requirements due to the latency and bandwidth limitations of fixed frame rate sensors, e.g., LiDAR and camera. To address this limitation, we introduce asynchronous event cameras into 3D object detection for the first time. We leverage their high temporal resolution and low bandwidth to enable high-speed 3D object detection. Our method enables detection even during inter-frame intervals when synchronized data is unavailable, by retrieving previous 3D information through the event camera. Furthermore, we introduce the first event-based 3D object detection dataset, DSEC-3DOD, which includes ground-truth 3D bounding boxes at 100 FPS, establishing the first benchmark for event-based 3D detectors. Our code and dataset will be publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Ev-3DOD, a novel approach to 3D object detection that leverages asynchronous event cameras to overcome the limitations of traditional fixed frame rate sensors like LiDAR and cameras. By utilizing the high temporal resolution and low bandwidth of event cameras, the method enables high-speed 3D object detection, even during inter-frame intervals when synchronized data is unavailable. The authors also present DSEC-3DOD, the first event-based 3D object detection dataset, which includes ground-truth 3D bounding boxes at 100 FPS, setting a new benchmark for event-based 3D detectors.",
        "Tags": [
            "3D Object Detection",
            "Autonomous Driving",
            "Event Cameras",
            "High Temporal Resolution",
            "DSEC-3DOD Dataset"
        ]
    },
    {
        "Title": "4DTAM: Non-Rigid Tracking and Mapping via Surface Gaussian Splatting",
        "Authors": "Hidenobu Matsuki \u00b7 Gwangbin Bae \u00b7 Andrew J. Davison",
        "Abstract": "We propose the first tracking and mapping approach for a single RGB-D camera capable of non-rigid surface reconstruction via differentiable rendering. We perform 4D scene capture from an online stream by joint optimization of geometry, appearance, dynamics, and camera ego-motion. Although the natural environment contains complex non-rigid motions, non-rigid SLAM has remained difficult; even with 2.5D sensor measurements, it is still ill-posed due to the high dimensionality of the optimization problem. Our novel SLAM method based on Gaussian surface primitives allows accurate 3D reconstruction and real-time rendering without any template, using a warp-field represented by a multi-layer perceptron (MLP) and regularization terms to enable spatio-temporal reconstruction. A challenge in non-rigid SLAM research is the lack of publicly available datasets with reliable ground truth and standardized evaluation protocols. To address this, we introduce a novel synthetic dataset of everyday objects featuring diverse motions, leveraging availability of large-scale objects and advancements in animation modeling.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces 4DTAM, a novel approach for non-rigid tracking and mapping using a single RGB-D camera, enabling real-time 4D scene capture through joint optimization of geometry, appearance, dynamics, and camera ego-motion. The method leverages Gaussian surface primitives and a warp-field represented by a multi-layer perceptron (MLP) to achieve accurate 3D reconstruction and rendering without templates. To address the lack of datasets for non-rigid SLAM, the authors also present a synthetic dataset of everyday objects with diverse motions, facilitating standardized evaluation.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "3D Reconstruction",
            "Non-Rigid SLAM",
            "Differentiable Rendering",
            "Synthetic Dataset"
        ]
    },
    {
        "Title": "Can Text-to-Video Generation help Video-Language Alignment?",
        "Authors": "Luca Zanella \u00b7 Massimiliano Mancini \u00b7 Willi Menapace \u00b7 Sergey Tulyakov \u00b7 Yiming Wang \u00b7 Elisa Ricci",
        "Abstract": "Recent video-language alignment models are trained on sets of videos, each with an associated positive caption and a negative caption generated by large language models. A problem with this procedure is that negative captions may introduce linguistic biases, i.e., concepts are seen only as negatives and never associated with a video. While a solution would be to collect videos for the negative captions, existing databases lack the fine-grained variations needed to cover all possible negatives. In this work, we study whether synthetic videos can help to overcome this issue. Our preliminary analysis with multiple generators shows that, while promising on some tasks, synthetic videos harm the performance of the model on others. We hypothesize this issue is linked to noise (semantic and visual) in the generated videos and develop a method, SynViTA, that accounts for those. SynViTA dynamically weights the contribution of each synthetic video based on how similar its target caption is w.r.t. the real counterpart. Moreover, a semantic consistency loss makes the model focus on fine-grained differences across captions, rather than differences in video appearance. Experiments show that, on average, SynViTA improves over existing methods on VideoCon test sets and SSv2-Temporal, SSv2-Events, and ATP-Hard benchmarks, being a first promising step for using synthetic videos when learning video-language models.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper investigates the use of synthetic videos to address linguistic biases in video-language alignment models, which are typically trained with positive and negative captions. The authors propose SynViTA, a method that dynamically weights synthetic videos based on caption similarity and employs a semantic consistency loss to focus on fine-grained caption differences rather than video appearance. Preliminary results indicate that SynViTA improves performance on several benchmarks, suggesting a promising direction for incorporating synthetic videos in video-language model training.",
        "Tags": [
            "Video-Language Models (VLMs)",
            "Text-to-Image Generation",
            "Synthetic Video Generation",
            "Semantic Consistency Loss",
            "Dynamic Weighting"
        ]
    },
    {
        "Title": "A General Adaptive Dual-level Weighting Mechanism for Remote Sensing Pansharpening",
        "Authors": "Jie Huang \u00b7 Haorui Chen \u00b7 Jiaxuan Ren \u00b7 Siran Peng \u00b7 Liang-Jian Deng",
        "Abstract": "Currently, deep learning-based methods for remote sensing pansharpening have advanced rapidly. However, many existing methods struggle to fully leverage feature heterogeneity and redundancy, thereby limiting their effectiveness. To address these challenges across two key dimensions, we introduce a general adaptive dual-level weighting mechanism (ADWM), designed to enhance a wide range of existing deep-learning methods. First, Intra-Feature Weighting (IFW) evaluates correlations among channels within each feature and selectively weighs to reduce redundancy and enhance unique information. Second, Cross-Feature Weighting (CFW) adjusts contributions across layers based on inter-layer correlations, refining the final output by preserving key distinctions across feature depths. This dual-level weighting is efficiently implemented through our proposed Correlation-Aware Covariance Weighting (CACW), which generates weights by utilizing the correlations captured within the covariance matrix. Extensive experiments demonstrate the superior performance of ADWM compared to recent state-of-the-art (SOTA) methods. Furthermore, we validate the effectiveness of our approach through generality experiments, ablation studies, comparison experiments, and detailed visual analysis.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces a general adaptive dual-level weighting mechanism (ADWM) to enhance deep learning-based methods for remote sensing pansharpening. ADWM addresses feature heterogeneity and redundancy through two key components: Intra-Feature Weighting (IFW), which evaluates and weighs channel correlations within features to reduce redundancy and enhance unique information, and Cross-Feature Weighting (CFW), which adjusts contributions across layers based on inter-layer correlations to refine the final output. The mechanism is efficiently implemented using Correlation-Aware Covariance Weighting (CACW), which generates weights based on correlations within the covariance matrix. The approach demonstrates superior performance compared to state-of-the-art methods, validated through extensive experiments.",
        "Tags": [
            "Remote Sensing Image Analysis",
            "Data Augmentation",
            "Feature Weighting",
            "Covariance Matrix",
            "Pansharpening"
        ]
    },
    {
        "Title": "Focus-N-Fix: Region-Aware Fine-Tuning for Text-to-Image Generation",
        "Authors": "Xiaoying Xing \u00b7 Avinab Saha \u00b7 Junfeng He \u00b7 Susan Hao \u00b7 Paul Vicol \u00b7 Moonkyung Ryu \u00b7 Gang Li \u00b7 Sahil Singla \u00b7 Sarah Young \u00b7 Yinxiao Li \u00b7 Feng Yang \u00b7 Deepak Ramachandran",
        "Abstract": "Text-to-image (T2I) generation has made significant advances in recent years, but challenges still remain in the generation of perceptual artifacts, misalignment with complex prompts, and safety. The prevailing approach to address these issues involves collecting human feedback on generated images, training reward models to estimate human feedback, and then fine-tuning T2I models based on the reward models to align them with human preferences. However, while existing reward fine-tuning methods can produce images with higher rewards, they may change model behavior in unexpected ways. For example, fine-tuning for one quality aspect (e.g., safety) may degrade other aspects (e.g., prompt alignment), or may lead to reward hacking (e.g., finding a way to increase rewards without having the intended effect). In this paper, we propose Focus-N-Fix, a region-aware fine-tuning method that trains models to correct only previously problematic image regions. The resulting fine-tuned model generates images with the same high-level structure as the original model but shows significant improvements in regions where the original model was deficient in safety (over-sexualization and violence), plausibility, or other criteria. Our experiments demonstrate that Focus-N-Fix improves these localized quality aspects with little or no degradation to others and typically imperceptible changes in the rest of the image.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Text-to-image (T2I) generation has advanced significantly, yet challenges like perceptual artifacts, prompt misalignment, and safety issues persist. Current methods involve human feedback, reward models, and fine-tuning to align T2I models with human preferences. However, these methods can lead to unintended model behavior, such as degrading other quality aspects or reward hacking. This paper introduces Focus-N-Fix, a region-aware fine-tuning method that corrects only problematic image regions. The fine-tuned model maintains the original high-level structure while improving deficient regions in safety, plausibility, and other criteria. Experiments show that Focus-N-Fix enhances localized quality aspects without degrading others and with minimal changes to the rest of the image.",
        "Tags": [
            "Text-to-Image Generation",
            "Fine-Tuning",
            "Region-Aware Fine-Tuning",
            "Reward Hacking",
            "Localized Quality Improvement"
        ]
    },
    {
        "Title": "PartGen: Part-level 3D Generation and Reconstruction with Multi-view Diffusion Models",
        "Authors": "Minghao Chen \u00b7 Roman Shapovalov \u00b7 Iro Laina \u00b7 Tom Monnier \u00b7 Jianyuan Wang \u00b7 David Novotny \u00b7 Andrea Vedaldi",
        "Abstract": "Text- or image-to-3D generators and 3D scanners can now produce 3D assets with high-quality shapes and textures. These assets typically consist of a single, fused representation, like an implicit neural field, a Gaussian mixture, or a mesh, without any useful structure.However, most applications and creative workflows require assets to be made of several meaningful parts that can be manipulated independently. To address this gap, we introduce ParGen, a novel approach that generates 3D objects composed of meaningful parts starting from text, an image, or an unstructured 3D object. First, given multiple views of a 3D object, generated or rendered, a multi-view diffusion model extracts a set of plausible and view-consistent part segmentations, dividing the object into parts. Then, a second multi-view diffusion model takes each part separately, fills in the occlusions, and uses those completed views for 3D reconstruction by feeding them to a 3D reconstruction network. This completion process considers the context of the entire object to ensure that the parts integrate cohesively. The generative completion model can make up for the information missing due to occlusions; in extreme cases, it can hallucinate entirely invisible parts based on the input 3D asset. We evaluate our method on generated and real 3D assets and show that it outperforms segmentation and part-extraction baselines by a large margin. We also showcase downstream applications such as 3D part editing.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces PartGen, a novel approach for generating 3D objects composed of meaningful parts from text, images, or unstructured 3D objects. The method uses multi-view diffusion models to extract consistent part segmentations and reconstruct each part by filling in occlusions, ensuring cohesive integration of parts. The approach outperforms existing segmentation and part-extraction methods and demonstrates potential for downstream applications like 3D part editing.",
        "Tags": [
            "3D Generation",
            "3D Reconstruction",
            "Multi-view Diffusion Models",
            "Part-level 3D Editing",
            "Occlusion Handling"
        ]
    },
    {
        "Title": "MM-OR: A Large Multimodal Operating Room Dataset for Semantic Understanding of High Intensity Surgical Environments",
        "Authors": "Ege \u00d6zsoy \u00b7 Chantal Pellegrini \u00b7 Tobias Czempiel \u00b7 Felix Tristram \u00b7 Kun yuan \u00b7 David Bani-Harouni \u00b7 Ulrich Eck \u00b7 Benjamin Busam \u00b7 Matthias Keicher \u00b7 Nassir Navab",
        "Abstract": "Operating rooms (ORs) are complex, high-stakes environments requiring precise understanding of interactions among medical staff, tools, and equipment for enhancing surgical assistance, situational awareness, and patient safety. Current datasets fall short in scale, realism and do not capture the multimodal nature of OR scenes, limiting progress in OR modeling. To this end, we introduce MM-OR, a realistic and large-scale multimodal spatiotemporal OR dataset, and the first dataset to enable multimodal scene graph generation. MM-OR captures comprehensive OR scenes containing RGB-D data, detail views, audio, speech transcripts, robotic logs, and tracking data and is annotated with panoptic segmentations, semantic scene graphs, and downstream task labels. Further, we propose MM2SG, the first multimodal large vision-language model for scene graph generation, and through extensive experiments, demonstrate its ability to effectively leverage multimodal inputs. Together, MM-OR and MM2SG establish a new benchmark for holistic OR understanding, and open the path towards multimodal scene analysis in complex, high-stakes environments. We will publish all our code and dataset upon acceptance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces MM-OR, a large-scale multimodal dataset designed for semantic understanding in high-intensity surgical environments, addressing the limitations of current datasets in scale and realism. MM-OR includes RGB-D data, audio, speech transcripts, robotic logs, and tracking data, annotated with panoptic segmentations and semantic scene graphs. The authors also propose MM2SG, a multimodal large vision-language model for scene graph generation, demonstrating its effectiveness in leveraging multimodal inputs. This work sets a new benchmark for holistic operating room understanding and advances multimodal scene analysis in complex environments.",
        "Tags": [
            "Multimodal Learning",
            "Semantic Segmentation",
            "Scene Graph Generation",
            "Medical Image Analysis",
            "RGB-D Data",
            "Panoptic Segmentation",
            "Vision-Language Model"
        ]
    },
    {
        "Title": "Token Cropr: Faster ViTs for Quite a Few Tasks",
        "Authors": "Benjamin Bergner \u00b7 Christoph Lippert \u00b7 Aravindh Mahendran",
        "Abstract": "The adoption of Vision Transformers (ViTs) in resource-constrained applications necessitates improvements in inference throughput.To this end several token pruning and merging approaches have been proposed that improve efficiency by successively reducing the number of tokens.However, it remains an open problem to design a token reduction method that is fast, maintains high performance, and is applicable to various vision tasks.In this work, we present a token pruner that uses auxiliary prediction heads that learn to select tokens end-to-end based on task relevance.These auxiliary heads can be removed after training, leading to throughput close to that of a random pruner.We evaluate our method on image classification, semantic segmentation, object detection, and instance segmentation, and show speedups of 1.5 to 4x with small drops in performance.As a best case, on the ADE20k semantic segmentation benchmark, we observe a 2x speedup relative to the no-pruning baseline, with a negligible performance penalty of 0.1 median mIoU across 5 seeds.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a token pruning method for Vision Transformers (ViTs) aimed at improving inference throughput for resource-constrained applications. The proposed approach uses auxiliary prediction heads to select task-relevant tokens during training, which can be removed post-training to achieve high throughput. The method is evaluated on multiple vision tasks, including image classification, semantic segmentation, object detection, and instance segmentation, demonstrating speedups of 1.5 to 4x with minimal performance degradation. Notably, on the ADE20k semantic segmentation benchmark, a 2x speedup is achieved with only a 0.1 median mIoU drop.",
        "Tags": [
            "Vision Transformer (ViT)",
            "Semantic Segmentation",
            "Object Detection",
            "Instance Segmentation",
            "Token Pruning",
            "Auxiliary Prediction Heads",
            "Inference Efficiency"
        ]
    },
    {
        "Title": "ArticulatedGS: Self-supervised Digital Twin Modeling of Articulated Objects using 3D Gaussian Splatting",
        "Authors": "Guo Junfu \u00b7 Yu Xin \u00b7 Gaoyi Liu \u00b7 Kai Xu \u00b7 Ligang Liu \u00b7 Ruizhen Hu",
        "Abstract": "We tackle the challenge of concurrent reconstruction at the part level with the RGB appearance and estimation of motion parameters for building digital twins of articulated objects using the 3D Gaussian Splatting (3D-GS) method. With two distinct sets of multi-view imagery, each depicting an object in separate static articulation configurations, we reconstruct the articulated object in 3D Gaussian representations with both appearance and geometry information at the same time. Our approach decoupled multiple highly interdependent parameters through a multi-step optimization process, thereby achieving a stable optimization procedure and high-quality outcomes. We introduce ArticulatedGS, a self-supervised, comprehensive framework that autonomously learns to model shapes and appearances at the part level and synchronizes the optimization of motion parameters, all without reliance on 3D supervision, motion cues, or semantic labels. Our experimental results demonstrate that, among comparable methodologies, our approach has achieved optimal outcomes in terms of part segmentation accuracy, motion estimation accuracy, and visual quality.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces ArticulatedGS, a self-supervised framework for creating digital twins of articulated objects using 3D Gaussian Splatting (3D-GS). The method reconstructs objects at the part level with RGB appearance and motion parameters from multi-view imagery of objects in different static configurations. It employs a multi-step optimization process to decouple interdependent parameters, ensuring stable optimization and high-quality results. The framework autonomously models shapes and appearances and synchronizes motion parameter optimization without 3D supervision, motion cues, or semantic labels. The approach outperforms comparable methods in part segmentation accuracy, motion estimation accuracy, and visual quality.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Avatars",
            "Self-Supervised Learning",
            "Digital Twin Modeling",
            "Part-Level Reconstruction",
            "Motion Parameter Estimation"
        ]
    },
    {
        "Title": "Rethinking Noisy Video-Text Retrieval via Relation-aware Alignment",
        "Authors": "Huakai Lai \u00b7 Guoxin Xiong \u00b7 Huayu Mai \u00b7 Xiang Liu \u00b7 Tianzhu Zhang",
        "Abstract": "Video-Text Retrieval (VTR) is a core task in multi-modal understanding, drawing growing attention from both academia and industry in recent years. While numerous VTR methods have achieved success, most of them assume accurate visual-text correspondences during training, which is difficult to ensure in practice due to ubiquitous noise, known as noisy correspondences (NC). In this work, we rethink how to mitigate the NC from the perspective of representative reference features (termed agents), and propose a novel relation-aware purified consistency (RPC) network to amend direct pairwise correlation, including representative agents construction and relation-aware ranking distribution alignment. The proposed RPC enjoys several merits. First, to learn the agents well without any correspondence supervision, we customize the agents construction according to the three characteristics of reliability, representativeness, and resilience. Second, the ranking distribution-based alignment process leverages the structural information inherent in inter-pair relationships, making it more robust compared to individual comparisons. Extensive experiments on five datasets under different settings demonstrate the efficacy and robustness of our method.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Video-Text Retrieval (VTR) is a critical task in multi-modal understanding, but existing methods often rely on accurate visual-text correspondences during training, which are difficult to maintain due to noisy correspondences (NC). This paper addresses the NC problem by introducing a relation-aware purified consistency (RPC) network, which focuses on representative reference features (agents) and relation-aware ranking distribution alignment. The RPC network constructs agents based on reliability, representativeness, and resilience, and aligns ranking distributions using structural information from inter-pair relationships, enhancing robustness. The method demonstrates strong performance across multiple datasets and settings.",
        "Tags": [
            "Video-Text Retrieval",
            "Noisy Correspondences",
            "Representative Agents",
            "Ranking Distribution Alignment",
            "Structural Information"
        ]
    },
    {
        "Title": "_x0008_APT: Adaptive Personalized Training for Diffusion Models with Limited Data",
        "Authors": "JungWoo Chae \u00b7 Jiyoon Kim \u00b7 Jaewoong Choi \u00b7 Kyungyul Kim \u00b7 Sangheum Hwang",
        "Abstract": "Personalizing diffusion models using limited data presents significant challenges, including overfitting, loss of prior knowledge, and degradation of text alignment. Overfitting leads to shifts in the noise prediction distribution, disrupting the denoising trajectory and causing the model to lose semantic coherence. In this paper, we propose Adaptive Personalized Training (APT), a novel framework that mitigates overfitting by employing adaptive training strategies and stabilizing the model's internal representations during fine-tuning. APT consists of three key components: (1) Adaptive Training Adjustment, which introduces an overfitting indicator to detect the degree of overfitting at each time step bin and applies adaptive data augmentation and adaptive loss weighting based on this indicator; (2) Representation Stabilization, which regularizes the mean and variance of intermediate feature maps to prevent excessive shifts in noise prediction; and (3) Attention Alignment for Prior Knowledge Preservation, which aligns the cross-attention maps of the fine-tuned model with those of the pretrained model to maintain prior knowledge and semantic coherence. Through extensive experiments, we demonstrate that APT effectively mitigates overfitting, preserves prior knowledge, and outperforms existing methods in generating high-quality, diverse images with limited reference data.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Personalizing diffusion models with limited data often leads to overfitting, loss of prior knowledge, and degraded text alignment. To address these challenges, the authors propose Adaptive Personalized Training (APT), a framework that mitigates overfitting through adaptive training strategies and stabilizes internal representations during fine-tuning. APT comprises three components: (1) Adaptive Training Adjustment, which uses an overfitting indicator to guide data augmentation and loss weighting; (2) Representation Stabilization, which regularizes feature maps to prevent noise prediction shifts; and (3) Attention Alignment for Prior Knowledge Preservation, which aligns cross-attention maps to maintain semantic coherence. APT demonstrates superior performance in generating high-quality, diverse images with limited data compared to existing methods.",
        "Tags": [
            "Diffusion Models",
            "Data Augmentation",
            "Overfitting Mitigation",
            "Cross-Attention Alignment",
            "Feature Map Regularization"
        ]
    },
    {
        "Title": "Mamba as a Bridge: Where VFM Meets VLM for Domain-Generalized Semantic Segmentation",
        "Authors": "Xin Zhang \u00b7 Robby T. Tan",
        "Abstract": "Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained traction in Domain Generalized Semantic Segmentation (DGSS) due to their strong generalization capabilities. However, existing DGSS methods often rely exclusively on either VFMs or VLMs, overlooking their complementary strengths. VFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g., CLIP) provide robust text alignment but struggle with coarse granularity. Despite their complementary strengths, effectively integrating VFMs and VLMs with attention mechanisms is challenging, as the increased patch tokens complicate long-sequence modeling. To address this, we propose MFuser, a novel Mamba-based fusion framework that efficiently combines the strengths of VFMs and VLMs while maintaining linear scalability in token length. MFuser consists of two key components: MVFuser, which acts as a co-adapter to jointly fine-tune the two models by capturing both sequential and spatial dynamics; and MTEnhancer, a hybrid attention-Mamba module that refines text embeddings by incorporating image priors. Our approach achieves precise feature locality and strong text alignment without incurring significant computational overhead. Extensive experiments demonstrate that MFuser significantly outperforms state-of-the-art DGSS methods, achieving 68.19 mIoU on synthetic-to-real and 71.87 mIoU on real-to-real benchmarks. The code will be released upon acceptance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces MFuser, a novel Mamba-based fusion framework designed to integrate the complementary strengths of Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) for Domain Generalized Semantic Segmentation (DGSS). VFMs, such as DINOv2, are adept at capturing fine-grained features, whereas VLMs like CLIP offer robust text alignment but face challenges with coarse granularity. The proposed MFuser framework addresses the difficulty of combining these models by maintaining linear scalability in token length and includes two main components: MVFuser, which fine-tunes both models by capturing sequential and spatial dynamics, and MTEnhancer, a hybrid attention-Mamba module that refines text embeddings with image priors. This approach achieves precise feature locality and strong text alignment efficiently, significantly outperforming existing DGSS methods on standard benchmarks.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Semantic Segmentation",
            "Mamba",
            "Domain Generalization",
            "Mamba-based fusion",
            "Linear scalability",
            "Hybrid attention-Mamba"
        ]
    },
    {
        "Title": "Noise Modeling in One Hour: Minimizing Preparation Efforts for Self-supervised Low-Light RAW Image Denoising",
        "Authors": "Feiran Li \u00b7 Haiyang Jiang \u00b7 Daisuke Iso",
        "Abstract": "Noise synthesis is a promising solution for addressing the data shortage problem in data-driven low-light RAW image denoising. However, accurate noise synthesis methods often necessitate labor-intensive calibration and profiling procedures during preparation, preventing them from landing to practice at scale. This work introduces a practically simple noise synthesis pipeline based on detailed analyses of noise properties and extensive justification of widespread techniques. Compared to other approaches, our proposed pipeline eliminates the cumbersome system gain calibration and signal-independent noise profiling steps, reducing the preparation time for noise synthesis from days to hours. Meanwhile, our method exhibits strong denoising performance, showing an up to 0.54dB PSNR improvement over the current state-of-the-art noise synthesis technique.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of data shortage in low-light RAW image denoising by proposing a simplified noise synthesis pipeline. Traditional methods require extensive calibration and profiling, which are time-consuming and impractical for large-scale use. The authors introduce a streamlined approach that eliminates the need for system gain calibration and signal-independent noise profiling, reducing preparation time from days to hours. The proposed method demonstrates superior denoising performance, achieving up to a 0.54dB PSNR improvement over existing state-of-the-art techniques.",
        "Tags": [
            "Self-Supervised Learning",
            "Low-Level Vision",
            "Denoising",
            "Noise Synthesis",
            "RAW Image Processing",
            "Efficient Calibration"
        ]
    },
    {
        "Title": "Not All Parameters Matter: Masking  Diffusion Models for Enhancing Generation Ability",
        "Authors": "Lei Wang \u00b7 Senmao Li \u00b7 Fei Yang \u00b7 Jianye Wang \u00b7 Ziheng Zhang \u00b7 Yuhan Liu \u00b7 Yaxing Wang \u00b7 Jian Yang",
        "Abstract": "The diffusion models, in early stages focus on constructing basic image structures, while the refined details, including local features and textures, are generated in later stages.  Thus the same network layers are forced to learn both structural and textural information simultaneously,  significantly differing from the traditional deep learning architectures (e.g., ResNet or GANs) which  captures or generates the image semantic information at different layers.  This difference inspires us to explore the time-wise diffusion models.  We initially investigate the key contributions of the U-Net parameters to the denoising process and identify that properly zeroing out certain parameters (including large parameters) contributes to denoising, substantially improving the generation quality on the fly. Capitalizing on this discovery, we propose a simple yet effective method\u2014termed \u201cMaskUNet\u201d\u2014 that enhances generation quality with negligible parameter numbers. Our method fully leverages timestep- and sample-dependent effective U-Net parameters. To optimize MaskUNet,  we offer two fine-tuning strategies: a training-based approach and a training-free approach, including tailored networks and optimization functions.  In zero-shot inference on the COCO dataset, MaskUNet achieves the best FID score and further demonstrates its effectiveness in downstream task evaluations.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper explores the unique behavior of diffusion models, which generate basic image structures early and refine details later, unlike traditional architectures like ResNet or GANs. The authors identify that selectively zeroing out certain U-Net parameters during the denoising process significantly improves generation quality. They propose 'MaskUNet,' a method that leverages timestep- and sample-dependent parameters to enhance generation quality with minimal additional parameters. Two fine-tuning strategies\u2014training-based and training-free\u2014are introduced to optimize MaskUNet. The method achieves state-of-the-art FID scores on the COCO dataset and demonstrates strong performance in downstream tasks.",
        "Tags": [
            "Diffusion Models",
            "Image Generation",
            "Parameter Masking",
            "U-Net Optimization",
            "Zero-Shot Inference"
        ]
    },
    {
        "Title": "Satellite Observations-guided Diffusion Model for Accurate Meteorological States at Arbitrary Resolution",
        "Authors": "Siwei Tu \u00b7 Ben Fei \u00b7 Weidong Yang \u00b7 Fenghua Ling \u00b7 Hao Chen \u00b7 Zili Liu \u00b7 Kun Chen \u00b7 Hang Fan \u00b7 Wanli Ouyang \u00b7 Lei Bai",
        "Abstract": "Accurate acquisition of surface meteorological conditions at arbitrary locations holds significant importance for weather forecasting and climate simulation. Due to the fact that meteorological states derived from satellite observations are often provided in the form of low-resolution grid fields, the direct application of spatial interpolation to obtain meteorological states for specific locations often results in significant discrepancies when compared to actual observations. Existing downscaling methods for acquiring meteorological state information at higher resolutions commonly overlook the correlation with satellite observations. To bridge the gap, we propose $\\textbf{S}$atellite-observations $\\textbf{G}$uided $\\textbf{D}$iffusion Model ($\\textbf{SGD}$), a conditional diffusion model pre-trained on ERA5 reanalysis data with satellite observations (GridSat) as conditions, which is employed for sampling downscaled meteorological states through a zero-shot guided sampling strategy and patch-based methods. During the training process, we propose to fuse the information from GridSat satellite observations into ERA5 maps via the attention mechanism, enabling SGD to generate atmospheric states that align more accurately with actual conditions. In the sampling, we employed optimizable convolutional kernels to simulate the upscale process, thereby generating high-resolution ERA5 maps using low-resolution ERA5 maps as well as observations from weather stations as guidance.  Moreover, our devised patch-based method promotes SGD to generate meteorological states at arbitrary resolutions. Experiments demonstrate SGD fulfills accurate meteorological states downscaling to 6.25km.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces the Satellite-observations Guided Diffusion Model (SGD), a novel approach for accurately downscaling meteorological states to arbitrary resolutions using satellite observations. Traditional methods often fail to consider the correlation with satellite data, leading to discrepancies. SGD leverages a conditional diffusion model pre-trained on ERA5 reanalysis data, incorporating satellite observations through an attention mechanism to enhance accuracy. The model employs a zero-shot guided sampling strategy and patch-based methods, along with optimizable convolutional kernels, to simulate the upscale process and generate high-resolution meteorological maps. This approach allows for the generation of meteorological states that closely align with actual conditions, achieving accurate downscaling to 6.25km.",
        "Tags": [
            "Diffusion Models",
            "Remote Sensing Image Analysis",
            "Attention Mechanism",
            "Zero-Shot Learning",
            "Patch-Based Methods"
        ]
    },
    {
        "Title": "The Power of Context: How Multimodality Improves Image Super-Resolution",
        "Authors": "Kangfu Mei \u00b7 Vishal M. Patel \u00b7 Mojtaba Sahraee-Ardakan \u00b7 Hossein Talebi \u00b7 Peyman Milanfar \u00b7 Mauricio Delbracio",
        "Abstract": "Single-image super-resolution (SISR) remains challenging due to the inherent difficulty of recovering fine-grained details and preserving perceptual quality from low-resolution inputs. Existing methods often rely on limited image priors, leading to suboptimal results. We propose a novel approach that leverages the rich contextual information available in multiple modalities -- including depth, segmentation, edges, and text prompts -- to learn a powerful generative prior for SISR within a diffusion model framework. We introduce a flexible network architecture that effectively fuses multimodal information, accommodating an arbitrary number of input modalities without requiring significant modifications to the diffusion process. Crucially, we mitigate hallucinations, often introduced by text prompts, by using spatial information from other modalities to guide regional text-based conditioning. Each modality's guidance strength can also be controlled independently, allowing steering outputs toward different directions, such as increasing bokeh through depth or adjusting object prominence via segmentation. Extensive experiments demonstrate that our model surpasses state-of-the-art generative SISR methods, achieving superior visual quality and fidelity.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the challenges of single-image super-resolution (SISR) by proposing a novel approach that leverages multimodal information, including depth, segmentation, edges, and text prompts, within a diffusion model framework. The introduced network architecture effectively fuses multimodal inputs without significant modifications to the diffusion process, mitigating hallucinations from text prompts by using spatial information from other modalities. This approach allows for independent control over each modality's guidance strength, enabling adjustments like increasing bokeh or object prominence. The model demonstrates superior visual quality and fidelity compared to state-of-the-art generative SISR methods.",
        "Tags": [
            "Super-Resolution",
            "Diffusion Models",
            "Multimodal Learning",
            "Multimodal Fusion",
            "Text-based Conditioning",
            "Perceptual Quality Enhancement"
        ]
    },
    {
        "Title": "Acc3D: Accelerating Single Image to 3D Diffusion Models via Edge Consistency Guided Score Distillation",
        "Authors": "Kendong Liu \u00b7 Zhiyu Zhu \u00b7 Hui LIU \u00b7 Junhui Hou",
        "Abstract": "We present Acc3D to tackle the challenge of accelerating the diffusion process for generating 3D models from single images. To derive accurate reconstruction through few-step inference, we emphasize the critical issue as the modeling of the score function at the endpoints (states of the random noise). To tackle such an issue, we propose edge consistency, i.e., consistent predictions across the low signal-to-noise ratio region, to enhance a pre-trained diffusion model, enabling a distillation-based refinement of the endpoint score function. Building on those distilled diffusion models, we introduce an adversarial augmentation strategy to further enrich generation detail. The two modules complement each other, mutually reinforcing to elevate generative performance. Extensive experiments show that our Acc3D not only achieves over a $20\\times$ increase in computational efficiency but also yields notable quality improvements, compared with state-of-the-art methods. Project webpage:  https://acc3d-object.github.io/",
        "Link": "https://acc3d-object.github.io/",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Acc3D, a method designed to accelerate the generation of 3D models from single images using diffusion models. The approach focuses on improving the modeling of the score function at the endpoints of the diffusion process through edge consistency, which ensures consistent predictions in low signal-to-noise ratio regions. This enhancement allows for a distillation-based refinement of the endpoint score function. Additionally, an adversarial augmentation strategy is employed to further enrich the detail of the generated models. The combination of these two modules significantly improves both computational efficiency and the quality of the generated 3D models, achieving over a 20\u00d7 increase in efficiency and notable quality improvements compared to state-of-the-art methods.",
        "Tags": [
            "3D Generation",
            "Diffusion Models",
            "Score Distillation",
            "Adversarial Augmentation",
            "Edge Consistency"
        ]
    },
    {
        "Title": "FreePCA: Integrating Consistency Information across Long-short Frames in Training-free Long Video Generation via Principal Component Analysis",
        "Authors": "Jiangtong Tan \u00b7 Hu Yu \u00b7 Jie Huang \u00b7 Jie Xiao \u00b7 Feng Zhao",
        "Abstract": "Long video generation involves generating extended videos using models trained on short videos, suffering from distribution shifts due to varying frame counts. It necessitates the use of local information from the original short frames to enhance visual and motion quality, and global information from the entire long frames to ensure appearance consistency. Existing training-free methods struggle to effectively integrate the benefits of both, as appearance and motion in videos are closely coupled, leading to inconsistency and poor quality. In this paper, we reveal that global and local information can be precisely decoupled into consistent appearance and motion intensity information by applying Principal Component Analysis (PCA), allowing for refined complementary integration of global consistency and local quality. With this insight, we propose FreePCA, a training-free long video generation paradigm based on PCA that simultaneously achieves high consistency and quality. Concretely, we decouple consistent appearance and motion intensity features by measuring cosine similarity in the principal component space. Critically, we progressively integrate these features to preserve original quality and ensure smooth transitions, while further enhancing consistency by reusing the mean statistics of the initial noise. Experiments demonstrate that FreePCA can be applied to various video diffusion models without requiring training, leading to substantial improvements.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Long video generation often faces challenges due to distribution shifts caused by varying frame counts, requiring the integration of local information from short frames for visual and motion quality, and global information from long frames for appearance consistency. Existing training-free methods fail to effectively combine these aspects, leading to inconsistency and poor quality. This paper introduces FreePCA, a training-free long video generation paradigm based on Principal Component Analysis (PCA), which decouples consistent appearance and motion intensity features by measuring cosine similarity in the principal component space. FreePCA progressively integrates these features to preserve original quality and ensure smooth transitions, while enhancing consistency by reusing the mean statistics of the initial noise. The method is applicable to various video diffusion models without requiring training, resulting in significant improvements.",
        "Tags": [
            "Video Generation",
            "Principal Component Analysis (PCA)",
            "Training-free Methods",
            "Cosine Similarity",
            "Noise Statistics Reuse"
        ]
    },
    {
        "Title": "Progressive Focused Transformer for Single Image Super-Resolution",
        "Authors": "Wei Long \u00b7 Xingyu Zhou \u00b7 Leheng Zhang \u00b7 Shuhang Gu",
        "Abstract": "Transformer-based methods have achieved remarkable results in image super-resolution tasks because they can capture non-local dependencies in low-quality input images. However, this feature-intensive modeling approach is computationally expensive because it calculates the similarities between numerous features that are irrelevant to the query features when obtaining attention weights. These unnecessary similarity calculations not only degrade the reconstruction performance but also introduce significant computational overhead. How to accurately identify the features that are important to the current query features and avoid similarity calculations between irrelevant features remains an urgent problem. To address this issue, we propose a novel and effective Progressive Focused Transformer (PFT) that links all isolated attention maps in the network through Progressive Focused Attention (PFA) to focus attention on the most important tokens. PFA not only enables the network to capture more critical similar features, but also significantly reduces the computational cost of the overall network by filtering out irrelevant features before calculating similarities. Extensive experiments demonstrate the effectiveness of the proposed method, achieving state-of-the-art performance on various single image super-resolution benchmarks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Transformer-based methods have shown significant success in image super-resolution by capturing non-local dependencies in low-quality images. However, these methods are computationally expensive due to unnecessary similarity calculations between irrelevant features, which degrade performance and increase overhead. To address this, the authors propose a Progressive Focused Transformer (PFT) that uses Progressive Focused Attention (PFA) to link attention maps and focus on the most important tokens. PFA improves performance by capturing critical features and reduces computational costs by filtering out irrelevant features before similarity calculations. The method achieves state-of-the-art results on single image super-resolution benchmarks.",
        "Tags": [
            "Super-Resolution",
            "Transformer",
            "Progressive Focused Attention",
            "Attention Mechanism",
            "Computational Efficiency"
        ]
    },
    {
        "Title": "PURA: Parameter Update-Recovery Test-Time Adaption for RGB-T Tracking",
        "Authors": "Zekai Shao \u00b7 Yufan Hu \u00b7 Bin Fan \u00b7 Hongmin Liu",
        "Abstract": "Maintaining stable tracking of objects in domain shift scenarios is crucial for RGB-T tracking, prompting us to explore the use of unlabeled test sample information for effective online model adaptation. However, current Test-Time Adaptation (TTA) methods in RGB-T tracking dramatically change the model's internal parameters during long-term adaptation. At the same time, the gradient computations involved in the optimization process impose a significant computational burden. To address these challenges, we propose a Parameter Update-Recovery Adaptation (PURA) framework based on parameter decomposition. Firstly, Our fast parameter update strategy adjusts model parameters using statistical information from test samples without requiring gradient calculations, ensuring consistency between the model and test data distribution. Secondly, our parameter decomposition recovery employs orthogonal decomposition to identify the principal update direction and recover parameters in this direction, aiding in the retention of critical knowledge. Finally, we leverage the information obtained from decomposition to provide feedback on the momentum during the update phase, ensuring a stable updating process. Experimental results demonstrate that PURA outperforms current state-of-the-art methods across multiple datasets, validating its effectiveness. The code is available in the Supplementary Materials.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces PURA, a Parameter Update-Recovery Adaptation framework designed to enhance RGB-T tracking in domain shift scenarios. PURA addresses the limitations of current Test-Time Adaptation methods, which often lead to significant changes in model parameters and high computational costs due to gradient calculations. The proposed framework employs a fast parameter update strategy that adjusts model parameters using test sample statistics without gradient computations, ensuring alignment with the test data distribution. Additionally, it uses parameter decomposition recovery to identify and recover principal update directions, preserving essential knowledge. The framework also incorporates feedback on momentum during updates to maintain stability. PURA demonstrates superior performance over existing methods across multiple datasets.",
        "Tags": [
            "Visual Tracking",
            "Test-Time Adaptation (TTA)",
            "RGB-T Tracking",
            "Parameter Decomposition",
            "Momentum Feedback",
            "Orthogonal Decomposition"
        ]
    },
    {
        "Title": "DiffPortrait360: Consistent Portrait Diffusion for 360 View Synthesis",
        "Authors": "Yuming Gu \u00b7 Phong Tran \u00b7 Yujian Zheng \u00b7 Hongyi Xu \u00b7 Heyuan Li \u00b7 Adilbek Karmanov \u00b7 Hao Li",
        "Abstract": "Generating high-quality 360-degree views of human heads from single-view images is essential for enabling accessible immersive telepresence applications and scalable personalized content creation.While cutting-edge methods for full head generation are limited to modeling realistic human heads, the latest diffusion-based approaches for style-omniscient head synthesis can produce only frontal views and struggle with view consistency, preventing their conversion into true 3D models for rendering from arbitrary angles.We introduce a novel approach that generates fully consistent 360-degree head views, accommodating human, stylized, and anthropomorphic forms, including accessories like glasses and hats. Our method builds on the DiffPortrait3D framework, incorporating a custom ControlNet for back-of-head detail generation and a dual appearance module to ensure global front-back consistency. By training on continuous view sequences and integrating a back reference image, our approach achieves robust, locally continuous view synthesis. Our model can be used to produce high-quality neural radiance fields (NeRFs) for real-time, free-viewpoint rendering, outperforming state-of-the-art methods in object synthesis and 360-degree head generation for very challenging input portraits.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces DiffPortrait360, a novel approach for generating consistent 360-degree views of human heads from single-view images, addressing limitations in current methods that struggle with view consistency and are restricted to frontal views. The method extends the DiffPortrait3D framework by incorporating a custom ControlNet for detailed back-of-head generation and a dual appearance module to ensure global consistency. It is trained on continuous view sequences and uses a back reference image to achieve robust, locally continuous view synthesis. The approach enables the creation of high-quality neural radiance fields (NeRFs) for real-time, free-viewpoint rendering, outperforming existing methods in 360-degree head generation and object synthesis.",
        "Tags": [
            "3D Generation",
            "Diffusion Models",
            "Neural Radiance Fields (NeRFs)",
            "ControlNet",
            "View Consistency"
        ]
    },
    {
        "Title": "D2SP: Dynamic Dual-Stage Purification Framework for Dual Noise Mitigation in Vision-based Affective Recognition.",
        "Authors": "Haoran Wang \u00b7 Xinji Mai \u00b7 Zeng Tao \u00b7 Xuan Tong \u00b7 Junxiong Lin \u00b7 Yan Wang \u00b7 Jiawen Yu \u00b7 Shaoqi Yan \u00b7 Ziheng Zhou \u00b7 Wenqiang Zhang",
        "Abstract": "The current advancements in Dynamic Facial Expression Recognition (DFER) methods mainly focus on better capturing the spatial and temporal features of facial expressions. However, DFER datasets contain a substantial amount of noisy samples, and few have addressed the issue of handling this noise. We identified two types of noise: one is caused by low-quality data resulting from factors such as occlusion, dim lighting, and blurriness; the other arises from mislabeled data due to annotation bias by annotators. Addressing the two types of noise, we have meticulously crafted a \\textbf{D}ynamic \\textbf{D}ual-\\textbf{S}tage \\textbf{P}urification (D2SP) Framework. This initiative aims to dynamically purify the DFER datasets of these two types of noise, ensuring that only high-quality and correctly labeled data is used in the training process. To mitigate low-quality samples, we introduce the Coarse-Grained Pruning (CGP) stage, which computes sample weights and prunes those low-weight samples. After CGP, the Fine-Grained Correction (FGC) stage evaluates prediction stability to correct mislabeled data. Moreover, D2SP is conceived as a general and plug-and-play framework, tailored to integrate seamlessly with prevailing DFER methods. Extensive experiments covering prevalent DFER datasets and deploying multiple benchmark methods have substantiated D2SP\u2019s ability to significantly enhance performance metrics.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces the Dynamic Dual-Stage Purification (D2SP) Framework to address noise in Dynamic Facial Expression Recognition (DFER) datasets, which includes low-quality data and mislabeled samples. The framework consists of two stages: Coarse-Grained Pruning (CGP) to remove low-quality samples and Fine-Grained Correction (FGC) to correct mislabeled data. D2SP is designed as a plug-and-play solution that can be integrated with existing DFER methods, and it has been shown to significantly improve performance metrics across various datasets and benchmark methods.",
        "Tags": [
            "Data Augmentation",
            "Self-Supervised Learning",
            "Noise Mitigation",
            "Dynamic Facial Expression Recognition",
            "Plug-and-Play Framework"
        ]
    },
    {
        "Title": "Channel Consistency Prior and Self-Reconstruction Strategy Based Unsupervised Image Deraining",
        "Authors": "Guanglu Dong \u00b7 Tianheng Zheng \u00b7 Yuanzhouhan Cao \u00b7 Linbo Qing \u00b7 Chao Ren",
        "Abstract": "Recently, deep image deraining models based on paired datasets have made a series of remarkable progress. However, they cannot be well applied in real-world applications due to the difficulty of obtaining real paired datasets and the poor generalization performance. In this paper, we propose a novel Channel Consistency Prior and Self-Reconstruction Strategy Based Unsupervised Image Deraining framework, CSUD, to tackle the aforementioned challenges. During training with unpaired data, CSUD is capable of generating high-quality pseudo clean and rainy image pairs which are used to enhance the performance of deraining network. Specifically, to preserve more image background details while transferring rain streaks from rainy images to the unpaired clean images, we propose a novel Channel Consistency Loss (CCLoss) by introducing the Channel Consistency Prior (CCP) of rain streaks into training process, thereby ensuring that the generated pseudo rainy images closely resemble the real ones. Furthermore, we propose a novel Self-Reconstruction (SR) strategy to alleviate the redundant information transfer problem of the generator, further improving the deraining performance and the generalization capability of our method. Extensive experiments on multiple synthetic and real-world datasets demonstrate that the deraining performance of CSUD surpasses other state-of-the-art unsupervised methods and CSUD exhibits superior generalization capability.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces CSUD, an unsupervised image deraining framework that leverages a Channel Consistency Prior and a Self-Reconstruction Strategy to address the challenges of poor generalization and the difficulty of obtaining real paired datasets. The framework generates high-quality pseudo clean and rainy image pairs from unpaired data, enhancing deraining network performance. A novel Channel Consistency Loss (CCLoss) is proposed to ensure the generated pseudo rainy images closely resemble real ones by preserving image background details and transferring rain streaks accurately. Additionally, a Self-Reconstruction (SR) strategy is introduced to mitigate redundant information transfer, further improving deraining performance and generalization. The method demonstrates superior performance and generalization capability across multiple synthetic and real-world datasets compared to state-of-the-art unsupervised methods.",
        "Tags": [
            "Image Editing",
            "Low-Level Vision",
            "Self-Supervised Learning",
            "Unsupervised Learning",
            "Image Deraining",
            "Channel Consistency Loss"
        ]
    },
    {
        "Title": "DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D Gaussian Splatting",
        "Authors": "Seungjun Lee \u00b7 Gim Hee Lee",
        "Abstract": "Reconstructing sharp 3D representations from blurry multi-view images are long-standing problem in computer vision. Recent works attempt to enhance high-quality novel view synthesis from the motion blur by leveraging event-based cameras, benefiting from high dynamic range and microsecond temporal resolution. However, they often reach sub-optimal visual quality in either restoring inaccurate color or losing fine-grained details. In this paper, we present DiET-GS, a diffusion prior and event stream-assisted motion deblurring 3DGS. Our framework effectively leverages blur-free event streams and diffusion prior in a two-stage training strategy. Specifically, we introduce the novel framework to constraint 3DGS with event double integral, achieving both accurate color and well-defined details. Additionally, we propose a simple technique to leverage diffusion prior to further enhance the edge details. Qualitative and quantitative results on both synthetic and real-world data demonstrate that our DiET-GS is capable of producing better quality of novel views compared to the existing baselines. The code will be publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "DiET-GS introduces a novel framework for reconstructing sharp 3D representations from blurry multi-view images by leveraging event-based cameras and diffusion priors. The method employs a two-stage training strategy that integrates blur-free event streams and diffusion priors to enhance the quality of novel view synthesis. The framework utilizes event double integral to constrain 3D Gaussian Splatting, ensuring accurate color restoration and fine-grained detail preservation. Additionally, a technique leveraging diffusion priors is proposed to further improve edge details. The approach demonstrates superior performance in generating high-quality novel views compared to existing baselines, as validated by qualitative and quantitative results on synthetic and real-world data.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Deblur",
            "Diffusion Models",
            "Event-based Cameras",
            "Novel View Synthesis",
            "Two-stage Training Strategy"
        ]
    },
    {
        "Title": "A Physics-Informed Blur Learning Framework for Imaging Systems",
        "Authors": "liqun.chen \u00b7 Yuxuan Li \u00b7 Jun Dai \u00b7 Jinwei Gu \u00b7 Tianfan Xue",
        "Abstract": "Accurate blur estimation is essential for high-performance imaging across various applications. Blur is typically represented by the point spread function (PSF). In this paper, we propose a physics-informed PSF learning framework for imaging system, consisting a simple calibration followed by a learning process. Our framework could achieve both high accuracy and universal applicability. Inspired by the Seidel PSF model for representing spatially varying PSF, we identify its limitations in optimization and introduce a novel wavefront-based PSF model accompanied by an optimization strategy, both reduce optimization complexity and improve estimation accuracy. Moreover, our wavefront-based PSF model is independent of lens parameters, eliminate the need for prior knowledge of the lens. To validate our approach, we compare it with recent PSF estimation methods (Degradation Transfer and Fast Two-step) through a deblurring task, where all the estimated PSFs are used to train state-of-the-art deblurring algorithms. Our approach demonstrates improvements in image quality in simulation, also showcase noticeable visual quality improvements on real captured images. Code and models are public.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces a physics-informed point spread function (PSF) learning framework for imaging systems, aimed at improving blur estimation accuracy and universal applicability. The framework involves a calibration step followed by a learning process, utilizing a novel wavefront-based PSF model that reduces optimization complexity and enhances estimation accuracy. Unlike traditional models, this approach does not require prior knowledge of lens parameters. The effectiveness of the proposed method is demonstrated through comparisons with existing PSF estimation techniques, showing superior image quality in both simulated and real-world scenarios.",
        "Tags": [
            "Deblur",
            "Low-Level Vision",
            "Wavefront-based PSF Model",
            "Physics-Informed Learning",
            "Spatially Varying PSF"
        ]
    },
    {
        "Title": "GigaHands: A Massive Annotated Dataset of Bimanual Hand Activities",
        "Authors": "Rao Fu \u00b7 Dingxi Zhang \u00b7 Alex Jiang \u00b7 Wanjia Fu \u00b7 Austin Funk \u00b7 Daniel Ritchie \u00b7 Srinath Sridhar",
        "Abstract": "Understanding bimanual human hand activities is a critical problem in AI and robotics. We cannot build large models of bimanual activities because existing datasets lack the scale, coverage of diverse hand activities, and detailed annotations. We introduce GigaHands, a massive annotated dataset capturing 34 hours of bimanual hand activities from 56 subjects and 417 objects, totaling 14k motion clips derived from 183 million frames paired with 84k text annotations. Our markerless capture setup and data acquisition protocol enable fully automatic 3D hand and object estimation while minimizing the effort required for text annotation. The scale and diversity of GigaHands enable broad applications, including text-driven action synthesis, hand motion captioning, and dynamic radiance field reconstruction.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces GigaHands, a large-scale dataset designed to address the lack of comprehensive data for understanding bimanual human hand activities. GigaHands includes 34 hours of bimanual hand activities from 56 subjects interacting with 417 objects, comprising 14,000 motion clips from 183 million frames and 84,000 text annotations. The dataset supports applications such as text-driven action synthesis, hand motion captioning, and dynamic radiance field reconstruction, facilitated by a markerless capture setup and an efficient data acquisition protocol.",
        "Tags": [
            "3D Reconstruction",
            "Datasets and Benchmarks",
            "Bimanual Hand Activities",
            "Markerless Capture",
            "Text Annotations"
        ]
    },
    {
        "Title": "Believing is Seeing: Unobserved Object Detection using Generative Models",
        "Authors": "Subhransu S. Bhattacharjee \u00b7 Dylan Campbell \u00b7 Rahul Shome",
        "Abstract": "Can objects that are not visible in an image---but are in the vicinity of the camera---be detected? This study introduces the novel tasks of 2D, 2.5D and 3D unobserved object detection for predicting the location of nearby objects that are occluded or lie outside the image frame.  We adapt several state-of-the-art pre-trained generative models to address this task, including 2D and 3D diffusion models and vision-language models, and show that they can be used to infer the presence of objects that are not directly observed.  To benchmark this task, we propose a suite of metrics that capture different aspects of performance.  Our empirical evaluation on indoor scenes from the RealEstate10k and NYU Depth v2 datasets demonstrate results that motivate the use of generative models for the unobserved object detection task.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This study introduces the novel tasks of 2D, 2.5D, and 3D unobserved object detection, which aim to predict the location of nearby objects that are occluded or outside the image frame. The authors adapt state-of-the-art pre-trained generative models, including 2D and 3D diffusion models and vision-language models, to infer the presence of unobserved objects. A suite of metrics is proposed to benchmark the task, and empirical evaluations on indoor scenes from the RealEstate10k and NYU Depth v2 datasets demonstrate the effectiveness of generative models for this task.",
        "Tags": [
            "Object Detection",
            "Diffusion Models",
            "Vision-Language Models (VLMs)",
            "Unobserved Object Detection",
            "Occlusion Handling",
            "Scene Understanding"
        ]
    },
    {
        "Title": "POSTA: A Go-to Framework for Customized Artistic Poster Generation",
        "Authors": "Haoyu Chen \u00b7 Xiaojie Xu \u00b7 Wenbo Li \u00b7 Jingjing Ren \u00b7 Tian Ye \u00b7 Songhua Liu \u00b7 Ying-Cong Chen \u00b7 Lei Zhu \u00b7 Xinchao Wang",
        "Abstract": "Poster design is a critical medium for visual communication. Prior work has explored automatic poster design using deep learning techniques, but these approaches lack text accuracy, user customization, and aesthetic appeal, limiting their applicability in artistic domains such as movies and exhibitions, where both clear content delivery and visual impact are essential. To address these limitations, we present POSTA: a modular framework powered by diffusion models and multimodal large language models (MLLMs) for customized artistic poster generation. The framework consists of three modules. Background Diffusion creates a themed background based on user input. Design MLLM then generates layout and typography elements that align with and complement the background style. Finally, to enhance the poster's aesthetic appeal, ArtText Diffusion applies additional stylization to key text elements. The final result is a visually cohesive and appealing poster, with a fully modular process that allows for complete customization. To train our models, we develop the PosterArt dataset, comprising high-quality artistic posters annotated with layout, typography, and pixel-level stylized text segmentation. Our comprehensive experimental analysis demonstrates POSTA\u2019s exceptional controllability and design diversity, outperforming existing models in both text accuracy and aesthetic quality.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "POSTA is a modular framework designed for customized artistic poster generation, addressing the limitations of prior deep learning approaches in text accuracy, user customization, and aesthetic appeal. The framework integrates diffusion models and multimodal large language models (MLLMs) to create visually cohesive posters. It consists of three modules: Background Diffusion for themed background creation, Design MLLM for layout and typography generation, and ArtText Diffusion for stylizing key text elements. POSTA is trained on the PosterArt dataset, which includes high-quality artistic posters with detailed annotations. The framework demonstrates superior controllability and design diversity, outperforming existing models in text accuracy and aesthetic quality.",
        "Tags": [
            "Diffusion Models",
            "Multimodal Large Language Models (MLLMs)",
            "Customized Poster Design",
            "Artistic Text Stylization",
            "Modular Framework"
        ]
    },
    {
        "Title": "SDBF: Steep-Decision-Boundary Fingerprinting for Hard-Label Tampering Detection of DNN Models",
        "Authors": "Xiaofan Bai \u00b7 Shixin Li \u00b7 Xiaojing Ma \u00b7 Bin Benjamin Zhu \u00b7 Dongmei Zhang \u00b7 Linchen Yu",
        "Abstract": "Cloud-based AI systems offer significant benefits but also introduce vulnerabilities, making deep neural network (DNN) models susceptible to malicious tampering. This tampering may involve harmful behavior injection or resource reduction, compromising model integrity and performance. To detect model tampering, hard-label fingerprinting techniques generate sensitive samples to probe and reveal tampering. Existing fingerprinting methods are mainly based on \\textbf{gradient-defined sensitivity} or \\textbf{decision boundary}, with the latter showing a manifest superior detection performance. However, existing decision-boundary-based fingerprinting methods remain conceptual, lacking a theoretical explanation for why samples near the decision boundary are more sensitive to tampering. Moreover, all existing fingerprinting methods either suffer from insufficient sensitivity or incur high computational costs.In this paper, we provide the first theoretical justification for why samples near the decision boundary are more sensitive to tampering-induced shifts than the faraway. Based on this, we further propose \\textbf{Steep-Decision-Boundary Fingerprinting (SDBF)}, a novel lightweight approach for hard-label tampering detection. SDBF places fingerprint samples near the \\textbf{steep decision boundary}, where the outputs of samples are inherently highly sensitive to tampering. We also design a \\textbf{Max Boundary Coverage Strategy (MBCS)}, which enhances samples'  diversity over the decision boundary. Theoretical analysis and extensive experimental results show that SDBF outperforms existing SOTA hard-label fingerprinting methods in both sensitivity and efficiency.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Cloud-based AI systems are vulnerable to malicious tampering of deep neural network (DNN) models, which can compromise model integrity and performance. Existing hard-label fingerprinting techniques for detecting tampering rely on gradient-defined sensitivity or decision boundaries, with the latter showing superior performance. However, these methods lack theoretical justification for why samples near the decision boundary are more sensitive to tampering. This paper provides the first theoretical explanation for this phenomenon and introduces Steep-Decision-Boundary Fingerprinting (SDBF), a lightweight approach for hard-label tampering detection. SDBF places fingerprint samples near the steep decision boundary, where outputs are highly sensitive to tampering. Additionally, a Max Boundary Coverage Strategy (MBCS) is designed to enhance sample diversity over the decision boundary. Theoretical analysis and experiments demonstrate that SDBF outperforms existing state-of-the-art methods in sensitivity and efficiency.",
        "Tags": [
            "Hard-Label Tampering Detection",
            "Decision Boundary",
            "Steep Decision Boundary",
            "Max Boundary Coverage Strategy",
            "Lightweight Detection"
        ]
    },
    {
        "Title": "DPFlow: Adaptive Optical Flow Estimation with a Dual-Pyramid Framework",
        "Authors": "Henrique Morimitsu \u00b7 Xiaobin Zhu \u00b7 Roberto M. Cesar Jr \u00b7 Xiangyang Ji \u00b7 Xu-Cheng Yin",
        "Abstract": "Optical flow estimation is essential for video processing tasks, such as restoration and action recognition. The quality of videos is constantly increasing, with current standards reaching 8K (7680 x 4320) resolution. However, optical flow methods are usually designed for low resolution and do not generalize to large inputs due to their rigid architectures. They adopt downscaling or input tiling to reduce the input size, causing a loss of details and global information. There is also a lack of optical flow benchmarks to judge the actual performance of existing methods on high-resolution samples. Previous works only conducted qualitative high-resolution evaluations on hand-picked samples. This paper fills this gap in optical flow estimation in two ways. We propose DPFlow, an adaptive optical flow architecture capable of generalizing up to 8K resolution inputs while trained with only low-resolution samples. We also introduce Kubric-NK, a new benchmark for evaluating optical flow methods with input resolutions ranging from 1K to 8K. Our high-resolution evaluation pushes the boundaries of existing methods and reveals new insights about their generalization capabilities. Extensive experimental results show that DPFlow achieves state-of-the-art results on the MPI-Sintel, KITTI 2015, Spring, and other high-resolution benchmarks. The code and dataset have been submitted as supplementary material for review and will be released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Optical flow estimation is crucial for video processing tasks like restoration and action recognition, but existing methods struggle with high-resolution inputs such as 8K due to rigid architectures. This paper introduces DPFlow, an adaptive optical flow architecture capable of generalizing to 8K resolution inputs while being trained on low-resolution samples. Additionally, the authors propose Kubric-NK, a new benchmark for evaluating optical flow methods across resolutions from 1K to 8K. DPFlow achieves state-of-the-art performance on multiple benchmarks, including MPI-Sintel and KITTI 2015, demonstrating its effectiveness in high-resolution scenarios.",
        "Tags": [
            "Optical Flow",
            "High-Resolution Video Processing",
            "Dual-Pyramid Framework",
            "8K Resolution Adaptation",
            "Optical Flow Benchmarking"
        ]
    },
    {
        "Title": "Empowering Vector Graphics with Consistently Arbitrary Viewing and View-dependent Visibility",
        "Authors": "Yidi Li \u00b7 Jun Xiao \u00b7 Zhengda Lu \u00b7 Yiqun Wang \u00b7 Haiyong Jiang",
        "Abstract": "This work presents a novel text-to-vector graphics generation approach, Dream3DVG, allowing for arbitrary viewpoint viewing, progressive detail optimization, and view-dependent occlusion awareness.  Our approach is a dual-branch optimization framework, consisting of an auxiliary 3D Gaussian Splatting optimization branch and a 3D vector graphics optimization branch.  The introduced 3DGS branch can bridge the domain gaps between text prompts and vector graphics with more consistent guidance. Moreover, 3DGS allows for progressive detail control by scheduling classifier-free guidance, facilitating guiding vector graphics with coarse shapes at the initial stages and finer details at later stages.  We also improve the view-dependent occlusions by devising a visibility-awareness rendering module.  Extensive results on 3D sketches and 3D iconographies, demonstrate the superiority of the method on different abstraction levels of details, cross-view consistency, and occlusion-aware stroke culling.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Dream3DVG, a novel text-to-vector graphics generation approach that enables arbitrary viewpoint viewing, progressive detail optimization, and view-dependent occlusion awareness. The method employs a dual-branch optimization framework, combining a 3D Gaussian Splatting optimization branch and a 3D vector graphics optimization branch. The 3DGS branch bridges the domain gap between text prompts and vector graphics, providing consistent guidance and progressive detail control. A visibility-awareness rendering module is also introduced to improve view-dependent occlusions. The approach demonstrates superior performance in generating 3D sketches and iconographies with varying levels of detail, cross-view consistency, and occlusion-aware stroke culling.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Text-to-Image Generation",
            "Dual-Branch Optimization",
            "Visibility-Aware Rendering",
            "Progressive Detail Control"
        ]
    },
    {
        "Title": "Semantic Library Adaptation: LoRA Retrieval and Fusion for Open-Vocabulary Semantic Segmentation",
        "Authors": "Reza Qorbani \u00b7 Gianluca Villani \u00b7 Theodoros Panagiotakopoulos \u00b7 Marc Botet Colomer \u00b7 Linus H\u00e4renstam-Nielsen \u00b7 Mattia Segu \u00b7 Pier Luigi Dovesi \u00b7 Jussi Karlgren \u00b7 Daniel Cremers \u00b7 Federico Tombari \u00b7 Matteo Poggi",
        "Abstract": "Open-vocabulary semantic segmentation models associate vision and text to label pixels from an undefined set of classes using textual queries, providing versatile performance on novel datasets. However, large shifts between training and test domains degrade their performance, requiring fine-tuning for effective real-world application. We introduce Semantic Library Adaptation (SemLa), a novel framework for training-free, test-time domain adaptation. SemLa leverages a library of LoRA-based adapters indexed with CLIP embeddings, dynamically merging the most relevant adapters based on proximity to the target domain in the embedding space. This approach constructs an ad-hoc model tailored to each specific input without additional training. Our method scales efficiently, enhances explainability by tracking adapter contributions, and inherently protects data privacy, making it ideal for sensitive applications. Comprehensive experiments on an 18-domain benchmark built over 10 standard datasets demonstrate SemLa's superior adaptability and performance across diverse settings, establishing a new standard in domain adaptation for open-vocabulary semantic segmentation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Semantic Library Adaptation (SemLa), a novel framework for test-time domain adaptation in open-vocabulary semantic segmentation. SemLa utilizes a library of LoRA-based adapters indexed with CLIP embeddings to dynamically merge the most relevant adapters based on the target domain's proximity in the embedding space. This approach allows for the construction of an ad-hoc model tailored to each specific input without the need for additional training. SemLa is scalable, enhances explainability by tracking adapter contributions, and inherently protects data privacy, making it suitable for sensitive applications. The framework demonstrates superior adaptability and performance across diverse settings, as evidenced by comprehensive experiments on an 18-domain benchmark built over 10 standard datasets.",
        "Tags": [
            "Open-Vocabulary Semantic Segmentation",
            "Domain Adaptation",
            "CLIP",
            "LoRA",
            "Test-Time Adaptation",
            "Dynamic Adapter Fusion",
            "Privacy-Preserving AI"
        ]
    },
    {
        "Title": "Spatial-Temporal Visual Representation for Self-Supervised Motion Planning",
        "Authors": "Yichen Xie \u00b7 Runsheng Xu \u00b7 Tong He \u00b7 Jyh-Jing Hwang \u00b7 Katie Z Luo \u00b7 Jingwei Ji \u00b7 Hubert Lin \u00b7 Letian Chen \u00b7 Yiren Lu \u00b7 Zhaoqi Leng \u00b7 Dragomir Anguelov \u00b7 Mingxing Tan",
        "Abstract": "The  latest  advancements  in  multi-modal  large  language models  (MLLMs)  have  spurred  a  strong  renewed  interest in end-to-end motion planning approaches for autonomous driving.   Many end-to-end approaches rely on human annotations to learn intermediate perception and prediction tasks,  while purely self-supervised approaches\u2014which directly learn from sensor inputs to generate planning trajectories without human annotations\u2014often underperform the state of the art. We observe a key gap in the input representation space:  end-to-end approaches built on MLLMs are often  pretrained  with  reasoning  tasks  in  perspective  view space rather than the native 3D space that autonomous vehicles plan in.  To this end, we propose PaLI-Driver, based on  the  popular  PaLI  vision-language  model.    PaLI-Driver uses a novel sparse volume strategy to seamlessly transform the strong visual representation of MLLMs from perspective view to 3D space without the need to finetune the vision encoder.   This representation aggregates multiview and multi-frame visual inputs and enables better pre diction  of  planning trajectories  in  3D  space.   To  validate our  method,  we  run  experiments  on  both  nuScenes  and our in-house collected dataset X-Planning.  Results show that PaLI-Driver performs favorably against existing supervised multi-task approaches while requiring no human annotations.  It also demonstrates great scalability when pretrained on large volumes of unannotated driving logs.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of self-supervised motion planning in autonomous driving by leveraging advancements in multimodal large language models (MLLMs). Existing end-to-end approaches often rely on human annotations for intermediate tasks, while self-supervised methods underperform due to limitations in input representation. The authors propose PaLI-Driver, a novel approach based on the PaLI vision-language model, which introduces a sparse volume strategy to transform visual representations from perspective view to 3D space without fine-tuning the vision encoder. This enables better prediction of planning trajectories in 3D space by aggregating multiview and multi-frame visual inputs. Experiments on nuScenes and an in-house dataset demonstrate that PaLI-Driver outperforms supervised multi-task approaches without requiring human annotations and shows scalability when pretrained on large volumes of unannotated driving logs.",
        "Tags": [
            "Autonomous Driving",
            "Self-Supervised Learning",
            "Multimodal Large Language Models (MLLMs)",
            "Sparse Volume Strategy",
            "3D Space Representation",
            "Multiview Aggregation"
        ]
    },
    {
        "Title": "ArtiFade: Learning to Generate High-quality Subject from Blemished Image",
        "Authors": "Shuya Yang \u00b7 Shaozhe Hao \u00b7 Yukang Cao \u00b7 Kwan-Yee K. Wong",
        "Abstract": "Subject-driven text-to-image generation has witnessed remarkable advancements in its ability to learn and capture characteristics of a subject using only a limited number of images. However, existing methods commonly rely on high-quality images for training and may struggle to generate reasonable images when the input images are blemished by artifacts. This is primarily attributed to the inadequate capability of current techniques in distinguishing subject-related features from disruptive artifacts. In this paper, we introduce ArtiFade to tackle this issue and successfully generate high-quality artifact-free images from blemished datasets. Specifically, ArtiFade exploits fine-tuning of a pre-trained text-to-image model, aiming to remove artifacts. The elimination of artifacts is achieved by utilizing a specialized dataset that encompasses both unblemished images and their corresponding blemished counterparts during fine-tuning. ArtiFade also ensures the preservation of the original generative capabilities inherent within the diffusion model, thereby enhancing the overall performance of subject-driven methods in generating high-quality and artifact-free images. We further devise evaluation benchmarks tailored for this task. Through extensive qualitative and quantitative experiments, we demonstrate the generalizability of ArtiFade in effective artifact removal under both in-distribution and out-of-distribution scenarios.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Subject-driven text-to-image generation often requires high-quality training images, but struggles with blemished inputs due to the inability to distinguish subject features from artifacts. ArtiFade addresses this by fine-tuning a pre-trained text-to-image model to remove artifacts, using a dataset of both unblemished and blemished images. This approach maintains the model's generative capabilities while improving artifact removal, as demonstrated through extensive experiments.",
        "Tags": [
            "Text-to-Image Generation",
            "Diffusion Models",
            "Artifact Removal",
            "Fine-Tuning",
            "Subject-Driven Generation"
        ]
    },
    {
        "Title": "Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget",
        "Authors": "Vikash Sehwag \u00b7 Xianghao Kong \u00b7 Jingtao Li \u00b7 Michael Spranger \u00b7 Lingjuan Lyu",
        "Abstract": "As scaling laws in generative AI push performance, they simultaneously concentrate the development of these models among actors with large computational resources. With a focus on text-to-image (T2I) generative models, we aim to unlock this bottleneck by demonstrating very low-cost training of large-scale T2I diffusion transformer models. As the computational cost of transformers increases with the number of patches in each image, we propose randomly masking up to 75% of the image patches during training. We propose a deferred masking strategy that preprocesses all patches using a patch-mixer before masking, thus significantly reducing the performance degradation with masking, making it superior to model downscaling in reducing computational cost. We also incorporate the latest improvements in transformer architecture, such as the use of mixture-of-experts layers, to improve performance and further identify the critical benefit of using synthetic images in micro-budget training. Finally, using only 37M publicly available real and synthetic images, we train a 1.16 billion parameter sparse transformer with only 1,890 USD economical cost and achieve a 12.7 FID in zero-shot generation on the COCO dataset. Notably, our model achieves competitive performance across both automated and human-centric evaluations, as well as high-quality generations, while incurring 118$\\times$ lower costs than Stable Diffusion models and 14$\\times$ lower costs than the current state-of-the-art approach, which costs \\$28,400. We also further investigate the influence of synthetic images on performance and demonstrate that micro-budget training on only synthetic images is sufficient for achieving high-quality data generation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of training large-scale text-to-image (T2I) diffusion transformer models on a micro-budget. The authors propose a method that randomly masks up to 75% of image patches during training, combined with a deferred masking strategy that preprocesses patches using a patch-mixer to minimize performance degradation. They also integrate advanced transformer architectures, such as mixture-of-experts layers, and highlight the importance of synthetic images in low-cost training. Using only 37M publicly available images, they train a 1.16 billion parameter sparse transformer at a cost of 1,890 USD, achieving a 12.7 FID in zero-shot generation on the COCO dataset. The model demonstrates competitive performance in automated and human-centric evaluations while significantly reducing costs compared to existing methods. The study also shows that synthetic images alone are sufficient for high-quality data generation in micro-budget training.",
        "Tags": [
            "Diffusion Models",
            "Text-to-Image Generation",
            "Patch-Masking Strategy",
            "Mixture-of-Experts Layers",
            "Synthetic Data Utilization"
        ]
    },
    {
        "Title": "EffiDec3D: An Optimized Decoder for High-Performance and Efficient 3D Medical Image Segmentation",
        "Authors": "Md Mostafijur Rahman \u00b7 Radu Marculescu",
        "Abstract": "Recent 3D deep networks such as SwinUNETR, SwinUNETRv2, and 3D UX-Net have shown promising performance by leveraging self-attention and large-kernel convolutions to capture the volumetric context. However, their substantial computational requirements limit their use in real-time and resource-constrained environments. The high #FLOPs and #Params in these networks stem largely fromcomplex decoder designs with high-resolution layers and excessive channel counts. In this paper, we propose EffiDec3D, an optimized 3D decoder that employs a channel reduction strategy across all decoder stages, which sets the number of channels to the minimum needed for accurate feature representation. Additionally, EffiDec3D removes the high-resolution layers when their contribution to segmentation quality is minimal. Our optimized EffiDec3D decoder achieves a 96.4% reduction in #Params and a 93.0% reduction in #FLOPs compared to the decoder of original 3D UX-Net. Similarly, for SwinUNETR and SwinUNETRv2 (which share an identical decoder), we observe reductions of 94.9% in #Params and 86.2% in #FLOPs. Our extensive experiments on 12 different medical imaging tasks confirm that EffiDec3D not only significantly reduces the computational demands, but also maintains a performance level comparable to original models, thus establishing a new standard for efficient 3D medical image segmentation. We will make the source code public upon paper acceptance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "EffiDec3D introduces an optimized 3D decoder designed to enhance the efficiency of 3D medical image segmentation networks. By implementing a channel reduction strategy and eliminating high-resolution layers that minimally impact segmentation quality, EffiDec3D significantly reduces computational demands. Compared to the decoders of 3D UX-Net, SwinUNETR, and SwinUNETRv2, EffiDec3D achieves substantial reductions in parameters and FLOPs while maintaining comparable performance across 12 medical imaging tasks. This advancement sets a new benchmark for efficient 3D medical image segmentation.",
        "Tags": [
            "3D Semantic Segmentation",
            "Medical Image Segmentation",
            "Channel Reduction Strategy",
            "High-Resolution Layer Optimization",
            "Computational Efficiency"
        ]
    },
    {
        "Title": "Generalizing Deepfake Video Detection with Plug-and-Play: Video-Level Blending and Spatiotemporal Adapter Tuning",
        "Authors": "Zhiyuan Yan \u00b7 Yandan Zhao \u00b7 Shen Chen \u00b7 Mingyi Guo \u00b7 Xinghe Fu \u00b7 Taiping Yao \u00b7 Shouhong Ding \u00b7 Yunsheng Wu \u00b7 Li Yuan",
        "Abstract": "Three key challenges hinder the development of current deepfake video detection: (1) Temporal features can be complex and diverse: how can we identify general temporal artifacts to enhance model generalization? (2) Spatiotemporal models often lean heavily on one type of artifact and ignore the other: how can we ensure balanced learning from both? (3) Videos are naturally resource-intensive: how can we tackle efficiency without compromising accuracy? This paper attempts to tackle the three challenges jointly. First, inspired by the notable generality of using image-level blending data for image forgery detection, we investigate whether and how video-level blending can be effective in video. We then perform a thorough analysis and identify a previously underexplored temporal forgery artifact: Facial Feature Drift (FFD), which commonly exists across different forgeries. To reproduce FFD, we then propose a novel Video-level Blending data (VB), where VB is implemented by blending the original image and its warped version frame-by-frame, serving as a hard negative sample to mine more general artifacts. Second, we carefully design a lightweight Spatiotemporal Adapter (StA) to equip a pre-trained image model with the ability to capture both spatial and temporal features jointly and efficiently. StA is designed with two-stream 3D-Conv with varying kernel sizes, allowing it to process spatial and temporal features separately. This eliminates the need to design a new deepfake-specific video architecture from scratch. Extensive experiments validate the effectiveness of the proposed methods; and show our approach can generalize well to previously unseen forgery videos.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses three key challenges in deepfake video detection: identifying general temporal artifacts, ensuring balanced learning from both spatial and temporal features, and improving efficiency without compromising accuracy. The authors propose two main contributions: (1) Video-level Blending (VB), a method to generate hard negative samples by blending original and warped frames, which helps uncover a previously underexplored temporal artifact called Facial Feature Drift (FFD). (2) A lightweight Spatiotemporal Adapter (StA) that enhances a pre-trained image model to jointly and efficiently capture spatial and temporal features using a two-stream 3D-Conv architecture with varying kernel sizes. The proposed methods demonstrate strong generalization to unseen forgery videos.",
        "Error_DeepSeek_Response": "{'tags': ['Deepfake Detection', 'Video Understanding', 'Spatiotemporal Adapter', 'Facial Feature Drift', 'Video-Level Blending'], 'abstract': 'This paper addresses three key challenges in deepfake video detection: identifying general temporal artifacts, ensuring balanced learning from both spatial and temporal features, and improving efficiency without compromising accuracy. The authors propose two main contributions: (1) Video-level Blending (VB), a method to generate hard negative samples by blending original and warped frames, which helps uncover a previously underexplored temporal artifact called Facial Feature Drift (FFD). (2) A lightweight Spatiotemporal Adapter (StA) that enhances a pre-trained image model to jointly and efficiently capture spatial and temporal features using a two-stream 3D-Conv architecture with varying kernel sizes. The proposed methods demonstrate strong generalization to unseen forgery videos.'}"
    },
    {
        "Title": "AniGrad: Anisotropic Gradient-Adaptive Resolution for 3D Reconstruction From Monocular Video",
        "Authors": "Noah Stier \u00b7 Alex Rich \u00b7 Pradeep Sen \u00b7 Tobias H\u00f6llerer",
        "Abstract": "Recent image-based 3D reconstruction methods have achieved excellent quality for indoor scenes using 3D convolutional neural networks. However, they rely on a high-resolution grid in order to achieve detailed output surfaces, which is quite costly in terms of compute time, and it results in large mesh sizes that are more expensive to store, transmit, and render. In this paper we propose a new solution to this problem, using adaptive sampling. By re-formulating the final layers of the network, we are able to analytically bound the local surface complexity, and set the local sample rate accordingly. Our method, AniGrad, achieves an order of magnitude reduction in both surface extraction latency and mesh size, while preserving mesh accuracy and detail.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces AniGrad, a novel method for 3D reconstruction from monocular video that addresses the computational and storage inefficiencies of high-resolution grids used in existing methods. By reformulating the final layers of a 3D convolutional neural network, AniGrad analytically bounds local surface complexity and adaptively sets the local sample rate. This approach significantly reduces surface extraction latency and mesh size by an order of magnitude while maintaining mesh accuracy and detail.",
        "Tags": [
            "3D Reconstruction",
            "3DGS (Gaussian Splatting)",
            "Adaptive Sampling",
            "Surface Complexity Analysis",
            "Mesh Optimization"
        ]
    },
    {
        "Title": "Towards Cost-Effective Learning: A Synergy of Semi-Supervised and Active Learning",
        "Authors": "Tianxiang Yin \u00b7 Ningzhong Liu \u00b7 Han Sun",
        "Abstract": "Active learning (AL) and semi-supervised learning (SSL) both aim to reduce annotation costs: AL selectively annotates high-value samples from the unlabeled data, while SSL leverages abundant unlabeled data to improve model performance. Although these two appear intuitively compatible, directly combining them remains challenging due to fundamental differences in their frameworks. Current semi-supervised active learning (SSAL) methods often lack theoretical foundations and often design AL strategies tailored to a specific SSL algorithm rather than genuinely integrating the two fields.In this paper, we incorporate AL objectives into the overall risk formulation within the mainstream pseudo-label-based SSL framework, clarifying key differences between SSAL and traditional AL scenarios. To bridge these gaps, we propose a feature re-alignment module that aligns the features of unlabeled data under different augmentations by leveraging clustering and consistency constraints. Experimental results demonstrate that our module enables flexible combinations of SOTA methods from both AL and SSL, yielding more efficient algorithm performance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of combining active learning (AL) and semi-supervised learning (SSL) to reduce annotation costs effectively. While AL focuses on annotating high-value samples and SSL leverages unlabeled data, integrating these approaches has been difficult due to their differing frameworks. The authors propose a novel method that incorporates AL objectives into the risk formulation of pseudo-label-based SSL, highlighting the distinctions between semi-supervised active learning (SSAL) and traditional AL. A feature re-alignment module is introduced to align features of unlabeled data under different augmentations using clustering and consistency constraints. This approach allows for flexible combinations of state-of-the-art AL and SSL methods, resulting in improved algorithm efficiency.",
        "Tags": [
            "Semi-Supervised Learning",
            "Active Learning",
            "Feature Re-alignment",
            "Pseudo-labeling",
            "Consistency Constraints"
        ]
    },
    {
        "Title": "A Unified Image-Dense Annotation Generation Model for Underwater Scenes",
        "Authors": "Hongkai Lin \u00b7 Dingkang Liang \u00b7 Zhenghao Qi \u00b7 Xiang Bai",
        "Abstract": "Underwater dense prediction, especially depth estimation and semantic segmentation, is crucial for comprehensively understanding underwater scenes.Nevertheless, high-quality and large-scale underwater datasets with dense annotations remained scarce because of the complex environment and the exorbitant data collection costs. This paper proposes a unified Text-to-Image and DEnse annotation generation method (TIDE) for underwater scenes. It relies solely on text as input to simultaneously generate realistic underwater images and multiple highly consistent dense annotations. Specifically, we unify the generation of text-to-image and text-to-dense annotations within a single model.The Implicit Layout Sharing mechanism (ILS) and cross-modal interaction method called Time Adaptive Normalization (TAN) are introduced to jointly optimize the consistency between image and dense annotations.We synthesize a large underwater dataset using TIDE to validate the effectiveness of our method in underwater dense prediction tasks.The results demonstrate that our method effectively improves the performance of existing underwater dense prediction models and mitigates the scarcity of underwater data with dense annotations.Our method can offer new perspectives on alleviating data scarcity issues in other fields.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces TIDE, a unified Text-to-Image and DEnse annotation generation method for underwater scenes, addressing the scarcity of high-quality, densely annotated underwater datasets. TIDE generates realistic underwater images and consistent dense annotations from text inputs alone, utilizing the Implicit Layout Sharing mechanism and Time Adaptive Normalization for optimization. The method significantly enhances the performance of underwater dense prediction models and offers a novel approach to mitigating data scarcity in various fields.",
        "Tags": [
            "Depth Estimation",
            "Semantic Segmentation",
            "Text-to-Image Generation",
            "Implicit Layout Sharing",
            "Time Adaptive Normalization"
        ]
    },
    {
        "Title": "Consistent and Controllable Image Animation with Motion Diffusion Models",
        "Authors": "Xin Ma \u00b7 Yaohui Wang \u00b7 Gengyun Jia \u00b7 Xinyuan Chen \u00b7 Tien-Tsin Wong \u00b7 Yuan-Fang Li \u00b7 Cunjian Chen",
        "Abstract": "Diffusion models have achieved great progress in image animation due to powerful generative capabilities. However, maintaining spatio-temporal consistency with detailed information from the input static image over time (e.g., style, background, and objects in the input static image) and ensuring motion smoothness in animated video narratives guided by textual prompts still remains challenging. In this paper, we introduce Cinemo, a novel image animation approach towards achieving better image consistency and motion smoothness. In general, we propose two effective strategies at the training and inference stages of Cinemo to accomplish our goal. At the training stage, Cinemo focuses on learning the distribution of motion residuals, rather than directly predicting subsequent via a motion diffusion model. At the inference stage, a noise refinement technique based on discrete cosine transformation is introduced to mitigate sudden motion changes. Such strategies enable Cinemo to produce highly consistent, smooth, and motion-controllable results. Compared to previous methods, Cinemo offers simpler and more precise user controllability. Extensive experiments against several state-of-the-art methods, including both commercial tools and research approaches, across multiple metrics, demonstrate the effectiveness and superiority of our proposed approach.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Cinemo, a novel image animation approach designed to enhance spatio-temporal consistency and motion smoothness in animated video narratives guided by textual prompts. Cinemo employs two key strategies: learning the distribution of motion residuals during training and applying a noise refinement technique based on discrete cosine transformation during inference. These strategies enable Cinemo to produce highly consistent, smooth, and motion-controllable results, offering simpler and more precise user controllability compared to previous methods.",
        "Tags": [
            "Diffusion Models",
            "Image Animation",
            "Motion Residuals",
            "Noise Refinement",
            "Discrete Cosine Transformation"
        ]
    },
    {
        "Title": "Fine-Grained Image-Text Correspondence with Cost Aggregation for Open-Vocabulary Part Segmentation",
        "Authors": "Jiho Choi \u00b7 Seonho Lee \u00b7 Minhyun Lee \u00b7 Seungho Lee \u00b7 Hyunjung Shim",
        "Abstract": "Open-Vocabulary Part Segmentation (OVPS) is an emerging field for recognizing fine-grained object parts in unseen categories. We identify two primary challenges in OVPS: (1) the difficulty in aligning part-level image-text correspondence, and (2) the lack of structural understanding in segmenting object parts. To address these issues, we propose PartCATSeg, a novel framework that integrates object-aware part-level cost aggregation, compositional loss, and structural guidance from DINO. Our approach employs a disentangled cost aggregation strategy that handles object and part-level costs separately, enhancing the precision of part-level segmentation. We also introduce a compositional loss to better capture part-object relationships, compensating for the limited part annotations. Additionally, structural guidance from DINO features improves boundary delineation and inter-part understanding. Extensive experiments on Pascal-Part-116, ADE20K-Part-234, and PartImageNet datasets demonstrate that our method significantly outperforms state-of-the-art approaches, setting a new baseline for robust generalization to unseen part categories.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Open-Vocabulary Part Segmentation (OVPS) addresses the challenge of recognizing fine-grained object parts in unseen categories. The paper identifies two main issues: aligning part-level image-text correspondence and lacking structural understanding in part segmentation. To tackle these, the authors propose PartCATSeg, a framework that integrates object-aware part-level cost aggregation, compositional loss, and structural guidance from DINO. The disentangled cost aggregation strategy separates object and part-level costs, improving segmentation precision. A compositional loss enhances part-object relationships, and DINO features provide structural guidance for better boundary delineation and inter-part understanding. The method outperforms state-of-the-art approaches on Pascal-Part-116, ADE20K-Part-234, and PartImageNet datasets, establishing a new baseline for generalization to unseen part categories.",
        "Tags": [
            "Semantic Segmentation",
            "Vision-Language Models (VLMs)",
            "Open-Vocabulary Part Segmentation",
            "Cost Aggregation",
            "Structural Guidance",
            "Disentangled Cost Aggregation",
            "Compositional Loss",
            "DINO Features"
        ]
    },
    {
        "Title": "Towards Open-Vocabulary Audio-Visual Event Localization",
        "Authors": "Jinxing Zhou \u00b7 Dan Guo \u00b7 Ruohao Guo \u00b7 Yuxin Mao \u00b7 Jingjing Hu \u00b7 Yiran Zhong \u00b7 Xiaojun Chang \u00b7 Meng Wang",
        "Abstract": "The Audio-Visual Event Localization (AVEL) task aims to temporally locate and classify video events that are both audible and visible.Most research in this field assumes a closed-set setting, which restricts these models' ability to handle test data containing event categories absent (unseen) during training. Recently, a few studies have explored AVEL in an open-set setting, enabling the recognition of unseen events as ``unknown'', but without providing category-specific semantics.In this paper, we advance the field by introducing the Open-Vocabulary Audio-Visual Event Localization (OV-AVEL) problem, which requires localizing audio-visual events and predicting explicit categories for both seen and unseen data at inference.To address this new task, we propose the OV-AVEBench dataset, comprising 24,800 videos across 67 real-life audio-visual scenes (seen:unseen = 46:21), each with manual segment-level annotation.We also establish three evaluation metrics for this task.Moreover, we investigate two baseline approaches, one training-free and one using a further fine-tuning paradigm.Specifically, we utilize the unified multimodal space from the pretrained ImageBind model to extract audio, visual, and textual (event classes) features.The training-free baseline then determines predictions by comparing the consistency of audio-text and visual-text feature similarities.The fine-tuning baseline incorporates lightweight temporal layers to encode temporal relations within the audio and visual modalities, using OV-AVEBench training data for model fine-tuning.We evaluate these baselines on the proposed OV-AVEBench dataset and discuss potential directions for future work in this new field.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces the Open-Vocabulary Audio-Visual Event Localization (OV-AVEL) problem, which extends the traditional Audio-Visual Event Localization (AVEL) task by requiring the localization and explicit category prediction of both seen and unseen audio-visual events. To facilitate this, the authors propose the OV-AVEBench dataset, containing 24,800 videos across 67 real-life scenes, with manual segment-level annotations. They also establish three evaluation metrics for the task. Two baseline approaches are investigated: a training-free method leveraging the pretrained ImageBind model for feature extraction and similarity comparison, and a fine-tuning approach that incorporates lightweight temporal layers to encode temporal relations. The baselines are evaluated on the OV-AVEBench dataset, and potential future research directions are discussed.",
        "Tags": [
            "Multimodal Learning",
            "Video Understanding",
            "Open-Vocabulary Learning",
            "Temporal Localization",
            "Multimodal Feature Extraction"
        ]
    },
    {
        "Title": "FisherTune: Fisher-Guided Robust Tuning of Vision Foundation Models for Domain Generalized Segmentation",
        "Authors": "Dong Zhao \u00b7 Jinlong Li \u00b7 Shuang Wang \u00b7 Mengyao Wu \u00b7 Qi Zang \u00b7 Nicu Sebe \u00b7 Zhun Zhong",
        "Abstract": "Vision Foundation Models (VFMs) excel in generalization due to large-scale pretraining, but fine-tuning them for Domain Generalized Semantic Segmentation (DGSS) while maintaining this ability remains challenging. Existing approaches either selectively fine-tune parameters or freeze the VFMs and update only the adapters, both of which may underutilize the VFMs' full potential in DGSS tasks. We observe that domain-sensitive parameters in VFMs, arising from task and distribution differences, can hinder generalization.To address this, we propose \\textbf{FisherTune}, a robust fine-tuning method guided by the Domain-Related Fisher Information Matrix (DR-FIM). DR-FIM measures parameter sensitivity across tasks and domains, enabling selective updates that preserve generalization and enhance DGSS adaptability. FisherTune incorporates variational inference to stabilize DR-FIM estimation, treating parameters as Gaussian-distributed variables and leveraging pre-trained priors. Extensive experiments show that FisherTune achieves superior cross-domain segmentation while maintaining generalization, outperforming selective-parameter and adapter-based methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Vision Foundation Models (VFMs) are highly effective in generalization due to large-scale pretraining, but fine-tuning them for Domain Generalized Semantic Segmentation (DGSS) without compromising their generalization ability is challenging. Existing methods either selectively fine-tune parameters or freeze VFMs and update only adapters, potentially underutilizing VFMs' capabilities. The authors propose FisherTune, a robust fine-tuning method guided by the Domain-Related Fisher Information Matrix (DR-FIM), which measures parameter sensitivity across tasks and domains. FisherTune uses variational inference to stabilize DR-FIM estimation, treating parameters as Gaussian-distributed variables and leveraging pre-trained priors. This approach achieves superior cross-domain segmentation while maintaining generalization, outperforming existing methods.",
        "Tags": [
            "Semantic Segmentation",
            "Domain Generalization",
            "Vision Foundation Models",
            "Fisher Information Matrix",
            "Variational Inference",
            "Parameter Sensitivity"
        ]
    },
    {
        "Title": "Detecting Open World Objects via Partial Attribute Assignment",
        "Authors": "Muli Yang \u00b7 Gabriel James Goenawan \u00b7 Huaiyuan Qin \u00b7 Kai Han \u00b7 Xi Peng \u00b7 Yanhua Yang \u00b7 Hongyuan Zhu",
        "Abstract": "Despite being trained on massive data, today's vision foundation models still fall short in detecting open world objects. Apart from recognizing known objects appeared in training, a successful Open World Object Detection (OWOD) system must also be able to detect unknown objects that were never seen before, without confusing them with the backgrounds. Unlike the prevailing former works that learn \"objectness\" using probability models, we focus on learning fine-grained class-agnostic attributes that can be used to detect both known and unknown object classes in an explainable manner. In this paper, we propose Partial Attribute Assignment (PASS), aiming to automatically select and optimize a small, relevant subset of attributes from a larger attribute pool. Specifically, we model attribute selection as a Partial Optimal Transport (POT) problem between known visual objects and the attribute pool, in which more relevant attributes signify more transported mass. PASS follows a curriculum schedule that progressively selects and optimizes a targeted subset of attributes during training, promoting stability and accuracy. Our method enjoys end-to-end optimization by minimizing the POT distance and the classification loss on known visual objects, demonstrating high training efficiency and superior OWOD performance among extensive experimental evaluations. Our code will be made public.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of Open World Object Detection (OWOD), where models must detect both known and unknown objects without confusing them with backgrounds. The authors propose Partial Attribute Assignment (PASS), a method that selects and optimizes a subset of class-agnostic attributes from a larger pool to detect objects in an explainable manner. PASS models attribute selection as a Partial Optimal Transport (POT) problem, where more relevant attributes correspond to more transported mass. The method employs a curriculum schedule to progressively select and optimize attributes during training, ensuring stability and accuracy. PASS achieves end-to-end optimization by minimizing the POT distance and classification loss on known objects, demonstrating high training efficiency and superior OWOD performance.",
        "Tags": [
            "Object Detection",
            "Open World Object Detection (OWOD)",
            "Partial Optimal Transport (POT)",
            "Explainable AI",
            "Curriculum Learning",
            "Attribute-Based Detection"
        ]
    },
    {
        "Title": "EnvPoser: Environment-aware Realistic Human Motion Estimation from Sparse Observations with Uncertainty Modeling",
        "Authors": "Songpengcheng Xia \u00b7 Yu Zhang \u00b7 Zhuo Su \u00b7 Xiaozheng Zheng \u00b7 Zheng Lv \u00b7 Guidong Wang \u00b7 Yongjie Zhang \u00b7 Qi Wu \u00b7 Lei Chu \u00b7 Ling Pei",
        "Abstract": "Estimating full-body motion using the tracking signals of head and hands from VR devices holds great potential for various applications. However, the sparsity and unique distribution of observations present a significant challenge, resulting in an ill-posed problem with multiple feasible solutions (i.e., hypotheses). This amplifies uncertainty and ambiguity in full-body motion estimation, especially for the lower-body joints. Therefore, we propose a new method, EnvPoser, that employs a two-stage framework to perform full-body motion estimation using sparse tracking signals and pre-scanned environment from VR devices. EnvPoser models the multi-hypothesis nature of human motion through an uncertainty-aware estimation module in the first stage. In the second stage, we refine these multi-hypothesis estimates by integrating semantic and geometric environmental constraints, ensuring that the final motion estimation aligns realistically with both the environmental context and physical interactions.Qualitative and quantitative experiments on two public datasets demonstrate that our method achieves state-of-the-art performance, highlighting significant improvements in human motion estimation within motion-environment interaction scenarios. Code will be released upon paper acceptance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "EnvPoser introduces a novel two-stage framework for estimating full-body human motion from sparse tracking signals of head and hands in VR environments. The method addresses the inherent uncertainty and ambiguity in motion estimation, particularly for lower-body joints, by first modeling multiple hypotheses through an uncertainty-aware module. In the second stage, it refines these hypotheses by incorporating semantic and geometric environmental constraints, ensuring realistic alignment with the physical context. The approach demonstrates state-of-the-art performance on public datasets, significantly improving motion estimation in scenarios involving interaction with the environment.",
        "Tags": [
            "Avatars",
            "3D Human Pose Estimation",
            "Uncertainty Modeling",
            "Environment-aware Motion Estimation",
            "VR-based Motion Tracking"
        ]
    },
    {
        "Title": "MAC-Ego3D: Multi-Agent Gaussian Consensus for Real-Time Collaborative Ego-Motion and Photorealistic 3D Reconstruction",
        "Authors": "Xiaohao Xu \u00b7 Feng Xue \u00b7 Shibo Zhao \u00b7 Yike Pan \u00b7 Sebastian Scherer \u00b7 Xiaonan Huang",
        "Abstract": "Real-time multi-agent collaboration for ego-motion estimation and high-fidelity 3D reconstruction is vital for scalable spatial intelligence. However, traditional methods produce sparse, low-detail maps, while recent dense mapping approaches struggle with high latency.To overcome these challenges, we present MAC-Ego3D, a novel framework for real-time collaborative photorealistic 3D reconstruction via Multi-Agent Gaussian Consensus. MAC-Ego3D enables agents to independently construct, align, and iteratively refine local maps using a unified Gaussian splat representation. Through Intra-Agent Gaussian Consensus, it enforces spatial coherence among neighboring Gaussian splats within an agent. For global alignment, parallelized Inter-Agent Gaussian Consensus, which asynchronously aligns and optimizes local maps by regularizing multi-agent Gaussian splats, seamlessly integrates them into a high-fidelity 3D model. Leveraging Gaussian primitives, MAC-Ego3D supports efficient RGB-D rendering, enabling rapid inter-agent Gaussian association and alignment.MAC-Ego3D bridges local precision and global coherence, delivering higher efficiency, largely reducing localization error, and improving mapping fidelity. It establishes a new SOTA on synthetic and real-world benchmarks, achieving a 15$\\times$ increase in inference speed, order-of-magnitude reductions in ego-motion estimation error for partial cases, and RGB PSNR gains of 4 to 10 dB. Our code will be made publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "MAC-Ego3D introduces a novel framework for real-time collaborative photorealistic 3D reconstruction using Multi-Agent Gaussian Consensus. The framework allows agents to independently construct, align, and refine local maps using a unified Gaussian splat representation. It employs Intra-Agent Gaussian Consensus for spatial coherence within an agent and Inter-Agent Gaussian Consensus for global alignment and optimization of local maps. This approach significantly improves efficiency, reduces localization error, and enhances mapping fidelity, achieving state-of-the-art performance on synthetic and real-world benchmarks with notable improvements in inference speed and RGB PSNR.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "3D Reconstruction",
            "Real-Time Collaboration",
            "Photorealistic Rendering",
            "Multi-Agent Systems"
        ]
    },
    {
        "Title": "Unleashing In-context Learning of Autoregressive Models for Few-shot Image Manipulation",
        "Authors": "Bolin Lai \u00b7 Felix Juefei-Xu \u00b7 Miao Liu \u00b7 Xiaoliang Dai \u00b7 Nikhil Mehta \u00b7 Chenguang Zhu \u00b7 Zeyi Huang \u00b7 James Rehg \u00b7 Sangmin Lee \u00b7 Ning Zhang \u00b7 Tong Xiao",
        "Abstract": "Text-guided image manipulation has experienced notable advancement in recent years. In order to mitigate linguistic ambiguity, few-shot learning with visual examples has been applied for instructions that are underrepresented in the training set, or difficult to describe purely in language. However, learning from visual prompts requires strong reasoning capability, which diffusion models are struggling with. To address this issue, we introduce a novel multi-modal autoregressive model, dubbed InstaManip, that can instantly learn a new image manipulation operation from textual and visual guidance via in-context learning, and apply it to new query images. Specifically, we propose an innovative group self-attention mechanism to break down the in-context learning process into two separate stages -- learning and applying, which simplifies the complex problem into two easier tasks. We also introduce a relation regularization method to further disentangle image transformation features from irrelevant contents in exemplar images. Extensive experiments suggest that our method surpasses previous few-shot image manipulation models by a notable margin (>=19% in human evaluation). We also find our model can be further boosted by increasing the number or diversity of exemplar images.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces InstaManip, a novel multi-modal autoregressive model designed for few-shot image manipulation using textual and visual guidance through in-context learning. The model employs a group self-attention mechanism to divide the learning process into two stages\u2014learning and applying\u2014and incorporates a relation regularization method to disentangle image transformation features from irrelevant content. InstaManip outperforms existing few-shot image manipulation models, achieving significant improvements in human evaluations, and benefits from increased exemplar image quantity and diversity.",
        "Tags": [
            "Image Editing",
            "Few-Shot Learning",
            "Multimodal Learning",
            "Autoregressive Models",
            "In-context Learning",
            "Group Self-Attention",
            "Relation Regularization"
        ]
    },
    {
        "Title": "Automatic Spectral Calibration of Hyperspectral Images: Method, Dataset and Benchmark",
        "Authors": "Zhuoran Du \u00b7 Shaodi You \u00b7 Cheng Cheng \u00b7 Shikui Wei",
        "Abstract": "Hyperspectral image (HSI) densely samples the world in both the space and frequency domain and therefore is more distinctive than RGB images. Usually, HSI needs to be calibrated to minimize the impact of various illumination conditions. The traditional way to calibrate HSI utilizes a physical reference, which involves manual operations, occlusions, and/or limits camera mobility.These limitations inspire this paper to automatically calibrate HSIs using a learning-based method.Towards this goal, a large-scale HSI calibration dataset is created, which has 765 high-quality HSI pairs covering diversified natural scenes and illuminations. The dataset is further expanded to 7650 pairs by combining with 10 different physically measured illuminations.A spectral illumination transformer (SIT) together with an illumination attention module is proposed. Extensive benchmarks demonstrate the SoTA performance of the proposed SIT. The benchmarks also indicate that low-light conditions are more challenging than normal conditions.The dataset and codes are anonymously available online: https://anonymous.4open.science/r/Automatic-spectral-calibration-of-HSI-0C5A",
        "Link": "https://anonymous.4open.science/r/Automatic-spectral-calibration-of-HSI-0C5A",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of calibrating hyperspectral images (HSI) to minimize the impact of varying illumination conditions, traditionally done using physical references that involve manual operations and limit camera mobility. The authors propose a learning-based method for automatic calibration, introducing a large-scale HSI calibration dataset with 765 high-quality HSI pairs, expanded to 7650 pairs using 10 different physically measured illuminations. A Spectral Illumination Transformer (SIT) with an illumination attention module is developed, demonstrating state-of-the-art performance in benchmarks, particularly highlighting the difficulty of low-light conditions.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Low-Level Vision",
            "Spectral Illumination Transformer",
            "Illumination Attention Module",
            "Hyperspectral Image Calibration"
        ]
    },
    {
        "Title": "The Scene Language: Representing Scenes with Programs, Words, and Embeddings",
        "Authors": "Yunzhi Zhang \u00b7 Zizhang Li \u00b7 Matt Zhou \u00b7 Shangzhe Wu \u00b7 Jiajun Wu",
        "Abstract": "We introduce the Scene Language, a visual scene representation that concisely and precisely describes the structure, semantics, and identity of visual scenes. It represents a scene with three key components: a program that specifies the hierarchical and relational structure of entities in the scene, words in natural language that summarize the semantic class of each entity, and embeddings that capture the visual identity of each entity. This representation can be inferred from pre-trained language models via a training-free inference technique, given text or image inputs. The resulting scene can be rendered into images using traditional, neural, or hybrid graphics renderers. Together, this forms an automated system for high-quality 3D and 4D scene generation. Compared with existing representations like scene graphs, our proposed Scene Language generates complex scenes with higher fidelity, while explicitly modeling the scene structures to enable precise control and editing.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces the Scene Language, a novel visual scene representation that describes scenes using three components: a program for hierarchical and relational structure, natural language words for semantic class, and embeddings for visual identity. This representation is inferred from pre-trained language models without additional training, using text or image inputs. The Scene Language enables high-quality 3D and 4D scene generation with superior fidelity and explicit structural modeling, allowing precise control and editing compared to traditional scene graphs.",
        "Tags": [
            "3D Generation",
            "Scene Graph Generation",
            "Hierarchical Scene Representation",
            "Training-Free Inference",
            "Hybrid Graphics Rendering"
        ]
    },
    {
        "Title": "Optimal Transport-Guided Source-Free Adaptation for Face Anti-Spoofing",
        "Authors": "Jack Li \u00b7 Tianchen Zhao \u00b7 Xiang Xu \u00b7 Zheng Zhang \u00b7 Zhihua Li \u00b7 Xuanbai Chen \u00b7 Qin ZHANG \u00b7 Alessandro Bergamo \u00b7 Anil Kumar Jain \u00b7 Yifan Xing",
        "Abstract": "Developing a face anti-spoofing model that meets the security requirements of clients worldwide is challenging due to the domain gap between training datasets and the diverse end-user test data. Moreover, for security and privacy reasons, it is undesirable for clients to share large amount of their face data with service providers. In this work, we introduce a novel method where the face anti-spoofing model can be adapted by the client itself to a target domain at test time using only a small sample of data, while keeping model parameters and training data inaccessible to the client. We develop a prototype-based base model and an optimal transport-guided adaptor that enable adaptation either in a light-weight training or training-free setting, without updating the base model's parameters. Moreover, we employ geodesic mixup, an optimal transport-based synthesis method that generates augmented training data along the geodesic path between source prototypes and the target data distribution. This allows training a lightweight classifier to effectively adapt to target-specific characteristics while retaining essential knowledge learned from the source domain. In cross-domain and cross-attack setting, compared with recent methods, our method achieves average improvements of 19.17\\% in HTER and 8.58\\% in AUC, respectively.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of adapting face anti-spoofing models to diverse end-user environments without compromising client data privacy. The authors propose a novel method that allows clients to adapt the model using minimal data, without accessing the model's parameters or training data. The approach involves a prototype-based base model and an optimal transport-guided adaptor, facilitating adaptation in either a lightweight training or training-free manner. Additionally, the method employs geodesic mixup for data augmentation, enhancing the model's ability to adapt to target-specific characteristics while preserving knowledge from the source domain. The proposed method demonstrates significant improvements in cross-domain and cross-attack scenarios, outperforming recent methods by 19.17% in HTER and 8.58% in AUC.",
        "Tags": [
            "Face Anti-Spoofing",
            "Domain Adaptation",
            "Optimal Transport",
            "Geodesic Mixup",
            "Privacy-Preserving Adaptation",
            "Prototype-Based Learning"
        ]
    },
    {
        "Title": "Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning",
        "Authors": "Buzhen Huang \u00b7 Chen Li \u00b7 Chongyang Xu \u00b7 Dongyue Lu \u00b7 Jinnan Chen \u00b7 Yangang Wang \u00b7 Gim Hee Lee",
        "Abstract": "Due to visual ambiguities and inter-person occlusions, existing human pose estimation methods cannot recover plausible close interactions from in-the-wild videos. Even state-of-the-art large foundation models~(\\eg, SAM) cannot accurately distinguish human semantics in such challenging scenarios. In this work, we find that human appearance can provide a straightforward cue to address these obstacles. Based on this observation, we propose a dual-branch optimization framework to reconstruct accurate interactive motions with plausible body contacts constrained by human appearances, social proxemics, and physical laws. Specifically, we first train a diffusion model to learn the human proxemic behavior and pose prior knowledge. The trained network and two optimizable tensors are then incorporated into a dual-branch optimization framework to reconstruct human motions and appearances. Several constraints based on 3D Gaussians, 2D keypoints, and mesh penetrations are also designed to assist the optimization. With the proxemics prior and diverse constraints, our method is capable of estimating accurate interactions from in-the-wild videos captured in complex environments. We further build a dataset with pseudo ground-truth interaction annotations, which may promote future research on pose estimation and human behavior understanding. Experimental results on several benchmarks demonstrate that our method outperforms existing approaches. The code and data will be publicly available for research purpose.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of reconstructing close human interactions from in-the-wild videos, where visual ambiguities and inter-person occlusions hinder accurate pose estimation. The authors propose a dual-branch optimization framework that leverages human appearance, social proxemics, and physical laws to reconstruct plausible interactive motions. A diffusion model is trained to learn proxemic behavior and pose priors, which are then integrated into the framework alongside constraints based on 3D Gaussians, 2D keypoints, and mesh penetrations. The method demonstrates superior performance on benchmarks and introduces a dataset with pseudo ground-truth interaction annotations to advance research in pose estimation and human behavior understanding.",
        "Tags": [
            "3D Human Pose Estimation",
            "3D Reconstruction",
            "Diffusion Models",
            "Social Proxemics",
            "3D Gaussians"
        ]
    },
    {
        "Title": "Enhanced then Progressive Fusion with View Graph for Multi-View Clustering",
        "Authors": "Zhibin Dong \u00b7 Meng Liu \u00b7 Siwei Wang \u00b7 KE LIANG \u00b7 Yi Zhang \u00b7 Suyuan Liu \u00b7 Jiaqi Jin \u00b7 Xinwang Liu \u00b7 En Zhu",
        "Abstract": "Multi-view clustering aims to improve clustering accuracy by effectively integrating complementary information from multiple perspectives. However, existing methods often encounter challenges such as feature conflicts between views and insufficient enhancement of individual view features, which hinder clustering performance. To address these challenges, we propose a novel framework, EPFMVC, which integrates feature enhancement with progressive fusion to more effectively align multi-view data. Specifically, we introduce two key innovations: (1) a Feature Channel Attention Encoder (FCAencoder), which adaptively enhances the most discriminative features in each view, and (2) a View Graph-based Progressive Fusion Mechanism, which constructs a view graph using optimal transport (OT) distance to progressively fuse similar views while minimizing inter-view conflicts. By leveraging multi-head attention, the fusion process gradually integrates complementary information, ensuring more consistent and robust shared representations. These innovations enable superior representation learning and effective fusion across views. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art techniques, achieving notable improvements in multi-view clustering tasks across various datasets and evaluation metrics.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces EPFMVC, a novel framework for multi-view clustering that addresses challenges such as feature conflicts and insufficient enhancement of individual view features. The framework integrates feature enhancement with progressive fusion, utilizing a Feature Channel Attention Encoder (FCAencoder) to enhance discriminative features and a View Graph-based Progressive Fusion Mechanism to minimize inter-view conflicts. This mechanism constructs a view graph using optimal transport distance and employs multi-head attention to progressively fuse similar views, ensuring robust shared representations. The method demonstrates superior performance in multi-view clustering tasks across various datasets and metrics.",
        "Tags": [
            "Multimodal Learning",
            "Graph Neural Networks (GNNs)",
            "Feature Channel Attention",
            "Optimal Transport Distance",
            "Multi-Head Attention"
        ]
    },
    {
        "Title": "Distilling Spatially-Heterogeneous Distortion Perception for Blind Image Quality Assessment",
        "Authors": "Xudong Li \u00b7 Wenjie Nie \u00b7 Yan Zhang \u00b7 Runze Hu \u00b7 Ke Li \u00b7 Xiawu Zheng \u00b7 Liujuan Cao",
        "Abstract": "In the Blind Image Quality Assessment (BIQA) field, accurately assessing the quality of authentically distorted images presents a substantial challenge due to the diverse distortion types in natural settings. Existing State-of-the-art IQA methods mix a sequence of distortions to entire images to establish distortion priors, but are inadequate for images with spatially local distortions. To address this, we introduce a novel IQA framework that employs knowledge distillation tailored to perceive spatially heterogeneous distortions, enhancing quality-distortion awareness. Specifically, we introduce a novel Block-wise Degradation Modelling approach that applies distinct distortions to different spatial blocks of an image, thereby expanding varied distortion priors. Following this, we present a Block-wise Aggregation and Filtering module that enables fine-grained attention to the quality perception within different distortion areas of the image. Furthermore, to enhance the granularity of distortion perception across various regions while maintaining quality perception, we incorporate strategies of Contrastive Knowledge Distillation and Affinity Knowledge Distillation to learn the distortion discrimination power and distortion correlation of different regions, respectively. Extensive experiments on seven standard BIQA datasets demonstrate the superior performance to the state-of-the-art BIQA methods, i.e., achieving the PLCC values of 0.947 ($\\uparrow 1.2\\%$ vs. 0.936 in KADID) and 0.735 ($\\uparrow 8.3\\%$ vs. 0.679 in LIVEFB).",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of Blind Image Quality Assessment (BIQA) for images with spatially heterogeneous distortions, which are common in natural settings. Existing methods often fail to accurately assess such images due to their uniform distortion application across entire images. The authors propose a novel IQA framework that utilizes knowledge distillation to better perceive spatially varied distortions. The framework includes a Block-wise Degradation Modelling approach that applies different distortions to various spatial blocks of an image, enhancing distortion priors. Additionally, a Block-wise Aggregation and Filtering module is introduced to focus attention on quality perception within different distortion areas. The framework also employs Contrastive Knowledge Distillation and Affinity Knowledge Distillation to improve distortion discrimination and correlation across regions. The proposed method demonstrates superior performance on seven standard BIQA datasets, outperforming state-of-the-art methods in terms of PLCC values.",
        "Tags": [
            "Image Quality Assessment",
            "Knowledge Distillation",
            "Spatially Heterogeneous Distortions",
            "Block-wise Degradation Modelling",
            "Contrastive Knowledge Distillation"
        ]
    },
    {
        "Title": "Uncertainty-guided Perturbation for Image Super-Resolution Diffusion Model",
        "Authors": "Leheng Zhang \u00b7 Weiyi You \u00b7 Kexuan Shi \u00b7 Shuhang Gu",
        "Abstract": "Diffusion-based image super-resolution methods have demonstrated significant advantages over GAN-based approaches, particularly in terms of perceptual quality. Building upon a lengthy Markov chain, diffusion-based methods possess remarkable modeling capacity, enabling them to achieve outstanding performance in real-world scenarios. Unlike previous methods that focus on modifying the noise schedule or sampling process to enhance performance, our approach emphasizes the improved utilization of LR information. We find that different regions of the LR image can be viewed as corresponding to different timesteps in a diffusion process, where flat areas are closer to the target HR distribution but edge and texture regions are farther away. In these flat areas, applying a slight noise is more advantageous for the reconstruction. We associate this characteristic with uncertainty and propose to apply uncertainty estimate to guide region-specific noise level control, a technique we refer to as Uncertainty-guided Noise Weighting. Pixels with lower uncertainty (i.e., flat regions) receive reduced noise to preserve more LR information, therefore improving performance. Furthermore, we modify the network architecture of previous methods to develop our Uncertainty-guided Perturbation Super-Resolution (UPSR) model. Extensive experimental results demonstrate that, despite reduced model size and training overhead, the proposed UWSR method outperforms current state-of-the-art methods across various datasets, both quantitatively and qualitatively.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces an uncertainty-guided approach to enhance diffusion-based image super-resolution methods, which traditionally outperform GAN-based methods in perceptual quality. The proposed method, Uncertainty-guided Perturbation Super-Resolution (UPSR), leverages the observation that different regions of a low-resolution (LR) image correspond to different timesteps in a diffusion process. Flat areas are closer to the high-resolution (HR) target distribution, while edge and texture regions are farther away. By associating these characteristics with uncertainty, the method applies region-specific noise level control, termed Uncertainty-guided Noise Weighting, where pixels with lower uncertainty receive reduced noise to preserve more LR information. This approach not only improves performance but also reduces model size and training overhead. The UPSR model demonstrates superior performance over state-of-the-art methods across various datasets, both quantitatively and qualitatively.",
        "Tags": [
            "Diffusion Models",
            "Super-Resolution",
            "Uncertainty Estimation",
            "Noise Weighting",
            "Image Reconstruction"
        ]
    },
    {
        "Title": "DeepLA-Net: Very Deep Local Aggregation Networks for Point Cloud Analysis",
        "Authors": "Ziyin Zeng \u00b7 Ziyin Zeng \u00b7 Mingyue Dong \u00b7 Jian Zhou \u00b7 Huan Qiu \u00b7 Zhen Dong \u00b7 Man Luo \u00b7 Bijun Li",
        "Abstract": "Due to the irregular and disordered data structure in 3D point clouds, prior works have focused on designing more sophisticated local representation methods to capture these complex local patterns. However, the recognition performance has saturated over the past few years, indicating that increasingly complex and redundant designs no longer make improvements to local learning. This phenomenon prompts us to diverge from the trend in 3D vision and instead pursue an alternative and successful solution: deeper neural networks. In this paper, we propose DeepLA-Net, a series of very deep networks for point cloud analysis. The key insight of our approach is to exploit a small but mighty local learning block, which reduces $10\\times$ fewer FLOPs, enabling the construction of very deep networks. Furthermore, we design a training supervision strategy to ensure smooth gradient backpropagation and optimization in very deep networks. We construct the DeepLA-Net family with a depth of up to 120 blocks --- at least $5\\times$ deeper than recent methods --- trained on a single RTX 3090. An ensemble of the DeepLA-Net achieves state-of-the-art performance on classification and segmentation tasks of S3DIS Area5 (+2.2\\% mIoU), ScanNet test set (+1.6\\% mIoU), ScanObjectNN (+2.1\\% OA), and ShapeNetPart (+0.9\\% cls.mIoU).",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "DeepLA-Net introduces a series of very deep networks for point cloud analysis, diverging from the trend of increasingly complex local representation methods. The approach leverages a compact local learning block that significantly reduces computational complexity, enabling the construction of networks up to 120 blocks deep. A novel training supervision strategy ensures effective gradient backpropagation and optimization. DeepLA-Net achieves state-of-the-art performance on multiple benchmarks for classification and segmentation tasks.",
        "Tags": [
            "3D Point Cloud",
            "3D Semantic Segmentation",
            "Deep Neural Networks",
            "Local Learning Block",
            "Training Supervision Strategy"
        ]
    },
    {
        "Title": "UniPose: A Unified MultiModal Framework for  Human Pose Comprehension, Generation and Editing",
        "Authors": "Yiheng Li \u00b7 RuiBing Hou \u00b7 Hong Chang \u00b7 Shiguang Shan \u00b7 Xilin Chen",
        "Abstract": "Human pose plays a crucial role in the digital age. While recent works have achieved impressive progress in understanding and generating human poses, they often support only a single modality of control signals and operate in isolation, limiting their application in real-world scenarios. This paper presents UniPose, a framework employing Large Language Models (LLMs) to comprehend, generate, and edit human poses across various modalities, including images, text, and 3D SMPL poses. Specifically,  we apply a pose tokenizer to convert 3D poses into discrete pose tokens, enabling seamless integration into the LLM within a unified vocabulary. To further enhance the fine-grained pose perception capabilities, we facilitate UniPose with a mixture of visual encoders, among them a pose-specific visual encoder. Benefiting from a unified learning strategy, UniPose effectively transfers knowledge across different pose-relevant tasks, adapts to unseen tasks, and exhibits extended capabilities. This work serves as the first attempt at building a general-purpose framework for pose comprehension, generation, and editing. Extensive experiments highlight UniPose's competitive and even superior performance across various pose-relevant tasks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "UniPose introduces a unified multimodal framework leveraging Large Language Models (LLMs) for human pose comprehension, generation, and editing across multiple modalities, including images, text, and 3D SMPL poses. The framework employs a pose tokenizer to convert 3D poses into discrete tokens, integrating them into the LLM's vocabulary. Enhanced by a mixture of visual encoders, including a pose-specific encoder, UniPose demonstrates effective knowledge transfer across pose-related tasks, adaptability to unseen tasks, and extended capabilities. This work represents the first general-purpose framework for multimodal human pose tasks, showcasing competitive performance across various applications.",
        "Tags": [
            "Large Language Models (LLMs)",
            "3D Human Pose Estimation",
            "Multimodal Learning",
            "Pose Tokenization",
            "Knowledge Transfer"
        ]
    },
    {
        "Title": "FireEdit: Fine-grained Instruction-based Image Editing via Region-aware Vision Language Model",
        "Authors": "Jun Zhou \u00b7 Jiahao Li \u00b7 Zunnan Xu \u00b7 Hanhui Li \u00b7 Yiji Cheng \u00b7 Fa-Ting Hong \u00b7 Qin Lin \u00b7 qinglin lu \u00b7 Xiaodan Liang",
        "Abstract": "Currently, instruction-based image editing methods have made significant progress by leveraging the powerful cross-modal understanding capabilities of visual language models (VLMs). However, they still face challenges in three key areas: 1) complex scenarios; 2) semantic consistency; and 3) fine-grained editing. To address these issues, we propose FireEdit, an innovative \\textbf{F}ine-grained \\textbf{I}nstruction-based image editing framework that exploits a REgion-aware VLM. FireEdit is designed to accurately comprehend user instructions and ensure effective control over the editing process. We employ a VLM to precisely localize the desired editing regions within complex scenes. To enhance the fine-grained visual perception capabilities of the VLM, we introduce additional region tokens that complement the holistic image features and are integrated into the user's instructions. Relying solely on the output of the Language Model (LLM) to guide the diffusion model may result in suboptimal editing outcomes.Therefore, we propose a Time-Aware Target Injection module and a Hybrid Visual Cross Attention module. The former dynamically adjusts the guidance strength at various denoising stages by integrating timestep embeddings with the text embeddings. The latter enhances visual details for image editing, thereby preserving semantic consistency between the edited result and the source image. By combining the VLM enhanced with fine-grained region tokens and the time-dependent diffusion model, FireEdit demonstrates significant advantages in comprehending editing instructions and maintaining high semantic consistency. Extensive experiments indicate that our approach surpasses the state-of-the-art instruction-based image editing methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "FireEdit introduces a fine-grained instruction-based image editing framework leveraging a region-aware Vision Language Model (VLM) to address challenges in complex scenarios, semantic consistency, and fine-grained editing. The framework employs a VLM for precise localization of editing regions and enhances fine-grained visual perception through additional region tokens. To improve editing outcomes, FireEdit incorporates a Time-Aware Target Injection module and a Hybrid Visual Cross Attention module, which dynamically adjust guidance strength and enhance visual details, respectively. The combination of the enhanced VLM and a time-dependent diffusion model allows FireEdit to achieve superior comprehension of editing instructions and maintain high semantic consistency, outperforming existing state-of-the-art methods.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Image Editing",
            "Fine-Grained Editing",
            "Time-Aware Target Injection",
            "Hybrid Visual Cross Attention"
        ]
    },
    {
        "Title": "PatchDEMUX: A Certifiably Robust Framework for Multi-label Classifiers Against Adversarial Patches",
        "Authors": "Dennis Jacob \u00b7 Chong Xiang \u00b7 Prateek Mittal",
        "Abstract": "Deep learning techniques have enabled vast improvements in computer vision technologies. Nevertheless, these models are vulnerable to adversarial patch attacks which  catastrophically impair performance. The physically realizable nature of these attacks calls for certifiable defenses, which feature provable guarantees on robustness. While certifiable defenses have been successfully applied to single-label classification, limited work has been done for multi-label classification. In this work, we present PatchDEMUX, a certifiably robust framework for multi-label classifiers against adversarial patches. Our approach is a generalizable method which can provably extend any existing certifiable defense for single-label classification; this is done by considering the multi-label classification task as a series of isolated binary classification problems. In addition, because one patch can only be placed at one location, we further develop a novel certification procedure that provides a tighter robustness certification bound. Using the current state-of-the-art (SOTA) single-label certifiable defense PatchCleanser as a backbone, we find that PatchDEMUX can achieve non-trivial robustness on the MSCOCO 2014 validation dataset while maintaining high clean performance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "PatchDEMUX is introduced as a certifiably robust framework designed to protect multi-label classifiers against adversarial patch attacks. Unlike previous efforts focused on single-label classification, PatchDEMUX generalizes existing certifiable defenses by treating multi-label classification as a series of binary classification tasks. The framework also introduces a novel certification procedure that tightens the robustness certification bound by leveraging the constraint that only one patch can be placed at a single location. By building on the state-of-the-art single-label certifiable defense, PatchCleanser, PatchDEMUX demonstrates significant robustness on the MSCOCO 2014 validation dataset while preserving high clean performance.",
        "Tags": [
            "Adversarial Robustness",
            "Multi-label Classification",
            "Certifiable Robustness",
            "Adversarial Patch Defense",
            "Binary Classification Decomposition"
        ]
    },
    {
        "Title": "GaussianIP: Identity-Preserving Realistic 3D Human Generation via Human-Centric Diffusion Prior",
        "Authors": "Zichen Tang \u00b7 Yuan Yao \u00b7 Miaomiao Cui \u00b7 Liefeng Bo \u00b7 Hongyu Yang",
        "Abstract": "Text-guided 3D human generation has advanced with the development of efficient 3D representations and 2D-lifting methods like score distillation sampling (SDS). However, current methods suffer from prolonged training times and often produce results that lack fine facial and garment details. In this paper, we propose GaussianIP, an effective two-stage framework for generating identity-preserving realistic 3D humans from text and image prompts. Our core insight is to leverage human-centric knowledge to facilitate the generation process. In stage 1, we propose a novel Adaptive Human Distillation Sampling (AHDS) method to rapidly generate a 3D human that maintains high identity consistency with the image prompt and achieves a realistic appearance. Compared to traditional SDS methods, AHDS better aligns with the human-centric generation process, enhancing visual quality with notably fewer training steps. To further improve the visual quality of the face and clothes regions, we design a View-Consistent Refinement (VCR) strategy in stage 2. Specifically, it produces detail-enhanced results of the multi-view images from stage 1 iteratively, ensuring the 3D texture consistency across views via mutual attention and distance-guided attention fusion. Then a polished version of the 3D human can be achieved by directly perform reconstruction with the refined images. Extensive experiments demonstrate that GaussianIP outperforms existing methods in both visual quality and training efficiency, particularly in generating identity-preserving results.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "GaussianIP introduces a two-stage framework for generating identity-preserving realistic 3D humans from text and image prompts, addressing the limitations of current methods such as prolonged training times and lack of fine details. The first stage employs an Adaptive Human Distillation Sampling (AHDS) method to rapidly generate 3D humans with high identity consistency and realistic appearance, significantly reducing training steps compared to traditional methods. The second stage enhances visual quality through a View-Consistent Refinement (VCR) strategy, which iteratively improves multi-view images for consistent 3D texture across views, leading to a polished 3D human model. GaussianIP demonstrates superior visual quality and training efficiency over existing methods, particularly in preserving identity.",
        "Tags": [
            "3D Generation",
            "Avatars",
            "Adaptive Human Distillation Sampling",
            "View-Consistent Refinement",
            "Identity-Preserving Generation"
        ]
    },
    {
        "Title": "AnimateAnything: Consistent and Controllable Animation for video generation",
        "Authors": "guojun lei \u00b7 Chi Wang \u00b7 Rong Zhang \u00b7 Yikai Wang \u00b7 Hong Li \u00b7 Weiwei Xu",
        "Abstract": "We propose a unified approach for video-controlled generation, enabling text-based guidance and manual annotations to control the generation of videos, similar to camera direction guidance. Specifically, we designed a two-stage algorithm. In the first stage, we convert all control information into frame-by-frame motion flows. In the second stage, we use these motion flows as guidance to control the final video generation. Additionally, to reduce instability in the generated videos caused by large motion variations (such as those from camera movement, object motion, or manual inputs), which can result in flickering or the intermittent disappearance of objects, we transform the temporal feature computation in the video model into frequency-domain feature computation. This is because frequency-domain signals better capture the essential characteristics of an image, and by ensuring consistency in the video's frequency-domain features, we can enhance temporal coherence and reduce flickering in the final generated video.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces a unified approach for video-controlled generation, leveraging text-based guidance and manual annotations to control video generation akin to camera direction guidance. The proposed method employs a two-stage algorithm: first, converting control information into frame-by-frame motion flows, and second, using these flows to guide the final video generation. To address instability issues like flickering or object disappearance due to large motion variations, the authors transform temporal feature computation into frequency-domain feature computation. This ensures better temporal coherence and reduces flickering in the generated videos.",
        "Tags": [
            "Video Generation",
            "Controllable Animation",
            "Frequency-Domain Feature Computation",
            "Temporal Coherence",
            "Motion Flow Guidance"
        ]
    },
    {
        "Title": "Rashomon Sets for Prototypical-Part Models: Editing Accurate Interpretable Models in Real-Time",
        "Authors": "Jon Donnelly \u00b7 Zhicheng Guo \u00b7 Alina Jade Barnett \u00b7 Hayden McTavish \u00b7 Chaofan Chen \u00b7 Cynthia Rudin",
        "Abstract": "Interpretability is critical for machine learning models in high-stakes settings because it allows users to verify the model's reasoning. In computer vision, prototypical part models (ProtoPNets) have become the dominant model type to meet this need.Users can easily identify flaws in ProtoPNets, but fixing problems in a ProtoPNet requires slow, difficult retraining that is not guaranteed to resolve the issue. This problem is called the \"interaction bottleneck.\"We solve the interaction bottleneck for ProtoPNets by simultaneously finding many equally good ProtoPNets (i.e., a draw from a \"Rashomon set\"). We show that our framework -- called Proto-RSet -- quickly produces many accurate, diverse ProtoPNets, allowing users to correct problems in real time while maintaining performance guarantees with respect to the training set. We demonstrate the utility of this method in two settings: 1) removing synthetic bias introduced to a bird-identification model and 2) debugging a skin cancer identification model. This tool empowers non-machine-learning experts, such as clinicians or domain experts, to quickly refine and correct machine learning models without repeated retraining by machine learning experts.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the 'interaction bottleneck' in prototypical part models (ProtoPNets), which are widely used for interpretability in high-stakes computer vision tasks. While ProtoPNets allow users to identify flaws, fixing these issues typically requires slow and uncertain retraining. The authors introduce Proto-RSet, a framework that generates multiple equally accurate and diverse ProtoPNets (a 'Rashomon set') in real time. This enables users to correct model flaws without retraining, maintaining performance guarantees. The method is demonstrated in two scenarios: removing synthetic bias in a bird-identification model and debugging a skin cancer identification model. Proto-RSet empowers non-experts, such as clinicians, to refine models efficiently without relying on machine learning specialists.",
        "Tags": [
            "Interpretability",
            "Prototypical-Part Models",
            "Rashomon Set",
            "Real-Time Model Editing",
            "Bias Removal"
        ]
    },
    {
        "Title": "WeakMCN: Multi-task Collaborative Network for Weakly Supervised Referring Expression Comprehension and Segmentation",
        "Authors": "Silin Cheng \u00b7 Yang Liu \u00b7 Xinwei He \u00b7 Sebastien Ourselin \u00b7 Lei Tan \u00b7 Gen Luo",
        "Abstract": "Weakly supervised referring expression comprehension (WREC) and segmentation (WRES) aim to learn object grounding based on a given expression using weak supervision signals like image-text pairs. While these tasks have traditionally been modeled separately, we argue that they can benefit from joint learning in a multi-task framework. To this end, we propose WeakMCN, a novel multi-task collaborative network that effectively combines WREC and WRES with a dual-branch architecture. Specifically, the WREC branch is formulated as anchor-based contrastive learning, which also acts as a teacher to supervise the WRES branch. In WeakMCN, we propose two innovative designs to facilitate multi-task collaboration, namely Dynamic Visual Feature Enhancement (DVFE) and Collaborative Consistency Module (CCM). DVFE dynamically combines various pre-trained visual knowledge to meet different task requirements, while CCM promotes cross-task consistency from the perspective of optimization. Extensive experimental results on three popular REC and RES benchmarks, i.e., RefCOCO, RefCOCO+, and RefCOCOg, consistently demonstrate performance gains of WeakMCN over state-of-the-art single-task alternatives, e.g., up to 3.91% and 13.11% on RefCOCO for WREC and WRES tasks, respectively. Furthermore, experiments also validate the strong generalization ability of WeakMCN in both semi-supervised REC and RES settings against existing methods, e.g., +8.94% for semi-REC and +7.71% for semi-RES on 1% RefCOCO.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "WeakMCN is a novel multi-task collaborative network designed for weakly supervised referring expression comprehension (WREC) and segmentation (WRES). It employs a dual-branch architecture where the WREC branch uses anchor-based contrastive learning and supervises the WRES branch. The network introduces two key components: Dynamic Visual Feature Enhancement (DVFE) for task-specific visual feature adaptation and Collaborative Consistency Module (CCM) for cross-task optimization consistency. WeakMCN demonstrates superior performance on benchmarks like RefCOCO, RefCOCO+, and RefCOCOg, outperforming state-of-the-art single-task models and showing strong generalization in semi-supervised settings.",
        "Tags": [
            "Referring Image Segmentation",
            "Weakly Supervised Learning",
            "Multi-task Learning",
            "Contrastive Learning",
            "Dynamic Feature Enhancement"
        ]
    },
    {
        "Title": "BIMBA: Selective-Scan Compression for Long-Range Video Question Answering",
        "Authors": "Md Mohaiminul Islam \u00b7 Tushar Nagarajan \u00b7 Huiyu Wang \u00b7 Gedas Bertasius \u00b7 Lorenzo Torresani",
        "Abstract": "Video Question Answering (VQA) in long videos poses the key challenge of extracting relevant information and modeling long-range dependencies from many redundant frames. The self-attention mechanism provides a general solution for sequence modeling, but it has a prohibitive cost when applied to a massive number of spatiotemporal tokens in long videos. To lower the computational cost, most prior methods rely on compression strategies, such as reducing the input length via sparse frame sampling or compressing the output sequence passed to the large language model (LLM) via space-time pooling. However, these naive approaches over-represent redundant information and often miss salient events or fast-occurring space-time patterns. In this work, we introduce \\model, an efficient state-space model to handle long-form videos. Our model leverages the selective scan algorithm to learn to effectively select critical information from high-dimensional video and transform it into a token sequence that is orders of magnitude smaller for efficient  LLM processing. Extensive experiments demonstrate that \\model\\  achieves state-of-the-art accuracy on multiple long-form VQA benchmarks, including EgoSchema, NextQA, TempCompass, and MVBench.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Video Question Answering (VQA) in long videos faces challenges in extracting relevant information and modeling long-range dependencies due to redundant frames. Traditional self-attention mechanisms are computationally expensive for spatiotemporal tokens in long videos. Prior methods reduce computational costs through compression strategies like sparse frame sampling or space-time pooling, but these often miss critical events. This paper introduces BIMBA, an efficient state-space model that uses a selective scan algorithm to identify and compress critical video information into a smaller token sequence for efficient processing by large language models (LLMs). BIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks.",
        "Tags": [
            "Video Understanding",
            "Large Language Models (LLMs)",
            "Selective Scan Algorithm",
            "State-Space Model",
            "Long-Form Video Processing"
        ]
    },
    {
        "Title": "ARM: Appearance Reconstruction Model for Relightable 3D Generation",
        "Authors": "Xiang Feng \u00b7 Chang Yu \u00b7 Zoubin Bi \u00b7 Yintong Shang \u00b7 Feng Gao \u00b7 Hongzhi Wu \u00b7 Kun Zhou \u00b7 Chenfanfu Jiang \u00b7 Yin Yang",
        "Abstract": "Recent image-to-3D reconstruction models have greatly advanced geometry generation, but they still struggle to faithfully generate realistic appearance. To address this, we introduce ARM, a novel method that reconstructs high-quality 3D meshes and realistic appearance from sparse-view images. The core of ARM lies in decoupling geometry from appearance, processing appearance within the UV texture space. Unlike previous methods, ARM improves texture quality by explicitly back-projecting measurements onto the texture map and processing them in a UV space module with a global receptive field. To resolve ambiguities between material and illumination in input images, ARM introduces a material prior that encodes semantic appearance information, enhancing the robustness of appearance decomposition. Trained on just 8 H100 GPUs, ARM outperforms existing methods both quantitatively and qualitatively.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces ARM, a novel method for reconstructing high-quality 3D meshes and realistic appearance from sparse-view images. ARM decouples geometry from appearance, processing appearance in the UV texture space to improve texture quality. It employs a material prior to resolve ambiguities between material and illumination, enhancing appearance decomposition. ARM demonstrates superior performance over existing methods in both quantitative and qualitative evaluations.",
        "Tags": [
            "3D Generation",
            "3D Reconstruction",
            "UV Texture Space",
            "Material Prior",
            "Appearance Decomposition"
        ]
    },
    {
        "Title": "Hearing Anywhere in Any Environment",
        "Authors": "Xiulong Liu \u00b7 Anurag Kumar \u00b7 Paul Calamia \u00b7 Sebastia Vicenc Amengual Gari \u00b7 Calvin Murdock \u00b7 Ishwarya Ananthabhotla \u00b7 Philip W Robinson \u00b7 Eli Shlizerman \u00b7 Vamsi Krishna Ithapu \u00b7 Ruohan Gao",
        "Abstract": "In mixed reality applications, a realistic acoustic experience in spatial environments is as crucial as the visual experience for achieving true immersion. Despite recent advances in neural approaches for Room Impulse Response (RIR) estimation, most existing methods are limited to the single environment on which they are trained, lacking the ability to generalize to new rooms with different geometries and surface materials. We aim to develop a unified model capable of reconstructing the spatial acoustic experience of any environment with minimum additional measurements. To this end, we present xRIR, a framework for cross-room RIR prediction. The core of our generalizable approach lies in combining a geometric feature extractor, which captures spatial context from panorama depth images, with a RIR encoder that extracts detailed acoustic features from only a few reference RIR samples. To evaluate our method, we introduce AcousticRooms, a new dataset featuring high-fidelity simulation of over 300,000 RIRs from 260 rooms. Experiments show that our method strongly outperforms a series of baselines. Furthermore, we successfully perform sim-to-real transfer by evaluating our model on four real-world environments, demonstrating the generalizability of our approach and the realism of our dataset.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces xRIR, a framework designed to predict Room Impulse Responses (RIR) across different environments, aiming to enhance the acoustic experience in mixed reality applications. Unlike existing methods that are limited to specific environments, xRIR combines a geometric feature extractor with a RIR encoder to generalize across various room geometries and materials using minimal additional measurements. The effectiveness of xRIR is validated through the introduction of AcousticRooms, a new dataset with over 300,000 RIRs from 260 rooms, and demonstrated by outperforming baseline methods and successfully transferring from simulation to real-world environments.",
        "Tags": [
            "3D Reconstruction",
            "Multimodal Learning",
            "Room Impulse Response Prediction",
            "Cross-room Generalization",
            "Sim-to-real Transfer"
        ]
    },
    {
        "Title": "PanoGS: Gaussian-based Panoptic Segmentation for 3D Open Vocabulary Scene Understanding",
        "Authors": "Hongjia Zhai \u00b7 Hai Li \u00b7 Zhenzhe Li \u00b7 Xiaokun Pan \u00b7 Yijia He \u00b7 Guofeng Zhang",
        "Abstract": "Recently, 3D Gaussian Splatting (3DGS) has shown encouraging performance for open vocabulary scene understanding tasks. However, previous methods can not distinguish 3D instance-level information, which usually predicts a heatmap between the scene feature and text query. In this paper, we propose PanoGS, a novel and efficient 3D panoptic open vocabulary scene understanding approach. Technical-wise, to learn accurate 3D language features that can scale to large indoor scenarios, we adopt the pyramid tri-planes to model the latent continuous parametric feature space and use a 3D feature decoder to regress the multi-view fused 2D feature cloud. Besides, we propose language-guided graph cuts that synergistically leverage reconstructed geometry and learned language cues to group 3D Gaussian primitives into a set of super-primitives. To obtain 3D consistent instance, we perform graph clustering based segmentation with SAM-guided edge affinity computation between different super-primitives. Extensive experiments on widely used datasets show better or more competitive performance on 3D panoptic open vocabulary scene understanding.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "PanoGS introduces a novel approach for 3D panoptic open vocabulary scene understanding using 3D Gaussian Splatting (3DGS). Unlike previous methods that lack instance-level distinction, PanoGS employs pyramid tri-planes to model a continuous parametric feature space and a 3D feature decoder for multi-view fused 2D feature cloud regression. It also introduces language-guided graph cuts to group 3D Gaussian primitives into super-primitives and uses SAM-guided edge affinity computation for 3D consistent instance segmentation. The method demonstrates superior or competitive performance on standard datasets.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Panoptic Segmentation",
            "Open Vocabulary Scene Understanding",
            "Language-Guided Graph Cuts",
            "SAM-Guided Edge Affinity"
        ]
    },
    {
        "Title": "Pattern Analogies: Learning to Perform Programmatic Image Edits by Analogy",
        "Authors": "Aditya Ganeshan \u00b7 Thibault Groueix \u00b7 Paul Guerrero \u00b7 Radomir Mech \u00b7 Matthew Fisher \u00b7 Daniel Ritchie",
        "Abstract": "Pattern images are everywhere in the digital and physical worlds, and tools to edit them are valuable. But editing pattern images is tricky: desired edits are often programmatic: structure-aware edits that alter the underlying program which generates the pattern. One could attempt to infer this underlying program, but current methods for doing so struggle with complex images and produce unorganized programs that make editing tedious. In this work, we introduce a novel approach to perform programmatic edits on pattern images. By using a pattern analogy\u2014a pair of simple patterns to demonstrate the intended edit\u2014and a learning-based generative model to execute these edits, our method allows users to intuitively edit patterns. To enable this paradigm, we introduce SplitWeave, a domain-specific language that, combined with a framework for sampling synthetic pattern analogies, enables the creation of a large, high-quality synthetic training dataset. We also present TriFuser, a Latent Diffusion Model (LDM) designed to overcome critical issues that arise when naively deploying LDMs to this task. Extensive experiments on real-world, artist-sourced patterns reveals that our method faithfully performs the demonstrated edit while also generalizing to related pattern styles beyond its training distribution.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a novel approach for performing programmatic edits on pattern images using pattern analogies\u2014pairs of simple patterns that demonstrate the intended edit. The method leverages a learning-based generative model to execute these edits intuitively. To support this approach, the authors develop SplitWeave, a domain-specific language for creating synthetic pattern analogies, and TriFuser, a Latent Diffusion Model (LDM) tailored to address challenges in deploying LDMs for this task. The method demonstrates the ability to faithfully perform edits and generalize to related pattern styles beyond its training distribution.",
        "Tags": [
            "Image Editing",
            "Latent Diffusion Models (LDMs)",
            "Programmatic Image Edits",
            "Pattern Analogies",
            "SplitWeave",
            "TriFuser"
        ]
    },
    {
        "Title": "Acquire and then Adapt: Squeezing out Text-to-Image Model for Image Restoration",
        "Authors": "Junyuan Deng \u00b7 Xinyi Wu \u00b7 Yongxing Yang \u00b7 Congchao Zhu \u00b7 Song Wang \u00b7 Zhenyao Wu",
        "Abstract": "Recently, pre-trained text-to-image (T2I) models have been extensively adopted for real-world image restoration because of their powerful generative prior. However, controlling these large models for image restoration usually requires a large number of high-quality images and immense computational resources for training, which is costly and not privacy-friendly. In this paper, we find that the well-trained large T2I model (i.e., Flux) is able to produce a variety of high-quality images aligned with real-world distributions, offering an unlimited supply of training samples to mitigate the above issue. Specifically, we proposed a training data construction pipeline for image restoration, namely FluxGen, which includes unconditional image generation, image selection, and degraded image simulation. A novel light-weighted adapter (FluxIR)  with squeeze-and-excitation layers is also carefully designed to control the large Diffusion Transformer (DiT)-based T2I model so that reasonable details can be restored. Experiments demonstrate that our proposed method enables the Flux model to adapt effectively to real-world image restoration tasks, achieving superior scores and visual quality on both synthetic and real-world degradation datasets - at only about 8.5\\% of the training cost compared to current approaches.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a novel approach to leverage pre-trained text-to-image (T2I) models for real-world image restoration, addressing the challenges of high computational costs and privacy concerns associated with traditional methods. The authors propose FluxGen, a pipeline for constructing training data that includes unconditional image generation, image selection, and degraded image simulation. Additionally, they design a lightweight adapter, FluxIR, equipped with squeeze-and-excitation layers, to efficiently control a large Diffusion Transformer (DiT)-based T2I model for restoring image details. The method significantly reduces training costs to about 8.5% of current approaches while achieving superior performance on both synthetic and real-world degradation datasets.",
        "Tags": [
            "Diffusion Models",
            "Image Restoration",
            "Squeeze-and-Excitation Layers",
            "Training Data Construction",
            "Lightweight Adapter"
        ]
    },
    {
        "Title": "Light3R-SfM: Towards Feed-forward Structure-from-motion",
        "Authors": "Sven Elflein \u00b7 Qunjie Zhou \u00b7 Laura Leal-Taixe",
        "Abstract": "We present Light3R-SfM, a feed-forward, end-to-end learnable framework for efficient large-scale Structure-from-Motion (SfM) from unconstrained image collections. Unlike existing SfM solutions that rely on costly matching and global optimization to achieve accurate 3D reconstructions, Light3R-SfM addresses this limitation through a novel latent global alignment module. This module replaces traditional global optimization with a learnable attention mechanism, effectively capturing multi-view constraints across images for robust and precise camera pose estimation. Light3R-SfM constructs a sparse scene graph via retrieval-score-guided shortest path tree to dramatically reduce memory usage and computational overhead compared to the naive approach. Extensive experiments demonstrate that Light3R-SfM achieves competitive accuracy while significantly reducing runtime, making it ideal for 3D reconstruction tasks in real-world applications with a runtime constraint. This work pioneers a data-driven, feed-forward SfM approach, paving the way toward scalable, accurate, and efficient 3D reconstruction in the wild.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Light3R-SfM introduces a feed-forward, end-to-end learnable framework for efficient large-scale Structure-from-Motion (SfM) from unconstrained image collections. It replaces traditional global optimization with a novel latent global alignment module using a learnable attention mechanism, enabling robust camera pose estimation. The framework constructs a sparse scene graph via a retrieval-score-guided shortest path tree, reducing memory usage and computational overhead. Light3R-SfM achieves competitive accuracy with significantly reduced runtime, making it suitable for real-world 3D reconstruction tasks with runtime constraints.",
        "Tags": [
            "3D Reconstruction",
            "Structure-from-Motion (SfM)",
            "Attention Mechanism",
            "Sparse Scene Graph",
            "Feed-forward Learning"
        ]
    },
    {
        "Title": "BLADE: Single-view Body Mesh Learning through Accurate Depth Estimation",
        "Authors": "Shengze Wang \u00b7 Jiefeng Li \u00b7 Tianye Li \u00b7 Ye Yuan \u00b7 Henry Fuchs \u00b7 Koki Nagano \u00b7 Shalini De Mello \u00b7 Michael Stengel",
        "Abstract": "Single-image human mesh recovery is a challenging task due to the ill-posed nature of simultaneous body shape, pose, and camera estimation. Existing estimators work well on images taken from afar, but they break down as the person moves close to the camera.Moreover, current methods fail to achieve both accurate 3D pose and 2D alignment at the same time. Error is mainly introduced by inaccurate perspective projection heuristically derived from orthographic parameters. To resolve this long-standing challenge, we present our method BLADE which accurately recovers perspective parameters from a single image without heuristic assumptions. We start from the inverse relationship between perspective distortion and the person's Z-translation $T_z$, and we show that $T_z$ can be reliably estimated from the image. We then discuss the important role of $T_z$ for accurate human mesh recovery estimated from close-range images. Finally, we show that, once $T_z$ and the 3D human mesh are estimated, one can accurately recover the focal length and full 3D translation. Extensive experiments on standard benchmarks and real-world close-range images show that our method is the first to accurately recover projection parameters from a single image, and consequently attain state-of-the-art accuracy on 3D pose estimation and 2D alignment for a wide range of images.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "BLADE addresses the challenge of single-image human mesh recovery, particularly in close-range scenarios where existing methods struggle due to inaccurate perspective projection. The method leverages the inverse relationship between perspective distortion and the person's Z-translation ($T_z$) to accurately estimate $T_z$ from a single image. This estimation is crucial for recovering accurate 3D human mesh and perspective parameters, including focal length and full 3D translation. BLADE achieves state-of-the-art accuracy in 3D pose estimation and 2D alignment across a wide range of images, demonstrating its effectiveness in overcoming the limitations of current approaches.",
        "Tags": [
            "3D Human Pose Estimation",
            "3D Human Mesh Estimation",
            "Perspective Projection",
            "Close-Range Imaging",
            "Z-Translation Estimation"
        ]
    },
    {
        "Title": "Align3R: Aligned Monocular Depth Estimation for Dynamic Videos",
        "Authors": "Jiahao Lu \u00b7 Tianyu Huang \u00b7 Peng Li \u00b7 Zhiyang Dou \u00b7 Cheng Lin \u00b7 Zhiming Cui \u00b7 Zhen Dong \u00b7 Sai-Kit Yeung \u00b7 Wenping Wang \u00b7 Yuan Liu",
        "Abstract": "Recent developments in monocular depth estimation methods enable high-quality depth estimation of single-view images but fail to estimate consistent video depth across different frames. Recent works address this problem by applying a video diffusion model to generate video depth conditioned on the input video, which is training-expensive and can only produce scale-invariant depth values without camera poses. In this paper, we propose a novel video-depth estimation method called Align3R to estimate temporal consistent depth maps for a dynamic video. Our key idea is to utilize the recent DUSt3R model to align estimated monocular depth maps of different timesteps. First, we fine-tune the DUSt3R model with additional estimated monocular depth as inputs for the dynamic scenes. Then, we apply optimization to reconstruct both depth maps and camera poses. Extensive experiments demonstrate that Align3R estimates consistent video depth and camera poses for a monocular video with superior performance than baseline methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Align3R, a novel method for estimating temporally consistent depth maps in dynamic videos. Unlike existing methods that rely on expensive video diffusion models and produce scale-invariant depth without camera poses, Align3R leverages the DUSt3R model to align monocular depth maps across different timesteps. The approach involves fine-tuning DUSt3R with additional monocular depth inputs for dynamic scenes and applying optimization to reconstruct both depth maps and camera poses. The method demonstrates superior performance in estimating consistent video depth and camera poses compared to baseline approaches.",
        "Tags": [
            "Depth Estimation",
            "3D Reconstruction",
            "Temporal Consistency",
            "Camera Pose Estimation",
            "Dynamic Scenes"
        ]
    },
    {
        "Title": "MotionPro: A Precise Motion Controller for Image-to-Video Generation",
        "Authors": "Zhongwei Zhang \u00b7 Fuchen Long \u00b7 Zhaofan Qiu \u00b7 Yingwei Pan \u00b7 Wu Liu \u00b7 Ting Yao \u00b7 Tao Mei",
        "Abstract": "Animating images with interactive motion control has garnered popularity for image-to-video (I2V) generation. Modern approaches typically regard the Gaussian filtered trajectory as sole motion control signal. Nevertheless, the flow approximation via Gaussian kernel limits the controllability of fine-grained movement, and commonly fails to disentangle object and camera moving. To alleviate these, we present MotionPro, a new recipe of region-wise motion controller that novelly leverages region-wise trajectory and motion mask to regulate fine-grained motion synthesis and identify exact target motion category (i.e., object or camera moving), respectively. Technically, MotionPro first estimates the flow maps on each training video via a tracking model, and then samples the region-wise trajectories from multiple local regions to simulate inference scenario. Instead of approximating flow distributions generally using a large Gaussian kernel, our region-wise trajectory provides a more precise control by directly employing trajectories in local region and thus manages to characterize fine-grained movement. A motion mask is simultaneously derived from the predicted flow maps to present holistic motion dynamics. To pursue natural motion control, MotionPro further strengthens video denoising with additional conditions of region-wise trajectory and motion mask in a feature modulation manner. More remarkably, we meticulously construct a benchmark, i.e., MC-Bench, with 1.1K user-annotated image-trajectory pairs, for the evaluation of both fine-grained and object-level I2V motion control. Extensive experiments conducted on WebVid-10M and MC-Bench demonstrate the effectiveness of MotionPro.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "MotionPro introduces a novel region-wise motion controller for image-to-video (I2V) generation, addressing limitations in fine-grained motion control and the disentanglement of object and camera movements. Unlike traditional methods that use Gaussian-filtered trajectories, MotionPro employs region-wise trajectories and motion masks to precisely control motion synthesis and identify motion categories. The approach involves estimating flow maps from training videos, sampling region-wise trajectories, and using motion masks to represent holistic motion dynamics. MotionPro also enhances video denoising by incorporating these elements in a feature modulation manner. A new benchmark, MC-Bench, with 1.1K annotated image-trajectory pairs, is introduced for evaluating fine-grained and object-level motion control. The effectiveness of MotionPro is validated through extensive experiments.",
        "Tags": [
            "Video Generation",
            "Image-to-Video Generation",
            "Region-wise Motion Control",
            "Motion Mask",
            "Feature Modulation"
        ]
    },
    {
        "Title": "Split Adaptation for Pre-trained Vision Transformers",
        "Authors": "Lixu Wang \u00b7 Bingqi Shang \u00b7 Yi Li \u00b7 Payal Mohapatra \u00b7 Wei Dong \u00b7 Xiao Wang \u00b7 Qi Zhu",
        "Abstract": "Vision Transformers (ViTs), extensively pre-trained on large-scale datasets, have become essential to foundation models, allowing excellent performance on diverse downstream tasks with minimal adaptation. Consequently, there is growing interest in adapting pre-trained ViTs across various fields, including privacy-sensitive domains where clients are often reluctant to share their data. Existing adaptation methods typically require direct data access, rendering them infeasible under these constraints. A straightforward solution may be sending the pre-trained ViT to clients for local adaptation, which poses issues of model intellectual property protection and incurs heavy client computation overhead. To address these issues, we propose a novel split adaptation (SA) method that enables effective downstream adaptation while protecting data and models. SA, inspired by split learning (SL), segments the pre-trained ViT into a frontend and a backend, with only the frontend shared with the client for data representation extraction. But unlike regular SL, SA replaces frontend parameters with low-bit quantized values, preventing direct exposure of the model. SA allows the client to add bi-level noise to the frontend and the extracted data representations, ensuring data protection. Accordingly, SA incorporates data-level and model-level out-of-distribution enhancements to mitigate noise injection's impact on adaptation performance. Our SA focuses on the challenging few-shot adaptation and adopts patch retrieval augmentation for overfitting alleviation. Extensive experiments on multiple datasets validate SA\u2019s superiority over state-of-the-art methods and demonstrate its defense against advanced data reconstruction attacks while preventing model leakage with minimal computation cost on the client side.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Split Adaptation (SA), a novel method for adapting pre-trained Vision Transformers (ViTs) to downstream tasks while addressing privacy and computational constraints. SA segments the ViT into a frontend and backend, sharing only the frontend with clients for data representation extraction. To protect data and model integrity, SA employs low-bit quantization and bi-level noise addition to the frontend and extracted representations. The method also incorporates data-level and model-level enhancements to counteract the impact of noise on adaptation performance. SA is particularly effective in few-shot adaptation scenarios and uses patch retrieval augmentation to prevent overfitting. Experimental results demonstrate SA's superiority over existing methods in terms of performance, privacy protection, and computational efficiency.",
        "Tags": [
            "Vision Transformer (ViT)",
            "Data Augmentation",
            "Privacy Protection",
            "Few-Shot Learning",
            "Quantization"
        ]
    },
    {
        "Title": "Advancing Multiple Instance Learning with Continual Learning for Whole Slide Imaging",
        "Authors": "Xianrui Li \u00b7 Yufei Cui \u00b7 Jun Li \u00b7 Antoni B. Chan",
        "Abstract": "Advances in medical imaging and deep learning have propelled progress in whole slide image (WSI) analysis, with multiple instance learning (MIL) showing promise for efficient and accurate diagnostics. However, conventional MIL models often lack adaptability to evolving datasets, as they rely on static training that cannot incorporate new information without extensive retraining. Applying continual learning (CL) to MIL models is a possible solution, but often sees limited improvements. In this paper, we analyze CL in the context of attention MIL models and find that the model forgetting is mainly concentrated in the attention layers of the MIL model. Using the results of this analysis we propose two components for improving CL on MIL:Attention Knowledge Distillation (AKD) and the Pseudo-Bag Memory Pool (PMP). AKD mitigates catastrophic forgetting by focusing on retaining attention layer knowledge between learning sessions, while PMP reduces the memory footprint by selectively storing only the most informative patches, or ''pseudo-bags'' from WSIs. Experimental evaluations demonstrate that our method significantly improves both accuracy and memory efficiency on diverse WSI datasets, outperforming current state-of-the-art CL methods. This work provides a foundation for CL in large-scale, weakly annotated clinical datasets, paving the way for more adaptable and resilient diagnostic models.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of adapting multiple instance learning (MIL) models for whole slide image (WSI) analysis to evolving datasets through continual learning (CL). The authors identify that model forgetting in attention-based MIL models primarily occurs in the attention layers. To mitigate this, they propose two novel components: Attention Knowledge Distillation (AKD) and the Pseudo-Bag Memory Pool (PMP). AKD focuses on preserving knowledge in the attention layers across learning sessions, while PMP optimizes memory usage by selectively storing the most informative patches from WSIs. The proposed method demonstrates significant improvements in accuracy and memory efficiency across diverse WSI datasets, outperforming existing CL approaches. This advancement lays the groundwork for more adaptable and resilient diagnostic models in clinical settings.",
        "Tags": [
            "Medical Image Analysis",
            "Continual Learning",
            "Attention Mechanisms",
            "Weakly Supervised Learning",
            "Memory Efficiency"
        ]
    },
    {
        "Title": "VELOCITI: Benchmarking Video-Language Compositional Reasoning with Strict Entailment",
        "Authors": "Darshana Saravanan \u00b7 Varun Gupta \u00b7 Darshan Singh \u00b7 Zeeshan Khan \u00b7 Vineet Gandhi \u00b7 Makarand Tapaswi",
        "Abstract": "A fundamental aspect of compositional reasoning in a video is associating people and their actions across time. Recent years have seen great progress in general-purpose vision/video models and a move towards long-video understanding. While exciting, we take a step back and ask: are today\u2019s models good at compositional reasoning on short videos? To this end, we introduce VELOCITI, a benchmark to study Video-LLMs by disentangling and assessing the comprehension of agents, actions, and their associations across multiple events. We adopt the Video-Language Entailment setup and propose StrictVLE that requires correct classification (rather than ranking) of the positive and negative caption. We evaluate several models and observe that even the best, LLaVA-OneVision (42.5%) and GPT-4o (44.3%), are far from human accuracy at 89.6%. Results show that action understanding lags behind agents, and negative captions created using entities appearing in the video perform worse than those obtained from pure text manipulation. We also present challenges with ClassicVLE and multiple-choice (MC) evaluation, strengthening our preference for StrictVLE. Finally, we validate that our benchmark requires visual inputs of multiple frames making it ideal to study video-language compositional reasoning.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces VELOCITI, a benchmark designed to evaluate the compositional reasoning capabilities of Video-Language Models (Video-LLMs) on short videos. The benchmark focuses on understanding agents, actions, and their associations across multiple events using a Video-Language Entailment setup. The authors propose StrictVLE, which requires correct classification of positive and negative captions rather than ranking. Evaluation of several models, including LLaVA-OneVision and GPT-4o, reveals significant gaps compared to human accuracy, particularly in action understanding. The study also highlights challenges with ClassicVLE and multiple-choice evaluations, advocating for the use of StrictVLE. The benchmark emphasizes the need for visual inputs from multiple frames, making it suitable for studying video-language compositional reasoning.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Video Understanding",
            "Strict Entailment",
            "Compositional Reasoning",
            "Video-Language Entailment"
        ]
    },
    {
        "Title": "Customized Condition Controllable Generation for Video Soundtrack",
        "Authors": "Fan Qi \u00b7 KunSheng Ma \u00b7 Changsheng Xu",
        "Abstract": "Recent advancements in latent diffusion models (LDMs) have led to innovative approaches in music generation, allowing for increased flexibility and integration with other modalities. However, existing methods often rely on a two-step process that fails to capture the artistic essence of videos, particularly in the context of complex videos requiring detailed sound effect and diverse instrumentation. In this paper, we propose a novel framework for generating video soundtracks that simultaneously produces music and sound effect tailored to the video content. Our method incorporates a Contrastive Visual-Sound-Music pretraining process that maps these modalities into a unified feature space, enhancing the model's ability to capture intricate audio dynamics. We design Spectrum Divergence Masked Attention for Unet to differentiate between the unique characteristics of sound effect and music. We utilize Score-guided Noise Iterative Optimization to provide musicians with customizable control during the generation process. Extensive evaluations on the FilmScoreDB and SymMV\\&HIMV datasets demonstrate that our approach significantly outperforms state-of-the-art baselines in both subjective and objective assessments, highlighting its potential as a robust tool for video soundtrack generation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a novel framework for generating video soundtracks that simultaneously produces music and sound effects tailored to video content. The method leverages a Contrastive Visual-Sound-Music pretraining process to map these modalities into a unified feature space, enhancing the model's ability to capture intricate audio dynamics. It incorporates Spectrum Divergence Masked Attention for Unet to differentiate between sound effects and music, and utilizes Score-guided Noise Iterative Optimization to offer customizable control during generation. The approach demonstrates superior performance over state-of-the-art baselines in both subjective and objective evaluations, showcasing its effectiveness in video soundtrack generation.",
        "Tags": [
            "Diffusion Models",
            "Video Generation",
            "Contrastive Learning",
            "Customizable Control",
            "Audio-Visual Integration"
        ]
    },
    {
        "Title": "RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness",
        "Authors": "Tianyu Yu \u00b7 Haoye Zhang \u00b7 Qiming Li \u00b7 Qixin Xu \u00b7 Yuan Yao \u00b7 Da Chen \u00b7 Xiaoman Lu \u00b7 Ganqu Cui \u00b7 Yunkai Dang \u00b7 Taiwen He \u00b7 Xiaocheng Feng \u00b7 Jun Song \u00b7 Bo Zheng \u00b7 Zhiyuan Liu \u00b7 Tat-seng Chua \u00b7 Maosong Sun",
        "Abstract": "Traditional feedback learning for hallucination reduction relies on labor-intensive manual labeling or expensive proprietary models.This leaves the community without  foundational knowledge about how to build high-quality feedback with open-source MLLMs.In this work, we introduce RLAIF-V, a novel framework that aligns MLLMs in a fully open-source paradigm. RLAIF-V maximally explores open-source MLLMs from two perspectives, including high-quality feedback data generation for preference learning and self-feedback guidance for inference-time scaling.Extensive experiments on seven benchmarks in both automatic and human evaluation show that RLAIF-V substantially enhances the trustworthiness of models at both preference learning and inference time. RLAIF-V 7B reduces object hallucination by 80.7\\% and overall hallucination by 33.7\\%. Remarkably, RLAIF-V 12B further reveals the self-alignment potential of open-source MLLMs, where the  model can learn from feedback of itself to achieve super GPT-4V trustworthiness.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces RLAIF-V, a novel framework that aligns Multimodal Large Language Models (MLLMs) in a fully open-source paradigm. RLAIF-V explores open-source MLLMs from two perspectives: high-quality feedback data generation for preference learning and self-feedback guidance for inference-time scaling. The framework significantly enhances model trustworthiness, reducing object hallucination by 80.7% and overall hallucination by 33.7%. Notably, RLAIF-V 12B demonstrates the self-alignment potential of open-source MLLMs, achieving trustworthiness superior to GPT-4V.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Self-Supervised Learning",
            "Hallucination Reduction",
            "Preference Learning",
            "Self-Alignment"
        ]
    },
    {
        "Title": "Detection-Friendly Nonuniformity Correction: A Union Framework for Infrared UAV Target Detection",
        "Authors": "Houzhang Fang \u00b7 Xiaolin Wang \u00b7 Zengyang Li \u00b7 Lu Wang \u00b7 Qingshan Li \u00b7 Yi Chang \u00b7 Luxin Yan",
        "Abstract": "Infrared unmanned aerial vehicle (UAV) images captured using thermal detectors are often affected by temperature-dependent low-frequency nonuniformity, which significantly reduces the contrast of the images. Detecting UAV targets under nonuniform conditions is crucial in UAV surveillance applications. Existing methods typically treat infrared nonuniformity correction (NUC) as a preprocessing step for detection, which leads to suboptimal performance. Balancing the two tasks while enhancing detection-beneficial information remains challenging. In this paper, we present a detection-friendly union framework, termed UniCD, that simultaneously addresses both infrared NUC and UAV target detection tasks in an end-to-end manner. We first model NUC as a small number of parameter estimation problem jointly driven by priors and data to generate detection-conducive images. Then, we incorporate a new auxiliary loss with target mask supervision into the backbone of the infrared UAV target detection network to strengthen target features while suppressing the background. To better balance correction and detection, we introduce a detection-guided self-supervised loss to reduce feature discrepancies between the two tasks, thereby enhancing detection robustness to varying nonuniformity levels. Additionally, we construct a new benchmark composed of 50,000 infrared images in various nonuniformity types, multi-scale UAV targets and rich backgrounds with target annotations, called IRBFD. Extensive experiments on IRBFD demonstrate that our UniCD is a robust union framework for NUC and UAV target detection while achieving real-time processing capabilities.  Dataset can be available at https://github.com/anonymous2025submit/UniCD.",
        "Link": "https://github.com/anonymous2025submit/UniCD",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Infrared UAV images often suffer from temperature-dependent nonuniformity, which degrades image contrast and complicates target detection. Traditional methods treat nonuniformity correction (NUC) as a preprocessing step, leading to suboptimal detection performance. This paper introduces UniCD, a detection-friendly union framework that jointly addresses NUC and UAV target detection in an end-to-end manner. UniCD models NUC as a parameter estimation problem driven by priors and data, generating detection-conducive images. It incorporates an auxiliary loss with target mask supervision to enhance target features and suppress background interference. A detection-guided self-supervised loss is introduced to balance correction and detection, improving robustness to varying nonuniformity levels. A new benchmark, IRBFD, comprising 50,000 infrared images with diverse nonuniformity types, multi-scale targets, and rich backgrounds, is also presented. Experiments demonstrate UniCD's effectiveness and real-time processing capabilities.",
        "Tags": [
            "Object Detection",
            "Low-Level Vision",
            "Infrared Image Processing",
            "End-to-End Learning",
            "Self-Supervised Learning"
        ]
    },
    {
        "Title": "COB-GS: Clear Object Boundaries in 3DGS Segmentation Based on Boundary-Adaptive Gaussian Splitting",
        "Authors": "Jiaxin Zhang \u00b7 Junjun Jiang \u00b7 Youyu Chen \u00b7 Kui Jiang \u00b7 Xianming Liu",
        "Abstract": "Accurate object segmentation is crucial for high-quality scene understanding in the 3D vision domain. However, 3D segmentation based on 3D Gaussian Splatting (3DGS) struggles with accurately delineating object boundaries, as Gaussian primitives often span across object edges due to their inherent volume and the lack of semantic guidance during training. In order to tackle these challenges, we introduce Clear Object Boundaries for 3DGS Segmentation (COB-GS), which aims to improve segmentation accuracy by clearly delineating blurry boundaries of interwoven Gaussian primitives within the scene. Unlike existing approaches that remove ambiguous Gaussians and sacrifice visual quality, COB-GS, as a 3DGS refinement method, jointly optimizes semantic and visual information, allowing the two different levels to cooperate with each other effectively. Specifically, for the semantic guidance, we introduce a boundary-adaptive Gaussian splitting technique that leverages semantic gradient statistics to identify and split ambiguous Gaussians, aligning them closely with object boundaries. For the visual optimization, we rectify the degraded suboptimal texture of the 3DGS scene, particularly along the refined boundary structures. Experimental results demonstrate that COB-GS substantially improves segmentation accuracy and robustness against inaccurate masks from pre-train model, yielding clear boundaries while preserving high visual quality.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Clear Object Boundaries for 3DGS Segmentation (COB-GS), a method designed to enhance the accuracy of object segmentation in 3D Gaussian Splatting (3DGS) by addressing the challenge of blurry object boundaries. COB-GS employs a boundary-adaptive Gaussian splitting technique that uses semantic gradient statistics to identify and split ambiguous Gaussians, aligning them more closely with object boundaries. Additionally, it optimizes the visual quality of the 3DGS scene by rectifying suboptimal textures, especially along the refined boundary structures. This approach significantly improves segmentation accuracy and robustness against inaccurate masks from pre-trained models, maintaining high visual quality.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Semantic Segmentation",
            "Boundary-Adaptive Gaussian Splitting",
            "Semantic Gradient Statistics",
            "Visual Quality Optimization"
        ]
    },
    {
        "Title": "ZeroVO: Visual Odometry with Minimal Assumptions",
        "Authors": "Lei Lai \u00b7 Zekai Yin \u00b7 Eshed Ohn-Bar",
        "Abstract": "We present a novel visual odometry (VO) algorithm that achieves zero-shot generalization across diverse cameras and environments, addressing traditional limitations in VO algorithms associated with specific sensors and predefined settings. Our approach incorporates three main innovations. First, we introduce a language-based prior that infuses semantic information, enhancing robust feature extraction and enabling effective generalization to previously unseen domains. Second, we design a calibration-free, geometry-aware network structure capable of handling noise in estimated depth and camera parameters. Third, we demonstrate that our flexible architecture can leverage an unconstrained, semi-supervised training process that iteratively adapts to new scenes using unlabeled data, further boosting its ability to generalize across diverse scenarios. We focus on autonomous driving contexts and validate our approach extensively on three standard benchmarks\u2014KITTI, nuScenes, and Argoverse 2\u2014as well as a newly generated, high-fidelity synthetic dataset from Grand Theft Auto (GTA). Our work advances the boundaries of VO applicability, offering a versatile solution for real-world deployment at scale.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces ZeroVO, a novel visual odometry (VO) algorithm designed for zero-shot generalization across diverse cameras and environments. The approach overcomes traditional VO limitations by incorporating three key innovations: (1) a language-based prior for semantic information infusion, enhancing feature extraction and domain generalization; (2) a calibration-free, geometry-aware network structure robust to noise in depth and camera parameters; and (3) a flexible, semi-supervised training process that adapts iteratively to new scenes using unlabeled data. Validated on standard benchmarks (KITTI, nuScenes, Argoverse 2) and a synthetic GTA dataset, ZeroVO demonstrates significant advancements in VO applicability for real-world autonomous driving scenarios.",
        "Tags": [
            "Autonomous Driving",
            "Visual Odometry",
            "Zero-Shot Learning",
            "Semi-Supervised Learning",
            "Geometry-Aware Networks"
        ]
    },
    {
        "Title": "Sim-to-Real Causal Transfer: A Metric Learning Approach to Causally-Aware Interaction Representations",
        "Authors": "Ahmad Rahimi \u00b7 Po-Chien Luan \u00b7 Yuejiang Liu \u00b7 Frano Raji\u010d \u00b7 Alex Alahi",
        "Abstract": "Modeling spatial-temporal interactions among neighboring agents is at the heart of multi-agent problems such as motion forecasting and crowd navigation. Despite notable progress, it remains unclear to which extent modern representations can capture the causal relationships behind agent interactions. In this work, we take an in-depth look at the causal awareness of these representations, from computational formalism to real-world practice. First, we revisit the notion of non-causal robustness studied in the recent CausalAgents benchmark. We show that existing representations are already partially resilient to perturbations of non-causal agents, and yet modeling indirect causal effects involving mediator agents remains challenging. To address this challenge, we introduce a metric learning approach that regularizes latent representations with causal annotations. Our controlled experiments show that this approach not only leads to higher degrees of causal awareness but also yields stronger out-of-distribution robustness. To further operationalize it in practice, we propose a sim-to-real causal transfer method via cross-domain multi-task learning. Experiments on trajectory prediction datasets show that our method can significantly boost generalization, even in the absence of real-world causal annotations, where we acquire higher prediction accuracy by only using 25% of real-world data. We hope our work provides a new perspective on the challenges and potential pathways toward causally-aware representations of multi-agent interactions. Our code is available in supplementary materials.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper investigates the causal awareness of spatial-temporal interaction representations in multi-agent systems, such as motion forecasting and crowd navigation. The authors revisit the concept of non-causal robustness and demonstrate that while existing representations are somewhat resilient to non-causal perturbations, modeling indirect causal effects remains difficult. To address this, they introduce a metric learning approach that regularizes latent representations using causal annotations, enhancing causal awareness and out-of-distribution robustness. Additionally, a sim-to-real causal transfer method is proposed, leveraging cross-domain multi-task learning to improve generalization in real-world scenarios, even without real-world causal annotations. The method achieves higher prediction accuracy using only 25% of real-world data, offering a new perspective on causally-aware representations for multi-agent interactions.",
        "Tags": [
            "Causal Transfer",
            "Metric Learning",
            "Sim-to-Real Learning",
            "Causal Awareness",
            "Out-of-Distribution Robustness",
            "Cross-Domain Learning"
        ]
    },
    {
        "Title": "Detecting Backdoor Attacks in Federated Learning via Direction Alignment Inspection",
        "Authors": "Jiahao Xu \u00b7 Zikai Zhang \u00b7 Rui Hu",
        "Abstract": "The distributed nature of training makes Federated Learning (FL) vulnerable to backdoor attacks, where malicious model updates aim to compromise the global model\u2019s performance on specific tasks. Existing defense methods show limited efficacy as they overlook the inconsistency between benign and malicious model updates regarding both general and fine-grained directions. To fill this gap, we introduce AlignIns, a novel defense method designed to safeguard FL systems against backdoor attacks. AlignIns looks into the direction of each model update through a direction alignment inspection process. Specifically, it examines the alignment of model updates with the overall update direction and analyzes the distribution of the signs of their significant parameters, comparing them with the principle sign across all model updates. Model updates that exhibit an unusual degree of alignment are considered malicious and thus be filtered out. We provide the theoretical analysis of the robustness of AlignIns and its propagation error in FL. Our empirical results on both independent and identically distributed (IID) and non-IID datasets demonstrate that AlignIns achieves higher robustness compared to the state-of-the-art defense methods. Code is available at \\url{https://anonymous.4open.science/r/AlignIns}.",
        "Link": "https://anonymous.4open.science/r/AlignIns",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Federated Learning (FL) is susceptible to backdoor attacks due to its distributed training nature, where malicious updates can compromise the global model's performance on specific tasks. Existing defenses are limited as they fail to address the inconsistency between benign and malicious updates in both general and fine-grained directions. To address this, the authors propose AlignIns, a novel defense method that inspects the direction of model updates through a direction alignment process. AlignIns evaluates the alignment of updates with the overall update direction and analyzes the distribution of significant parameter signs, filtering out updates with unusual alignment. Theoretical analysis supports AlignIns' robustness and propagation error in FL. Empirical results on IID and non-IID datasets show that AlignIns outperforms state-of-the-art defense methods in robustness.",
        "Tags": [
            "Federated Learning",
            "Backdoor Attacks",
            "Direction Alignment Inspection",
            "Model Update Analysis",
            "Robustness in Federated Learning"
        ]
    },
    {
        "Title": "Disco4D: Disentangled 4D Human Generation and Animation from a Single Image",
        "Authors": "Hui En Pang \u00b7 Shuai Liu \u00b7 Zhongang Cai \u00b7 Lei Yang \u00b7 Tianwei Zhang \u00b7 Ziwei Liu",
        "Abstract": "We present $\\textbf{Disco4D}$, a novel Gaussian Splatting framework for 4D human generation and animation from a single image. Different from existing methods, Disco4D distinctively disentangles clothings (with Gaussian models) from the human body (with SMPL-X model), significantly enhancing the generation details and flexibility. It has the following technical innovations. $\\textbf{1)}$ Disco4D learns to efficiently fit the clothing Gaussians over the SMPL-X Gaussians. $\\textbf{2)}$ It adopts diffusion models to enhance the 3D generation process, $\\textit{e.g.}$, modeling occluded parts not visible in the input image. $\\textbf{3)}$ It learns an identity encoding for each clothing Gaussian to facilitate the separation and extraction of clothing assets. Furthermore, Disco4D naturally supports 4D human animation with vivid dynamics. Extensive experiments demonstrate the superiority of Disco4D on 4D human generation and animation tasks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Disco4D introduces a novel Gaussian Splatting framework for 4D human generation and animation from a single image. It uniquely disentangles clothing (modeled with Gaussians) from the human body (modeled with SMPL-X), improving generation details and flexibility. Key innovations include: 1) efficient fitting of clothing Gaussians over SMPL-X Gaussians, 2) use of diffusion models to enhance 3D generation, particularly for occluded parts, and 3) identity encoding for clothing Gaussians to enable separation and extraction of clothing assets. Disco4D also supports vivid 4D human animation, demonstrating superior performance in 4D human generation and animation tasks.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Avatars",
            "Diffusion Models",
            "4D Human Animation",
            "Clothing Disentanglement",
            "Identity Encoding"
        ]
    },
    {
        "Title": "Video Depth without Video Models",
        "Authors": "Bingxin Ke \u00b7 Dominik Narnhofer \u00b7 Shengyu Huang \u00b7 Lei Ke \u00b7 Torben Peters \u00b7 Katerina Fragkiadaki \u00b7 Anton Obukhov \u00b7 Konrad Schindler",
        "Abstract": "Video depth estimation lifts monocular video clips to 3D by inferring dense depth at every frame. Recent advances in single-image depth estimation, brought about by the rise of large foundation models and the use of synthetic training data, have fueled a renewed interest in video depth. However, naively applying a single-image depth estimator to every frame of a video disregards temporal continuity, which not only leads to flickering but may also break when camera motion causes sudden changes in depth range. An obvious and principled solution would be to build on top of video foundation models, but these come with their own limitations; including expensive training and inference, imperfect 3D consistency, and stitching routines for the fixed-length (short) outputs. We take a step back and demonstrate how to turn a single-image latent diffusion model (LDM) into a state-of-the-art video depth estimator. Our model, which we call \\textbf{RollingDepth}, has two main ingredients: (i) a multi-frame depth estimator that is derived from a single-image LDM and maps very short video snippets (typically frame triplets) to depth snippets. (ii) a robust, optimization-based registration algorithm that optimally assembles depth snippets sampled at various different frame rates back into a consistent video. RollingDepth is able to efficiently handle long videos with hundreds of frames and delivers more accurate depth videos than both dedicated video depth estimators and high-performing single-frame models.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces RollingDepth, a novel approach to video depth estimation that leverages a single-image latent diffusion model (LDM) to infer dense depth across video frames. Unlike traditional methods that apply single-image depth estimators frame-by-frame, RollingDepth incorporates a multi-frame depth estimator for short video snippets and an optimization-based registration algorithm to ensure temporal consistency and accuracy across long videos. This method outperforms both dedicated video depth estimators and high-performing single-frame models, offering a more efficient and accurate solution for video depth estimation without relying on video foundation models.",
        "Tags": [
            "Depth Estimation",
            "Video Understanding",
            "Latent Diffusion Models",
            "3D Reconstruction",
            "Latent Diffusion Models",
            "Optimization-based Registration",
            "Temporal Consistency"
        ]
    },
    {
        "Title": "Narrating the Video: Boosting Text-Video Retrieval via Comprehensive Utilization of Frame-Level Captions",
        "Authors": "Chan Hur \u00b7 Jeong-hun Hong \u00b7 Dong-hun Lee \u00b7 Dabin Kang \u00b7 Semin Myeong \u00b7 Sang-hyo Park \u00b7 Hyeyoung Park",
        "Abstract": "In recent text-video retrieval, the use of additional captions from vision-language models has shown promising effects on the performance. However, existing models using additional captions often have struggled to capture the rich semantics, including temporal changes, inherent in the video. In addition, incorrect information caused by generative models can lead to inaccurate retrieval. To address these issues, we propose a new framework, Narrating the Video (NarVid), which strategically leverages the comprehensive information available from frame-level captions, the narration. The proposed NarVid exploits narration in multiple ways: 1) feature enhancement through cross-modal interactions between narration and video, 2) query-aware adaptive filtering to suppress irrelevant or incorrect information, 3) dual-modal matching score by adding query-video similarity and query-narration similarity, and 4) hard-negative loss to learn discriminative features from multiple perspectives using the two similarities from different views. Experimental results demonstrate that NarVid achieves state-of-the-art performance on various benchmark datasets. The code will be available at [github]",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces a novel framework, Narrating the Video (NarVid), designed to enhance text-video retrieval by leveraging frame-level captions, or narrations, more effectively. NarVid addresses the limitations of existing models by utilizing narrations in four key ways: enhancing features through cross-modal interactions, applying query-aware adaptive filtering to reduce irrelevant or incorrect information, combining query-video and query-narration similarities for dual-modal matching, and employing a hard-negative loss to improve feature discrimination. The framework demonstrates superior performance on multiple benchmark datasets, setting new standards for text-video retrieval.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Text-Video Retrieval",
            "Cross-Modal Interaction",
            "Query-Aware Filtering",
            "Dual-Modal Matching"
        ]
    },
    {
        "Title": "CLOC: Contrastive Learning for Ordinal Classification with Multi-Margin N-pair Loss",
        "Authors": "Dileepa Pitawela \u00b7 Gustavo Carneiro \u00b7 Hsiang-Ting Chen",
        "Abstract": "In ordinal classification, misclassifying neighboring ranks is common, yet the consequences of these errors are not the same.For example, misclassifying benign tumor categories is less consequential, compared to an error at the pre-cancerous to cancerous threshold, which could profoundly influence treatment choices. Despite this, existing ordinal classification methods do not account for the varying importance of these margins, treating all neighboring classes as equally significant. To address this limitation, we propose CLOC, a new margin-based contrastive learning method for ordinal classification that learns an ordered representation based on the optimization of multiple margins with a novel multi-margin n-pair loss (MMNP).CLOC enables flexible decision boundaries across key adjacent categories, facilitating smooth transitions between classes and reducing the risk of overfitting to biases present in the training data.We provide empirical discussion regarding the properties of MMNP and show experimental results on five real-world image datasets (Adience, Historical Colour Image Dating, Knee Osteoarthritis, Indian Diabetic Retinopathy Image, and Breast Carcinoma Subtyping) and one synthetic dataset simulating clinical decision bias.Our results demonstrate that CLOC outperforms existing ordinal classification methods and show the interpretability and controllability of CLOC in learning meaningful, ordered representations that align with clinical and practical needs.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces CLOC, a novel margin-based contrastive learning method for ordinal classification that addresses the varying importance of misclassification margins between neighboring ranks. Unlike existing methods that treat all neighboring classes as equally significant, CLOC employs a multi-margin n-pair loss (MMNP) to learn ordered representations, enabling flexible decision boundaries and reducing overfitting to training biases. The method is validated on five real-world image datasets and one synthetic dataset, demonstrating superior performance over existing ordinal classification methods and highlighting its interpretability and alignment with clinical needs.",
        "Tags": [
            "Ordinal Classification",
            "Contrastive Learning",
            "Multi-Margin N-pair Loss",
            "Clinical Decision Bias",
            "Ordered Representation Learning"
        ]
    },
    {
        "Title": "Occlusion-aware Text-Image-Point Cloud Pretraining for Open-World 3D Object Recognition",
        "Authors": "Khanh Nguyen \u00b7 Ghulam Mubashar Hassan \u00b7 Ajmal Mian",
        "Abstract": "Recent open-world representation learning approaches have leveraged CLIP to enable zero-shot 3D object recognition. However, performance on real point clouds with occlusions still falls short due to the unrealistic pretraining settings. Additionally, these methods incur high inference costs because they rely on Transformer's attention modules. In this paper, we make two contributions to address these limitations. First, we propose occlusion-aware text-image-point cloud pretraining to reduce the training-testing domain gap. From 52K synthetic 3D objects, our framework generates nearly 630K partial point clouds for pretraining, consistently improving real-world recognition performances of existing popular 3D networks. Second, to reduce computational requirements, we introduce DuoMamba, a two-stream linear state space model tailored for point clouds. By integrating two space-filling curves with 1D convolutions, DuoMamba effectively models spatial dependencies between point tokens, offering a powerful alternative to Transformer. When pretrained with our framework, DuoMamba surpasses current state-of-the-art methods while reducing latency and FLOPs, highlighting the potential of our approach for real-world applications. We will release our data and code to facilitate future research.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenges of open-world 3D object recognition, particularly in handling occlusions and reducing computational costs. The authors propose an occlusion-aware text-image-point cloud pretraining framework that generates 630K partial point clouds from 52K synthetic 3D objects, improving real-world recognition performance. Additionally, they introduce DuoMamba, a two-stream linear state space model designed for point clouds, which uses space-filling curves and 1D convolutions to model spatial dependencies efficiently. DuoMamba outperforms current state-of-the-art methods in terms of performance, latency, and FLOPs, demonstrating its potential for practical applications.",
        "Tags": [
            "3D Point Cloud",
            "Zero-Shot Learning",
            "CLIP",
            "Occlusion-aware Pretraining",
            "DuoMamba",
            "Space-filling Curves"
        ]
    },
    {
        "Title": "Three-view Focal Length Recovery From Homographies",
        "Authors": "Yaqing Ding \u00b7 Viktor Kocur \u00b7 Zuzana Berger Haladova \u00b7 Qianliang Wu \u00b7 Shen Cai \u00b7 Jian Yang \u00b7 Zuzana Kukelova",
        "Abstract": "In this paper, we propose a novel approach for recovering focal lengths from three-view homographies. By examining the consistency of normal vectors between two homographies, we derive new explicit constraints between the focal lengths and homographies using an elimination technique. We demonstrate that three-view homographies provide two additional constraints, enabling the recovery of one or two focal lengths. We discuss four possible cases, including three cameras having an unknown equal focal length, three cameras having two different unknown focal lengths, three cameras where one focal length is known, and the other two cameras have equal or different unknown focal lengths. All the problems can be converted into solving polynomials in one or two unknowns, which can be efficiently solved using Sturm sequence or hidden variable technique. Evaluation using both synthetic and real data shows that the proposed solvers are both faster and more accurate than methods relying on existing two-view solvers.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a novel method for recovering focal lengths from three-view homographies. The approach leverages the consistency of normal vectors between homographies to derive explicit constraints linking focal lengths and homographies. The method can recover one or two focal lengths by utilizing additional constraints provided by three-view homographies. Four scenarios are considered: cameras with equal unknown focal lengths, cameras with two different unknown focal lengths, cameras with one known focal length and two equal or different unknown focal lengths. The problems are reduced to solving polynomials in one or two unknowns, which are efficiently addressed using Sturm sequence or hidden variable techniques. The proposed solvers outperform existing two-view solvers in both speed and accuracy, as demonstrated by evaluations on synthetic and real data.",
        "Tags": [
            "3D Reconstruction",
            "Depth Estimation",
            "Focal Length Recovery",
            "Homography Analysis",
            "Polynomial Solving"
        ]
    },
    {
        "Title": "Q-Bench-Video: Benchmark the Video Quality Understanding of LMMs",
        "Authors": "Zicheng Zhang \u00b7 Ziheng Jia \u00b7 Haoning Wu \u00b7 Chunyi Li \u00b7 Zijian Chen \u00b7 Yingjie Zhou \u00b7 Wei Sun \u00b7 Xiaohong Liu \u00b7 Xiongkuo Min \u00b7 Weisi Lin \u00b7 Guangtao Zhai",
        "Abstract": "With the rising interest in research on Large Multi-modal Models (LMMs) for video understanding, many studies have emphasized general video comprehension capabilities, neglecting the systematic exploration into video quality understanding. To address this oversight, we introduce Q-Bench-Video in this paper, a new benchmark specifically designed to evaluate LMMs' proficiency in discerning video quality. a) To ensure video source diversity, Q-Bench-Video encompasses videos from natural scenes, AI-generated Content (AIGC), and Computer Graphics (CG). b) Building on the traditional multiple-choice questions format with the Yes-or-No and What-How categories, we include Open-ended questions to better evaluate complex scenarios. Additionally, we incorporate the video pair quality comparison question to enhance comprehensiveness. c) Beyond the traditional Technical, Aesthetic, and Temporal distortions, we have expanded our evaluation aspects to include the dimension of AIGC distortions, which addresses the increasing demand for video generation. Finally, we collect a total of 2,378 question-answer pairs and test them on 12 open-source & 5 proprietary LMMs. Our findings indicate that while LMMs have a foundational understanding of video quality, their performance remains incomplete and imprecise, with a notable discrepancy compared to human performance. Through Q-Bench-Video, we seek to catalyze community interest, stimulate further research, and unlock the untapped potential of LMMs to close the gap in video quality understanding.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Q-Bench-Video, a benchmark designed to evaluate the video quality understanding capabilities of Large Multi-modal Models (LMMs). Q-Bench-Video includes diverse video sources such as natural scenes, AI-generated Content (AIGC), and Computer Graphics (CG), and employs a variety of question formats including multiple-choice, open-ended, and video pair quality comparison questions. The benchmark also expands evaluation aspects to include AIGC distortions, addressing the growing need for video generation. Testing on 12 open-source and 5 proprietary LMMs reveals that while these models have a basic understanding of video quality, their performance is incomplete and imprecise compared to human standards. The goal of Q-Bench-Video is to foster further research and improve LMMs' video quality understanding.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Video Understanding",
            "Video Quality Assessment",
            "AI-Generated Content (AIGC)",
            "Benchmarking"
        ]
    },
    {
        "Title": "CASAGPT: Cuboid Arrangement and Scene Assembly for Interior Design",
        "Authors": "Weitao Feng \u00b7 Hang Zhou \u00b7 Jing Liao \u00b7 Li Cheng \u00b7 Wenbo Zhou",
        "Abstract": "We present a novel approach for indoor scene synthesis, which learns to arrange decomposed cuboid primitives to represent 3D objects within a scene. Unlike conventional methods that use bounding boxes to determine the placement and scale of 3D objects, our approach leverages cuboids as a straightforward yet highly effective alternative for modeling objects. This allows for compact scene generation while minimizing object intersections. Our approach, coined CASAGPT for Cuboid Arrangement and Scene Assembly, employs an autoregressive model to sequentially arrange cuboids, producing physically plausible scenes. By applying rejection sampling during the fine-tuning stage to filter out scenes with object collisions, our model further reduces intersections and enhances scene quality. Additionally, we introduce a refined dataset, 3DFRONT-NC, which eliminates significant noise presented in the original dataset, 3D-FRONT. Extensive experiments on the 3D-FRONT dataset as well as our dataset demonstrate that our approach consistently outperforms the state-of-the-art methods, enhancing the realism of generated scenes, and providing a promising direction for 3D scene synthesis.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces CASAGPT, a novel approach for indoor scene synthesis that uses cuboid primitives to represent 3D objects, offering a more effective alternative to traditional bounding box methods. This method minimizes object intersections and enhances scene realism through an autoregressive model that sequentially arranges cuboids. The approach is further refined by employing rejection sampling to filter out scenes with object collisions, improving scene quality. A new dataset, 3DFRONT-NC, is introduced to reduce noise from the original 3D-FRONT dataset. The method demonstrates superior performance over existing techniques in generating realistic 3D scenes.",
        "Tags": [
            "3D Generation",
            "3D Reconstruction",
            "Cuboid Primitives",
            "Autoregressive Model",
            "Rejection Sampling"
        ]
    },
    {
        "Title": "HistoFS: Non-IID Histopathologic Whole Slide Image Classification via Federated Style Transfer with RoI-Preserving",
        "Authors": "Farchan Hakim Raswa \u00b7 Chun-Shien Lu \u00b7 Jia-Ching Wang",
        "Abstract": "Federated learning for pathological whole slide image (WSI) classification allows multiple clients to train a global multiple instance learning (MIL) model without sharing their privacy-sensitive WSIs.To accommodate the non-independent and identically distributed (non-i.i.d.) feature shifts, cross-client style transfer has been popularly used but is subject to two fundamental issues: (1) WSIs contain multiple morphological structures due to tissue heterogeneity, and (2) the  region of interests (RoIs) is not guaranteed, particularly after augmenting local WSIs data trough style transfer. To address these challenges, we propose HistoFS, a federated learning framework for computational pathology on non-i.i.d. feature shifts in WSI classification. Specifically, we introduce pseudo bag styles that capture multiple style variations within a single WSI. In addition, an authenticity module is introduced to ensure that  RoIs are preserved, allowing local models to learn WSIs with diverse styles while maintaining essential RoIs. Extensive experiments validate the superiority of HistoFS over state-of-the-art methods on three clinical datasets.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "HistoFS is a federated learning framework designed for non-independent and identically distributed (non-i.i.d.) histopathologic whole slide image (WSI) classification. It addresses challenges in cross-client style transfer, such as tissue heterogeneity and the preservation of regions of interest (RoIs). The framework introduces pseudo bag styles to capture multiple style variations within a single WSI and an authenticity module to ensure RoIs are preserved during style transfer. This allows local models to learn from WSIs with diverse styles while maintaining critical RoIs. HistoFS demonstrates superior performance over state-of-the-art methods on three clinical datasets.",
        "Tags": [
            "Federated Learning",
            "Medical Image Analysis",
            "Style Transfer",
            "Region of Interest Preservation",
            "Multiple Instance Learning"
        ]
    },
    {
        "Title": "PO3AD: Predicting Point Offsets toward Better 3D Point Cloud Anomaly Detection",
        "Authors": "Jianan Ye \u00b7 Weiguang Zhao \u00b7 Xi Yang \u00b7 Guangliang Cheng \u00b7 Kaizhu Huang",
        "Abstract": "Point cloud anomaly detection under the anomaly-free setting poses significant challenges as it requires accurately capturing the features of 3D normal data to identify deviations indicative of anomalies. Current efforts focus on devising reconstruction tasks, such as acquiring normal data representations by restoring normal samples from altered, pseudo-anomalous counterparts. Our findings reveal that distributing attention equally across normal and pseudo-anomalous data tends to dilute the model's focus on anomalous deviations. The challenge is further compounded by the inherently disordered and sparse nature of 3D point cloud data. In response to those predicaments, we introduce an innovative approach that emphasizes learning $\\textit{point offsets}$, targeting more informative pseudo-abnormal points, thus fostering more effective distillation of normal data representations. We also have crafted an augmentation technique that is steered by $\\textit{normal vectors}$, facilitating the creation of credible pseudo anomalies that enhance the efficiency of the training process. Our comprehensive experimental evaluation on the Anomaly-ShapeNet and Real3D-AD datasets evidences that our proposed method outperforms existing state-of-the-art approaches, achieving an average enhancement of 9.0% and 1.4% in the AUC-ROC detection metric across these datasets, respectively.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of 3D point cloud anomaly detection under anomaly-free conditions by proposing a novel approach that focuses on learning point offsets to better capture deviations indicative of anomalies. The method emphasizes targeting informative pseudo-abnormal points to improve the distillation of normal data representations. Additionally, a normal vector-guided augmentation technique is introduced to generate credible pseudo anomalies, enhancing training efficiency. The proposed method demonstrates superior performance on the Anomaly-ShapeNet and Real3D-AD datasets, achieving significant improvements in the AUC-ROC detection metric.",
        "Tags": [
            "3D Point Cloud",
            "Anomaly Detection",
            "Point Offsets",
            "Normal Vector Augmentation",
            "Pseudo-Anomaly Generation"
        ]
    },
    {
        "Title": "Efficient Data Driven Mixture-of-Expert Extraction from Trained Networks",
        "Authors": "Uranik Berisha \u00b7 Jens Mehnert \u00b7 Alexandru Paul Condurache",
        "Abstract": "Vision Transformers (ViTs) have emerged as the state-of-the-art models in various Computer Vision (CV) tasks, but their high computational and resource demands pose significant challenges. While Mixture of Experts (MoE) can make these models more efficient, they often require costly retraining or even training from scratch. Recent developments aim to reduce these computational costs by leveraging pretrained networks. These have been shown to produce sparse activation patterns in the Multi-Layer Perceptrons (MLPs) of the encoder blocks, allowing for conditional activation of only relevant subnetworks for each sample. Building on this idea, we propose a new method to construct MoE variants from pretrained models. Our approach extracts expert subnetworks from the model\u2019s MLP layers post-training in two phases. First, we cluster output activations to identify distinct activation patterns. In the second phase, we use these clusters to extract the corresponding subnetworks responsible for producing them. On ImageNet-1k recognition tasks, we demonstrate that these extracted experts can perform surprisingly well out of the box and require only minimal fine-tuning to regain 98% of the original performance, all while reducing FLOPs and model size, by up to 36% and 32% respectively.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Vision Transformers (ViTs) are state-of-the-art in Computer Vision tasks but are computationally demanding. Mixture of Experts (MoE) can enhance efficiency but typically requires costly retraining. This paper introduces a method to construct MoE variants from pretrained models by extracting expert subnetworks from the MLP layers post-training. The process involves clustering output activations to identify distinct patterns and using these clusters to extract corresponding subnetworks. On ImageNet-1k tasks, the extracted experts perform well with minimal fine-tuning, achieving 98% of the original performance while reducing FLOPs by up to 36% and model size by 32%.",
        "Tags": [
            "Vision Transformer (ViT)",
            "Mixture of Experts (MoE)",
            "Sparse Activation Patterns",
            "Post-Training Extraction",
            "Efficiency Optimization"
        ]
    },
    {
        "Title": "Efficient Diffusion as Low Light Enhancer",
        "Authors": "Guanzhou Lan \u00b7 Qianli Ma \u00b7 YUQI YANG \u00b7 Zhigang Wang \u00b7 Dong Wang \u00b7 Xuelong Li \u00b7 Bin Zhao",
        "Abstract": "The computational burden of the iterative sampling process remains a major challenge in diffusion-based Low-Light Image Enhancement (LLIE). Current acceleration methods, whether training-based or training-free, often lead to significant performance degradation, highlighting the trade-off between performance and efficiency.In this paper, we identify two primary factors contributing to performance degradation: fitting errors and the inference gap. Our key insight is that fitting errors can be mitigated by linearly extrapolating the incorrect score functions, while the inference gap can be reduced by shifting the Gaussian flow to a reflectance-aware residual space.Based on the above insights, we design Reflectance-Aware Trajectory Refinement (RATR) module, a simple yet effective module to refine the teacher trajectory using the reflectance component of images. Following this, we introduce Reflectance-aware Diffusion with Distilled Trajectory ReDDiT, an efficient and flexible distillation framework tailored for LLIE. Our framework achieves comparable performance to previous diffusion-based methods with redundant steps in just 2 steps while establishing new state-of-the-art (SOTA) results with 8 or 4 steps. Comprehensive experimental evaluations on 10 benchmark datasets validate the effectiveness of our method, consistently outperforming existing SOTA methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the computational inefficiency of diffusion-based Low-Light Image Enhancement (LLIE) by identifying and mitigating two key factors causing performance degradation: fitting errors and the inference gap. The authors propose a Reflectance-Aware Trajectory Refinement (RATR) module to refine the teacher trajectory using image reflectance components. They also introduce Reflectance-aware Diffusion with Distilled Trajectory (ReDDiT), a distillation framework that achieves state-of-the-art performance in just 2 steps, with further improvements at 4 or 8 steps. The method is validated across 10 benchmark datasets, demonstrating superior performance over existing approaches.",
        "Tags": [
            "Diffusion Models",
            "Low-Light Image Enhancement",
            "Reflectance-Aware Trajectory Refinement",
            "Distillation Framework",
            "Efficient Sampling"
        ]
    },
    {
        "Title": "ProbeSDF: Light Field Probes For Neural Surface Reconstruction",
        "Authors": "Briac Toussaint \u00b7 Diego Thomas \u00b7 Jean-S\u00e9bastien Franco",
        "Abstract": "SDF-based differential rendering frameworks have achieved state-of-the-art multiview 3D shape reconstruction. In this work, we re-examine this family of approaches by minimally reformulating its core appearance model in a way that simultaneously yields faster computation and increased performance. To this goal, we exhibit a physically-inspired minimal radiance parametrization decoupling angular and spatial contributions, by encoding them with a small number of features stored in two respective volumetric grids of different resolutions. Requiring as little as four parameters per voxel, and a tiny MLP call inside a single fully fused kernel, our approach allows to enhance performance with both surface and image (PSNR) metrics, while providing a significant training speedup and real-time rendering. We show this performance to be consistently achieved on real data over two widely different and popular application fields, generic object and human subject shape reconstruction, using four representative and challenging datasets.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces ProbeSDF, a novel approach to SDF-based differential rendering for multiview 3D shape reconstruction. By reformulating the core appearance model, the method achieves faster computation and improved performance. The approach uses a physically-inspired minimal radiance parametrization that decouples angular and spatial contributions, encoded with a small number of features in two volumetric grids of different resolutions. This requires only four parameters per voxel and a tiny MLP call within a single fully fused kernel, enhancing performance in surface and image metrics (PSNR) while enabling significant training speedup and real-time rendering. The method demonstrates consistent performance on real data across two application fields: generic object and human subject shape reconstruction, using four challenging datasets.",
        "Tags": [
            "3D Reconstruction",
            "Neural Radiance Fields (NeRF)",
            "Radiance Parametrization",
            "Real-Time Rendering",
            "Volumetric Grids"
        ]
    },
    {
        "Title": "MonoDGP: Monocular 3D Object Detection with Decoupled-Query and Geometry-Error Priors",
        "Authors": "Fanqi Pu \u00b7 Yifan Wang \u00b7 Jiru Deng \u00b7 Wenming Yang",
        "Abstract": "Perspective projection has been extensively utilized in monocular 3D object detection methods. It introduces geometric priors from 2D bounding boxes and 3D object dimensions to reduce the uncertainty of depth estimation. However, due to depth errors originating from the object's visual surface, the height of the bounding box often fails to represent the actual projected central height, which undermines the effectiveness of geometric depth. Direct prediction for the projected height unavoidably results in a loss of 2D priors, while multi-depth prediction with complex branches does not fully leverage geometric depth. This paper presents a Transformer-based monocular 3D object detection method called MonoDGP, which adopts perspective-invariant geometry errors to modify the projection formula. We also try to systematically discuss and explain the mechanisms and efficacy behind geometry errors, which serve as a simple but effective alternative to multi-depth prediction. Additionally, MonoDGP decouples the depth-guided decoder and constructs a 2D decoder only dependent on visual features, providing 2D priors and initializing object queries without the disturbance of 3D detection. To further optimize and fine-tune input tokens of the transformer decoder, we also introduce a Region Segmentation Head (RSH) that generates enhanced features and segment embeddings. Our monocular method demonstrates state-of-the-art performance on the KITTI benchmark without extra data. Code is available in the supplementary materials and will be publicly released after the review.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces MonoDGP, a Transformer-based monocular 3D object detection method that addresses the limitations of perspective projection in depth estimation. MonoDGP employs perspective-invariant geometry errors to refine the projection formula and systematically explores their mechanisms and efficacy. The method decouples the depth-guided decoder, creating a 2D decoder reliant solely on visual features to provide 2D priors and initialize object queries without interference from 3D detection. Additionally, a Region Segmentation Head (RSH) is introduced to optimize and fine-tune input tokens of the transformer decoder, generating enhanced features and segment embeddings. MonoDGP achieves state-of-the-art performance on the KITTI benchmark without requiring additional data.",
        "Tags": [
            "3D Object Detection",
            "Depth Estimation",
            "Transformer-based",
            "Perspective-invariant Geometry Errors",
            "Region Segmentation Head"
        ]
    },
    {
        "Title": "Chain of Semantics Programming in 3D Gaussian Splatting Representation for 3D Vision Grounding",
        "Authors": "Jiaxin Shi \u00b7 Mingyue Xiang \u00b7 Hao Sun \u00b7 Yixuan Huang \u00b7 Zhi Weng",
        "Abstract": "3D Vision Grounding (3DVG) is a fundamental research area that enables agents to perceive and interact with the 3D world. The challenge of the 3DVG task lies in understanding fine-grained semantics and spatial relationships within both the utterance and 3D scene. To address this challenge, we propose a zero-shot neuro-symbolic framework that utilizes a large language model (LLM) as neuro-symbolic functions to ground the object within the 3D Gaussian Splatting (3DGS) representation. By utilizing 3DGS representation, we can dynamically render high-quality 2D images from various viewpoints to enrich the semantic information. Given the complexity of spatial relationships, we construct a relationship graph and chain of semantics that decouple spatial relationships and facilitate step-by-step reasoning within 3DGS representation. Additionally, we employ a grounded-aware self-check mechanism to enable the LLM to reflect on its responses and mitigate the effects of ambiguity in spatial reasoning. We evaluate our method using two publicly available datasets, Nr3D and Sr3D, achieving accuracies of 60.8\\% and 91.4\\%, respectively. Notably, our method surpasses current state-of-the-art zero-shot methods on the Nr3D dataset. In addition, it outperforms the recent supervised models on the Sr3D dataset.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a zero-shot neuro-symbolic framework for 3D Vision Grounding (3DVG), leveraging large language models (LLMs) as neuro-symbolic functions to ground objects within 3D Gaussian Splatting (3DGS) representations. The approach dynamically renders high-quality 2D images from various viewpoints to enhance semantic information and constructs a relationship graph and chain of semantics for step-by-step reasoning. A grounded-aware self-check mechanism is employed to improve spatial reasoning by allowing the LLM to reflect on its responses. The method achieves superior performance on the Nr3D and Sr3D datasets, surpassing current state-of-the-art zero-shot and supervised models.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Large Language Models (LLMs)",
            "3D Vision Grounding",
            "Zero-Shot Learning",
            "Spatial Reasoning",
            "Neuro-symbolic Framework",
            "Grounded-aware Self-check Mechanism",
            "Dynamic 2D Image Rendering"
        ]
    },
    {
        "Title": "Detect-and-Guide: Self-regulation of Diffusion Models for Safe Text-to-Image Generation via Guideline Token Optimization",
        "Authors": "Feifei Li \u00b7 Mi Zhang \u00b7 Yiming Sun \u00b7 Min Yang",
        "Abstract": "Text-to-image diffusion models have achieved state-of-the-art results in synthesis tasks; however, there is a growing concern about their potential misuse in creating harmful content. To mitigate these risks, post-hoc model intervention techniques, such as concept unlearning and safety guidance, have been developed.However, fine-tuning model weights or adapting the hidden states of the diffusion model operates in an uninterpretable way, making it unclear which part of the intermediate variables is responsible for unsafe generation. These interventions severely affect the sampling trajectory when erasing harmful concepts from complex, multi-concept prompts, thus hindering their practical use in real-world settings.Despite their effectiveness on single-concept prompts, current methods still face challenges when  as they struggle to precisely remove harmful concepts without disrupting the semantics of benign ones. In this work, we propose the safe generation framework   Detect-and-Guide (DAG), leveraging the internal knowledge of diffusion models to perform self-diagnosis and fine-grained self-regulation during the sampling process.DAG first detects harmful concepts from noisy latents using refined cross-attention maps of optimized tokens, then applies safety guidance with adaptive strength and editing regions to negate unsafe generation.The optimization only requires a small annotated dataset and can provide precise detection maps with generalizability and concept specificity. Moreover, DAG does not require fine-tuning of diffusion models, and therefore introduces no loss to their generation diversity. Experiments on erasing sexual content show that DAG achieves state-of-the-art safe generation performance, balancing harmfulness mitigation and text-following performance on multi-concept real-world prompts.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Detect-and-Guide (DAG), a framework designed to enhance the safety of text-to-image diffusion models by preventing the generation of harmful content. Unlike existing methods that require fine-tuning or uninterpretable adjustments to model weights, DAG utilizes the model's internal knowledge to self-diagnose and regulate the generation process. It detects harmful concepts through optimized cross-attention maps and applies safety guidance with adaptive strength, ensuring that benign concepts remain unaffected. DAG operates without fine-tuning the model, preserving its generation diversity, and achieves state-of-the-art performance in mitigating harmful content while maintaining text-following accuracy on complex, multi-concept prompts.",
        "Tags": [
            "Diffusion Models",
            "Text-to-Image Generation",
            "Self-regulation",
            "Safety Guidance",
            "Cross-Attention Optimization"
        ]
    },
    {
        "Title": "RELOCATE: A Simple Training-Free Baseline for Visual Query Localization Using Region-Based Representations",
        "Authors": "Savya Khosla \u00b7 Sethuraman T V \u00b7 Alexander G. Schwing \u00b7 Derek Hoiem",
        "Abstract": "We present RELOCATE, a simple training-free baseline designed to perform the challenging task of visual query localization in long videos. To eliminate the need for task-specific training and efficiently handle long videos, RELOCATE leverages a region-based representation derived from pretrained vision models. At a high level, it follows the classic object localization approach: (1) identify all objects in each video frame, (2) compare the objects with the given query and select the most similar ones, and (3) perform bidirectional tracking to get a spatio-temporal response. However, we propose some key enhancements to handle small objects, cluttered scenes, partial visibility, and varying appearances. Notably, we refine the selected objects for accurate localization and generate additional visual queries to capture visual variations. We evaluate RELOCATE on the challenging Ego4D Visual Query 2D Localization dataset, establishing a new baseline that outperforms prior task-specific methods by 49% (relative improvement) in spatio-temporal average precision.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "RELOCATE is a training-free baseline for visual query localization in long videos, leveraging region-based representations from pretrained vision models. It identifies objects in video frames, compares them with the query, and uses bidirectional tracking for spatio-temporal localization. Key enhancements address challenges like small objects, cluttered scenes, and varying appearances. RELOCATE outperforms prior task-specific methods by 49% in spatio-temporal average precision on the Ego4D Visual Query 2D Localization dataset.",
        "Tags": [
            "Visual Tracking",
            "Object Detection",
            "Training-Free Approach",
            "Bidirectional Tracking",
            "Region-Based Representations"
        ]
    },
    {
        "Title": "Exact: Exploring Space-Time Perceptive Clues for Weakly Supervised Satellite Image Time Series Semantic Segmentation",
        "Authors": "Hao Zhu \u00b7 Yan Zhu \u00b7 Jiayu Xiao \u00b7 Tianxiang Xiao \u00b7 Yike Ma \u00b7 Yucheng Zhang \u00b7 Feng Dai",
        "Abstract": "Automated crop mapping through Satellite Image Time Series (SITS) has emerged as a crucial avenue for agricultural monitoring and management. However, due to the low resolution and unclear parcel boundaries, annotating pixel-level masks is exceptionally complex and time-consuming in SITS. This paper embraces the weakly supervised paradigm (i.e., only image-level categories available) to liberate the crop mapping task from the exhaustive annotation burden. The unique characteristics of SITS give rise to several challenges in weakly supervised learning: (1) noise perturbation from spatially neighboring regions, and (2) erroneous semantic bias from anomalous temporal periods. To address the above difficulties, we propose a novel method, termed exploring space-time perceptive clues (Exact). First, we introduce a set of spatial clues to explicitly capture the representative patterns of different crops from the most class-relative regions. Besides, we leverage the temporal-to-class interaction of the model to emphasize the contributions of pivotal clips, thereby enhancing the model perception for crop regions. Build upon the space-time perceptive clues, we derive the clue-based CAMs to effectively supervise the SITS segmentation network. Our method demonstrates impressive performance on various SITS benchmarks. Remarkably, the segmentation network trained on  Exact-generated masks achieves 95% of its fully supervised performance, showing the bright promise of weakly supervised paradigm in crop mapping scenario. All code will be publicly available in the future.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of automated crop mapping using Satellite Image Time Series (SITS) by proposing a weakly supervised learning method called Exact. The method leverages spatial and temporal clues to overcome noise from neighboring regions and semantic bias from anomalous temporal periods. Exact introduces spatial clues to capture representative crop patterns and uses temporal-to-class interactions to emphasize pivotal clips, enhancing model perception. The derived clue-based CAMs effectively supervise the SITS segmentation network, achieving 95% of the performance of fully supervised methods. This demonstrates the potential of weakly supervised learning in crop mapping.",
        "Tags": [
            "Semantic Segmentation",
            "Weakly Supervised Learning",
            "Remote Sensing Image Analysis",
            "Crop Mapping",
            "Satellite Image Time Series",
            "Class Activation Maps (CAMs)"
        ]
    },
    {
        "Title": "Synthetic Visual Genome",
        "Authors": "Jae Sung Park \u00b7 Zixian Ma \u00b7 Linjie Li \u00b7 Chenhao Zheng \u00b7 Cheng-Yu Hsieh \u00b7 Ximing Lu \u00b7 Khyathi Chandu \u00b7 Quan Kong \u00b7 Norimasa Kobori \u00b7 Ali Farhadi \u00b7 Yejin Choi \u00b7 Ranjay Krishna",
        "Abstract": "Understanding and reasoning over visual relationships\u2014spatial, functional, interactional, social, etc.\u2014are considered to be a fundamental component of human cognition.Yet, despite the major advances in visual comprehension in multimodal language models, precise reasoning over relationships remains a challenge. We introduce Robin: an MLM instruction-tuned with densely annotated relationships capable of constructing high-quality dense scene graphs at scale. To train Robin, we curate SVG, a scene graph based instruction tuning dataset containing $33K$ images and $855K$ relationships for $170K$ objects by completing the missing relations in existing scene graphs using GPT4-V and a carefully designed filtering process\u2014combining rule-based and model-based filtering techniques to ensure high-quality. To generate more accurate and rich scene graphs at scale for any image,  we introduce SG-EDIT: a self-distillation framework where GPT-4o refines Robin's predicted scene graphs by removing unlikely relations and/or suggesting relevant ones. Results show that our Robin-3B model, despite being trained on less than $3$ million instances, outperforms similar-size models trained on over $300$ million instances on relationship understanding benchmarks, and even surpasses larger models up to 13B parameters. Notably, it achieves state-of-the-art performance in referring expression comprehension with a score of $88.2$, surpassing the previous best of $87.4$. Our results suggest that training on the refined scene graph data is crucial to maintaining high performance across diverse visual reasoning tasks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Robin, a multimodal language model (MLM) fine-tuned for precise reasoning over visual relationships, capable of generating high-quality dense scene graphs at scale. To train Robin, the authors curate SVG, a dataset containing 33K images and 855K relationships for 170K objects, enhanced using GPT-4-V and a filtering process to ensure quality. Additionally, they propose SG-EDIT, a self-distillation framework where GPT-4 refines Robin's predicted scene graphs. The Robin-3B model, trained on fewer than 3 million instances, outperforms similar-sized models trained on over 300 million instances and even surpasses larger models up to 13B parameters. It achieves state-of-the-art performance in referring expression comprehension, demonstrating the importance of refined scene graph data for visual reasoning tasks.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Scene Graph Generation",
            "Self-Distillation",
            "GPT-4 Integration",
            "Dense Scene Graphs"
        ]
    },
    {
        "Title": "Towards Realistic Example-based Modeling via 3D Gaussian Stitching",
        "Authors": "Xinyu Gao \u00b7 Ziyi Yang \u00b7 Bingchen Gong \u00b7 Xiaoguang Han \u00b7 Sipeng Yang \u00b7 Xiaogang Jin",
        "Abstract": "Using parts of existing models to rebuild new models, commonly termed as example-based modeling, is a classical methodology in the realm of computer graphics. Previous works mostly focus on shape composition, making them very hard to use for realistic composition of 3D objects captured from real-world scenes. This leads to combining multiple NeRFs into a single 3D scene to achieve seamless appearance blending. However, the current SeamlessNeRF method struggles to achieve interactive editing and harmonious stitching for real-world scenes due to its gradient-based strategy and grid-based representation.To this end, we present an example-based modeling method that combines multiple Gaussian fields in a point-based representation using sample-guided synthesis. Specifically, as for composition, we create a GUI to segment and transform multiple fields in real time, easily obtaining a semantically meaningful composition of models represented by 3D Gaussian Splatting (3DGS). For texture blending, due to the discrete and irregular nature of 3DGS, straightforwardly applying gradient propagation as SeamlssNeRF is not supported. Thus, a novel sampling-based cloning method is proposed to harmonize the blending while preserving the original rich texture and content. Our workflow consists of three steps: 1) real-time segmentation and transformation of a Gaussian model using a well-tailored GUI, 2) KNN analysis to identify boundary points in the intersecting area between the source and target models, and 3) two-phase optimization of the target model using sampling-based cloning and gradient constraints. Extensive experimental results validate that our approach significantly outperforms previous works in terms of realistic synthesis, demonstrating its practicality.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces an example-based modeling method that leverages 3D Gaussian Splatting (3DGS) for realistic composition of 3D objects from real-world scenes. Unlike previous methods that rely on gradient-based strategies and grid-based representations, the proposed approach uses sample-guided synthesis to combine multiple Gaussian fields in a point-based representation. A GUI is developed for real-time segmentation and transformation of Gaussian models, enabling semantically meaningful compositions. For texture blending, a novel sampling-based cloning method is introduced to harmonize blending while preserving texture details. The workflow involves real-time segmentation, KNN analysis for boundary identification, and a two-phase optimization process. The method demonstrates superior performance in realistic synthesis compared to existing techniques.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "3D Reconstruction",
            "Sample-guided Synthesis",
            "Real-time GUI",
            "Texture Blending"
        ]
    },
    {
        "Title": "Synthetic Data is an Elegant GIFT for Continual Vision-Language Models",
        "Authors": "Bin Wu \u00b7 Wuxuan Shi \u00b7 Jinqiao Wang \u00b7 Mang Ye",
        "Abstract": "Pre-trained Vision-Language Models (VLMs) require Continual Learning (CL) to efficiently update their knowledge and adapt to various downstream tasks without retraining from scratch. However, for VLMs, in addition to the loss of knowledge previously learned from downstream tasks, pre-training knowledge is also corrupted during continual fine-tuning. This issue is exacerbated by the unavailability of original pre-training data, leaving VLM's generalization ability degrading. In this paper, we propose GIFT, a novel continual fine-tuning approach that utilizes synthetic data to overcome catastrophic forgetting in VLMs. Taking advantage of recent advances in text-to-image synthesis, we employ a pre-trained diffusion model to recreate both pre-training and learned downstream task data. In this way, the VLM can revisit previous knowledge through distillation on matching diffusion-generated images and corresponding text prompts. Leveraging the broad distribution and high alignment between synthetic image-text pairs in VLM's feature space, we propose a contrastive distillation loss along with an image-text alignment constraint. To further combat in-distribution overfitting and enhance distillation performance with limited amount of generated data, we incorporate adaptive weight consolidation, utilizing Fisher information from these synthetic image-text pairs and achieving a better stability-plasticity balance. Extensive experiments demonstrate that our method consistently outperforms previous state-of-the-art approaches across various settings.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces GIFT, a novel approach for continual fine-tuning of Vision-Language Models (VLMs) to address catastrophic forgetting. GIFT leverages synthetic data generated by a pre-trained diffusion model to recreate both pre-training and downstream task data, enabling VLMs to revisit previous knowledge through distillation on synthetic image-text pairs. The method employs a contrastive distillation loss and an image-text alignment constraint to enhance alignment in the VLM's feature space. Additionally, adaptive weight consolidation is used to balance stability and plasticity, improving distillation performance with limited synthetic data. The approach demonstrates superior performance over existing methods in various settings.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Continual Learning",
            "Synthetic Data",
            "Diffusion Models",
            "Contrastive Distillation"
        ]
    },
    {
        "Title": "Style Quantization for Data-Efficient GAN Training",
        "Authors": "Jian Wang \u00b7 Xin Lan \u00b7 Ji-Zhe Zhou \u00b7 Yuxin Tian \u00b7 Jiancheng Lv",
        "Abstract": "Under limited data setting, GANs often struggle to navigate and effectively exploit the input latent space. Consequently, images generated from adjacent variables in a sparse input latent space may exhibit significant discrepancies in realism, leading to suboptimal consistency regularization (CR) outcomes. To address this, we propose \\textit{SQ-GAN}, a novel approach that enhances CR by introducing a style space quantization scheme. This method transforms the sparse, continuous input latent space into a compact, structured discrete proxy space, allowing each element to correspond to a specific data point, thereby improving CR performance. Instead of direct quantization, we first map the input latent variables into a less entangled ``style'' space and apply quantization using a learnable codebook. This enables each quantized code to control distinct factors of variation. Additionally, we optimize the optimal transport distance to align the codebook codes with features extracted from the training data by a foundation model, embedding external knowledge into the codebook and establishing a semantically rich vocabulary that properly describes the training dataset. Extensive experiments demonstrate significant improvements in both discriminator robustness and generation quality with our method.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces \textit{SQ-GAN}, a novel approach to improve consistency regularization (CR) in GANs under limited data settings. By transforming the sparse, continuous input latent space into a compact, structured discrete proxy space through style space quantization, \textit{SQ-GAN} enhances CR performance. The method involves mapping input latent variables into a less entangled style space and applying quantization using a learnable codebook, which allows each quantized code to control distinct factors of variation. Additionally, the optimal transport distance is optimized to align codebook codes with features extracted from training data by a foundation model, embedding external knowledge into the codebook. This results in a semantically rich vocabulary that accurately describes the training dataset, leading to significant improvements in discriminator robustness and generation quality.",
        "Tags": [
            "Generative Adversarial Networks (GANs)",
            "Data Augmentation",
            "Style Space Quantization",
            "Optimal Transport",
            "Learnable Codebook"
        ]
    },
    {
        "Title": "Meta-Learning Hyperparameters for Foundation Model Adaptation in Remote-Sensing Imagery",
        "Authors": "Zichen Tian \u00b7 Yaoyao Liu \u00b7 Yaoyao Liu \u00b7 Qianru Sun",
        "Abstract": "Training large foundation models of remote-sensing (RS) images is almost impossible due to the limited and long-tailed data problems. Fine-tuning natural image pre-trained models on RS images is a straightforward solution. To reduce computational costs and improve performance on tail classes, existing methods apply parameter-efficient fine-tuning (PEFT) techniques, such as LoRA and AdaptFormer. However, we observe that fixed hyperparameters -- such as intra-layer positions, layer depth, and scaling factors, can considerably hinder PEFT performance, as fine-tuning on RS images proves highly sensitive to these settings. To address this, we propose MetaPEFT, a method incorporating adaptive scalers that dynamically adjust module influence during fine-tuning. MetaPEFT dynamically adjusts three key factors of PEFT on RS images: module insertion, layer selection, and module-wise learning rates, which collectively control the influence of PEFT modules across the network. We conduct extensive experiments on three transfer-learning scenarios and five datasets. The results show that MetaPEFT achieves state-of-the-art performance in cross-spectral adaptation, requiring only a small amount of trainable parameters and improving tail-class accuracy significantly. Our code is available in the supplementary materials for review.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Training large foundation models for remote-sensing (RS) imagery is challenging due to limited and long-tailed data. Fine-tuning pre-trained natural image models on RS images is a common approach, but fixed hyperparameters in parameter-efficient fine-tuning (PEFT) techniques like LoRA and AdaptFormer can hinder performance. To address this, the authors propose MetaPEFT, a method that dynamically adjusts module influence during fine-tuning by optimizing three key factors: module insertion, layer selection, and module-wise learning rates. MetaPEFT achieves state-of-the-art performance in cross-spectral adaptation, improves tail-class accuracy, and requires minimal trainable parameters.",
        "Tags": [
            "Remote Sensing Image Analysis",
            "Parameter-Efficient Fine-Tuning (PEFT)",
            "Meta-Learning",
            "Dynamic Hyperparameter Optimization",
            "Long-Tail Learning"
        ]
    },
    {
        "Title": "All-directional Disparity Estimation for Real-world QPD Images",
        "Authors": "Hongtao Yu \u00b7 Shaohui Song \u00b7 Lihu Sun \u00b7 Wenkai Su \u00b7 Xiaodong Yang \u00b7 Chengming Liu",
        "Abstract": "Quad Photodiode (QPD) sensors represent an evolution by providing four sub-views, whereas dual-pixel (DP) sensors are limited to two sub-views. In addition to enhancing auto-focus performance, QPD sensors also enable disparity estimation in horizontal and vertical directions. However, the characteristics of QPD sensors, including uneven illumination across sub-views and the narrow baseline, render algorithm design difficult. Furthermore, effectively utilizing the two-directional disparity of QPD sensors remains a challenge. The scarcity of QPD disparity datasets also limits the development of learning-based methods. In this work, we address these challenges by first proposing a DPNet for DP disparity estimation. Specifically, we design an illumination-invariant module to reduce the impact of illumination, followed by a coarse-to-fine module to estimate sub-pixel disparity. Building upon the DPNet, we further propose a QuadNet, which integrates the two-directional disparity via an edge-aware fusion module. To facilitate the evaluation of our approaches, we propose the first QPD disparity dataset QPD2K, comprising 2,100 real-world QPD images and corresponding disparity maps. Experiments demonstrate that our approaches achieve state-of-the-art performance in DP and QPD disparity estimation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenges of disparity estimation using Quad Photodiode (QPD) sensors, which provide four sub-views compared to the two sub-views of dual-pixel (DP) sensors. The authors propose a DPNet for DP disparity estimation, featuring an illumination-invariant module and a coarse-to-fine module for sub-pixel disparity estimation. Building on DPNet, they introduce QuadNet, which integrates two-directional disparity using an edge-aware fusion module. Additionally, the authors contribute the first QPD disparity dataset, QPD2K, containing 2,100 real-world QPD images with corresponding disparity maps. The proposed methods achieve state-of-the-art performance in DP and QPD disparity estimation.",
        "Tags": [
            "Depth Estimation",
            "Low-Level Vision",
            "Quad Photodiode Sensors",
            "Edge-Aware Fusion",
            "Illumination-Invariant Module"
        ]
    },
    {
        "Title": "Relative Pose Estimation through Affine Corrections of Monocular Depth Priors",
        "Authors": "Yifan Yu \u00b7 Shaohui Liu \u00b7 R\u00e9mi Pautrat \u00b7 Marc Pollefeys \u00b7 Viktor Larsson",
        "Abstract": "Monocular depth estimation (MDE) models have undergone significant advancements over recent years. Many MDE models aim to predict affine-invariant relative depth from monocular images, while recent developments in large-scale training and vision foundation models enable reasonable estimation of metric (absolute) depth. However, effectively leveraging these predictions for geometric vision tasks, in particular relative pose estimation, remains relatively under explored. While depths provide rich constraints for cross-view image alignment, the intrinsic noise and ambiguity from the monocular depth priors present practical challenges to improving upon classic keypoint-based solutions. In this paper, we develop three solvers for relative pose estimation that explicitly account for independent affine (scale and shift) ambiguities, covering both calibrated and uncalibrated conditions. We further propose a hybrid estimation pipeline that combines our proposed solvers with classic point-based solvers and epipolar constraints. We find that the affine correction modeling is beneficial to not only the relative depth priors but also, surprisingly, the \"metric\" ones. Results across multiple datasets demonstrate large improvements of our approach over classic keypoint-based baselines and PnP-based solutions, under both calibrated and uncalibrated setups. We also show that our method improves consistently with different feature matchers and MDE models, and can further benefit from very recent advances on both modules.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the challenge of leveraging monocular depth estimation (MDE) models for relative pose estimation, a task that remains underexplored despite advancements in MDE. The authors introduce three solvers designed to account for affine ambiguities in depth predictions, applicable to both calibrated and uncalibrated scenarios. A hybrid pipeline combining these solvers with traditional point-based methods and epipolar constraints is proposed, demonstrating significant improvements over conventional keypoint-based and PnP-based solutions. The approach is shown to be robust across different datasets, feature matchers, and MDE models, benefiting from recent advancements in these areas.",
        "Tags": [
            "Depth Estimation",
            "Relative Pose Estimation",
            "Affine Correction",
            "Hybrid Estimation Pipeline",
            "Epipolar Constraints"
        ]
    },
    {
        "Title": "Conditional Balance: Improving Multi-Conditioning Trade-Offs in Image Generation",
        "Authors": "Nadav Z. Cohen \u00b7 Oron Nir \u00b7 Ariel Shamir",
        "Abstract": "Balancing content fidelity and artistic style is a pivotal challenge in image generation. While traditional style transfer methods and modern Denoising Diffusion Probabilistic Models (DDPMs) strive to achieve this balance, they often struggle to do so without sacrificing either style, content, or sometimes both. This work addresses this challenge by analyzing the ability of DDPMs to maintain content and style equilibrium. We introduce a novel method to identify sensitivities within the DDPM attention layers, identifying specific layers that correspond to different stylistic aspects. By directing conditional inputs only to these sensitive layers, our approach enables fine-grained control over style and content, significantly reducing issues arising from over-constrained inputs. Our findings demonstrate that this method enhances recent stylization techniques by better aligning style and content, ultimately improving the quality of generated visual content.",
        "Link": "https://nadavc220.github.io/conditional-balance.github.io/",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of balancing content fidelity and artistic style in image generation, particularly with Denoising Diffusion Probabilistic Models (DDPMs). The authors propose a novel method to identify sensitive layers within DDPM attention mechanisms that correspond to specific stylistic aspects. By selectively applying conditional inputs to these layers, the method achieves fine-grained control over style and content, reducing issues from over-constrained inputs. The approach enhances stylization techniques, improving the alignment and quality of generated visual content.",
        "Tags": [
            "Diffusion Models",
            "Image Generation",
            "Attention Mechanisms",
            "Style-Content Trade-Off",
            "Fine-Grained Control"
        ]
    },
    {
        "Title": "Rethink Visual-language Pretraining for Deepfake Detection: Multi-modal Interpretable Forged Face Detection",
        "Authors": "Xiao Guo \u00b7 Xiufeng Song \u00b7 Yue Zhang \u00b7 Xiaohong Liu \u00b7 Xiaoming Liu",
        "Abstract": "Deepfake detection is a long-established research topic crucial for combating the spread of malicious misinformation. Unlike previous methods that provide either binary classification results or textual explanations for deepfake detection, we propose a novel method that delivers both simultaneously. Our method harnesses the multi-modal learning power of the pre-trained CLIP and the unprecedented interpretability of large language models (LLMs) to enhance both the generalization and interpretability of deepfake detection. Specifically, we introduce a multi-modal face forgery detector (M2F2-Det) that employs specially designed face forgery prompt learning, integrating zero-shot learning capabilities of the pre-trained CLIP to improve generalization to unseen forgeries.Also, M2F2-Det incorporates the LLM to provide detailed explanations for detection decisions, offering strong interpretability by bridging the gap between natural language and the subtle nuances of facial forgery detection. Empirically, we evaluate M2F2-Det for both detection and sentence generation tasks, on both of which M2F2-Det achieves state-of-the-art performance, showing its effectiveness in detecting and explaining diverse and unseen forgeries. Code and models will be released upon publication.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a novel method for deepfake detection that combines binary classification with textual explanations, leveraging the strengths of pre-trained CLIP and large language models (LLMs). The proposed multi-modal face forgery detector (M2F2-Det) utilizes face forgery prompt learning to enhance generalization to unseen forgeries and incorporates LLMs for detailed explanations of detection decisions, thereby improving interpretability. The method demonstrates state-of-the-art performance in both detection and sentence generation tasks, effectively detecting and explaining diverse and unseen forgeries.",
        "Tags": [
            "Deepfake Detection",
            "Multimodal Learning",
            "CLIP",
            "Large Language Models (LLMs)",
            "Zero-Shot Learning",
            "Face Forgery Prompt Learning",
            "Sentence Generation",
            "Interpretable AI"
        ]
    },
    {
        "Title": "Unconstrained 3D gaze estimation with Gaze-Aware 3D Context Encoding",
        "Authors": "Yuki Kawana \u00b7 Shintaro Shiba \u00b7 Quan Kong \u00b7 Norimasa Kobori",
        "Abstract": "We propose a novel 3D gaze estimation approach that learns spatial relationships between the subject, scene, and objects and outputs 3D gaze direction. Our method targets unconstrained settings, including cases where close-up views of the subject\u2019s eyes are unavailable, such as when the subject is distant or facing away. Previous methods rely on either 2D appearance alone or limited spatial cues by using depth maps in non-learnable post-processing. Estimating 3D gaze direction from 2D observations in these settings is challenging; besides variations in subject, scene, and gaze direction, different camera poses produce various 2D appearances and 3D gaze directions, even when targeting the same 3D scene. To address this issue, we propose gaze-aware 3D context encoding. Our method represents the subject and scene as 3D poses and object positions as 3D context, learning spatial relationships in 3D space. Inspired by human vision, we geometrically align this context in a subject-centric space, significantly reducing the spatial complexity. Furthermore, we propose D$^3$ (distance-direction-decomposed) positional encoding to better capture the spatial relationship between 3D context and gaze direction in direction and distance space. Experiments show substantial improvement, reducing mean angle error of leading baselines by 13\\%\u201337\\% on benchmark datasets in single-view settings.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Error_DeepSeek_Response": "```json\n{\n  \"tags\": [\"3D Human Pose Estimation\", \"3D Reconstruction\"],\n  \"abstract\": \"This paper introduces a novel 3D gaze estimation method that leverages spatial relationships between the subject, scene, and objects to predict 3D gaze direction. The approach is designed for unconstrained settings, such as when the subject is distant or not facing the camera, making it robust even without close-up eye views. Unlike previous methods that rely on 2D appearance or non-learnable depth maps, the proposed method encodes 3D context by representing the subject and scene as 3D poses and object positions. It geometrically aligns this context in a subject-centric space, inspired by human vision, to reduce spatial complexity. Additionally, a D$^3$ (distance-direction-decomposed) positional encoding is introduced to better model the spatial relationship between 3D context and gaze direction. The method demonstrates significant improvements, reducing mean angle error by 13\\%\u201337\\% compared to leading baselines on benchmark datasets.\",\n  \"extra_tags\": [\"Gaze Estimation\", \"3D Context Encoding\", \"Subject-Centric Alignment\"]\n}\n```"
    },
    {
        "Title": "VideoGuide: Improving Video Diffusion Models without Training Through a Teacher's Guide",
        "Authors": "Dohun Lee \u00b7 Bryan Sangwoo Kim \u00b7 Geon Yeong Park \u00b7 Jong Chul Ye",
        "Abstract": "Text-to-image (T2I) diffusion models have revolutionized visual content creation, but extending these capabilities to text-to-video (T2V) generation remains a challenge, particularly in preserving temporal consistency. Existing methods that aim to improve consistency often cause trade-offs such as reduced imaging quality and impractical computational time. To address these issues we introduce VideoGuide, a novel framework that enhances the temporal consistency of pretrained T2V models without the need for additional training or fine-tuning. Instead, VideoGuide leverages any pretrained video diffusion model (VDM) or itself as a guide during the early stages of inference, improving temporal quality by interpolating the guiding model\u2019s denoised samples into the sampling model's denoising process. The proposed method brings about significant improvement in temporal consistency and image fidelity, providing a cost-effective and practical solution that synergizes the strengths of various video diffusion models. Furthermore, we demonstrate prior distillation, revealing that base models can achieve enhanced text coherence by utilizing the superior data prior of the guiding model through the proposed method. Project Page: https://videoguide2025.github.io/",
        "Link": "https://videoguide2025.github.io/",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "VideoGuide is a novel framework designed to enhance the temporal consistency of pretrained text-to-video (T2V) diffusion models without requiring additional training or fine-tuning. By leveraging a pretrained video diffusion model as a guide during the early stages of inference, VideoGuide interpolates the guide's denoised samples into the sampling model's denoising process. This approach significantly improves both temporal consistency and image fidelity, offering a cost-effective and practical solution. Additionally, the method demonstrates prior distillation, enabling base models to achieve better text coherence by utilizing the superior data prior of the guiding model.",
        "Tags": [
            "Video Generation",
            "Diffusion Models",
            "Temporal Consistency",
            "Prior Distillation",
            "Inference Optimization"
        ]
    },
    {
        "Title": "MANTA: Diffusion Mamba for Efficient and Effective Stochastic Long-Term Dense Anticipation",
        "Authors": "Olga Zatsarynna \u00b7 Emad Bahrami \u00b7 Yazan Abu Farha \u00b7 Gianpiero Francesca \u00b7 J\u00fcrgen Gall",
        "Abstract": "Our work addresses the problem of stochastic long-term dense anticipation. The goal of this task is to predict actions and their durations several minutes into the future based on provided video observations. Anticipation over extended horizons introduces high uncertainty, as a single observation can lead to multiple plausible future outcomes. To address this uncertainty, stochastic models are designed to predict several potential future action sequences. Recent work has further proposed to incorporate uncertainty modelling for observed frames by simultaneously predicting per-frame past and future actions in a unified manner. While such joint modelling of actions is beneficial, it requires long-range temporal capabilities to connect events across distant past and future time points. However, the previous work struggles to achieve such a long-range understanding due to its limited and/or sparse receptive field. To alleviate this issue, we propose a novel MANTA (MAmba for ANTicipation) network. Our model enables effective long-term temporal modelling even for very long sequences while maintaining linear complexity in sequence length. We demonstrate that our approach achieves state-of-the-art results on three datasets\u2014Breakfast, 50Salads, and Assembly101\u2014while also significantly improving computational and memory efficiency.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces MANTA, a novel network designed for stochastic long-term dense anticipation, which involves predicting future actions and their durations based on video observations. The model addresses the challenge of high uncertainty in long-term predictions by enabling effective long-range temporal modeling with linear complexity in sequence length. MANTA achieves state-of-the-art performance on the Breakfast, 50Salads, and Assembly101 datasets, while also enhancing computational and memory efficiency.",
        "Tags": [
            "Video Understanding",
            "Action Detection",
            "Diffusion Models",
            "Mamba",
            "Long-Range Temporal Modeling",
            "Stochastic Prediction",
            "Linear Complexity"
        ]
    },
    {
        "Title": "Unveiling Visual Perception in Language Models: A Attention Head Analysis Approach",
        "Authors": "Jing Bi \u00b7 Lianggong Bruce Wen \u00b7 Zhang Liu \u00b7 JunJia Guo \u00b7 Yunlong Tang \u00b7 Bingjie Wang \u00b7 Chenliang Xu",
        "Abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated remarkable progress in visual understanding. This impressive leap raises a compelling question: how can language models, initially trained solely on linguistic data, effectively interpret and process visual content? This paper aims to address this question with systematic investigation across 4 model families and 4 model scales, uncovering a unique class of attention heads that focus specifically on visual content. Our analysis reveals a strong correlation between the behavior of these attention heads, the distribution of attention weights, and their concentration on visual tokens within the input. These findings enhance our understanding of how LLMs adapt to multimodal tasks, demonstrating their potential to bridge the gap between textual and visual understanding. This work paves the way for the development of AI systems capable of engaging with diverse modalities.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper investigates how language models, initially trained on linguistic data, interpret and process visual content. Through a systematic analysis across multiple model families and scales, the study identifies a unique class of attention heads that specifically focus on visual tokens. The findings reveal a strong correlation between the behavior of these attention heads, their attention weight distribution, and their concentration on visual content. This work enhances understanding of how large language models adapt to multimodal tasks, bridging the gap between textual and visual understanding and paving the way for AI systems capable of engaging with diverse modalities.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Vision-Language Models (VLMs)",
            "Attention Mechanisms",
            "Visual Token Processing",
            "Multimodal Adaptation"
        ]
    },
    {
        "Title": "Paint by Inpaint: Learning to Add Image Objects by Removing Them First",
        "Authors": "Navve Wasserman \u00b7 Noam Rotstein \u00b7 Roy Ganz \u00b7 Ron Kimmel",
        "Abstract": "Image editing has advanced significantly with the introduction of text-conditioned diffusion models. Despite this progress, seamlessly adding objects to images based on textual instructions without requiring user-provided input masks remains a challenge. We address this by leveraging the insight that removing objects (Inpaint) is significantly simpler than its inverse process of adding them (Paint), attributed to the utilization of segmentation mask datasets alongside inpainting models that inpaint within these masks. Capitalizing on this realization, by implementing an automated and extensive pipeline, we curate a filtered large-scale image dataset containing pairs of images and their corresponding object-removed versions. Using these pairs, we train a diffusion model to inverse the inpainting process, effectively adding objects into images. Unlike other editing datasets, ours features natural target images instead of synthetic ones; moreover, it maintains consistency between source and target by construction. Additionally, we utilize a large Vision-Language Model to provide detailed descriptions of the removed objects and a Large Language Model to convert these descriptions into diverse, natural-language instructions. Our quantitative and qualitative results show that the trained model surpasses existing models in both object addition and general editing tasks. To propel future research, we will release the dataset alongside the trained models.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces a novel approach to image editing by leveraging the insight that removing objects (inpainting) is simpler than adding them. The authors curate a large-scale dataset of image pairs, where one image contains an object and the other has the object removed. Using this dataset, they train a diffusion model to reverse the inpainting process, enabling the addition of objects to images based on textual instructions without requiring user-provided masks. The dataset features natural target images and maintains consistency between source and target images. The model is enhanced by using a Vision-Language Model for detailed object descriptions and a Large Language Model to generate diverse, natural-language instructions. The trained model outperforms existing models in object addition and general editing tasks, and the dataset and models will be released to support future research.",
        "Tags": [
            "Image Editing",
            "Diffusion Models",
            "Text-Conditioned Editing",
            "Vision-Language Models",
            "Large Language Models"
        ]
    },
    {
        "Title": "High Dynamic Range Video Compression: A Large-Scale Benchmark Dataset and A Learned Bit-depth Scalable Compression Algorithm",
        "Authors": "zhaoyi tian \u00b7 Feifeng Wang \u00b7 Shiwei Wang \u00b7 Zihao Zhou \u00b7 Yao Zhu \u00b7 Liquan Shen",
        "Abstract": "Recently, learned video compression (LVC) is undergoing a period of rapid development. However, due to absence of large and high-quality high dynamic range (HDR) video training data, LVC on HDR video is still unexplored. In this paper, we are the first to collect a large-scale HDR video benchmark dataset, named HDRVD2K, featuring huge quantity, diverse scenes and multiple motion types. HDRVD2K fills gaps of video training data and facilitate the development of LVC on HDR videos. Based on HDRVD2K, we further propose the first learned bit-depth scalable video compression (LBSVC) network for HDR videos by effectively exploiting bit-depth redundancy between videos of multiple dynamic ranges. To achieve this, we first propose a compression-friendly bit-depth enhancement module (BEM) to effectively predict original HDR videos based on compressed tone-mapped low dynamic range (LDR) videos and dynamic range prior, instead of reducing redundancy only through spatio-temporal predictions. Our method greatly improves the reconstruction quality and compression performance on HDR videos. Extensive experiments demonstrate the effectiveness of HDRVD2K on learned HDR video compression and great compression performance of our proposed LBSVC network.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the lack of large-scale, high-quality high dynamic range (HDR) video datasets for learned video compression (LVC) by introducing HDRVD2K, a comprehensive HDR video benchmark dataset. The dataset features a vast quantity of videos, diverse scenes, and multiple motion types, facilitating advancements in LVC for HDR videos. Additionally, the authors propose a novel learned bit-depth scalable video compression (LBSVC) network that leverages bit-depth redundancy across videos of varying dynamic ranges. A key innovation is the compression-friendly bit-depth enhancement module (BEM), which predicts original HDR videos from compressed tone-mapped low dynamic range (LDR) videos using dynamic range prior, significantly improving reconstruction quality and compression performance. Experimental results validate the effectiveness of HDRVD2K and the superior performance of the proposed LBSVC network.",
        "Tags": [
            "Video Compression",
            "High Dynamic Range (HDR)",
            "Datasets and Benchmarks",
            "Bit-depth Scalable Compression",
            "Dynamic Range Prior",
            "Tone-mapped LDR Videos"
        ]
    },
    {
        "Title": "Uncertain Multimodal Intention and Emotion Understanding in the Wild",
        "Authors": "Qu Yang \u00b7 QingHongYa Shi \u00b7 Tongxin Wang \u00b7 Mang Ye",
        "Abstract": "Understanding intention and emotion from social media poses unique challenges due to the inherent uncertainty in multimodal data, where posts often contain incomplete or missing modalities. While this uncertainty reflects real-world scenarios, it remains underexplored within the computer vision community, particularly in conjunction with the intrinsic relationship between emotion and intention. To address these challenges, we introduce the Multimodal IntentioN and Emotion Understanding in the Wild (MINE) dataset, comprising over 20,000 topic-specific social media posts with natural modality variations across text, image, video, and audio. MINE is distinctively constructed to capture both the uncertain nature of multimodal data and the implicit correlations between intentions and emotions, providing extensive annotations for both aspects. To tackle these scenarios, we propose the Bridging Emotion-Intention via Implicit Label Reasoning (BEAR) framework. BEAR consists of two key components: a BEIFormer that leverages emotion-intention correlations, and a Modality Asynchronous Prompt that handles modality uncertainty. Experiments show that BEAR outperforms existing methods in processing uncertain multimodal data while effectively mining emotion-intention relationships for social media content understanding. Dataset and code will be released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the challenge of understanding intention and emotion from social media, where multimodal data often contains incomplete or missing modalities, reflecting real-world uncertainty. To tackle this, the authors introduce the MINE dataset, which includes over 20,000 topic-specific social media posts with natural variations across text, image, video, and audio, capturing both uncertainty and the implicit correlations between intentions and emotions. They propose the BEAR framework, which consists of a BEIFormer to leverage emotion-intention correlations and a Modality Asynchronous Prompt to handle modality uncertainty. BEAR demonstrates superior performance in processing uncertain multimodal data and effectively mining emotion-intention relationships.",
        "Tags": [
            "Multimodal Learning",
            "Emotion Understanding",
            "Uncertainty Handling",
            "Implicit Label Reasoning",
            "Modality Asynchronous Prompt"
        ]
    },
    {
        "Title": "Towards Effective and Sparse Adversarial Attack on Spiking Neural Networks via Breaking Invisible Surrogate Gradients",
        "Authors": "Li Lun \u00b7 Kunyu Feng \u00b7 Qinglong Ni \u00b7 Ling Liang \u00b7 Yuan Wang \u00b7 Ying Li \u00b7 dunshan yu \u00b7 Xiaoxin CUI",
        "Abstract": "Spiking neural networks (SNNs) have shown their competence in handling spatial-temporal event-based data with low energy consumption. Similar to conventional artificial neural networks (ANNs), SNNs are also vulnerable to gradient-based adversarial attacks, wherein gradients are calculated by spatial-temporal back-propagation (STBP) and surrogate gradients (SGs). However, the SGs may be invisible for an inference-only model as they do not influence the inference results, and current gradient-based attacks are ineffective for binary dynamic images captured by the dynamic vision sensor (DVS). While some approaches addressed the issue of invisible SGs through universal SGs, their SGs lack a correlation with the victim model, resulting in sub-optimal performance. Moreover, the imperceptibility of existing SNN-based binary attacks is still insufficient. In this paper, we introduce an innovative potential-dependent surrogate gradient (PDSG) method to establish a robust connection between the SG and the model, thereby enhancing the adaptability of adversarial attacks across various models with invisible SGs. Additionally, we propose the sparse dynamic attack (SDA) to effectively attack binary dynamic images. Utilizing a generation-reduction paradigm, SDA can fully optimize the sparsity of adversarial perturbations. Experimental results demonstrate that our PDSG and SDA outperform state-of-the-art SNN-based attacks across various models and datasets. Specifically, our PDSG achieves 100% attack success rate on ImageNet, and our SDA obtains 82% attack success rate by modifying only 0.24% of the pixels on CIFAR10DVS.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Spiking neural networks (SNNs) are effective for spatial-temporal event-based data but are vulnerable to gradient-based adversarial attacks. Current attacks struggle with binary dynamic images from dynamic vision sensors (DVS) due to invisible surrogate gradients (SGs) that do not influence inference. Existing universal SGs lack correlation with the victim model, leading to sub-optimal performance. This paper introduces a potential-dependent surrogate gradient (PDSG) method to strengthen the connection between SGs and the model, improving adversarial attack adaptability. Additionally, a sparse dynamic attack (SDA) is proposed to effectively target binary dynamic images by optimizing the sparsity of adversarial perturbations. The PDSG achieves a 100% attack success rate on ImageNet, while the SDA achieves an 82% success rate by modifying only 0.24% of pixels on CIFAR10DVS.",
        "Tags": [
            "Spiking Neural Networks (SNNs)",
            "Adversarial Attacks",
            "Potential-Dependent Surrogate Gradient (PDSG)",
            "Sparse Dynamic Attack (SDA)",
            "Binary Dynamic Images"
        ]
    },
    {
        "Title": "Advancing Fine-Grained Compositional Alignment in Video-Text Models",
        "Authors": "Dahun Kim \u00b7 AJ Piergiovanni \u00b7 Ganesh Satish Mallya \u00b7 Anelia Angelova",
        "Abstract": "We introduce a benchmark and learning framework for advancing video-text compositionality understanding, aimed at enhancing vision-language models (VLMs) in fine-grained temporal alignment. Unlike existing benchmarks focused on static image-text compositionality or isolated single-event videos, our benchmark focuses on fine-grained video-text alignment in continuous multi-event videos. Leveraging video-text datasets with temporally localized event captions (\\eg ActivityNet-Captions, YouCook2), we create challenging negative samples with subtle temporal disruptions such as reordering, action word replacements, partial captioning, and combined disruptions that comprehensively test models\u2019 compositional sensitivity across extended, cohesive video-text sequences. To enhance model performance, we propose a hierarchical pairwise preference loss that strengthens alignment with temporally accurate pairs and progressively reduces similarity for increasingly disrupted pairs, encouraging fine-grained compositional alignment. To mitigate the limited availability of densely annotated video data, we introduce a pretraining strategy that concatenates short video-caption pairs to simulate multi-event sequences, facilitating effective compositional learning. We evaluate large multimodal models (LMMs) on our benchmark, identifying both strengths and areas for improvement in video-text compositionality. Our work provides a comprehensive framework for assessing and advancing model capabilities in achieving fine-grained, temporally coherent video-text alignment.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a benchmark and learning framework to enhance fine-grained temporal alignment in video-text models, focusing on continuous multi-event videos rather than static image-text or single-event scenarios. The benchmark utilizes datasets with temporally localized event captions to create challenging negative samples, testing models' compositional sensitivity. A hierarchical pairwise preference loss is proposed to improve alignment with accurate pairs and reduce similarity for disrupted pairs. Additionally, a pretraining strategy is introduced to simulate multi-event sequences, addressing the scarcity of densely annotated video data. The framework evaluates large multimodal models, highlighting strengths and areas for improvement in video-text compositionality.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Video Understanding",
            "Temporal Alignment",
            "Hierarchical Pairwise Preference Loss",
            "Pretraining Strategy"
        ]
    },
    {
        "Title": "ImagineFSL: Self-Supervised Pretraining Matters on Imagined Base Set for VLM-based Few-shot Learning",
        "Authors": "Haoyuan Yang \u00b7 Xiaoou Li \u00b7 Jiaming Lv \u00b7 Xianjun Cheng \u00b7 Qilong Wang \u00b7 Peihua Li",
        "Abstract": "Adapting CLIP models for few-shot recognition has recently attracted significant attention. Despite considerable progress, these adaptations remain hindered by the pervasive challenge of data scarcity. Text-to-image models, capable of generating abundant photorealistic labeled images, offer a promising solution. However, existing approaches treat synthetic images merely as complements to real images, rather than as standalone knowledge repositories stemming from distinct foundation models. To overcome this limitation, we reconceptualize synthetic images as an imagined base set, i.e., a unique, large-scale synthetic dataset encompassing diverse concepts. We introduce a novel CLIP adaptation methodology called ImagineFSL, involving pretraining on the imagined base set followed by fine-tuning on downstream few-shot tasks. We find that, compared to no pretraining, both supervised   and self-supervised pretraining are beneficial, with the latter providing better performance. Building on this finding, we propose an improved self-supervised method tailored for few-shot scenarios, enhancing the transferability of representations from synthetic to real image domains. Additionally, we present an image generation pipeline that employs chain-of-thought and in-context learning techniques, harnessing foundation models to automatically generate diverse, realistic images. Our methods are validated across eleven datasets, consistently outperforming state-of-the-art methods by substantial margins.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces ImagineFSL, a novel methodology for adapting CLIP models to few-shot recognition tasks by leveraging synthetic images generated from text-to-image models. These synthetic images are treated as an 'imagined base set,' a unique, large-scale dataset that serves as a standalone knowledge repository. The approach involves pretraining on this imagined base set followed by fine-tuning on downstream few-shot tasks. The study finds that self-supervised pretraining outperforms supervised pretraining, leading to better transferability of representations from synthetic to real image domains. An improved self-supervised method tailored for few-shot scenarios is proposed, along with an image generation pipeline that uses chain-of-thought and in-context learning techniques to produce diverse, realistic images. The methodology is validated across eleven datasets, demonstrating significant improvements over state-of-the-art methods.",
        "Tags": [
            "CLIP",
            "Self-Supervised Learning",
            "Few-Shot Learning",
            "Text-to-Image Generation",
            "Transfer Learning",
            "Synthetic Data"
        ]
    },
    {
        "Title": "The Art of Deception: Color Visual Illusions and Diffusion Models",
        "Authors": "Alexandra Gomez-Villa \u00b7 Kai Wang \u00b7 C.Alejandro Parraga \u00b7 Bart\u0142omiej Twardowski \u00b7 Jesus Malo \u00b7 Javier Vazquez-Corral \u00b7 Joost van de Weijer",
        "Abstract": "Visual illusions in humans arise when interpreting out-of-distribution stimuli: if the observer is adapted to certain statistics, perception of outliers deviates from reality. Recent studies have shown that artificial neural networks (ANNs) can also be deceived by visual illusions.This revelation raises profound questions about the nature of visual information. Why are two independent systems, both human brains and ANNs, susceptible to the same illusions? Should any ANN be capable of perceiving visual illusions? Are these perceptions a feature or a flaw? In this work, we study how visual illusions are encoded in diffusion models. Remarkably, we show that they present human-like brightness/color shifts in their latent space. We use this fact to demonstrate that diffusion models can predict visual illusions.  Furthermore, we also show how to generate new unseen visual illusions in realistic images using text-to-image diffusion models.  We validate this ability through psychophysical experiments that show how our model-generated illusions also fool humans.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper explores how diffusion models encode and predict visual illusions, demonstrating that they exhibit human-like brightness and color shifts in their latent space. The study reveals that diffusion models can not only predict existing visual illusions but also generate new, realistic illusions using text-to-image models. These model-generated illusions are validated through psychophysical experiments, showing they can deceive human perception similarly to natural illusions.",
        "Tags": [
            "Diffusion Models",
            "Visual Illusions",
            "Latent Space Analysis",
            "Text-to-Image Generation",
            "Psychophysical Validation"
        ]
    },
    {
        "Title": "VIRES: Video Instance Repainting with Sketch and Text Guidance",
        "Authors": "Shuchen Weng \u00b7 Haojie Zheng \u00b7 Peixuan Zhang \u00b7 Yuchen Hong \u00b7 Han Jiang \u00b7 Si Li \u00b7 Boxin Shi",
        "Abstract": "We introduce VIRES, a video instance repainting method with sketch and text guidance, enabling video instance repainting, replacement, generation, and removal. Existing approaches struggle with temporal consistency and accurate alignment with the provided sketch sequence. VIRES leverages the generative priors of text-to-video models to maintain temporal consistency and produce visually pleasing results. We propose the Sequential ControlNet with the standardized self-scaling, which effectively extracts structure layouts and adaptively captures high-contrast sketch details. We further augment the diffusion transformer backbone with the sketch attention to interpret and inject fine-grained sketch semantics. A sketch-aware encoder ensures that repainted results are aligned with the provided sketch sequence. Additionally, we contribute the VireSet, a dataset with detailed annotations tailored for training and evaluating video instance editing methods. Experimental results demonstrate the effectiveness of VIRES, which outperforms state-of-the-art methods in visual quality, temporal consistency, condition alignment, and human ratings.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "VIRES is a novel video instance repainting method that utilizes sketch and text guidance for tasks such as repainting, replacement, generation, and removal. It addresses challenges in temporal consistency and sketch alignment by leveraging generative priors from text-to-video models. The method introduces Sequential ControlNet with self-scaling for structure layout extraction and sketch detail capture, and enhances a diffusion transformer backbone with sketch attention for fine-grained semantic interpretation. A sketch-aware encoder ensures alignment with the sketch sequence. VIRES also introduces VireSet, a dataset for training and evaluating video instance editing methods. The approach demonstrates superior performance in visual quality, temporal consistency, and condition alignment compared to existing methods.",
        "Tags": [
            "Video Generation",
            "Diffusion Models",
            "Sequential ControlNet",
            "Sketch Attention",
            "VireSet Dataset"
        ]
    },
    {
        "Title": "UniScene: Unified Occupancy-centric Driving Scene Generation",
        "Authors": "Bohan Li \u00b7 Jiazhe Guo \u00b7 Hongsi Liu \u00b7 Yingshuang Zou \u00b7 Yikang Ding \u00b7 Xiwu Chen \u00b7 Hu ZHU \u00b7 Feiyang Tan \u00b7 Chi Zhang \u00b7 Tiancai Wang \u00b7 Shuchang Zhou \u00b7 Li Zhang \u00b7 Xiaojuan Qi \u00b7 Hao Zhao \u00b7 Mu Yang \u00b7 Wenjun Zeng \u00b7 Xin Jin",
        "Abstract": "Generating high-fidelity, controllable, and annotated training data is critical for autonomous driving. Existing methods typically generate a single data form directly from a coarse scene layout, which not only fails to output rich data forms required for diverse downstream tasks but also struggles to model the direct layout-to-data distribution. In this paper, we introduce UniScene, the first unified framework for generating three key data forms \u2014 semantic occupancy, video, and LiDAR \u2014 in driving scenes. UniScene employs a progressive generation process that decomposes the complex task of scene generation into two hierarchical steps: (a) first generating semantic occupancy from a customized scene layout as a meta scene representation rich in both semantic and geometric information, and then (b) conditioned on occupancy, generating video and LiDAR data, respectively, with two novel transfer strategies of Gaussian-based Joint Rendering and Prior-guided Sparse Modeling. This occupancy-centric approach reduces the generation burden, especially for intricate scenes, while providing detailed intermediate representations for the subsequent generation stages. Extensive experiments demonstrate that UniScene outperforms previous SOTAs in the occupancy, video, and LiDAR generation, which also indeed benefits downstream driving tasks. The code is available in the supplementary.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "UniScene introduces a unified framework for generating three key data forms\u2014semantic occupancy, video, and LiDAR\u2014in driving scenes. The framework employs a progressive generation process that decomposes scene generation into two hierarchical steps: first generating semantic occupancy from a customized scene layout, and then generating video and LiDAR data based on the occupancy. This occupancy-centric approach reduces the generation burden and provides detailed intermediate representations for subsequent stages. UniScene outperforms previous state-of-the-art methods in generating occupancy, video, and LiDAR data, benefiting downstream driving tasks.",
        "Tags": [
            "Autonomous Driving",
            "3D Generation",
            "Semantic Segmentation",
            "Gaussian-based Joint Rendering",
            "Prior-guided Sparse Modeling",
            "Occupancy-centric Generation"
        ]
    },
    {
        "Title": "SciBench: Addressing Scientific Illusions in Image Synthesis",
        "Authors": "Jialuo Li \u00b7 Wenhao Chai \u00b7 XINGYU FU \u00b7 Haiyang Xu \u00b7 Saining Xie",
        "Abstract": "This paper presents a novel approach to integrating scientific knowledge into generative models, enhancing their realism and consistency in image synthesis. We present SciScore, an end-to-end reward model that refines the assessment of generated images based on scientific knowledge, which is achieved by augmenting both the scientific comprehension and visual capabilities of pre-trained CLIP model. We also introduce SciBench, an expert-annotated adversarial dataset comprising 30k image pairs with 9k prompts, covering wide distinct scientific knowledge categories. Leveraging SciBench, we propose a two-stage training framework, comprising a supervised fine-tuning phase and a masked online fine-tuning phase, to incorporate scientific knowledge into existing generative models. Through comprehensive experiments, we demonstrate the effectiveness of our framework in establishing new standards for evaluating the scientific realism of generated content. Specifically, SciScore attains performance comparable to human-level, demonstrating a 5% improvement similar to evaluations conducted by experienced human experts. Furthermore, by applying our proposed fine-tuning method to FLUX, we achieve a performance enhancement exceeding 50% based on SciScore.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces SciBench, a novel approach to enhance the realism and scientific consistency of image synthesis by integrating scientific knowledge into generative models. The authors develop SciScore, an end-to-end reward model that evaluates generated images based on scientific knowledge, leveraging the capabilities of the pre-trained CLIP model. SciBench, an expert-annotated adversarial dataset with 30k image pairs and 9k prompts, is introduced to facilitate the integration of scientific knowledge. A two-stage training framework, involving supervised and masked online fine-tuning, is proposed to incorporate this knowledge into existing generative models. The framework demonstrates significant improvements in scientific realism, with SciScore achieving human-level performance and a 50% performance enhancement when applied to FLUX.",
        "Tags": [
            "Generative Adversarial Networks (GANs)",
            "Image Generation",
            "Scientific Knowledge Integration",
            "Reward Model",
            "Adversarial Dataset"
        ]
    },
    {
        "Title": "Removing Reflections from RAW Photos",
        "Authors": "Eric Kee \u00b7 Adam Pikielny \u00b7 Kevin Blackburn-Matzen \u00b7 Marc Levoy",
        "Abstract": "We describe a system to remove real-world reflections from images for consumer photography. Our system operates on linear (RAW) photos, and accepts an optional contextual photo looking in the opposite direction (e.g., the \"selfie\" camera on a mobile device). This optional photo helps disambiguate what should be considered the reflection. The system is trained solely on synthetic mixtures of real-world RAW images, which we combine using a reflection simulation that is photometrically and geometrically accurate. Our system comprises a base model that accepts the captured photo and optional context photo as input, and runs at 256p, followed by an up-sampling model that transforms 256p images to full resolution. The system can produce images for review at 1K in 4.5 to 6.5 seconds on a MacBook or iPhone 14 Pro. We test on RAW photos that were captured in the field and embody typical consumer photos, and show that our RAW-image simulation yields SOTA performance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper presents a system designed to remove real-world reflections from RAW photos, targeting consumer photography. The system leverages an optional contextual photo (e.g., from a selfie camera) to help distinguish reflections. It is trained on synthetic mixtures of real-world RAW images using a photometrically and geometrically accurate reflection simulation. The system includes a base model operating at 256p resolution and an up-sampling model to achieve full resolution. It demonstrates state-of-the-art performance on field-captured RAW photos, processing 1K images in 4.5 to 6.5 seconds on consumer devices like MacBook and iPhone 14 Pro.",
        "Tags": [
            "Image Editing",
            "Low-Level Vision",
            "RAW Image Processing",
            "Reflection Removal",
            "Synthetic Data Training"
        ]
    },
    {
        "Title": "WISE: A Framework for Gigapixel Whole-Slide-Image Lossless Compression",
        "Authors": "Yu Mao \u00b7 Jun Wang \u00b7 Nan Guan \u00b7 Chun Jason Xue",
        "Abstract": "Whole-Slide Images (WSIs) have revolutionized medical analysis by presenting high-resolution images of the whole tissue slide. Despite avoiding the physical storage of the slides, WSIs require considerable data volume, which makes the storage and maintenance of WSI records costly and unsustainable. To this end, this work presents the first investigation of lossless compression of WSI images. Interestingly, we find that most existing compression methods fail to compress the WSI images effectively. Furthermore, our analysis reveals that the failure of existing compressors is mainly due to information irregularity in WSI images. To resolve this issue, we develop a simple yet effective lossless compressor called WISE, specifically designed for WSI images. WISE employs a hierarchical encoding strategy to extract effective bits, reducing the entropy of the image and then adopting a dictionary-based method to handle the irregular frequency patterns. Through extensive experiments, we show that WISE can effectively compress the gigapixel WSI images to 36 times on average and up to 136 times.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Whole-Slide Images (WSIs) are crucial for medical analysis but require significant storage due to their high resolution. This paper introduces WISE, the first lossless compression framework specifically designed for WSI images. Existing compression methods often fail due to information irregularity in WSIs. WISE addresses this by using a hierarchical encoding strategy to reduce image entropy and a dictionary-based method to manage irregular frequency patterns. The framework demonstrates significant compression efficiency, reducing gigapixel WSI images by an average of 36 times and up to 136 times.",
        "Tags": [
            "Medical Image Analysis",
            "Data Augmentation",
            "Lossless Compression",
            "Hierarchical Encoding",
            "Dictionary-Based Compression"
        ]
    },
    {
        "Title": "Neuro-3D: Towards 3D Visual Decoding from EEG Signals",
        "Authors": "Zhanqiang Guo \u00b7 Jiamin Wu \u00b7 Yonghao Song \u00b7 Jiahui Bu \u00b7 Weijian Mai \u00b7 Qihao Zheng \u00b7 Wanli Ouyang \u00b7 Chunfeng Song",
        "Abstract": "Human's perception of the visual world is shaped by the stereo processing of 3D information. Understanding how the brain perceives and processes 3D visual stimuli in the real world has been a longstanding endeavor in neuroscience. Towards this goal, we introduce a new neuroscience task: decoding 3D visual perception from EEG signals, a neuroimaging technique that enables real-time monitoring of neural dynamics enriched with complex visual cues. To provide the essential benchmark, we first present EEG-3D, a pioneering dataset featuring multimodal analysis data and extensive EEG recordings from 12 subjects viewing 72 categories of 3D objects rendered in both videos and images. Furthermore, we propose Neuro-3D, a 3D visual decoding framework based on EEG signals. This framework adaptively integrates EEG features derived from static and dynamic stimuli to learn complementary and robust neural representations, which are subsequently utilized to recover both the shape and color of 3D objects through the proposed diffusion-based colored point cloud decoder. To the best of our knowledge, we are the first to explore EEG-based 3D visual decoding. Experiments indicate that Neuro-3D not only reconstructs colored 3D objects with high fidelity,  but also learns effective neural representations that enable insightful brain region analysis. The dataset and associated code will be made publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Neuro-3D, a novel framework for decoding 3D visual perception from EEG signals, addressing the challenge of understanding how the brain processes 3D visual stimuli. The authors present EEG-3D, a comprehensive dataset with EEG recordings from subjects viewing 3D objects in various formats. The Neuro-3D framework integrates EEG features from both static and dynamic stimuli to reconstruct 3D objects' shapes and colors using a diffusion-based colored point cloud decoder. This approach not only achieves high-fidelity 3D reconstructions but also provides insights into neural representations and brain region analysis.",
        "Tags": [
            "3D Reconstruction",
            "EEG Signal Processing",
            "Multimodal Learning",
            "Diffusion Models",
            "Colored Point Cloud Decoder",
            "Neural Representation Learning"
        ]
    },
    {
        "Title": "O-TPT: Orthogonality Constraints for Calibrating Test-time Prompt Tuning in Vision-Language Models",
        "Authors": "Ashshak Sharifdeen \u00b7 Muhammad Akhtar Munir \u00b7 Sanoojan Baliah \u00b7 Salman Khan \u00b7 Muhammad Haris Khan",
        "Abstract": "Test-time prompt tuning for vision-language models (VLMs) are getting attention due to their ability to learn with unlabeled data without fine-tuning. Although test-time prompt tuning methods for VLMs can boost accuracy, the resulting models tend to demonstrate poor calibration, which casts doubts on the reliability and trustworthiness of these models. Notably, more attention needs to be devoted to calibrating the test-time prompt tuning in vision-language models. To this end, we propose a new approach, called O-TPT that introduces orthogonality constraints on the textual features corresponding to the learnable prompts for calibrating test-time prompt tuning in VLMsTowards introducing orthogonality constraints, we make the following contributions. First, we uncover new insights behind the suboptimal calibration performance of existing methods relying on textual feature dispersion. Second, we show that imposing a simple orthogonalization of textual features is a more effective approach towards obtaining textual dispersion.We conduct extensive experiments on various datasets with different backbones and baselines. Results indicate that our method consistently outperforms the state-of-the-art in significantly reducing the overall average calibration error. Also, our method surpasses the zero-shot calibration performance on fine-grained classification tasks. Our code will be made public upon acceptance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Test-time prompt tuning for vision-language models (VLMs) has gained attention for its ability to learn from unlabeled data without fine-tuning. However, these methods often suffer from poor calibration, raising concerns about their reliability. To address this, the authors propose O-TPT, a novel approach that introduces orthogonality constraints on textual features to improve calibration during test-time prompt tuning. The study reveals insights into the suboptimal calibration of existing methods and demonstrates that orthogonalizing textual features effectively enhances textual dispersion. Extensive experiments show that O-TPT significantly reduces calibration error and outperforms state-of-the-art methods, particularly in fine-grained classification tasks.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Test-time Prompt Tuning",
            "Calibration",
            "Orthogonality Constraints",
            "Textual Feature Dispersion",
            "Fine-Grained Classification"
        ]
    },
    {
        "Title": "Test-Time Backdoor Detection for Object Detection Models",
        "Authors": "Hangtao Zhang \u00b7 Yichen Wang \u00b7 Shihui Yan \u00b7 Chenyu Zhu \u00b7 Ziqi Zhou \u00b7 Linshan Hou \u00b7 Shengshan Hu \u00b7 Minghui Li \u00b7 Yanjun Zhang \u00b7 Leo Yu Zhang",
        "Abstract": "Object detection models are vulnerable to backdoor attacks, where attackers poison a small subset of training samples by embedding a predefined trigger to manipulate prediction. Detecting poisoned samples (i.e., those containing triggers) at test time can prevent backdoor activation. However, unlike image classification tasks, the unique characteristics of object detection---particularly its output of numerous objects---pose fresh challenges for backdoor detection. The complex attack effects (e.g., \"ghost\" object emergence or \"vanishing\" object) further render current defenses fundamentally inadequate. To this end, we design TRAnsformation Consistency Evaluation (TRACE), a brand-new method for detecting poisoned samples at test time in object detection. Our journey begins with two intriguing observations: (1) poisoned samples exhibit significantly more consistent detection results than clean ones across varied backgrounds. (2) clean samples show higher detection consistency when introduced to different focal information. Based on these phenomena, TRACE applies foreground and background transformations to each test sample, then assesses transformation consistency by calculating the variance in objects confidences. TRACE achieves black-box, universal backdoor detection, with extensive experiments showing a 30% improvement in AUROC over state-of-the-art defenses and resistance to adaptive attacks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Object detection models are susceptible to backdoor attacks, where attackers embed triggers in training samples to manipulate predictions. Detecting these poisoned samples at test time is crucial to prevent backdoor activation. The unique challenges of object detection, such as the output of multiple objects and complex attack effects like 'ghost' or 'vanishing' objects, make existing defenses inadequate. To address this, the authors propose TRAnsformation Consistency Evaluation (TRACE), a novel method for detecting poisoned samples. TRACE leverages two key observations: poisoned samples show more consistent detection results across varied backgrounds, while clean samples exhibit higher consistency when exposed to different focal information. TRACE applies foreground and background transformations to test samples and evaluates consistency by calculating variance in object confidences. The method achieves black-box, universal backdoor detection, demonstrating a 30% improvement in AUROC over state-of-the-art defenses and robustness against adaptive attacks.",
        "Tags": [
            "Object Detection",
            "Backdoor Detection",
            "Transformation Consistency",
            "Black-Box Detection",
            "Adaptive Attack Resistance"
        ]
    },
    {
        "Title": "Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sampling",
        "Authors": "Junha Hyung \u00b7 Kinam Kim \u00b7 Susung Hong \u00b7 Min-Jung Kim \u00b7 Jaegul Choo",
        "Abstract": "Diffusion models have emerged as a powerful tool for generating high-quality images, videos, and 3D content. While sampling guidance techniques like CFG improve quality, they reduce diversity and motion. Autoguidance mitigates these issues but demands extra weak model training, limiting its practicality for large-scale models.In this work, we introduce Spatiotemporal Skip Guidance (STG), a simple training-free sampling guidance method for enhancing transformer-based video diffusion models.STG employs an implicit weak model via self-perturbation, avoiding the need for external models or additional training. By selectively skipping spatiotemporal layers, STG produces an aligned, degraded version of the original model to boost sample quality without compromising diversity or dynamic degree. Our contributions include: (1) introducing STG as an efficient, high-performing guidance technique for video diffusion models, (2) eliminating the need for auxiliary models by simulating a weak model through layer skipping, and (3) ensuring quality-enhanced guidance without compromising sample diversity or dynamics unlike CFG.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Spatiotemporal Skip Guidance (STG), a training-free sampling guidance method designed to enhance transformer-based video diffusion models. STG utilizes self-perturbation to simulate a weak model, eliminating the need for external models or additional training. By selectively skipping spatiotemporal layers, STG improves sample quality without sacrificing diversity or dynamic motion, addressing limitations of existing guidance techniques like CFG. Key contributions include the introduction of STG, the elimination of auxiliary models, and the preservation of sample diversity and dynamics.",
        "Tags": [
            "Diffusion Models",
            "Video Generation",
            "Training-Free Guidance",
            "Self-Perturbation",
            "Layer Skipping"
        ]
    },
    {
        "Title": "What\u2019s in the Image? A Deep-Dive into the Vision of Vision Language Models",
        "Authors": "Omri Kaduri \u00b7 Shai Bagon \u00b7 Tali Dekel",
        "Abstract": "Vision-Language Models (VLMs) have recently demonstrated remarkable capabilities in comprehending complex visual content. However, the mechanisms underlying how VLMs process visual information remain largely unexplored. In this paper,  we conduct a thorough empirical analysis, focusing on the attention modules across layers, by which we reveal several key insights about how these models process visual data: (i) the internal representation of the query tokens (e.g., representations of \"describe the image\"), is utilized by the model to store global image information; we demonstrate that the model generates surprisingly descriptive responses solely from these tokens, without direct access to image tokens.  (ii) Cross-modal information flow is predominantly influenced by the middle layers (approximately 25% of all layers), while early and late layers contribute only marginally. (iii) Fine-grained visual attributes and object details are directly extracted from image tokens in a spatially localized manner, i.e., the generated tokens associated with a specific object or attribute attend strongly to their corresponding regions in the image.  We propose novel quantitative evaluation to validate our observations, leveraging real-world complex visual scenes. Finally, we demonstrate the potential of our findings in facilitating efficient visual processing in state-of-the-art VLMs.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper investigates the mechanisms by which Vision-Language Models (VLMs) process visual information, focusing on the attention modules across layers. Key findings include: (i) query tokens store global image information, enabling descriptive responses without direct access to image tokens; (ii) cross-modal information flow is primarily influenced by middle layers, with early and late layers contributing minimally; (iii) fine-grained visual attributes and object details are extracted from image tokens in a spatially localized manner. The study proposes novel quantitative evaluations using real-world complex visual scenes and highlights the potential of these insights for improving visual processing in VLMs.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Self-Supervised Learning",
            "Attention Mechanisms",
            "Cross-Modal Learning",
            "Visual Information Processing"
        ]
    },
    {
        "Title": "LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware Omni-Modal Perception of Long Videos",
        "Authors": "Tiantian Geng \u00b7 Jinrui Zhang \u00b7 Qingni Wang \u00b7 Teng Wang \u00b7 Jinming Duan \u00b7 Feng Zheng",
        "Abstract": "Despite impressive advancements in video understanding, most efforts remain limited to coarse-grained or visual-only video tasks. However, real-world videos encompass omni-modal information (vision, audio, and speech) with a series of events forming a cohesive storyline. The lack of multi-modal video data with fine-grained event annotations and the high cost of manual labeling are major obstacles to comprehensive omni-modality video perception. To address this gap, we propose an automatic pipeline consisting of high-quality multi-modal video filtering, semantically coherent omni-modal event boundary detection, and cross-modal correlation-aware event captioning. In this way, we present LongVALE, the first-ever Vision-Audio-Language Event understanding benchmark comprising 105K omni-modal events with precise temporal boundaries and detailed relation-aware captions within 8.4K high-quality long videos. Further, we build a baseline that leverages LongVALE to enable video large language models (LLMs) for omni-modality fine-grained temporal video understanding for the first time. Extensive experiments demonstrate the effectiveness and great potential of LongVALE in advancing comprehensive multi-modal video understanding.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces LongVALE, a novel benchmark for omni-modal video understanding that integrates vision, audio, and language with fine-grained event annotations. Addressing the limitations of existing video understanding tasks, which are often coarse-grained or visual-only, LongVALE provides a dataset of 105K omni-modal events within 8.4K long videos, each annotated with precise temporal boundaries and detailed captions. The authors also propose an automatic pipeline for generating this dataset, which includes multi-modal video filtering, event boundary detection, and cross-modal event captioning. Additionally, a baseline model leveraging LongVALE is developed to facilitate fine-grained temporal video understanding using video large language models (LLMs). The benchmark demonstrates significant potential in advancing comprehensive multi-modal video understanding.",
        "Tags": [
            "Video Understanding",
            "Multimodal Learning",
            "Event Boundary Detection",
            "Cross-Modal Correlation",
            "Temporal Video Understanding"
        ]
    },
    {
        "Title": "OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts",
        "Authors": "Yuxuan Wang \u00b7 Yueqian Wang \u00b7 Bo Chen \u00b7 Tong Wu \u00b7 Dongyan Zhao \u00b7 Zilong Zheng",
        "Abstract": "The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has propelled the development of Omni language models, designed to process and proactively respond to continuous streams of multi-modal data. Despite their potential, evaluating their real-world interactive capabilities in streaming video contexts remains a formidable challenge. In this work, we introduce OmniMMI, a comprehensive multi-modal interaction benchmark tailored for OmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 real-world interactive videos and 2,290 questions, addressing two critical yet underexplored challenges in existing video benchmarks: streaming video understanding and proactive reasoning, across six distinct subtasks.  Moreover, we propose a novel framework, Multi-modal Multiplexing Modeling (M4), designed to enhance real-time interactive reasoning with minimum finetuning on pre-trained MLLMs. Extensive experimental results reveal that the existing MLLMs fall short in interactive streaming understanding, particularly struggling with proactive tasks and multi-turn queries. Our proposed M4, though lightweight, demonstrates a significant improvement in handling proactive tasks and real-time interactions.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces OmniMMI, a benchmark designed to evaluate the interactive capabilities of multi-modal language models (MLLMs) in streaming video contexts. It includes over 1,121 videos and 2,290 questions, focusing on streaming video understanding and proactive reasoning across six subtasks. A novel framework, Multi-modal Multiplexing Modeling (M4), is proposed to enhance real-time interactive reasoning with minimal finetuning on pre-trained MLLMs. Experimental results show that existing MLLMs struggle with proactive tasks and multi-turn queries, while M4 significantly improves performance in these areas.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Video Understanding",
            "Proactive Reasoning",
            "Real-time Interaction",
            "Streaming Video Contexts"
        ]
    },
    {
        "Title": "OSMamba: Omnidirectional Spectral Mamba with Dual-Domain Prior Generator for Exposure Correction",
        "Authors": "Gehui Li \u00b7 Bin Chen \u00b7 Chen Zhao \u00b7 Lei Zhang \u00b7 Jian Zhang",
        "Abstract": "Exposure correction is a fundamental problem in computer vision and image processing. Recently, frequency domain-based methods have achieved impressive improvement, yet they still struggle with complex real-world scenarios under extreme exposure conditions. This is due to the local convolutional receptive fields failing to model long-range dependencies in the spectrum, and the non-generative learning paradigm being inadequate for retrieving lost details from severely degraded regions. In this paper, we propose Omnidirectional Spectral Mamba (OSMamba), a novel exposure correction network that incorporates the advantages of state space models and generative diffusion models to address these limitations. Specifically, OSMamba introduces an omnidirectional spectral scanning mechanism that adapts Mamba to the frequency domain to capture comprehensive long-range dependencies in both the amplitude and phase spectra of deep image features, hence enhancing illumination correction and structure recovery. Furthermore, we develop a dual-domain prior generator that learns from well-exposed images to generate a degradation-free diffusion prior containing correct information about severely under- and over-exposed regions for better detail restoration. Extensive experiments on multiple-exposure and mixed-exposure datasets demonstrate that the proposed OSMamba achieves state-of-the-art performance both quantitatively and qualitatively.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Omnidirectional Spectral Mamba (OSMamba), a novel exposure correction network that combines state space models and generative diffusion models to address limitations in existing frequency domain-based methods. OSMamba employs an omnidirectional spectral scanning mechanism to capture long-range dependencies in both amplitude and phase spectra, improving illumination correction and structure recovery. Additionally, a dual-domain prior generator is developed to learn from well-exposed images and generate degradation-free diffusion priors for better detail restoration in severely under- and over-exposed regions. The proposed method demonstrates state-of-the-art performance on multiple-exposure and mixed-exposure datasets.",
        "Tags": [
            "Exposure Correction",
            "Mamba",
            "Diffusion Models",
            "Omnidirectional Spectral Scanning",
            "Dual-Domain Prior Generator",
            "Frequency Domain Processing"
        ]
    },
    {
        "Title": "DepthSplat: Connecting Gaussian Splatting and Depth",
        "Authors": "Haofei Xu \u00b7 Songyou Peng \u00b7 Fangjinhua Wang \u00b7 Hermann Blum \u00b7 Daniel Barath \u00b7 Andreas Geiger \u00b7 Marc Pollefeys",
        "Abstract": "Gaussian splatting and single/multi-view depth estimation are typically studied in isolation. In this paper, we present DepthSplat to connect Gaussian splatting and depth estimation and study their interactions. More specifically, we first contribute a robust multi-view depth model by leveraging pre-trained monocular depth features, leading to high-quality feed-forward 3D Gaussian splatting reconstructions. We also show that Gaussian splatting can serve as an unsupervised pre-training objective for learning powerful depth models from large-scale unlabeled datasets. We validate the synergy between Gaussian splatting and depth estimation through extensive ablation and cross-task transfer experiments. Our DepthSplat achieves state-of-the-art performance on ScanNet, RealEstate10K and DL3DV datasets in terms of both depth estimation and novel view synthesis, demonstrating the mutual benefits of connecting both tasks. We invite the readers to view our supplementary video for feed-forward reconstruction results of large-scale or 360 scenes from up to 12 input views at $512 \\times 960$ resolutions. Our code and models will be publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "DepthSplat bridges the gap between Gaussian splatting and depth estimation, exploring their synergistic relationship. The paper introduces a robust multi-view depth model that utilizes pre-trained monocular depth features to enhance 3D Gaussian splatting reconstructions. Additionally, it demonstrates that Gaussian splatting can act as an unsupervised pre-training objective for depth models, leveraging large-scale unlabeled datasets. The proposed DepthSplat achieves state-of-the-art results on ScanNet, RealEstate10K, and DL3DV datasets for both depth estimation and novel view synthesis, highlighting the mutual benefits of integrating these tasks.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Depth Estimation",
            "3D Reconstruction",
            "Multi-View Depth Estimation",
            "Unsupervised Pre-Training",
            "Novel View Synthesis"
        ]
    },
    {
        "Title": "Towards Efficient Foundation Model for Zero-shot Amodal Segmentation",
        "Authors": "Zhaochen Liu \u00b7 Limeng Qiao \u00b7 Xiangxiang Chu \u00b7 Lin Ma \u00b7 Tingting Jiang",
        "Abstract": "Aiming to predict the complete shape of partially occluded objects, amodal segmentation is an important capacity towards visual intelligence. In order to promote the practicability, zero-shot foundation model competent for the open world gains growing attention in this field. Nevertheless, prior models exhibit deficiencies in efficiency and stability. To address this problem, utilizing the implicit prior knowledge, we propose the first SAM-based amodal segmentation foundation model, SAMBA. Methodologically, a novel framework with multilevel facilitation is designed to better adapt the task characteristics and unleash the potential capabilities of SAM. In the modality level, a separation-to-fusion structure is employed that jointly learns modal and amodal segmentation to enhance mutual coordination. In the instance level, to ease the complexity of amodal feature extraction, we introduce a principal focusing mechanism to indicate objects of interest. In the pixel level, mixture-of-experts is incorporated with a specialized distribution loss, by which distinct occlusion rates correspond to different experts to improve the accuracy. Experiments are conducted on several eminent datasets, and the results show that the performance of SAMBA is superior to existing zero-shot and even supervised approaches. Furthermore, our proposed model has notable advantages in terms of speed and size. The model and code will be made publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces SAMBA, the first SAM-based foundation model for zero-shot amodal segmentation, which predicts the complete shape of partially occluded objects. SAMBA employs a novel multilevel facilitation framework to enhance task adaptation and leverage SAM's capabilities. The framework includes a separation-to-fusion structure at the modality level for joint modal and amodal segmentation learning, a principal focusing mechanism at the instance level to simplify amodal feature extraction, and a mixture-of-experts approach at the pixel level with a specialized distribution loss to handle varying occlusion rates. SAMBA outperforms existing zero-shot and supervised approaches in performance, speed, and model size.",
        "Tags": [
            "Zero-Shot Learning",
            "Semantic Segmentation",
            "Amodal Segmentation",
            "Foundation Model",
            "Mixture-of-Experts"
        ]
    },
    {
        "Title": "PartRM: Modeling Part-Level Dynamics with Large 4D Reconstruction Model",
        "Authors": "Mingju Gao \u00b7 Yike Pan \u00b7 Huan-ang Gao \u00b7 Zongzheng Zhang \u00b7 Wenyi Li \u00b7 Hao Dong \u00b7 Hao Tang \u00b7 Li Yi \u00b7 Hao Zhao",
        "Abstract": "As interest grows in world models that predict future states from current observations and actions, accurately modeling part-level dynamics has become increasingly relevant for various applications. Existing approaches, such as Puppet-Master, rely on fine-tuning large-scale pre-trained video diffusion models, which are impractical for real-world use due to the limitations of 2D video representation and slow processing times. To overcome these challenges, we present PartRM, a novel 4D reconstruction framework that simultaneously models appearance, geometry, and part-level motion from multi-view images of a static object. PartRM builds upon large 3D Gaussian reconstruction models, leveraging their extensive knowledge of appearance and geometry in static objects. To address data scarcity in 4D, we introduce the PartDrag-4D dataset, providing multi-view observations of part-level dynamics across over 20,000 states. We enhance the model\u2019s understanding of interaction conditions with a multi-scale drag embedding module that captures dynamics at varying granularities. To prevent catastrophic forgetting during fine-tuning, we implement a two-stage training process that focuses sequentially on motion and appearance learning. Experimental results show that PartRM establishes a new state-of-the-art in part-level motion learning and can be applied in manipulation tasks in robotics. Our code, data, and models will be made publicly available to facilitate future research.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "PartRM introduces a novel 4D reconstruction framework that models appearance, geometry, and part-level motion from multi-view images of static objects, addressing the limitations of existing approaches like Puppet-Master. It leverages large 3D Gaussian reconstruction models and introduces the PartDrag-4D dataset to overcome data scarcity in 4D. The framework includes a multi-scale drag embedding module for understanding interaction conditions and employs a two-stage training process to prevent catastrophic forgetting. PartRM achieves state-of-the-art performance in part-level motion learning and is applicable in robotics manipulation tasks.",
        "Tags": [
            "3D Reconstruction",
            "4D Generation",
            "3DGS (Gaussian Splatting)",
            "Part-Level Dynamics",
            "Multi-Scale Drag Embedding",
            "Two-Stage Training"
        ]
    },
    {
        "Title": "A Hubness Perspective on Representation Learning for Graph-Based Multi-View Clustering",
        "Authors": "Zheming Xu \u00b7 He Liu \u00b7 Congyan Lang \u00b7 Tao Wang \u00b7 Yidong Li \u00b7 Michael C. Kampffmeyer",
        "Abstract": "Recent graph-based multi-view clustering (GMVC) methods typically encode view features into high-dimensional spaces and construct graphs based on distance similarity. However, the high dimensionality of the embeddings often leads to the hubness problem, where a few points repeatedly appear in the nearest neighbor lists of other points. We show that this negatively impacts the extracted graph structures and message passing, thus degrading clustering performance. To the best of our knowledge, we are the first to highlight the detrimental effect of hubness in GMVC methods and introduce the hubREP (hub-aware Representation Embedding and Pairing) framework. Specifically, we propose a simple yet effective encoder that reduces hubness while preserving neighborhood topology within each view. Additionally, we propose a hub-aware pairing module to maintain structure consistency across views, efficiently enhancing the view-specific representations. The proposed hubREP is lightweight compared to the conventional autoencoders used in state-of-the-art GMVC methods and can be integrated into existing GMVC methods that mostly focus on novel fusion mechanisms, further boosting their performance. Comprehensive experiments performed on eight benchmarks confirm the superiority of our method. Code is included in the supplementary material.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the hubness problem in graph-based multi-view clustering (GMVC), where high-dimensional embeddings lead to a few points dominating nearest neighbor lists, negatively affecting graph structures and clustering performance. The authors introduce the hubREP framework, which includes a hubness-reducing encoder and a hub-aware pairing module to enhance view-specific representations while maintaining neighborhood topology and cross-view consistency. The proposed method is lightweight, outperforms existing GMVC methods, and can be integrated into current approaches to improve their performance.",
        "Tags": [
            "Graph Neural Networks (GNNs)",
            "Multimodal Learning",
            "Hubness Problem",
            "Representation Learning",
            "Multi-View Clustering"
        ]
    },
    {
        "Title": "SAIST: Segment Any Infrared Small Target Model Guided by Contrastive Language-Image Pretraining",
        "Authors": "Mingjin Zhang \u00b7 Xiaolong Li \u00b7 Fei Gao \u00b7 Jie Guo \u00b7 Xinbo Gao \u00b7 Jing Zhang",
        "Abstract": "Infrared Small Target Detection (IRSTD) aims to identify low signal-to-noise ratio small targets in infrared images with complex backgrounds, which is crucial for various applications. However, existing IRSTD methods typically rely solely on image modalities for processing, which fail to fully capture contextual information, leading to limited detection accuracy and adaptability in complex environments. Inspired by vision-language models, this paper proposes a novel framework, SAIST, which integrates textual information with image modalities to enhance IRSTD performance. The framework consists of two main components: Scene Recognition Contrastive Language-Image Pretraining (SR-CLIP) and CLIP-guided Segment Anything Model (CG-SAM).  SR-CLIP generates a set of visual descriptions through object-object similarity and object-scene relevance, embedding them into learnable prompts to refine the textual description set. This reduces the domain gap between vision and language, generating precise textual and visual prompts. CG-SAM utilizes the prompts generated by SR-CLIP to accurately guide the Mask Decoder in learning prior knowledge of background features, while incorporating infrared imaging equations to improve small target recognition in complex backgrounds and significantly reduce the false alarm rate. Additionally, this paper introduces the first multimodal IRSTD dataset, MIRSTD, which contains abundant image-text pairs. Experimental results demonstrate that the proposed SAIST method outperforms existing state-of-the-art approaches. The dataset and code will be made publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces SAIST, a novel framework for Infrared Small Target Detection (IRSTD) that integrates textual information with image modalities to improve detection accuracy in complex environments. The framework comprises two components: Scene Recognition Contrastive Language-Image Pretraining (SR-CLIP) and CLIP-guided Segment Anything Model (CG-SAM). SR-CLIP generates visual descriptions to refine textual prompts, bridging the gap between vision and language. CG-SAM uses these prompts to guide the Mask Decoder, incorporating infrared imaging equations to enhance target recognition and reduce false alarms. The paper also presents the first multimodal IRSTD dataset, MIRSTD, containing image-text pairs. SAIST demonstrates superior performance over existing methods.",
        "Tags": [
            "Infrared Small Target Detection",
            "Vision-Language Models (VLMs)",
            "Multimodal Learning",
            "Contrastive Language-Image Pretraining (CLIP)",
            "Infrared Imaging Equations"
        ]
    },
    {
        "Title": "ReCap: Better Gaussian Relighting with Cross-Environment Captures",
        "Authors": "Jingzhi Li \u00b7 Zongwei Wu \u00b7 Eduard Zamfir \u00b7 Radu Timofte",
        "Abstract": "Accurate 3D objects relighting in diverse unseen environments is crucial for realistic virtual object placement. Due to the albedo-lighting ambiguity, existing methods often fall short in producing faithful relights. Without proper constraints, observed training views can be explained by numerous combinations of lighting and material attributes, lacking physical correspondence with the actual environment maps used for relighting. In this work, we present ReCap, treating cross-environment captures as multi-task target to provide the missing supervision that cuts through the entanglement. Specifically, ReCap jointly optimizes multiple lighting representations that share a common set of material attributes. This naturally harmonizes a coherent set of lighting representations around the mutual material attributes, exploiting commonalities and differences across varied object appearances. Such coherence enables physically sound lighting reconstruction and robust material estimation \u2014 both essential for accurate relighting. Together with a streamlined shading function and effective post-processing, ReCap outperforms the leading competitor by $3.4$ dB in PSNR on an expanded relighting benchmark.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "ReCap introduces a novel approach to 3D object relighting by leveraging cross-environment captures as a multi-task target to address the albedo-lighting ambiguity. By jointly optimizing multiple lighting representations that share common material attributes, ReCap achieves physically sound lighting reconstruction and robust material estimation. This method, combined with a streamlined shading function and effective post-processing, significantly outperforms existing methods in terms of PSNR on an expanded relighting benchmark.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "NeRF (Neural Radiance Fields)",
            "3D Reconstruction",
            "Material Estimation",
            "Lighting Reconstruction",
            "Cross-Environment Captures"
        ]
    },
    {
        "Title": "Ferret: An Efficient Online Continual Learning Framework under Varying Memory Constraints",
        "Authors": "Yuhao Zhou \u00b7 Yuxin Tian \u00b7 Jindi Lv \u00b7 Mingjia Shi \u00b7 Yuanxi Li \u00b7 Qing Ye \u00b7 Shuhao Zhang \u00b7 Jiancheng Lv",
        "Abstract": "In the realm of high-frequency data streams, achieving real-time learning within varying memory constraints is paramount. This paper presents Ferret, a comprehensive framework designed to enhance \\emph{online accuracy} of Online Continual Learning (OCL) algorithms while dynamically adapting to varying memory budgets.Ferret employs a fine-grained pipeline parallelism strategy combined with an iterative gradient compensation algorithm, ensuring seamless handling of high-frequency data with minimal latency, and effectively counteracting the challenge of stale gradients in parallel training. To adapt to varying memory budgets, its automated model partitioning and pipeline planning optimizes performance regardless of memory limitations. Extensive experiments across 20 benchmarks and 5 integrated OCL algorithms show Ferret\u2019s remarkable efficiency, achieving up to 3.7$\\times$ lower memory overhead to reach the same online accuracy compared to competing methods.Furthermore, Ferret consistently outperforms these methods across diverse memory budgets, underscoring its superior adaptability. These findings position Ferret as a premier solution for efficient and adaptive OCL framework in real-time environments.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Ferret, a framework designed to improve the online accuracy of Online Continual Learning (OCL) algorithms while dynamically adapting to varying memory constraints. Ferret utilizes a fine-grained pipeline parallelism strategy and an iterative gradient compensation algorithm to handle high-frequency data streams efficiently, minimizing latency and addressing stale gradients in parallel training. The framework's automated model partitioning and pipeline planning optimize performance under different memory budgets. Ferret demonstrates significant efficiency, achieving up to 3.7\u00d7 lower memory overhead for the same online accuracy compared to other methods, and consistently outperforms them across various memory constraints.",
        "Tags": [
            "Online Continual Learning (OCL)",
            "Memory Constraints",
            "Pipeline Parallelism",
            "Gradient Compensation",
            "Model Partitioning"
        ]
    },
    {
        "Title": "SketchFusion: Learning Universal Sketch Features through Fusing Foundation Models",
        "Authors": "Subhadeep Koley \u00b7 Tapas Kumar Dutta \u00b7 Aneeshan Sain \u00b7 Pinaki Nath Chowdhury \u00b7 Ayan Kumar Bhunia \u00b7 Yi-Zhe Song",
        "Abstract": "While foundation models have revolutionised computer vision, their effectiveness for sketch understanding remains limited by the unique challenges of abstract, sparse visual inputs. Through systematic analysis, we uncover two fundamental limitations: Stable Diffusion (SD) struggles to extract meaningful features from abstract sketches (unlike its success with photos), and exhibits a pronounced frequency-domain bias that suppresses essential low-frequency components needed for sketch understanding. Rather than costly retraining, we address these limitations by strategically combining SD with CLIP, whose strong semantic understanding naturally compensates for SD's spatial-frequency biases. By dynamically injecting CLIP features into SD's denoising process and adaptively aggregating features across semantic levels, our method achieves state-of-the-art performance in sketch retrieval (+3.35\\%), recognition (+1.06\\%), segmentation (+29.42\\%), and correspondence learning (+21.22\\%), demonstrating the first truly universal sketch feature representation in the era of foundation models.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the limitations of foundation models, particularly Stable Diffusion (SD), in understanding abstract and sparse sketch inputs. It identifies two key issues: SD's difficulty in extracting meaningful features from sketches and its frequency-domain bias that suppresses low-frequency components crucial for sketch understanding. To overcome these challenges, the authors propose a method that strategically combines SD with CLIP, leveraging CLIP's semantic understanding to compensate for SD's biases. This approach involves dynamically injecting CLIP features into SD's denoising process and adaptively aggregating features across semantic levels. The resulting method achieves state-of-the-art performance in sketch retrieval, recognition, segmentation, and correspondence learning, establishing the first universal sketch feature representation in the era of foundation models.",
        "Tags": [
            "Sketch Understanding",
            "Foundation Models",
            "CLIP",
            "Stable Diffusion",
            "Feature Fusion",
            "Dynamic Feature Injection",
            "Adaptive Feature Aggregation",
            "Universal Sketch Representation"
        ]
    },
    {
        "Title": "Evaluating Vision-Language Models as Evaluators in Path Planning",
        "Authors": "Mohamed Aghzal \u00b7 Xiang Yue \u00b7 Erion Plaku \u00b7 Ziyu Yao",
        "Abstract": "Despite their promise to perform complex reasoning, large language models (LLMs) have been shown to have limited effectiveness in end-to-end planning. This has inspired an intriguing question: if these models cannot plan well, can they still contribute to the planning framework as a helpful plan evaluator? In this work, we generalize this question to consider LLMs augmented with visual understanding, i.e., Vision-Language Models (VLMs). We introduce PathEval, a novel benchmark evaluating VLMs as plan evaluators in complex path-planning scenarios. Succeeding in the benchmark requires a VLM to be able to abstract traits of optimal paths from the scenario description, demonstrate precise low-level perception on each path, and integrate this information to decide the better path.  Our analysis of state-of-the-art VLMs reveals that these models face significant challenges on the benchmark. We observe that the VLMs can precisely abstract given scenarios to identify the desired traits and exhibit mixed performance in integrating the provided information. Yet, their vision component presents a critical bottleneck, with models struggling to perceive low-level details about a path. Our experimental results show that this issue cannot be trivially addressed via end-to-end fine-tuning; rather, task-specific discriminative adaptation of these vision encoders is needed for these VLMs to become effective path evaluators.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper investigates the potential of Vision-Language Models (VLMs) as evaluators in path-planning scenarios, despite their limitations in end-to-end planning. The authors introduce PathEval, a benchmark designed to assess VLMs' ability to abstract optimal path traits, demonstrate precise low-level perception, and integrate information to evaluate paths effectively. Analysis of state-of-the-art VLMs reveals significant challenges, particularly in low-level visual perception, which hinders their effectiveness as path evaluators. The study concludes that task-specific adaptation of vision encoders is necessary for VLMs to perform well in this role, as end-to-end fine-tuning is insufficient.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Path Planning",
            "Plan Evaluation",
            "Low-Level Perception",
            "Discriminative Adaptation"
        ]
    },
    {
        "Title": "Unveiling the Ignorance of MLLMs: Seeing Clearly, Answering Incorrectly",
        "Authors": "Yexin Liu \u00b7 Zhengyang Liang \u00b7 Yueze Wang \u00b7 Xianfeng Wu \u00b7 feilong tang \u00b7 Muyang He \u00b7 Jian Li \u00b7 Zheng Liu \u00b7 Harry Yang \u00b7 Ser-Nam Lim \u00b7 Bo Zhao",
        "Abstract": "Multimodal Large Language Models (MLLMs) have displayed remarkable performance in multimodal tasks, particularly in visual comprehension. However, we reveal that MLLMs often generate incorrect answers even when they understand the visual content. To this end, we manually construct a benchmark with 12 categories and design evaluation metrics that assess the degree of error in MLLM responses even when the visual content is seemingly understood. Based on this benchmark, we test 15 leading MLLMs and analyze the distribution of attention maps and logits of some MLLMs. Our investigation identifies two primary issues: 1) most instruction tuning datasets predominantly feature questions that ``directly\" relate to the visual content, leading to a bias in MLLMs' responses to other indirect questions, and 2) MLLMs\u2019 attention to visual tokens is notably lower than to system and question tokens. We further observe that attention scores between questions and visual tokens as well as the model's confidence in the answers are lower in response to misleading questions than to straightforward ones. To address the first challenge, we introduce a paired positive and negative data construction pipeline to diversify the dataset. For the second challenge, we propose to enhance the model's focus on visual content during decoding by refining the text and visual prompt. For the text prompt, we propose a content-guided refinement strategy that performs preliminary visual content analysis to generate structured information before answering the question. Additionally, we employ a visual attention refinement strategy that highlights question-relevant visual tokens to increase the model\u2019s attention to visual content that aligns with the question. Extensive experiments demonstrate that these challenges can be significantly mitigated with our proposed dataset and techniques. The benchmark, training set, and code will be available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This study investigates the limitations of Multimodal Large Language Models (MLLMs) in generating accurate responses despite understanding visual content. A manually constructed benchmark with 12 categories and specific evaluation metrics was developed to assess MLLM errors. Testing 15 leading MLLMs revealed two main issues: a bias towards direct questions due to the nature of instruction tuning datasets, and insufficient attention to visual tokens compared to system and question tokens. To address these, the authors introduced a data construction pipeline to diversify the dataset and proposed strategies to enhance the model's focus on visual content during decoding. These strategies include content-guided refinement of text prompts and visual attention refinement to highlight relevant visual tokens. The findings suggest that these approaches can significantly mitigate the identified challenges.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Visual Comprehension",
            "Attention Mechanism",
            "Instruction Tuning",
            "Visual Token Enhancement"
        ]
    },
    {
        "Title": "OW-OVD: Unified Open World and Open Vocabulary Object Detection",
        "Authors": "Xing Xi \u00b7 Yangyang Huang \u00b7 Ronghua Luo \u00b7 YuQiu",
        "Abstract": "Open world perception expands traditional closed-set frameworks, which assume a predefined set of known categories, to encompass dynamic real-world environments. Open World Object Detection (OWOD) and Open Vocabulary Object Detection (OVD) are two main research directions, each addressing unique challenges in dynamic environments. However, existing studies often focus on only one of these tasks, leaving the combined challenges of OWOD and OVD largely underexplored. In this paper, we propose a novel detector, OW-OVD, which inherits the zero-shot generalization capability of OVD detectors while incorporating the ability to actively detect unknown objects and progressively optimize performance through incremental learning, as seen in OWOD detectors. To achieve this, we start with a standard OVD detector and adapt it for OWOD tasks. For attribute selection, we propose the Visual Similarity Attribute Selection (VSAS) method, which identifies the most generalizable attributes by computing similarity distributions across annotated and unannotated regions. Additionally, to ensure the diversity of attributes, we incorporate a similarity constraint in the iterative process. Finally, to preserve the standard inference process of OVD, we propose the Hybrid Attribute-Uncertainty Fusion (HAUF) method. This method combines attribute similarity with known class uncertainty to infer the likelihood of an object belonging to an unknown class. We validated the effectiveness of OW-OVD through evaluations on two OWOD benchmarks, M-OWODB and S-OWODB. The results demonstrate that OW-OVD outperforms existing state-of-the-art models, achieving a +15.3 improvement in unknown object recall (U-Recall) and a +15.5 increase in unknown class average precision (U-mAP). Our code is available at: https://anonymous.4open.science/r/OW_OVD-E336.",
        "Link": "https://anonymous.4open.science/r/OW_OVD-E336",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces OW-OVD, a novel detector that unifies Open World Object Detection (OWOD) and Open Vocabulary Object Detection (OVD) to address the challenges of detecting unknown objects and generalizing to unseen categories in dynamic environments. OW-OVD combines the zero-shot generalization of OVD with the incremental learning capabilities of OWOD. The proposed Visual Similarity Attribute Selection (VSAS) method identifies generalizable attributes by analyzing similarity distributions across annotated and unannotated regions, while the Hybrid Attribute-Uncertainty Fusion (HAUF) method integrates attribute similarity and class uncertainty to infer unknown objects. Evaluations on M-OWODB and S-OWODB benchmarks show significant improvements in unknown object recall and class average precision, demonstrating OW-OVD's superior performance over existing models.",
        "Tags": [
            "Object Detection",
            "Open World Object Detection (OWOD)",
            "Open Vocabulary Object Detection (OVD)",
            "Zero-Shot Generalization",
            "Incremental Learning",
            "Hybrid Attribute-Uncertainty Fusion"
        ]
    },
    {
        "Title": "VidComposition: Can MLLMs Analyze Compositions in Compiled Video?",
        "Authors": "Yunlong Tang \u00b7 JunJia Guo \u00b7 Hang Hua \u00b7 Susan Liang \u00b7 Mingqian Feng \u00b7 Xinyang Li \u00b7 Rui Mao \u00b7 Chao Huang \u00b7 Jing Bi \u00b7 Zeliang Zhang \u00b7 Pooyan Fazli \u00b7 Chenliang Xu",
        "Abstract": "The advancement of Multimodal Large Language Models (MLLMs) has enabled significant progress in multimodal understanding, expanding their capacity to analyze video content.However, existing evaluation benchmarks for MLLMs primarily focus on abstract video comprehension, lacking a detailed assessment of their ability to understand video compositions, the nuanced interpretation of how visual elements combine and interact within highly compiled video contexts.We introduce VidComposition, a new benchmark specifically designed to evaluate the video composition understanding capabilities of MLLMs using carefully curated compiled videos and cinematic-level annotations.VidComposition includes 982 videos with 1706 multiple-choice questions, covering various compositional aspects such as camera movement, angle, shot size, narrative structure, character actions and emotions, etc.Our comprehensive evaluation of 33 open-source and proprietary MLLMs reveals a significant performance gap between human and model capabilities. This highlights the limitations of current MLLMs in understanding complex, compiled video compositions and offers insights into areas for further improvement.Our benchmark will be publicly available for evaluating more models.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces VidComposition, a benchmark designed to evaluate the video composition understanding capabilities of Multimodal Large Language Models (MLLMs). VidComposition includes 982 videos with 1706 multiple-choice questions, covering various compositional aspects such as camera movement, angle, shot size, narrative structure, character actions, and emotions. The evaluation of 33 MLLMs reveals a significant performance gap between human and model capabilities, highlighting the limitations of current MLLMs in understanding complex, compiled video compositions. The benchmark aims to provide insights into areas for further improvement and will be publicly available for evaluating more models.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Video Understanding",
            "Video Composition Analysis",
            "Cinematic-Level Annotations",
            "Multiple-Choice Questions"
        ]
    },
    {
        "Title": "Generative Sparse-View Gaussian Splatting",
        "Authors": "Hanyang Kong \u00b7 Xingyi Yang \u00b7 Xinchao Wang",
        "Abstract": "Novel view synthesis from limited observations remains a significant challenge due to the lack of information in under-sampled regions, often resulting in noticeable artifacts. We introduce Generative Sparse-view Gaussian Splatting (GS-GS), a general pipeline designed to enhance the rendering quality of 3D/4D Gaussian Splatting (GS) when training views are sparse. Our method generates unseen views using generative models, specifically leveraging pre-trained image diffusion models to iteratively refine view consistency and hallucinate additional images at pseudo views. This approach improves 3D/4D scene reconstruction by explicitly enforcing semantic correspondences during the generation of unseen views, thereby enhancing geometric consistency\u2014unlike purely generative methods that often fail to maintain view consistency. Extensive evaluations on various 3D/4D datasets\u2014including Blender, LLFF, Mip-NeRF360, and Neural 3D Video\u2014demonstrate that our GS-GS outperforms existing state-of-the-art methods in rendering quality without sacrificing efficiency.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Generative Sparse-view Gaussian Splatting (GS-GS), a pipeline aimed at improving the rendering quality of 3D/4D Gaussian Splatting (GS) under sparse training views. By leveraging pre-trained image diffusion models, GS-GS generates and refines unseen views, ensuring semantic and geometric consistency. This approach outperforms existing methods in rendering quality across multiple 3D/4D datasets, maintaining efficiency.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Novel View Synthesis",
            "Image Diffusion Models",
            "View Consistency",
            "Semantic Correspondences"
        ]
    },
    {
        "Title": "Unified Uncertainty-Aware Diffusion for Multi-Agent Trajectory Modeling",
        "Authors": "Guillem Font Font \u00b7 Antonio Rubio \u00b7 Luis Ferraz \u00b7 Antonio Agudo",
        "Abstract": "Multi-agent trajectory modeling has primarily focused on forecasting future states, often overlooking broader tasks like trajectory completion, which are crucial for real-world applications such as correct tracking data. Existing methods also generally predict agents' states without offering any state-wise measure of uncertainty. Moreover, popular multi-modal sampling methods lack any error probability estimates for each generated scene under the same prior observations, making it difficult to rank the predictions during inference time. We introduce U2Diff, a unified diffusion model designed to handle trajectory completion while providing state-wise uncertainty estimates jointly. This uncertainty estimation is achieved by augmenting the simple denoising loss with the negative log-likelihood of the predicted noise and propagating latent space uncertainty to the real state space. Additionally, we incorporate a Rank Neural Network in post-processing to enable error probability estimation for each generated mode, demonstrating a strong correlation with the error relative to ground truth. Our method outperforms the state-of-the-art solutions in trajectory completion and forecasting across four challenging sports datasets (NBA, Basketball-U, Football-U, Soccer-U), highlighting the effectiveness of uncertainty and error probability estimation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces U2Diff, a unified diffusion model for multi-agent trajectory modeling that addresses trajectory completion and forecasting while providing state-wise uncertainty estimates. Unlike existing methods, U2Diff augments the denoising loss with the negative log-likelihood of predicted noise and propagates latent space uncertainty to the real state space. It also incorporates a Rank Neural Network for error probability estimation of each generated mode, which correlates strongly with ground truth errors. The method demonstrates superior performance in trajectory completion and forecasting across four sports datasets (NBA, Basketball-U, Football-U, Soccer-U), emphasizing the importance of uncertainty and error probability estimation.",
        "Tags": [
            "Diffusion Models",
            "Trajectory Modeling",
            "Uncertainty Estimation",
            "Rank Neural Network",
            "Trajectory Completion"
        ]
    },
    {
        "Title": "Shading Meets Motion: Self-supervised Indoor 3D Reconstruction Via Simultaneous Shape-from-Shading and Structure-from-Motion",
        "Authors": "Guoyu Lu",
        "Abstract": "Scene reconstruction has a wide range of applications in computer vision and robotics. To build practical constraints and feature correspondences, rich textures and distinguished gradient variations are particularly required in classic and learning-based SfM. When building low-texture regions with repeated patterns, especially mostly white indoor rooms, there is a significant drop in the performance. Inthis work, we propose Shading-SfM-Net (Shading & structure-from motion network), a novel framework for simultaneously learninga shape-from-shading network based on the inverse rendering constraint and a structure-from-motion framework based on warped keypoint and geometric consistency, to improve structure-from-motion and surface reconstruction for low-texture indoor scenes. Shading-SfM-Net tightly incorporates the surface shape consistency and 3D geometric registration loss in order to dig into their mutual informationand further overcome the instability on flat regions. We evaluate the proposed framework on texture-less indoor scenes (NYU v2and ScanNet), and show that for each individual network without simultaneous training, our method is able to achieve comparableresult to the state of the art methods. By simultaneously learning shape and motion from the two networks, our pipeline is able toachieve state-of-the-art performance with superior generalization capability for unseen texture-less datasets.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Shading-SfM-Net, a novel framework for improving 3D reconstruction in low-texture indoor scenes by simultaneously learning shape-from-shading and structure-from-motion. The framework integrates surface shape consistency and 3D geometric registration loss to enhance performance on flat and texture-less regions. Evaluations on texture-less indoor datasets (NYU v2 and ScanNet) demonstrate that the proposed method achieves state-of-the-art performance and superior generalization capabilities for unseen datasets.",
        "Tags": [
            "3D Reconstruction",
            "Self-Supervised Learning",
            "Shape-from-Shading",
            "Structure-from-Motion",
            "Low-Texture Scenes"
        ]
    },
    {
        "Title": "MatAnyone: Stable Video Matting with Consistent Memory Propagation",
        "Authors": "Peiqing Yang \u00b7 Shangchen Zhou \u00b7 Jixin Zhao \u00b7 Qingyi Tao \u00b7 Chen Change Loy",
        "Abstract": "Auxiliary-free human video matting methods, which rely solely on input frames, often struggle with complex or ambiguous backgrounds. To tackle this, we propose MatAnyone, a practical framework designed for target-assigned video matting. Specifically, building on a memory-based framework, we introduce a consistent memory propagation module via region-adaptive memory fusion, which adaptively combines memory from the previous frame. This ensures stable semantic consistency in core regions while maintaining fine details along object boundaries. For robust training, we present a larger, high-quality, and diverse dataset for video matting. Additionally, we incorporate a novel training strategy that efficiently leverages large-scale segmentation data, further improving matting stability. With this new network design, dataset, and training strategy, MatAnyone delivers robust, accurate video matting in diverse real-world scenarios, outperforming existing methods. The code and model will be publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "MatAnyone is a novel framework for target-assigned video matting that addresses challenges posed by complex or ambiguous backgrounds in auxiliary-free methods. It introduces a consistent memory propagation module using region-adaptive memory fusion, ensuring semantic consistency in core regions and preserving fine details along object boundaries. The framework is supported by a larger, high-quality dataset and a novel training strategy that leverages large-scale segmentation data, enhancing matting stability. MatAnyone demonstrates superior performance in diverse real-world scenarios compared to existing methods.",
        "Tags": [
            "Video Matting",
            "Self-Supervised Learning",
            "Memory Propagation",
            "Region-Adaptive Fusion",
            "Video Matting Dataset"
        ]
    },
    {
        "Title": "DeCafNet: Delegate and Conquer for Efficient Temporal Grounding in Long Videos",
        "Authors": "Zijia Lu \u00b7 ASM Iftekhar \u00b7 Gaurav Mittal \u00b7 Tianjian Meng \u00b7 Xiawei Wang \u00b7 Cheng Zhao \u00b7 Rohith Kukkala \u00b7 Ehsan Elhamifar \u00b7 Mei Chen",
        "Abstract": "Long Video Temporal Grounding (LVTG) aims at identifying specific moments within lengthy videos based on user-provided text queries for effective content retrieval. The approach taken by existing methods of dividing video into clips and processing each clip via a full-scale expert encoder is challenging to scale due to prohibitive computational costs of processing a large number of clips in long videos. To address this issue, we introduce DeCafNet, an approach employing \"delegate-and-conquer\" strategy to achieve computation efficiency without sacrificing grounding performance. DeCafNet introduces a sidekick encoder that performs dense feature extraction over all video clips in a resource-efficient manner, while generating a saliency map to identify the most relevant clips for full processing by the expert encoder. To effectively leverage features from sidekick and expert encoders that exist at different temporal resolutions, we introduce DeCaf-Grounder, which unifies and refines them via query-aware temporal aggregation and multi-scale temporal refinement for accurate grounding. Experiments on two LTVG benchmark datasets demonstrate that DeCafNet reduces computation by up to 47% while still outperforming existing methods, establishing a new state-of-the-art for LTVG in terms of both efficiency and performance. Code and model will be released upon acceptance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "DeCafNet addresses the computational challenges in Long Video Temporal Grounding (LVTG) by introducing a 'delegate-and-conquer' strategy. This approach uses a sidekick encoder for efficient dense feature extraction across video clips and generates a saliency map to identify relevant clips for detailed processing by an expert encoder. The DeCaf-Grounder component unifies and refines features from both encoders through query-aware temporal aggregation and multi-scale temporal refinement, enhancing grounding accuracy. DeCafNet achieves up to 47% reduction in computation while surpassing existing methods in performance, setting a new benchmark in LTVG efficiency and effectiveness.",
        "Tags": [
            "Video Understanding",
            "Temporal Grounding",
            "Saliency Map",
            "Query-Aware Temporal Aggregation",
            "Multi-Scale Temporal Refinement"
        ]
    },
    {
        "Title": "Image Quality Assessment: From Human to Machine Preference",
        "Authors": "Chunyi Li \u00b7 Yuan Tian \u00b7 Xiaoyue Ling \u00b7 Zicheng Zhang \u00b7 Haodong Duan \u00b7 Haoning Wu \u00b7 Ziheng Jia \u00b7 Xiaohong Liu \u00b7 Xiongkuo Min \u00b7 Guo Lu \u00b7 Weisi Lin \u00b7 Guangtao Zhai",
        "Abstract": "Image Quality Assessment (IQA) based on human subjective preferences has undergone extensive research in the past decades. However, with the development of communication protocols, the visual data consumption volume of machines has gradually surpassed that of humans. For machines, the preference depends on downstream tasks such as segmentation and detection, rather than visual appeal. Considering the huge gap between human and machine visual systems, this paper proposes the topic: Image Quality Assessment for Machine Vision for the first time. Specifically, we (1) defined the subjective preferences of machines, including downstream tasks, test models, and evaluation metrics; (2) established the Machine Preference Database (MPD), which contains 2.25M fine-grained annotations and 30k reference/distorted image pair instances; (3) verified the performance of mainstream IQA algorithms on MPD. Experiments show that current IQA metrics are human-centric and cannot accurately characterize machine preferences. We sincerely hope that MPD can promote the evolution of IQA from human to machine preferences.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces the concept of Image Quality Assessment (IQA) tailored for machine vision, addressing the growing disparity between human and machine visual systems. The authors define machine subjective preferences based on downstream tasks, test models, and evaluation metrics, and establish the Machine Preference Database (MPD) containing 2.25M fine-grained annotations and 30k image pairs. The study evaluates mainstream IQA algorithms on MPD, revealing their inadequacy in capturing machine preferences. The work aims to shift IQA focus from human-centric to machine-centric metrics.",
        "Tags": [
            "Image Quality Assessment",
            "Machine Vision",
            "Machine Preference Database",
            "Downstream Task Evaluation",
            "Fine-Grained Annotations"
        ]
    },
    {
        "Title": "Revisiting Source-Free Domain Adaptation: Insights into Representativeness, Generalization, and Diversity",
        "Authors": "Ronghang Zhu \u00b7 Mengxuan Hu \u00b7 Weiming Zhuang \u00b7 Lingjuan Lyu \u00b7 Xiang Yu \u00b7 Sheng Li",
        "Abstract": "Domain adaptation addresses the challenge where the distribution of target inference data differs from that of the source training data. Recently, data privacy has become a significant constraint, limiting access to the source domain. To mitigate this issue, Source-Free Domain Adaptation (SFDA) methods bypass source domain data by generating source-like data or pseudo-labeling the unlabeled target domain. However, these approaches often lack theoretical grounding. In this work, we provide a theoretical analysis of the SFDA problem, focusing on the general empirical risk of the unlabeled target domain. Our analysis offers a comprehensive understanding of how representativeness, generalization, and variety contribute to controlling the upper bound of target domain empirical risk in SFDA settings. We further explore how to balance this trade-off from three perspectives: sample selection, semantic domain alignment, and a progressive learning framework. These insights inform the design of novel algorithms. Experimental results demonstrate that our proposed method achieves state-of-the-art performance on three benchmark datasets\u2014Office-Home, DomainNet, and VisDA-C\u2014yielding relative improvements of $3.2\\%$, $9.1\\%$, and $7.5\\%$, respectively, over the representative SFDA method, SHOT.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper revisits Source-Free Domain Adaptation (SFDA), a method addressing domain adaptation challenges without access to source domain data due to privacy constraints. The authors provide a theoretical analysis of SFDA, focusing on the empirical risk of the unlabeled target domain. They explore how representativeness, generalization, and diversity influence the upper bound of target domain empirical risk. The study proposes balancing these factors through sample selection, semantic domain alignment, and a progressive learning framework, leading to the design of novel algorithms. The proposed method demonstrates state-of-the-art performance on benchmark datasets, outperforming the representative SFDA method, SHOT, by significant margins.",
        "Tags": [
            "Domain Adaptation",
            "Source-Free Domain Adaptation (SFDA)",
            "Empirical Risk Analysis",
            "Semantic Domain Alignment",
            "Progressive Learning Framework"
        ]
    },
    {
        "Title": "Community Forensics: Using Thousands of Generators to Train Fake Image Detectors",
        "Authors": "Jeongsoo Park \u00b7 Andrew Owens",
        "Abstract": "One of the key challenges of detecting AI-generated images is spotting images that have been created by previously unseen generative models. We argue that the limited diversity of the training data is a major obstacle to addressing this problem, and we propose a new dataset that is significantly larger and more diverse than prior works. As part of creating this dataset, we systematically download thousands of text-to-image latent diffusion models and sample images from them. We also collect images from dozens of popular open source and commercial models. The resulting dataset contains 2.7M images that have been sampled from 4803 different models. These images collectively capture a wide range of scene content, generator architectures, and image processing settings. Using this dataset, we study the generalization abilities of fake image detectors. Our experiments suggest that detection performance improves as the number of models in the training set increases, even when these models have similar architectures. We also find that increasing the diversity of the models improves detection performance, and that our trained detectors generalize better than those trained on other datasets.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the challenge of detecting AI-generated images, particularly those from unseen generative models, by highlighting the limited diversity in training data as a major obstacle. To overcome this, the authors introduce a significantly larger and more diverse dataset comprising 2.7 million images sampled from 4803 different text-to-image latent diffusion models, including popular open-source and commercial ones. This dataset captures a wide range of scene content, generator architectures, and image processing settings. The study demonstrates that increasing the number and diversity of models in the training set enhances the generalization abilities of fake image detectors, outperforming detectors trained on other datasets.",
        "Tags": [
            "Diffusion Models",
            "Image Generation",
            "Datasets and Benchmarks",
            "Fake Image Detection",
            "Latent Diffusion Models",
            "Generalization in AI"
        ]
    },
    {
        "Title": "Shift the Lens: Environment-Aware Unsupervised Camouflaged Object Detection",
        "Authors": "Ji Du \u00b7 Fangwei Hao \u00b7 Mingyang Yu \u00b7 Desheng Kong \u00b7 Jiesheng Wu \u00b7 Bin Wang \u00b7 Jing XU \u00b7 Ping Li",
        "Abstract": "Camouflaged Object Detection (COD) seeks to distinguish objects from their highly similar backgrounds. Existing work has essentially focused on isolating camouflaged objects from the environment, demonstrating ever-improving performance but at the cost of extensive annotations and complex optimizations. In this paper, we diverge from this paradigm and shift the lens to isolating the salient environment from the camouflaged object. We introduce EASE, an Environment-Aware unSupErvised COD framework that identifies the environment by referencing an environment prototype library and detects camouflaged objects by inverting the retrieved environmental features. Specifically, our approach (DiffPro) uses large multimodal models, diffusion models, and vision-foundation models to construct the environment prototype library. To retrieve environments from the library and refrain from confusing foreground and background, we incorporate three retrieval schemes: Kernel Density Estimation-based Adaptive Threshold (KDE-AT), Global-to-Local pixel-level retrieval (G2L), and Self-Retrieval (SR). Our experiments demonstrate significant improvements over current unsupervised methods, with EASE achieving an average gain of over 10\\% on the COD10K dataset. When integrated with SAM, EASE surpasses prompt-based segmentation approaches and performs competitively with state-of-the-art fully-supervised methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces EASE, an Environment-Aware unSupErvised framework for Camouflaged Object Detection (COD), which shifts the focus from isolating camouflaged objects to isolating the salient environment from the object. By referencing an environment prototype library constructed using large multimodal models, diffusion models, and vision-foundation models, EASE detects camouflaged objects through the inversion of retrieved environmental features. The framework employs three retrieval schemes\u2014Kernel Density Estimation-based Adaptive Threshold (KDE-AT), Global-to-Local pixel-level retrieval (G2L), and Self-Retrieval (SR)\u2014to accurately retrieve environments and avoid confusion between foreground and background. EASE demonstrates significant improvements over existing unsupervised methods, achieving an average gain of over 10% on the COD10K dataset and performing competitively with state-of-the-art fully-supervised methods when integrated with SAM.",
        "Tags": [
            "Camouflaged Object Detection",
            "Unsupervised Learning",
            "Diffusion Models",
            "Vision-Foundation Models",
            "Environment Prototype Library",
            "Inversion of Environmental Features",
            "Retrieval Schemes"
        ]
    },
    {
        "Title": "BF-STVSR: B-Splines and Fourier---Best Friends for High Fidelity Spatial-Temporal Video Super-Resolution",
        "Authors": "Eunjin Kim \u00b7 HYEONJIN KIM \u00b7 Kyong Hwan Jin \u00b7 Jaejun Yoo",
        "Abstract": "Enhancing low-resolution, low-frame-rate videos to high-resolution, high-frame-rate quality is essential for a seamless user experience, motivating advancements in Continuous Spatial-Temporal Video Super Resolution (C-STVSR). While prior methods employ Implicit Neural Representation (INR) for continuous encoding, they often struggle to capture the complexity of video data, relying on simple coordinate concatenation and pre-trained optical flow network for motion representation.  Interestingly, we find that adding position encoding, contrary to common observations, does not improve\u2014and even degrade\u2014performance. This issue becomes particularly pronounced when combined with pre-trained optical flow networks, which can limit the model\u2019s flexibility. To address these issues, we propose BF-STVSR, a C-STVSR framework with two key modules tailored to better represent spatial and temporal characteristics of video: 1) B-spline Mapper for smooth temporal interpolation, and 2) Fourier Mapper for capturing dominant spatial frequencies. Our approach achieves state-of-the-art PSNR and SSIM performance, showing enhanced spatial details and natural temporal consistency. Our code will be available soon.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces BF-STVSR, a novel framework for Continuous Spatial-Temporal Video Super Resolution (C-STVSR) that addresses limitations in existing methods. While prior approaches rely on Implicit Neural Representation (INR) and pre-trained optical flow networks, they often fail to capture the complexity of video data effectively. The authors identify that position encoding, contrary to common practices, can degrade performance when combined with pre-trained optical flow networks. To overcome these challenges, BF-STVSR incorporates two key modules: a B-spline Mapper for smooth temporal interpolation and a Fourier Mapper for capturing dominant spatial frequencies. This approach achieves state-of-the-art performance in terms of PSNR and SSIM, delivering enhanced spatial details and natural temporal consistency.",
        "Tags": [
            "Super-Resolution",
            "Video Understanding",
            "B-spline Mapper",
            "Fourier Mapper",
            "Temporal Interpolation"
        ]
    },
    {
        "Title": "Open-World Objectness Modeling Unifies Novel Object Detection",
        "Authors": "Shan Zhang \u00b7 Yao Ni \u00b7 Jinhao Du \u00b7 Yuan Xue \u00b7 Philip H.S. Torr \u00b7 Piotr Koniusz \u00b7 Anton van den Hengel",
        "Abstract": "The challenge in open-world object detection, as in many few- and zero-shot learning problems, is to generalize beyond the class distribution of the training data. We thus propose a general class-agnostic objectness measure to reduce bias toward labeled samples. To prevent previously unseen objects from being filtered as background or misclassified as known categories by classifers, we explicitly model the joint distribution of objectness and category labels using variational approximation. Without sufficient labeled data, minimizing the KL divergence between the estimated posterior and a static normal prior fails to converge, however. Theoretical analysis illuminates the root cause and motivates adopting a Gaussian prior with variance dynamically adapted to the estimated posterior as a surrogate. To further reduce misclassification, we introduce an energy-based margin loss that encourages unknown objects to move toward high-density regions of the distribution, thus reducing the uncertainty of unknown detections. We introduce an energy-based Open-World OBJectness modeling (OWOBJ) to boost novel object detection, especially in low-data settings.  As a flexible plugin, OWOBJ outperforms baselines in Open-World, Few-Shot, and zero-shot Open-Vocabulary Object Detection. Code will be released upon acceptance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the challenge of open-world object detection, where the goal is to generalize beyond the class distribution of training data. The authors propose a class-agnostic objectness measure to reduce bias toward labeled samples and prevent unseen objects from being misclassified or filtered as background. They model the joint distribution of objectness and category labels using variational approximation and introduce a dynamically adapted Gaussian prior to address convergence issues. Additionally, an energy-based margin loss is proposed to reduce uncertainty in detecting unknown objects by encouraging them to move toward high-density regions of the distribution. The proposed method, Open-World OBJectness modeling (OWOBJ), is shown to outperform baselines in open-world, few-shot, and zero-shot open-vocabulary object detection tasks.",
        "Error_DeepSeek_Response": "{'tags': ['Object Detection', 'Zero-Shot Learning', 'Open-World Learning', 'Energy-Based Models', 'Variational Approximation'], 'abstract': 'The paper addresses the challenge of open-world object detection, where the goal is to generalize beyond the class distribution of training data. The authors propose a class-agnostic objectness measure to reduce bias toward labeled samples and prevent unseen objects from being misclassified or filtered as background. They model the joint distribution of objectness and category labels using variational approximation and introduce a dynamically adapted Gaussian prior to address convergence issues. Additionally, an energy-based margin loss is proposed to reduce uncertainty in detecting unknown objects by encouraging them to move toward high-density regions of the distribution. The proposed method, Open-World OBJectness modeling (OWOBJ), is shown to outperform baselines in open-world, few-shot, and zero-shot open-vocabulary object detection tasks.'}"
    },
    {
        "Title": "ControlFace: Harnessing Facial Parametric Control for Face Rigging",
        "Authors": "Wooseok Jang \u00b7 Youngjun Hong \u00b7 Geonho Cha \u00b7 Seungryong Kim",
        "Abstract": "Manipulation of facial images to meet specific controls such as pose, expression, and lighting, also referred to as face rigging is a complex task in computer vision. Existing methods are limited by their reliance on image datasets, which necessitates individual-specific fine-tuning and limits their ability to retain fine-grained identity and semantic details, reducing practical usability. To overcome these limitations, we introduce ControlFace, a novel face rigging method conditioned on 3DMM renderings that enables flexible, high-fidelity control. ControlFace employs a dual-branch U-Nets: one, referred to as FaceNet, captures identity and fine details, while the other focuses on generation. To enhance control precision, control mixer module encodes the correlated features between the target-aligned control and reference-aligned control, and a novel guidance method, reference control guidance, steers the generation process for better control adherence. By training on a facial video dataset, we fully utilize FaceNet\u2019s rich representations while ensuring control adherence. Extensive experiments demonstrate ControlFace\u2019s superior performance in identity preservation, and control precision, highlighting its practicality. Code and pre-trained weights will be publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "ControlFace introduces a novel face rigging method that leverages 3DMM renderings to achieve flexible and high-fidelity control over facial images. The method employs a dual-branch U-Net architecture, where one branch (FaceNet) captures identity and fine details, and the other focuses on generation. A control mixer module and a novel reference control guidance method are used to enhance control precision and adherence. Trained on a facial video dataset, ControlFace demonstrates superior performance in identity preservation and control precision, making it highly practical for various applications.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Image Editing",
            "3DMM Renderings",
            "Dual-Branch U-Net",
            "Reference Control Guidance"
        ]
    },
    {
        "Title": "Co-Speech Gesture Video Generation with Implicit Motion-Audio Entanglement",
        "Authors": "Xinjie Li \u00b7 Ziyi Chen \u00b7 Xinlu Yu \u00b7 Iek-Heng Chu \u00b7 Peng Chang \u00b7 Jing Xiao",
        "Abstract": "Co-speech gestures are essential to non-verbal communication, enhancing both the naturalness and effectiveness of human interaction. Although recent methods have made progress in generating co-speech gesture videos, many rely on strong visual controls, such as pose images or TPS keypoint movements, which often lead to artifacts like blurry hands and distorted fingers. In response to these challenges, we present the Implicit Motion-Audio Entanglement (IMAE) method for co-speech gesture video generation. IMAE strengthens audio control by entangling implicit motion parameters, including pose and expression, with audio inputs. Our method utilizes a two-branch framework that combines an audio-to-motion generation branch with a video diffusion branch, enabling realistic gesture generation without requiring additional inputs during inference. To improve training efficiency, we propose a two-stage slow-fast training strategy that balances memory constraints while facilitating the learning of meaningful gestures from long frame sequences.Extensive experimental results demonstrate that our method achieves state-of-the-art performance across multiple metrics.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces the Implicit Motion-Audio Entanglement (IMAE) method for generating co-speech gesture videos, addressing challenges like blurry hands and distorted fingers in existing methods. IMAE enhances audio control by integrating implicit motion parameters with audio inputs, using a two-branch framework that combines audio-to-motion generation with video diffusion. A two-stage slow-fast training strategy is proposed to improve efficiency and learning from long frame sequences. The method demonstrates state-of-the-art performance across various metrics.",
        "Tags": [
            "Video Generation",
            "Multimodal Learning",
            "Implicit Motion-Audio Entanglement",
            "Two-Stage Slow-Fast Training",
            "Video Diffusion"
        ]
    },
    {
        "Title": "DiGIT: Multi-Dilated Gated Encoder and Central-Adjacent Region Integrated Decoder for Temporal Action Detection Transformer",
        "Authors": "Ho-Joong Kim \u00b7 Yearang Lee \u00b7 Jung-Ho Hong \u00b7 Seong-Whan Lee",
        "Abstract": "In this paper, we examine a key limitation in query-based detectors for temporal action detection (TAD), which arises from their direct adaptation of originally designed architectures for object detection. Despite the effectiveness of the existing models, they struggle to fully address the unique challenges of TAD, such as the redundancy in multi-scale features and the limited ability to capture sufficient temporal context. To address these issues, we propose a multi-dilated gated encoder and central-adjacent region integrated decoder for temporal action detection transformer (DiGIT). Our approach replaces the existing encoder that consists of multi-scale deformable attention and feedforward network with our multi-dilated gated encoder. Our proposed encoder reduces the redundant information caused by multi-level features while maintaining the ability to capture fine-grained and long-range temporal information. Furthermore, we introduce a central-adjacent region integrated decoder that leverages a more comprehensive sampling strategy for deformable cross-attention to capture the essential information. Extensive experiments demonstrate that DiGIT achieves state-of-the-art performance on THUMOS14, ActivityNet v1.3, and HACS-Segment.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses limitations in query-based detectors for temporal action detection (TAD), which stem from their adaptation of object detection architectures. These models struggle with redundancy in multi-scale features and insufficient temporal context capture. The authors propose DiGIT, a transformer-based model featuring a multi-dilated gated encoder and a central-adjacent region integrated decoder. The encoder reduces redundant information while preserving fine-grained and long-range temporal details. The decoder employs a comprehensive sampling strategy for deformable cross-attention to capture essential information. DiGIT achieves state-of-the-art performance on benchmark datasets.",
        "Tags": [
            "Temporal Action Detection",
            "Transformers",
            "Multi-Dilated Gated Encoder",
            "Central-Adjacent Region Integrated Decoder",
            "Deformable Cross-Attention"
        ]
    },
    {
        "Title": "Navigating the Unseen: Zero-shot Scene Graph Generation via Capsule-Based Equivariant Features",
        "Authors": "Wenhuan Huang \u00b7 Yi JI \u00b7 guiqian zhu \u00b7 Ying Li \u00b7 chunping Liu",
        "Abstract": "In scene graph generation (SGG), the accurate prediction of unseen triples is essential for its effectiveness in downstream vision-language tasks. We hypothesize that the predicates of unseen triples can be viewed as transformations of seen predicates in feature space, and the essence of the zero-shot task is to bridge the gap caused by this transformation. Traditional models, however, have difficulty addressing this challenge, which we attribute to their inability to model the predicates equivariant. To overcome this limitation, we introduce a novel framework based on\u00a0capsule networks (CAPSGG). We propose a $\\textbf{Three-Stream Pipeline}$ that generates modality-specific representations for predicates, while building low-level predicate capsules of these modalities. Then these capsules are aggregated into high-level predicate capsules using a $\\textbf{Routing Capsule Layer}$. In addition, we introduce $\\textbf{GroupLoss}$ to aggregate capsules with the same predicate label into groups. This replaces\u00a0the global loss with the intra-group loss, effectively balancing the learning of predicate invariance and equivariant features, while mitigating the impact of the severe long-tail distribution of the predicate categories. Our extensive experiments demonstrate the notable superiority of our approach over state-of-the-art methods, with zero-shot indicators outperforming up to $\\textbf{132.26\\\\%}$  on SGCls task than the T-CAR [21]. Our code will be available upon publication.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of zero-shot scene graph generation (SGG) by proposing a novel framework based on capsule networks (CAPSGG). The authors hypothesize that unseen predicates can be modeled as transformations of seen predicates in feature space. To achieve this, they introduce a Three-Stream Pipeline that generates modality-specific representations and builds low-level predicate capsules, which are then aggregated into high-level predicate capsules using a Routing Capsule Layer. Additionally, a GroupLoss mechanism is introduced to balance the learning of predicate invariance and equivariant features, mitigating the impact of long-tail distribution in predicate categories. The proposed method demonstrates significant improvements over state-of-the-art methods in zero-shot SGG tasks.",
        "Tags": [
            "Scene Graph Generation",
            "Zero-Shot Learning",
            "Capsule Networks",
            "Equivariant Features",
            "Long-Tail Distribution"
        ]
    },
    {
        "Title": "Vid2Sim: Realistic and Interactive Simulation from Video for Urban Navigation",
        "Authors": "Ziyang Xie \u00b7 Zhizheng Liu \u00b7 Zhenghao Peng \u00b7 Wayne Wu \u00b7 Bolei Zhou",
        "Abstract": "Sim-to-real gap has long posed a significant challenge for robot learning in simulation, preventing the deployment of learned models in the real world. Previous work has primarily focused on domain randomization and system identification to mitigate this gap. However, these methods are often limited by the inherent constraints of the simulation and graphics engines. In this work, we propose Vid2Sim, a novel framework that effectively bridges the sim2real gap through a scalable and cost-efficient real2sim pipeline for neural 3D scene reconstruction and simulation. Given a monocular video as input, Vid2Sim can generate photo-realistic and physically interactable 3D simulation environments to enable the reinforcement learning of visual navigation agents in complex urban environments. Extensive experiments demonstrate that Vid2Sim significantly improves the performance of urban navigation in the digital twins and real world by 31.2% and 68.3% in success rate compared with agents trained with prior simulation methods. Code and data will be made publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The sim-to-real gap remains a major obstacle in robot learning, limiting the deployment of simulation-trained models in real-world scenarios. Traditional approaches like domain randomization and system identification are constrained by simulation and graphics engine limitations. This paper introduces Vid2Sim, a novel framework that bridges the sim-to-real gap using a scalable and cost-efficient real-to-sim pipeline for neural 3D scene reconstruction and simulation. Vid2Sim generates photo-realistic and physically interactable 3D environments from monocular video, enabling reinforcement learning for visual navigation agents in complex urban settings. The framework significantly enhances urban navigation performance in both digital twins and real-world environments, achieving improvements of 31.2% and 68.3% in success rates compared to prior simulation methods.",
        "Tags": [
            "3D Reconstruction",
            "Autonomous Driving",
            "Neural 3D Scene Reconstruction",
            "Reinforcement Learning",
            "Urban Navigation"
        ]
    },
    {
        "Title": "AlphaPre: Amplitude-Phase Disentanglement Model for Precipitation Nowcasting",
        "Authors": "Kenghong Lin \u00b7 Baoquan Zhang \u00b7 Demin Yu \u00b7 Wenzhi Feng \u00b7 Shidong Chen \u00b7 Feifan Gao \u00b7 Xutao Li \u00b7 Yunming Ye",
        "Abstract": "Precipitation nowcasting involves using current radar observation sequences to predict future radar sequences and determine future precipitation distribution, which is crucial for disaster warning, traffic planning, and agricultural production. Despite numerous advancements, challenges persist in accurately predicting both the location and intensity of precipitation, as these factors are often interdependent, with complex atmospheric dynamics and moisture distribution causing position and intensity changes to be intricately coupled. Inspired by the fact that in the frequency domain, phase variations are shown to correspond to changes in the position of precipitation, while amplitude variations are linked to intensity changes, we propose an amplitude-phase disentanglement model called AlphaPre, which separately learn the position and intensity changes of precipitation. AlphaPre comprises three key components: a phase network, an amplitude network, and an AlphaMixer. The phase network captures positional changes by learning phase variations, while the amplitude network models intensity changes by alternating between the frequency and spatial domains. The AlphaMixer then integrates these components to produce a refined precipitation forecast. Extensive experiments on four datasets demonstrate the effectiveness and superiority of our method over state-of-the-art approaches.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Precipitation nowcasting is critical for disaster warning, traffic planning, and agricultural production, yet accurately predicting both the location and intensity of precipitation remains challenging due to their interdependence and complex atmospheric dynamics. Inspired by frequency domain analysis, where phase variations correspond to positional changes and amplitude variations to intensity changes, the authors propose AlphaPre, an amplitude-phase disentanglement model. AlphaPre consists of a phase network to capture positional changes, an amplitude network to model intensity changes, and an AlphaMixer to integrate these components for refined precipitation forecasts. The model demonstrates superior performance over state-of-the-art methods across four datasets.",
        "Tags": [
            "Precipitation Nowcasting",
            "Frequency Domain Analysis",
            "Amplitude-Phase Disentanglement",
            "Phase Network",
            "AlphaMixer"
        ]
    },
    {
        "Title": "Turbo3D: Ultra-fast Text-to-3D Generation",
        "Authors": "Hanzhe Hu \u00b7 Tianwei Yin \u00b7 Fujun Luan \u00b7 Yiwei Hu \u00b7 Hao Tan \u00b7 Zexiang Xu \u00b7 Sai Bi \u00b7 Shubham Tulsiani \u00b7 Kai Zhang",
        "Abstract": "We present Turbo3D, an ultra-fast text-to-3D system capable of generating high-quality Gaussian splatting assets in under one second. Turbo3D employs a rapid 4-step, 4-view diffusion generator, and an efficient feed-forward Gaussian reconstructor, both operating in latent space. The 4-step, 4-view generator is a student model distilled through a novel Dual-Teacher approach, which encourages the student to learn view consistency from a multi-view teacher and photo-realism from a single-view teacher. By shifting the Gaussian reconstructor's inputs from pixel space to latent space, we eliminate the extra image decoding time and halve the transformer sequence length for maximum efficiency. Our method demonstrates superior 3D generation results compared to previous baselines, while operating in a fraction of their runtime.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Turbo3D is an ultra-fast text-to-3D generation system that produces high-quality Gaussian splatting assets in under one second. It utilizes a 4-step, 4-view diffusion generator and an efficient feed-forward Gaussian reconstructor, both operating in latent space. The generator is distilled using a novel Dual-Teacher approach, ensuring view consistency and photo-realism. By processing inputs in latent space, the system eliminates image decoding time and reduces transformer sequence length, achieving superior 3D generation results with significantly reduced runtime compared to previous methods.",
        "Tags": [
            "3D Generation",
            "Gaussian Splatting",
            "Dual-Teacher Distillation",
            "Latent Space Processing",
            "Ultra-fast 3D Generation"
        ]
    },
    {
        "Title": "EDEN: Enhanced Diffusion for High-quality Large-motion Video Frame Interpolation",
        "Authors": "Zihao Zhang \u00b7 Haoran Chen \u00b7 Haoyu Zhao \u00b7 Guansong Lu \u00b7 Yanwei Fu \u00b7 Hang Xu \u00b7 Zuxuan Wu",
        "Abstract": "Handling complex or nonlinear motion patterns has long posed challenges for video frame interpolation. Although recent advances in diffusion-based methods offer improvements over traditional optical flow-based approaches, they still struggle to generate sharp, temporally consistent frames in scenarios with large motion. To address this limitation, we introduce EDEN, an Enhanced Diffusion for high-quality large-motion vidEo frame iNterpolation. Our approach first utilizes a transformer-based tokenizer to produce refined latent representations of the intermediate frames for diffusion models. We then enhance the diffusion transformer with temporal attention across the process and incorporate a start-end frame difference embedding to guide the generation of dynamic motion. Extensive experiments demonstrate that EDEN achieves state-of-the-art results across popular benchmarks, including nearly a 10% LPIPS reduction on DAVIS and SNU-FILM, and an 8% improvement on DAIN-HD.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces EDEN, an Enhanced Diffusion model designed for high-quality video frame interpolation, particularly in scenarios with large motion. Traditional methods, including recent diffusion-based approaches, often fail to produce sharp and temporally consistent frames under such conditions. EDEN addresses this by employing a transformer-based tokenizer to refine latent representations of intermediate frames and enhancing the diffusion transformer with temporal attention. Additionally, it incorporates a start-end frame difference embedding to better guide dynamic motion generation. The model demonstrates significant improvements, achieving state-of-the-art results on benchmarks like DAVIS, SNU-FILM, and DAIN-HD.",
        "Tags": [
            "Video Frame Interpolation",
            "Diffusion Models",
            "Transformer-based Tokenizer",
            "Temporal Attention",
            "Dynamic Motion Generation"
        ]
    },
    {
        "Title": "Mosaic3D: Foundation Dataset and Model for Open-Vocabulary 3D Segmentation",
        "Authors": "Junha Lee \u00b7 Chunghyun Park \u00b7 Jaesung Choe \u00b7 Yu-Chiang Frank Wang \u00b7 Jan Kautz \u00b7 Minsu Cho \u00b7 Chris Choy",
        "Abstract": "We tackle open-vocabulary 3D scene understanding by introducing a novel data generation pipeline and training framework. Our method addresses three critical requirements for effective training: precise 3D region segmentation, comprehensive textual descriptions, and sufficient dataset scale. By leveraging state-of-the-art open-vocabulary image segmentation models and region-aware Vision-Language Models (VLM), we develop an automatic pipeline that generates high-quality 3D mask-text pairs. Applying this pipeline to multiple 3D scene datasets, we create Mosaic3D-5.6M, a dataset of over 30K annotated scenes with 5.6M mask-text pairs\u2014significantly larger than existing datasets. Building upon this data, we propose Mosaic3D, a foundation model combining a 3D encoder trained with contrastive learning and a lightweight mask decoder for open-vocabulary 3D semantic and instance segmentation. Our approach achieves state-of-the-art results on open-vocabulary 3D semantic and instance segmentation tasks including ScanNet200, Matterport3D, and ScanNet++, with ablation studies validating the effectiveness of our large-scale training data.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Mosaic3D, a novel approach for open-vocabulary 3D scene understanding, addressing key requirements such as precise 3D region segmentation, comprehensive textual descriptions, and large-scale dataset generation. The authors develop an automatic pipeline leveraging open-vocabulary image segmentation models and region-aware Vision-Language Models (VLMs) to create Mosaic3D-5.6M, a dataset with 5.6M mask-text pairs across 30K annotated scenes. The proposed Mosaic3D foundation model combines a 3D encoder trained with contrastive learning and a lightweight mask decoder, achieving state-of-the-art results on open-vocabulary 3D semantic and instance segmentation tasks.",
        "Tags": [
            "3D Semantic Segmentation",
            "Vision-Language Models (VLMs)",
            "Open-Vocabulary Learning",
            "Contrastive Learning",
            "3D Mask-Text Pair Generation"
        ]
    },
    {
        "Title": "Beyond Image Classification: A Video Benchmark and Dual-Branch Hybrid Discrimination Framework for Compositional Zero-Shot Learning",
        "Authors": "Dongyao Jiang \u00b7 Haodong Jing \u00b7 Yongqiang Ma \u00b7 Nanning Zheng",
        "Abstract": "Human reasoning naturally combines concepts to identify unseen compositions, a capability that Compositional Zero-Shot Learning (CZSL) aims to replicate in machine learning models. However, we observe that focusing solely on typical image classification tasks in CZSL may limit models' compositional generalization potential. To address this, we introduce C-EgoExo, a video-based benchmark, along with a compositional action recognition task to enable more comprehensive evaluations. Inspired by human reasoning processes, we propose a Dual-branch Hybrid Discrimination (DHD) framework, featuring two branches that decode visual inputs in distinct observation sequences. Through a cross-attention mechanism and a contextual dependency encoder, DHD effectively mitigates challenges posed by conditional variance. We further design a Copula-based orthogonal decoding loss to counteract contextual interference in primitive decoding. Our approach demonstrates outstanding performance across diverse CZSL tasks, excelling in both image-based and video-based modalities and in attribute-object and action-object compositions, setting a new benchmark for CZSL evaluation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the limitations of Compositional Zero-Shot Learning (CZSL) by introducing C-EgoExo, a video-based benchmark, and a compositional action recognition task to enhance evaluation comprehensiveness. The authors propose a Dual-branch Hybrid Discrimination (DHD) framework, which decodes visual inputs through two distinct observation sequences, utilizing a cross-attention mechanism and a contextual dependency encoder to mitigate conditional variance challenges. Additionally, a Copula-based orthogonal decoding loss is designed to reduce contextual interference in primitive decoding. The DHD framework demonstrates superior performance across various CZSL tasks, including image-based and video-based modalities, and excels in attribute-object and action-object compositions, establishing a new benchmark for CZSL evaluation.",
        "Error_DeepSeek_Response": "{'tags': ['Compositional Zero-Shot Learning', 'Video Understanding', 'Dual-Branch Hybrid Discrimination', 'Cross-Attention Mechanism', 'Copula-based Orthogonal Decoding Loss'], 'abstract': 'This paper addresses the limitations of Compositional Zero-Shot Learning (CZSL) by introducing C-EgoExo, a video-based benchmark, and a compositional action recognition task to enhance evaluation comprehensiveness. The authors propose a Dual-branch Hybrid Discrimination (DHD) framework, which decodes visual inputs through two distinct observation sequences, utilizing a cross-attention mechanism and a contextual dependency encoder to mitigate conditional variance challenges. Additionally, a Copula-based orthogonal decoding loss is designed to reduce contextual interference in primitive decoding. The DHD framework demonstrates superior performance across various CZSL tasks, including image-based and video-based modalities, and excels in attribute-object and action-object compositions, establishing a new benchmark for CZSL evaluation.'}"
    },
    {
        "Title": "Link-based Contrastive Learning for One-Shot Unsupervised Domain Adaptation",
        "Authors": "Yue Zhang \u00b7 Mingyue Bin \u00b7 Yuyang Zhang \u00b7 Zhongyuan Wang \u00b7 Zhen Han \u00b7 Chao Liang",
        "Abstract": "Unsupervised domain adaptation (UDA) aims to learn discriminative features from a labeled source domain by supervised learning and to transfer the knowledge to an unlabeled target domain via distribution alignment. However, in some real-world scenarios, e.g., public safety or access control, it\u2019s difficult to obtain sufficient source domain data, which hinders the application of existing UDA methods. To this end, this paper investigates a realistic but rarely studied problem called one-shot unsupervised domain adaptation (OSUDA), where there is only one example per category in the source domain and abundant unlabeled samples in the target domain. Compared with UDA, OSUDA faces dual challenges in both feature learning and domain alignment due to the lack of sufficient source data. To address these challenges, we propose a simple but effective link-based contrastive learning (LCL) method for OSUDA. On the one hand, with the help of in-domain links that indicate whether two samples are from the same cluster, LCL can learn discriminative features with abundant unlabeled target data. On the other hand, by constructing cross-domain links that show whether two clusters are bidirectionally matched, LCL can realize accurate domain alignment with only one source sample per category. Extensive experiments conducted on 4 public domain adaptation benchmarks, including VisDA-2017, Office-31, Office-Home, and DomainNet, demonstrate the effectiveness of the proposed LCL under the OSUDA setting. In addition, we build a realistic OSUDA surveillance video face recognition dataset, where LCL consistently improves the recognition accuracy across various face recognition methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of one-shot unsupervised domain adaptation (OSUDA), where only one labeled example per category is available in the source domain, and abundant unlabeled data exists in the target domain. The authors propose a link-based contrastive learning (LCL) method to tackle the dual challenges of feature learning and domain alignment in OSUDA. LCL leverages in-domain links to learn discriminative features from unlabeled target data and constructs cross-domain links to achieve accurate domain alignment with minimal source data. The method is validated on multiple domain adaptation benchmarks and a newly constructed surveillance video face recognition dataset, demonstrating consistent improvements in recognition accuracy.",
        "Tags": [
            "Unsupervised Domain Adaptation",
            "Contrastive Learning",
            "One-Shot Learning",
            "Domain Alignment",
            "Surveillance Video Analysis"
        ]
    },
    {
        "Title": "M3GYM: A Large-Scale Multimodal Multi-view Multi-person Pose Dataset for Fitness Activity Understanding in Real-world Settings",
        "Authors": "Qingzheng Xu \u00b7 Ru Cao \u00b7 Xin Shen \u00b7 Heming Du \u00b7 Sen Wang \u00b7 Xin Yu",
        "Abstract": "Human pose estimation is a critical task in computer vision for applications in sports analysis, healthcare monitoring, and human-computer interaction. However, existing human pose datasets are collected either from custom-configured laboratories with complex devices or they only include data on single individuals, and both types typically capture daily activities. In this paper, we introduce the M3GYM dataset, a large-scale multimodal, multi-view, and multi-person pose dataset collected from a real gym to address the limitations of existing datasets.Specifically, we collect videos for 82 sessions from the gym, each session lasting between 40 to 60 minutes. These videos are gathered by 8 cameras, including over 50 subjects and 47 million frames. These sessions include 51 Normal fitness exercise sessions as well as 17 Pilates and 14 Yoga sessions. The exercises cover a wide range of poses and typical fitness activities, particularly in Yoga and Pilates, featuring poses with stretches, bends, and twists, \\eg, humble warrior, fire hydrants and knee hover side twists.Each session involves multiple subjects, leading to significant self-occlusion and mutual occlusion in single views.Moreover, the gym has two symmetric floor mirrors, a feature not seen in previous datasets, and seven lighting conditions. We provide frame-level multimodal annotations, including 2D\\&3D keypoints, subject IDs, and meshes. Additionally, M3GYM uniquely offers labels for over 500 actions along with corresponding assessments from sports experts.We benchmark a variety of state-of-the-art methods for several tasks, \\ie, 2D human pose estimation, single-view and multi-view 3D human pose estimation, and human mesh recovery. To simulate real-world applications, we also conduct cross-domain experiments across Normal, Yoga, and Pilates sessions. The results show that M3GYM significantly improves model generalization in complex real-world settings.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The M3GYM dataset is introduced as a large-scale, multimodal, multi-view, and multi-person pose dataset collected from a real gym to address limitations in existing human pose datasets. It includes videos from 82 sessions, captured by 8 cameras, featuring over 50 subjects and 47 million frames. The dataset covers a variety of fitness activities, including Normal exercises, Pilates, and Yoga, with complex poses and occlusions. It provides detailed annotations such as 2D and 3D keypoints, subject IDs, meshes, and labels for over 500 actions with expert assessments. Benchmarking results demonstrate that M3GYM enhances model generalization in complex, real-world settings.",
        "Tags": [
            "3D Human Pose Estimation",
            "Multimodal Learning",
            "Multi-view Pose Estimation",
            "Real-world Occlusion Handling",
            "Fitness Activity Analysis"
        ]
    },
    {
        "Title": "Transformers without Normalization",
        "Authors": "Jiachen Zhu \u00b7 Xinlei Chen \u00b7 Kaiming He \u00b7 Yann LeCun \u00b7 Zhuang Liu",
        "Abstract": "Normalization layers are ubiquitous in modern neural networks and have long been considered essential.In this work, we demonstrate that strong performance can be achieved on Transformers without normalization layers, by using a remarkably simple technique.We introduce Dynamic Tanh (DyT), an element-wise operation $\\mathrm{DyT}(\\boldsymbol{x}) = \\tanh(\\alpha \\boldsymbol{x})$, as a drop-in replacement for normalization layers in Transformers.DyT is inspired by the observation that layer normalization layers often produce tanh-like, $S$-shaped input-output mappings.By incorporating DyT, Transformers without any normalization layers can match or exceed the performance of their normalized counterparts, mostly without tuning training hyperparameters.We validate the efficacy of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models.These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep neural networks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper challenges the necessity of normalization layers in Transformers by introducing Dynamic Tanh (DyT), an element-wise operation that replaces normalization layers. DyT, inspired by the tanh-like behavior of layer normalization, enables Transformers to achieve strong performance without normalization layers across various tasks and domains, including recognition, generation, supervised, and self-supervised learning. The findings suggest that normalization layers may not be indispensable in modern neural networks, providing new insights into their role.",
        "Tags": [
            "Transformers",
            "Self-Supervised Learning",
            "Dynamic Tanh (DyT)",
            "Normalization-Free Networks",
            "Element-wise Operations"
        ]
    },
    {
        "Title": "Iterative Predictor-Critic Code Decoding for Real-World Image Dehazing",
        "Authors": "Jiayi Fu \u00b7 Siyu Liu \u00b7 Zikun Liu \u00b7 Chun-Le Guo \u00b7 Hyunhee Park \u00b7 Rui-Qi Wu \u00b7 Guoqing Wang \u00b7 Chongyi Li",
        "Abstract": "We propose a novel real-world image dehazing method, abbreviated as IPC-Dehaze, by leveraging the high-quality codebook prior encapsulated in a pre-train VQGAN. Apart from previous codebook-based methods that rely on one-shot decoding, our method utilizes high-quality codes obtained in the previous iteration to guide the prediction of the Code-Predictor in the subsequent iteration, improving code prediction accuracy and ensuring stable dehazing performance. Our idea stems from the observations that 1) the degradation of hazy images varies with haze density and scene depth, and 2) clear regions play crucial cues in restoring dense haze regions. However, it is nontrivial to progressively refine the obtained codes in subsequent iterations, owing to the difficulty in determining which codes should be retained or replaced at each iteration. Another key insight of our study is to propose Code-Critic to capture interrelations among codes. The Code-Critic is used to evaluate code correlations and then resample a set of codes with the highest mask scores, i.e., a higher score indicates that the code is more likely to be rejected, which helps retain more accurate codes and predict difficult ones. Extensive experiments demonstrate the superiority of our method over state-of-the-art methods in real-world dehazing. Our code will be made publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces IPC-Dehaze, a novel real-world image dehazing method that leverages a pre-trained VQGAN's high-quality codebook prior. Unlike traditional one-shot decoding approaches, IPC-Dehaze iteratively refines code predictions using high-quality codes from previous iterations, guided by a Code-Predictor. The method addresses the challenge of determining which codes to retain or replace by introducing a Code-Critic, which evaluates code correlations and resamples codes based on mask scores. This approach ensures more accurate code retention and prediction, particularly for dense haze regions. The method demonstrates superior performance in real-world dehazing compared to state-of-the-art techniques.",
        "Error_DeepSeek_Response": "{'tags': ['Image Dehazing', 'Low-Level Vision', 'VQGAN', 'Iterative Refinement', 'Codebook Prior'], 'abstract': \"The paper introduces IPC-Dehaze, a novel real-world image dehazing method that leverages a pre-trained VQGAN's high-quality codebook prior. Unlike traditional one-shot decoding approaches, IPC-Dehaze iteratively refines code predictions using high-quality codes from previous iterations, guided by a Code-Predictor. The method addresses the challenge of determining which codes to retain or replace by introducing a Code-Critic, which evaluates code correlations and resamples codes based on mask scores. This approach ensures more accurate code retention and prediction, particularly for dense haze regions. The method demonstrates superior performance in real-world dehazing compared to state-of-the-art techniques.\"}"
    },
    {
        "Title": "Learning Bijective Surface Parameterization for Inferring Signed Distance Functions from Sparse Point Clouds with Grid Deformation",
        "Authors": "Takeshi Noda \u00b7 Chao Chen \u00b7 Junsheng Zhou \u00b7 Weiqi Zhang \u00b7 Yu-Shen Liu \u00b7 Zhizhong Han",
        "Abstract": "Inferring signed distance functions (SDFs) from sparse point clouds remains a challenge in surface reconstruction. The key lies in the lack of detailed geometric information in sparse point clouds, which is essential for learning a continuous field. To resolve this issue, we present a novel approach that learns a dynamic deformation network to predict SDFs in an end-to-end manner. To parameterize a continuous surface from sparse points, we propose a bijective surface parameterization (BSP) that learns the global shape from local patches. Specifically, we construct a bijective mapping for sparse points from the parametric domain to 3D local patches, integrating patches into the global surface. Meanwhile, we introduce grid deformation optimization (GDO) into the surface approximation to optimize the deformation of grid points and further refine the parametric surfaces. Experimental results on synthetic and real scanned datasets demonstrate that our method significantly outperforms the current state-of-the-art methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of inferring signed distance functions (SDFs) from sparse point clouds for surface reconstruction. The authors propose a novel approach that employs a dynamic deformation network to predict SDFs end-to-end. A key innovation is the introduction of bijective surface parameterization (BSP), which learns global shapes from local patches by constructing a bijective mapping from the parametric domain to 3D local patches. Additionally, grid deformation optimization (GDO) is introduced to refine parametric surfaces by optimizing grid point deformation. The method demonstrates superior performance over state-of-the-art techniques on both synthetic and real-world datasets.",
        "Tags": [
            "3D Reconstruction",
            "Signed Distance Functions (SDFs)",
            "Bijective Mapping",
            "Grid Deformation Optimization",
            "Surface Parameterization"
        ]
    },
    {
        "Title": "Percept, Memory, and Imagine: World Feature Simulating for Open-Domain Unknown Object Detection",
        "Authors": "Aming Wu \u00b7 Cheng Deng",
        "Abstract": "To accelerate the safe deployment of object detectors, we focus on reducing the impact of both covariate and semantic shifts. And we consider a realistic yet challenging scenario, namely Open-Domain Unknown Object Detection (ODU-OD), which aims to detect unknown objects in unseen target domains without accessing any auxiliary data. Towards ODU-OD, it is feasible to learn a robust discriminative boundary by synthesizing virtual features. Generally, perception, memory, and imagination are three essential capacities for human beings. Through multi-level perception and rich memory about known objects, the characteristics of unknown objects can be imagined sufficiently, enhancing the ability of discriminating known from unknown objects. Inspired by this idea, an approach of World Feature Simulation (WFS) is proposed, mainly consisting of a multi-level perception, memory recorder, and unknown-feature generator. Specifically, after extracting the features of the input, we separately employ a Mamba and Graph Network to obtain the global-level and connective-level representations. Next, a codebook containing multiple learnable codewords is defined to preserve fragmented memory of known objects. Meanwhile, we perform a modulated operation on the memory to form the imagination bank involving unknown characteristics. Finally, to alleviate the impact of lacking supervision data, based on the multi-level representation and imagination bank, a dedicated unknown-feature generator is designed to recurrently synthesize outlier features deviating from in-distribution (ID) objects. In the experiments, our method is evaluated on four different detection tasks. The significant performance gains over baselines demonstrate the superiorities of our method.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the challenge of Open-Domain Unknown Object Detection (ODU-OD), which involves detecting unknown objects in unseen domains without auxiliary data. To tackle this, the authors propose a World Feature Simulation (WFS) approach inspired by human cognitive processes of perception, memory, and imagination. The method employs a multi-level perception mechanism using Mamba and Graph Networks for global and connective-level feature representations. A memory recorder with a learnable codebook preserves known object features, while an imagination bank synthesizes unknown object characteristics. An unknown-feature generator is designed to create outlier features, enhancing the model's ability to distinguish known from unknown objects. The approach demonstrates significant performance improvements across multiple detection tasks.",
        "Tags": [
            "Object Detection",
            "Open-Domain Unknown Object Detection",
            "Mamba",
            "Graph Neural Networks (GNNs)",
            "Feature Synthesis",
            "Cognitive-Inspired AI",
            "Outlier Feature Generation"
        ]
    },
    {
        "Title": "Factored-NeuS: Reconstructing Surfaces, Illumination, and Materials of Possibly Glossy Objects",
        "Authors": "Yue Fan \u00b7 Ningjing Fan \u00b7 Ivan Skorokhodov \u00b7 Oleg Voynov \u00b7 Savva Ignatyev \u00b7 Evgeny Burnaev \u00b7 Peter Wonka \u00b7 Yiqun Wang",
        "Abstract": "We develop a method that recovers the surface, materials, and illumination of a scene from its posed multi-view images. In contrast to prior work, it does not require any additional data and can handle glossy objects or bright lighting. It is a progressive inverse rendering approach, which consists of three stages. In the first stage, we reconstruct the scene radiance and signed distance function (SDF) with a novel regularization strategy for specular reflections. We propose to explain a pixel color using both surface and volume rendering jointly, which allows for handling complex view-dependent lighting effects for surface reconstruction. In the second stage, we distill light visibility and indirect illumination from the learned SDF and radiance field using learnable mapping functions. Finally, we design a method for estimating the ratio of incoming direct light reflected in a specular manner and use it to reconstruct the materials and direct illumination. Experimental results demonstrate that the proposed method outperforms the current state-of-the-art in recovering surfaces, materials, and lighting without relying on any additional data.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Factored-NeuS, a method for reconstructing surfaces, materials, and illumination from posed multi-view images without requiring additional data. The approach handles glossy objects and bright lighting through a progressive inverse rendering process. It first reconstructs scene radiance and signed distance function (SDF) with a novel regularization strategy for specular reflections, combining surface and volume rendering to manage complex view-dependent lighting effects. Next, it distills light visibility and indirect illumination from the learned SDF and radiance field. Finally, it estimates the ratio of specularly reflected direct light to reconstruct materials and direct illumination. The method outperforms state-of-the-art techniques in recovering surfaces, materials, and lighting.",
        "Tags": [
            "3D Reconstruction",
            "NeRF (Neural Radiance Fields)",
            "Inverse Rendering",
            "Specular Reflections",
            "Light Visibility Estimation"
        ]
    },
    {
        "Title": "LatentHOI: On the Generalizable Hand Object Motion Generation with Latent Hand Diffusion",
        "Authors": "Muchen Li \u00b7 Sammy Christen \u00b7 Chengde Wan \u00b7 Yujun Cai \u00b7 Renjie Liao \u00b7 Leonid Sigal \u00b7 Shugao Ma",
        "Abstract": "Current research on generating 3D hand-object interaction motion primarily focuses on in-domain objects. Generalization to unseen objects is essential for practical applications, yet it remains both challenging and largely unexplored.In this paper, we propose LatentHOI, a novel approach designed to tackle the challenges of generalizing hand-object interaction to unseen objects.Our main insight lies in decoupling high-level temporal motion from fine-grained spatial hand-object interactions with a latent diffusion model coupled with a Grasping Variational Autoencoder (GraspVAE). This configuration not only enhances the conditional dependency between spatial grasp and temporal motion but also improves data utilization and reduces overfitting through regularization in the latent space. We conducted extensive experiments in an unseen-object setting on both single-hand grasping and bi-manual motion datasets, including GRAB, DexYCB, and OakInk.Quantitative and qualitative evaluations demonstrate that our method significantly enhances the realism and physical plausibility of generated motions for unseen objects, both in single and bimanual manipulations, compared to the state-of-the-art.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces LatentHOI, a novel approach for generating 3D hand-object interaction motions that generalizes to unseen objects, addressing a significant gap in current research. By decoupling high-level temporal motion from fine-grained spatial interactions using a latent diffusion model and a Grasping Variational Autoencoder (GraspVAE), the method improves the realism and physical plausibility of generated motions. This approach enhances conditional dependency between spatial grasp and temporal motion, optimizes data utilization, and reduces overfitting through latent space regularization. Evaluations on datasets like GRAB, DexYCB, and OakInk demonstrate superior performance in generating realistic motions for both single-hand and bimanual manipulations with unseen objects.",
        "Tags": [
            "3D Generation",
            "Avatars",
            "Latent Diffusion Model",
            "GraspVAE",
            "Unseen Object Generalization"
        ]
    },
    {
        "Title": "RORem: Training a Robust Object Remover with Human-in-the-Loop",
        "Authors": "Ruibin Li \u00b7 Tao Yang \u00b7 Song Guo \u00b7 Lei Zhang",
        "Abstract": "Despite the significant advancements, existing object removal methods struggle with incomplete removal, incorrect content synthesis and blurry synthesized regions, resulting in low success rates. Such issues are mainly caused by the lack of high-quality paired training data, as well as the self-supervised training paradigm adopted in these methods, which forces the model to in-paint the masked regions, leading to ambiguity between synthesizing the masked objects and restoring the background. To address these issues, we propose a semi-supervised learning strategy with human-in-the-loop to create high-quality paired training data, aiming to train a Robust Object Remover (RORem). We first collect 60K training pairs from open-source datasets to train an initial object removal model for generating removal samples, and then utilize human feedback to select a set of high-quality object removal pairs, with which we train a discriminator to automate the following training data generation process. By iterating this process for several rounds, we finally obtain a substantial object removal dataset with over 200K pairs. Fine-tuning the pre-trained stable diffusion model with this dataset, we obtain our RORem, which demonstrates state-of-the-art object removal performance in terms of both reliability and image quality. Particularly, RORem improves the object removal success rate over previous methods by more than 18\\%. The dataset, source code and trained model will be released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Existing object removal methods often suffer from incomplete removal, incorrect content synthesis, and blurry regions due to the lack of high-quality paired training data and self-supervised training paradigms. To address these issues, the authors propose RORem, a Robust Object Remover trained using a semi-supervised learning strategy with human-in-the-loop. The approach involves collecting 60K training pairs from open-source datasets to train an initial model, followed by iterative human feedback to refine the dataset, ultimately creating over 200K high-quality object removal pairs. Fine-tuning a pre-trained stable diffusion model with this dataset results in RORem, which achieves state-of-the-art performance, improving the object removal success rate by over 18% compared to previous methods.",
        "Tags": [
            "Object Removal & Replacement",
            "Data Augmentation",
            "Human-in-the-Loop",
            "Stable Diffusion",
            "Semi-Supervised Learning"
        ]
    },
    {
        "Title": "Star with Bilinear Mapping",
        "Authors": "Zelin Peng \u00b7 Yu Huang \u00b7 Zhengqin Xu \u00b7 feilong tang \u00b7 Ming Hu \u00b7 Xiaokang Yang \u00b7 Wei Shen",
        "Abstract": "Contextual modeling is crucial for robust visual representation learning, especially in computer vision. Although Transformers have become a leading architecture for vision tasks due to their attention mechanism, the quadratic complexity of full attention operations presents substantial computational challenges. To address this, we introduce Star with Bilinear Mapping (SBM), a Transformer-like architecture that achieves global contextual modeling with linear complexity. SBM employs a bilinear mapping module (BM) with low-rank decomposition strategy and star operations (element-wise multiplication) to efficiently capture global contextual information. Our model demonstrates competitive performance on image classification and semantic segmentation tasks, delivering significant computational efficiency gains compared to traditional attention-based models.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Star with Bilinear Mapping (SBM), a Transformer-like architecture designed to achieve global contextual modeling with linear complexity, addressing the computational challenges of traditional attention mechanisms. SBM utilizes a bilinear mapping module with low-rank decomposition and star operations to efficiently capture global contextual information. The model demonstrates competitive performance in image classification and semantic segmentation tasks while offering significant computational efficiency improvements over conventional attention-based models.",
        "Tags": [
            "Vision Transformer (ViT)",
            "Self-Supervised Learning",
            "Linear Complexity",
            "Bilinear Mapping",
            "Low-Rank Decomposition"
        ]
    },
    {
        "Title": "Hardware-Rasterized Ray-Based Gaussian Splatting",
        "Authors": "Samuel Rota Bul\u00f2 \u00b7 Lorenzo Porzi \u00b7 Nemanja Bartolovic \u00b7 Peter Kontschieder",
        "Abstract": "We present a novel, hardware rasterized rendering approach for ray-based 3D Gaussian Splatting (RayGS), obtaining both fast and high-quality results for novel view synthesis. Our work contains a mathematically rigorous and geometrically intuitive derivation about how to efficiently estimate all relevant quantities for rendering RayGS models, structured with respect to standard hardware rasterization shaders. Our solution is the first enabling rendering RayGS models at sufficiently high frame rates to support quality-sensitive applications like Virtual and Mixed Reality. Our second contribution enables alias-free rendering for RayGS, by addressing mip-related issues arising when rendering diverging scales during training and testing. We demonstrate significant performance gains, across different benchmark scenes, while retaining state-of-the-art appearance quality of RayGS.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a hardware-rasterized rendering approach for ray-based 3D Gaussian Splatting (RayGS), achieving fast and high-quality novel view synthesis. The authors provide a mathematically rigorous and geometrically intuitive derivation for efficiently estimating rendering quantities, structured for standard hardware rasterization shaders. The approach enables high frame rates suitable for quality-sensitive applications like Virtual and Mixed Reality. Additionally, it addresses mip-related issues to achieve alias-free rendering for RayGS, demonstrating significant performance gains while maintaining state-of-the-art appearance quality across various benchmark scenes.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Novel View Synthesis",
            "Hardware Rasterization",
            "Alias-Free Rendering",
            "Virtual Reality"
        ]
    },
    {
        "Title": "Object Detection using Event Camera: A MoE Heat Conduction based Detector and A New Benchmark Dataset",
        "Authors": "Xiao Wang \u00b7 Yu Jin \u00b7 Wentao Wu \u00b7 Wei Zhang \u00b7 Lin Zhu \u00b7 Bo Jiang \u00b7 Yonghong Tian",
        "Abstract": "Object detection in event streams has emerged as a cutting-edge research area, demonstrating superior performance in low-light conditions, scenarios with motion blur, and rapid movements. Current detectors leverage spiking neural networks, Transformers, or convolutional neural networks as their core architectures, each with its own set of limitations including restricted performance, high computational overhead, or limited local receptive fields. This paper introduces a novel MoE (Mixture of Experts) heat conduction-based object detection algorithm that strikingly balances accuracy and computational efficiency. Initially, we employ a stem network for event data embedding, followed by processing through our innovative MoE-HCO blocks. Each block integrates various expert modules to mimic heat conduction within event streams. Subsequently, an IoU-based query selection module is utilized for efficient token extraction, which is then channeled into a detection head for the final object detection process. Furthermore, we are pleased to introduce EvDET200K, a novel benchmark dataset for event-based object detection. Captured with a high-definition Prophesee EVK4-HD event camera, this dataset encompasses 10 distinct categories, 200,000 bounding boxes, and 10,054 samples, each spanning 2 to 5 seconds. We also provide comprehensive results from over 15 state-of-the-art detectors, offering a solid foundation for future research and comparison.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper presents a novel Mixture of Experts (MoE) heat conduction-based object detection algorithm designed for event streams, addressing limitations of current methods such as restricted performance and high computational overhead. The algorithm uses a stem network for event data embedding, processes data through MoE-HCO blocks that mimic heat conduction, and employs an IoU-based query selection module for efficient token extraction. Additionally, the authors introduce EvDET200K, a new benchmark dataset for event-based object detection, captured with a high-definition event camera, containing 10 categories, 200,000 bounding boxes, and 10,054 samples. The dataset provides a foundation for future research and comparison, with results from over 15 state-of-the-art detectors.",
        "Tags": [
            "Object Detection",
            "Event Camera",
            "Datasets and Benchmarks",
            "Mixture of Experts (MoE)",
            "Heat Conduction",
            "IoU-based Query Selection"
        ]
    },
    {
        "Title": "Test-Time Fine-Tuning of Image Compression Models for Multi-Task Adaptability",
        "Authors": "unki park \u00b7 Seongmoon Jeong \u00b7 Jang Youngchan \u00b7 Gyeong-Moon Park \u00b7 Jong Hwan Ko",
        "Abstract": "The field of computer vision initially focused on human visual perception and has progressively expanded to encompass machine vision, with ongoing advancements in technology expected to drive further expansion. Consequently, image compressors must effectively respond not only to human visual perception but also to the current and future machine vision tasks. Towards this goal, this paper proposes a Test-Time Fine-Tuning (TTFT) approach for adapting Learned Image Compression (LIC) to multiple tasks. A large-scale LIC model, originally trained for human perception, is adapted to both closed-set and open-set machine tasks through TTFT using Singular Value Decomposition based Low Rank Adaptation (SVD-LoRA). The SVD-LoRA layers are applied to both the encoder and decoder of backbone LIC model, with a modified learning scheme employed for the decoder during TTFT to train only the singular values, preventing excessive bitstream overhead. This enables instance-specific optimization for the target task. Experimental results demonstrate that the proposed method effectively adapts the backbone compressor to diverse machine tasks, outperforming competing methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a Test-Time Fine-Tuning (TTFT) approach to adapt Learned Image Compression (LIC) models for multi-task adaptability, addressing both human visual perception and machine vision tasks. The proposed method employs Singular Value Decomposition based Low Rank Adaptation (SVD-LoRA) to fine-tune a large-scale LIC model originally trained for human perception. SVD-LoRA layers are applied to both the encoder and decoder, with a modified learning scheme for the decoder to optimize singular values and minimize bitstream overhead. The approach enables instance-specific optimization for target tasks, demonstrating superior performance in adapting the backbone compressor to diverse machine tasks compared to competing methods.",
        "Tags": [
            "Image Compression",
            "Test-Time Fine-Tuning",
            "Multimodal Learning",
            "SVD-LoRA",
            "Instance-Specific Optimization",
            "Bitstream Overhead Reduction"
        ]
    },
    {
        "Title": "Post-pre-training for Modality Alignment in Vision-Language Foundation Models",
        "Authors": "Shin'ya Yamaguchi \u00b7 Dewei Feng \u00b7 Sekitoshi Kanai \u00b7 Kazuki Adachi \u00b7 Daiki Chijiwa",
        "Abstract": "Contrastive language image pre-training (CLIP) is an essential component of building modern vision-language foundation models. While CLIP demonstrates remarkable zero-shot performance on downstream tasks, the multi-modal feature spaces still suffer from a modality gap, which is a gap between image and text feature clusters and limits downstream task performance. Although existing works attempt to address the modality gap by modifying pre-training or fine-tuning, they struggle with heavy training costs with large datasets or degradations of zero-shot performance. This paper presents CLIP-Refine, a post-pre-training method for CLIP models at a phase between pre-training and fine-tuning. CLIP-Refine aims to align the feature space with 1 epoch training on small image-text datasets without zero-shot performance degradations. To this end, we introduce two techniques: random feature alignment (RaFA) and hybrid contrastive-distillation (HyCD). RaFA aligns the image and text features to follow a shared prior distribution by minimizing the distance to random reference vectors sampled from the prior. HyCD updates the model with hybrid soft labels generated by combining ground-truth image-text pair labels and outputs from the pre-trained CLIP model. This contributes to achieving both maintaining the past knowledge and learning new knowledge to align features. Our extensive experiments with multiple classification and retrieval tasks show that CLIP-Refine succeeds in mitigating the modality gap and improving the zero-shot performance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces CLIP-Refine, a post-pre-training method designed to address the modality gap in CLIP models, which hinders downstream task performance. CLIP-Refine operates between pre-training and fine-tuning phases, requiring only 1 epoch of training on small datasets to align image and text feature spaces without degrading zero-shot performance. The method employs two novel techniques: random feature alignment (RaFA) and hybrid contrastive-distillation (HyCD). RaFA aligns features to a shared prior distribution, while HyCD uses hybrid soft labels to preserve pre-trained knowledge and learn new alignments. Experiments across various tasks demonstrate that CLIP-Refine effectively reduces the modality gap and enhances zero-shot performance.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Self-Supervised Learning",
            "Modality Alignment",
            "Feature Space Optimization",
            "Hybrid Learning Techniques"
        ]
    },
    {
        "Title": "Rethinking Reconstruction and Denoising in the Dark: New Perspective, General Architecture and Beyond",
        "Authors": "Long Ma \u00b7 Tengyu Ma \u00b7 Ziye Li \u00b7 Yuetong Wang \u00b7 Jinyuan Liu \u00b7 Chengpei Xu \u00b7 Risheng Liu",
        "Abstract": "Recently, enhancing image quality in the original RAW domain has garnered significant attention, with denoising and reconstruction emerging as fundamental tasks. Although some works attempt to couple these tasks, they primarily focus on multi-stage learning while neglecting task associativity within a broader parameter space, leading to suboptimal performance. This work introduces a novel approach by rethinking denoising and reconstruction from a \"backbone-head\" perspective, leveraging the stronger shared parameter space offered by the backbone, compared to the encoder used in existing works. We derive task-specific heads with fewer parameters to mitigate learning pressure. By incorporating Chromaticity-aware attention into the backbone and introducing an adaptive denoising prior during training, we enable simultaneous reconstruction and denoising. Additionally, we design a dual-head interaction module to capture the latent correspondence between the two tasks, significantly enhancing multi-task accuracy. Extensive experiments validate the superiority of our approach.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper rethinks the tasks of denoising and reconstruction in the context of enhancing image quality in the RAW domain. It introduces a novel 'backbone-head' architecture that leverages a shared parameter space within the backbone, enabling more effective multi-task learning. The approach incorporates Chromaticity-aware attention and an adaptive denoising prior to facilitate simultaneous reconstruction and denoising. A dual-head interaction module is also introduced to capture the latent correspondence between the two tasks, improving multi-task accuracy. The proposed method demonstrates superior performance compared to existing approaches.",
        "Tags": [
            "Low-Level Vision",
            "Denoising",
            "Reconstruction",
            "Chromaticity-aware Attention",
            "Dual-head Interaction Module",
            "Adaptive Denoising Prior"
        ]
    },
    {
        "Title": "RUBIK: A Structured Benchmark for Image Matching across Geometric Challenges",
        "Authors": "Thibaut Loiseau \u00b7 Guillaume Bourmaud",
        "Abstract": "Camera pose estimation is crucial for many computer vision applications, yet existing benchmarks offer limited insight into method limitations across different geometric challenges. We introduce RUBIK, a novel benchmark that systematically evaluates image matching methods across well-defined geometric difficulty levels. Using three complementary criteria - overlap, scale ratio, and viewpoint angle - we organize 16.5K image pairs from nuScenes into 33 difficulty levels. Our comprehensive evaluation of 14 methods reveals that while recent detector-free approaches achieve the best performance (>47% success rate), they come with significant computational overhead compared to detector-based methods (150-600ms vs. 40-70ms). Even the best performing method succeeds on only 54.8% of the pairs, highlighting substantial room for improvement, particularly in challenging scenarios combining low overlap, large scale differences, and extreme viewpoint changes. Benchmark will be made publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces RUBIK, a structured benchmark designed to evaluate image matching methods across varying geometric challenges. RUBIK organizes 16.5K image pairs from the nuScenes dataset into 33 difficulty levels based on overlap, scale ratio, and viewpoint angle. The evaluation of 14 methods shows that detector-free approaches perform best (>47% success rate) but are computationally intensive (150-600ms) compared to detector-based methods (40-70ms). The top-performing method achieves only a 54.8% success rate, indicating significant room for improvement, especially in scenarios with low overlap, large scale differences, and extreme viewpoint changes.",
        "Tags": [
            "Feature Matching",
            "Datasets and Benchmarks",
            "Geometric Difficulty Levels",
            "Camera Pose Estimation",
            "Detector-Free vs Detector-Based Methods"
        ]
    },
    {
        "Title": "DreamRelation: Bridging Customization and Relation Generation",
        "Authors": "Qingyu Shi \u00b7 Lu Qi \u00b7 Jianzong Wu \u00b7 Jinbin Bai \u00b7 Jingbo Wang \u00b7 Yunhai Tong \u00b7 Xiangtai Li",
        "Abstract": "Customized image generation is essential for delivering personalized content based on user-provided prompts, enabling large-scale text-to-image diffusion models to better align with individual needs. However, existing models often neglect the relationships between customized objects in generated images. In contrast, this work addresses this gap by focusing on relation-aware customized image generation, which seeks to preserve the identities from image prompts while maintaining the predicate relations specified in text prompts. Specifically, we introduce DreamRelation, a framework that disentangles identity and relation learning using a carefully curated dataset. Our training data consists of relation-specific images, independent object images containing identity information, and text prompts to guide relation generation. Then, we propose two key modules to tackle the two main challenges\u2014generating accurate and natural relations, especially when significant pose adjustments are required, and avoiding object confusion in cases of overlap. First, we introduce a keypoint matching loss that effectively guides the model in adjusting object poses closely tied to their relationships. Second, we incorporate local features from the image prompts to better distinguish between objects, preventing confusion in overlapping cases. Extensive results on our proposed benchmarks demonstrate the superiority of DreamRelation in generating precise relations while preserving object identities across a diverse set of objects and relations. The source code and trained models will be made available to the public.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "DreamRelation introduces a framework for relation-aware customized image generation, addressing the challenge of preserving object identities and specified predicate relations in generated images. The framework utilizes a dataset of relation-specific images, independent object images, and text prompts to guide relation generation. It features two key modules: a keypoint matching loss for adjusting object poses related to their relationships and local features from image prompts to prevent object confusion in overlapping cases. DreamRelation demonstrates superior performance in generating precise relations while maintaining object identities across various objects and relations.",
        "Tags": [
            "Text-to-Image Generation",
            "Customization",
            "Relation-Aware Generation",
            "Keypoint Matching Loss",
            "Local Feature Integration"
        ]
    },
    {
        "Title": "3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer",
        "Authors": "Jiajun Deng \u00b7 Tianyu He \u00b7 Li Jiang \u00b7 Tianyu Wang \u00b7 Feras Dayoub \u00b7 Ian Reid",
        "Abstract": "Current 3D Large Multimodal Models (3D LMMs) have shown tremendous potential in 3D-vision-based dialogue and reasoning. However, how to further enhance 3D LMMs to achieve fine-grained scene understanding and facilitate flexible human-agent interaction remains a challenging problem. In this work, we introduce \\textbf{3D-LLaVA}, a simple yet highly powerful 3D LMM designed to act as an intelligent assistant in comprehending, reasoning, and interacting with the 3D world. Unlike existing top-performing methods that rely on complicated pipelines\u2014such as offline multi-view feature extraction or additional task-specific heads\u20143D-LLaVA adopts a minimalist design with integrated architecture and only takes point clouds as input. At the core of 3D-LLaVA is a new Omni Superpoint Transformer (OST), which integrates three functionalities: (1) a \\textbf{visual feature selector} that converts and selects visual tokens, (2) a \\textbf{visual prompt encoder} that embeds interactive visual prompts into the visual token space, and (3) a \\textbf{referring mask decoder} that produces 3D masks based on text description. This versatile OST is empowered by the hybrid pretraining to obtain perception priors and leveraged as the visual connector that bridges the 3D data to the LLM. After performing unified instruction tuning, our 3D-LLaVA reports impressive results on various benchmarks. The code and model will be released to promote future exploration.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces 3D-LLaVA, a 3D Large Multimodal Model (3D LMM) designed for fine-grained scene understanding and human-agent interaction in 3D environments. Unlike existing methods that use complex pipelines, 3D-LLaVA employs a minimalist architecture that processes point clouds directly. The model features an Omni Superpoint Transformer (OST) with three key functionalities: a visual feature selector, a visual prompt encoder, and a referring mask decoder. These components enable the model to convert visual tokens, embed interactive prompts, and generate 3D masks based on text descriptions. The OST is pretrained to bridge 3D data with a large language model (LLM), and after unified instruction tuning, 3D-LLaVA achieves strong performance on multiple benchmarks.",
        "Tags": [
            "3D Point Cloud",
            "Large Language Models (LLMs)",
            "Multimodal Large Language Models (MLLMs)",
            "Omni Superpoint Transformer",
            "Point Cloud Processing",
            "Human-Agent Interaction"
        ]
    },
    {
        "Title": "BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian Splatting",
        "Authors": "Yiren Lu \u00b7 Yunlai Zhou \u00b7 Disheng Liu \u00b7 tuo liang \u00b7 Yu Yin",
        "Abstract": "3D Gaussian Splatting (3DGS) has shown remarkable potential for static scene reconstruction, and recent advancements have extended its application to dynamic scenes. However, the quality of reconstructions depends heavily on high-quality input images and precise camera poses, which is not that trivial to fulfill in the real-world scenarios. Capturing dynamic scenes with handheld monocular cameras, for instance, typically involves simultaneous movement of both the camera and objects within a single exposure. This combined motion frequently results in image blur that existing methods cannot adequately handle. To address these challenges, we introduce BARD-GS, a novel approach for robust dynamic scene reconstruction that effectively handles blurry inputs and imprecise camera poses. Our method comprises two main components: 1) camera motion deblurring and 2) object motion deblurring. By explicitly decomposing motion blur into camera motion blur and object motion blur and modeling them separately, we achieve significantly improved rendering results in dynamic regions. In addition, we collect a real-world motion blur dataset of dynamic scenes to evaluate our approach. Extensive experiments demonstrate that BARD-GS effectively reconstructs high-quality dynamic scenes under realistic conditions, significantly outperforming existing methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "BARD-GS introduces a novel approach for robust dynamic scene reconstruction using 3D Gaussian Splatting (3DGS), addressing challenges posed by blurry inputs and imprecise camera poses. The method decomposes motion blur into camera motion blur and object motion blur, modeling them separately to achieve improved rendering results in dynamic regions. A real-world motion blur dataset is collected for evaluation, and experiments show that BARD-GS outperforms existing methods in reconstructing high-quality dynamic scenes under realistic conditions.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "3D Reconstruction",
            "Deblur",
            "Motion Decomposition",
            "Dynamic Scene Reconstruction",
            "Blur Handling"
        ]
    },
    {
        "Title": "DV-Matcher: Deformation-based Non-rigid Point Cloud Matching Guided by Pre-trained Visual Features",
        "Authors": "Zhangquan Chen \u00b7 Puhua Jiang \u00b7 Ruqi Huang",
        "Abstract": "In this paper, we present DV-Matcher, a novel learning-based framework for estimating dense correspondences between non-rigidly deformable point clouds. Learning directly from unstructured point clouds without meshing or manual labelling, our framework delivers high-quality dense correspondences, which is of significant practical utility in point cloud processing. Our key contributions are two-fold: First, we propose a scheme to inject prior knowledge from pre-trained vision models into geometric feature learning, which effectively complements the local nature of geometric features with global and semantic information; Second, we propose a novel deformation-based module to promote the extrinsic alignment induced by the learned correspondences, which effectively enhances the feature learning. Experimental results show that our method achieves state-of-the-art results in matching non-rigid point clouds in both near-isometric and heterogeneous shape collection as well as more realistic partial and noisy data.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces DV-Matcher, a learning-based framework for estimating dense correspondences between non-rigidly deformable point clouds. The framework operates directly on unstructured point clouds, eliminating the need for meshing or manual labeling. Key contributions include a method to integrate prior knowledge from pre-trained vision models into geometric feature learning, enhancing local geometric features with global and semantic information, and a novel deformation-based module that improves extrinsic alignment through learned correspondences. The framework demonstrates state-of-the-art performance in matching non-rigid point clouds across various conditions, including near-isometric and heterogeneous shapes, as well as partial and noisy data.",
        "Tags": [
            "3D Point Cloud",
            "3D Registration",
            "Deformation-based Matching",
            "Pre-trained Visual Features",
            "Non-rigid Point Cloud Alignment"
        ]
    },
    {
        "Title": "DEIM: DETR with Improved Matching for Fast Convergence",
        "Authors": "Shihua Huang \u00b7 Zhichao Lu \u00b7 Xiaodong Cun \u00b7 Yongjun YU \u00b7 Xiao Zhou \u00b7 Xi Shen",
        "Abstract": "We introduce DEIM, an innovative and efficient training framework designed to accelerate convergence in real-time object detection with Transformer-based architectures (DETR). To mitigate the sparse supervision inherent in one-to-one (O2O) matching in DETR models, DEIM employs a Dense O2O matching strategy. This approach increases the number of positive samples per image by incorporating additional targets, using standard data augmentation techniques. While Dense O2O matching speeds up convergence, it also introduces numerous low-quality matches that could affect performance. To address this, we propose the Matchability-Aware Loss (MAL), a novel loss function that optimizes matches across various quality levels, enhancing the effectiveness of Dense O2O.    Extensive experiments on the COCO dataset validate the efficacy of DEIM. When integrated with RT-DETR and D-FINE, it consistently boosts performance while reducing training time by 50\\%. Notably, paired with RT-DETRv2, DEIM achieves 53.2\\% AP in a single day of training on an NVIDIA 4090 GPU. Additionally, DEIM-trained real-time models outperform leading real-time object detectors, with DEIM-D-FINE-L and DEIM-D-FINE-X achieving 54.7\\% and 56.4\\% AP at 124 and 78 FPS on an NVIDIA T4 GPU, respectively, without the need for additional data. We believe DEIM sets a new baseline for advancements in real-time object detection. Our code will be made available upon publication.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "DEIM is introduced as an efficient training framework to accelerate convergence in real-time object detection using Transformer-based architectures (DETR). It addresses the sparse supervision issue in one-to-one (O2O) matching by employing a Dense O2O matching strategy, which increases positive samples per image through standard data augmentation. To handle low-quality matches introduced by this strategy, DEIM proposes the Matchability-Aware Loss (MAL), a novel loss function that optimizes matches across quality levels. Experiments on the COCO dataset demonstrate DEIM's effectiveness, significantly reducing training time while improving performance. DEIM-trained models outperform leading real-time detectors, achieving high accuracy and speed without additional data.",
        "Tags": [
            "DETR (Detection Transformer)",
            "Object Detection",
            "Dense O2O Matching",
            "Matchability-Aware Loss (MAL)",
            "Real-Time Object Detection"
        ]
    },
    {
        "Title": "DAMM-Diffusion: Learning Divergence-Aware Multi-Modal Diffusion Model for Nanoparticles Distribution Prediction",
        "Authors": "Junjie Zhou \u00b7 Shouju Wang \u00b7 Yuxia Tang \u00b7 Qi Zhu \u00b7 Daoqiang Zhang \u00b7 WEI SHAO",
        "Abstract": "The prediction of nanoparticles (NPs) distribution is crucial for the diagnosis and treatment of tumors. Recent studies indicate that the heterogeneity of tumor microenvironment (TME) highly affects the distribution of NPs across tumors. Hence, it has become a research hotspot to generate the NPs distribution by the aid of multi-modal TME components. However, the distribution divergence among multi-modal TME components may cause side effects i.e., the best uni-modal model may outperform the joint generative model. To address the above issues, we propose a \\Divergence-Aware Multi-Modal Diffusion model (i.e., DAMM-Diffusion) to adaptively generate the prediction results from uni-modal and multi-modal branches in a unified network. In detail, the uni-modal branch is composed of the U-Net architecture while the multi-modal branch extends it by introducing two novel fusion modules i.e., Multi-Modal Fusion Module (MMFM) and Uncertainty-Aware Fusion Module (UAFM). Specifically, the MMFM is proposed to fuse features from multiple modalities, while the UAFM module is introduced to learn the uncertainty map for cross-attention computation. Following the individual prediction results from each branch, the Divergence-Aware Multi-Modal Predictor (DAMMP) module is proposed to assess the consistency of multi-modal data with the uncertainty map, which determines whether the final prediction results come from  multi-modal or uni-modal predictions. We predict the NPs distribution given the TME components of tumor vessels and cell nuclei, and the experimental results show that DAMM-Diffusion can generate the distribution of NPs with higher accuracy than the comparing methods. Additional results on the multi-modal brain image synthesis task further validate the effectiveness of the proposed method.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The prediction of nanoparticles (NPs) distribution is critical for tumor diagnosis and treatment, influenced by the heterogeneity of the tumor microenvironment (TME). To address the challenge of distribution divergence among multi-modal TME components, the authors propose DAMM-Diffusion, a Divergence-Aware Multi-Modal Diffusion model. This model adaptively generates predictions from both uni-modal and multi-modal branches within a unified network. The uni-modal branch uses a U-Net architecture, while the multi-modal branch incorporates two novel fusion modules: the Multi-Modal Fusion Module (MMFM) and the Uncertainty-Aware Fusion Module (UAFM). The MMFM fuses features from multiple modalities, and the UAFM learns an uncertainty map for cross-attention computation. The Divergence-Aware Multi-Modal Predictor (DAMMP) module assesses the consistency of multi-modal data with the uncertainty map to determine the final prediction source. The model demonstrates higher accuracy in predicting NPs distribution compared to existing methods, validated through experiments on tumor vessels and cell nuclei data, as well as multi-modal brain image synthesis tasks.",
        "Tags": [
            "Diffusion Models",
            "Medical Image Analysis",
            "Uncertainty-Aware Fusion",
            "Multi-Modal Fusion",
            "Tumor Microenvironment Analysis"
        ]
    },
    {
        "Title": "ViiNeuS: Volumetric Initialization for Implicit Neural Surface reconstruction of urban scenes with limited image overlap",
        "Authors": "Hala Djeghim \u00b7 Nathan Piasco \u00b7 Moussab Bennehar \u00b7 Luis Guillermo Roldao Jimenez \u00b7 Dzmitry Tsishkou \u00b7 D\u00e9sir\u00e9 Sidib\u00e9",
        "Abstract": "Neural implicit surface representation methods have recently shown impressive 3D reconstruction results. However, existing solutions struggle to reconstruct driving scenes due to their large size, highly complex nature and limited visual observation overlap.Hence, to achieve accurate reconstructions, additional supervision data such as LiDAR, strong geometric priors, and long training times are required.To tackle such limitations, we present ViiNeuS, a new hybrid implicit surface learning method that efficiently initializes the signed distance field to reconstruct large driving scenes from 2D street view images.ViiNeuS's hybrid architecture models two separate implicit fields: one representing the volumetric density of the scene, and another one representing the signed distance to the surface.To accurately reconstruct urban outdoor driving scenarios, we introduce a novel volume-rendering strategy that relies on self-supervised probabilistic density estimation to sample points near the surface and transition progressively from volumetric to surface representation. Our solution permits a proper and fast initialization of the signed distance field without relying on any geometric prior on the scene, compared to concurrent methods.By conducting extensive experiments on four outdoor driving datasets, we show that ViiNeuS can learn an accurate and detailed 3D surface scene representation in various driving scenarios while being two times faster to train compared to previous state-of-the-art solutions.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces ViiNeuS, a hybrid implicit surface learning method designed to reconstruct large urban driving scenes from 2D street view images with limited visual overlap. ViiNeuS employs a novel architecture that models two implicit fields: one for volumetric density and another for signed distance to the surface. A key innovation is a self-supervised probabilistic density estimation strategy that facilitates a smooth transition from volumetric to surface representation, enabling accurate and efficient initialization of the signed distance field without relying on geometric priors. The method demonstrates superior performance in reconstructing detailed 3D surfaces across various driving scenarios, achieving faster training times compared to existing state-of-the-art solutions.",
        "Tags": [
            "3D Reconstruction",
            "Implicit Neural Representations",
            "Self-Supervised Learning",
            "Volume Rendering",
            "Urban Scene Reconstruction"
        ]
    },
    {
        "Title": "Dense-SfM: Structure from Motion with Dense Consistent Matching",
        "Authors": "JongMin Lee \u00b7 Sungjoo Yoo",
        "Abstract": "We present Dense-SfM, a novel Structure from Motion (SfM) framework designed for dense and accurate 3D reconstruction from multi-view images. Sparse keypoint matching, which traditional SfM methods often rely on, limits both accuracy and point density, especially in texture-less areas. Dense-SfM addresses this limitation by integrating dense matching with a Gaussian Splatting (GS) based track extension which gives more consistent, longer feature tracks. To further improve reconstruction accuracy, Dense-SfM is equipped with a multi-view kernelized matching module leveraging transformer and Gaussian Process architectures, for robust track refinement across multi-views. Evaluations on the ETH3D and Texture-Poor SfM datasets show that Dense-SfM offers significant improvements in accuracy and density over state-of-the-art methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Dense-SfM introduces a novel Structure from Motion (SfM) framework for dense and accurate 3D reconstruction from multi-view images. Unlike traditional SfM methods that rely on sparse keypoint matching, Dense-SfM integrates dense matching with a Gaussian Splatting-based track extension to achieve more consistent and longer feature tracks. Additionally, it employs a multi-view kernelized matching module, utilizing transformer and Gaussian Process architectures, to refine tracks robustly across multiple views. The framework demonstrates significant improvements in accuracy and density over state-of-the-art methods on benchmark datasets.",
        "Tags": [
            "3D Reconstruction",
            "3DGS (Gaussian Splatting)",
            "Dense Matching",
            "Transformer Architecture",
            "Gaussian Process"
        ]
    },
    {
        "Title": "Dense Dispersed Structured Light for Hyperspectral 3D Imaging of Dynamic Scenes",
        "Authors": "Suhyun Shin \u00b7 Seungwoo Yoon \u00b7 Ryota Maeda \u00b7 Seung-Hwan Baek",
        "Abstract": "Hyperspectral 3D imaging captures both depth maps and hyperspectral images, enabling comprehensive geometric and material analysis. Recent methods achieve high spectral and depth accuracy; however, they require long acquisition times\u2014often over several minutes\u2014or rely on large, expensive systems, restricting their use to static scenes. We present Dense Dispersed Structured Light (DDSL), an accurate hyperspectral 3D imaging method for dynamic scenes that utilizes stereo RGB cameras and an RGB projector equipped with an affordable diffraction grating film.We design spectrally multiplexed DDSL patterns that significantly reduce the number of required projector patterns, thereby accelerating acquisition speed. Additionally, we formulate an image formation model and a reconstruction method to estimate a hyperspectral image and depth map from captured stereo images. As the first practical and accurate hyperspectral 3D imaging method for dynamic scenes, we experimentally demonstrate that DDSL achieves a spectral resolution of 15.5 nm full width at half maximum (FWHM), a depth error of 4 mm, and a frame rate of 6.6 fps.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Dense Dispersed Structured Light (DDSL), a novel method for hyperspectral 3D imaging of dynamic scenes. Unlike existing methods that are either slow or require expensive equipment, DDSL uses stereo RGB cameras and an RGB projector with a diffraction grating film to achieve high spectral and depth accuracy. The method employs spectrally multiplexed patterns to reduce the number of required projector patterns, thus speeding up acquisition. An image formation model and reconstruction method are developed to estimate hyperspectral images and depth maps from stereo images. DDSL achieves a spectral resolution of 15.5 nm FWHM, a depth error of 4 mm, and a frame rate of 6.6 fps, making it the first practical and accurate hyperspectral 3D imaging method for dynamic scenes.",
        "Tags": [
            "3D Reconstruction",
            "Hyperspectral Imaging",
            "Dynamic Scene Imaging",
            "Spectral Multiplexing",
            "Diffraction Grating"
        ]
    },
    {
        "Title": "SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models",
        "Authors": "Jaerin Lee \u00b7 Daniel Jung \u00b7 Kanggeon Lee \u00b7 Kyoung Mu Lee",
        "Abstract": "We introduce SemanticDraw, a new paradigm of interactive content creation where high-quality images are generated in near real-time from given multiple hand-drawn regions, each encoding prescribed semantic meaning. In order to maximize the productivity of content creators and to fully realize their artistic imagination, it requires both quick interactive interfaces and fine-grained regional controls in their tools. Despite astonishing generation quality from recent diffusion models, we find that existing approaches for regional controllability are very slow (52 seconds for 512 x 512 image) while not compatible with acceleration methods such as LCM, blocking their huge potential in interactive content creation. From this observation, we build our solution for interactive content creation in two steps: (1) we establish compatibility between region-based controls and acceleration techniques for diffusion models, maintaining high fidelity of multi-prompt image generation with x 10 reduced number of inference steps, (2) we increase the generation throughput with our new multi-prompt stream batch pipeline, enabling low-latency generation from multiple, region-based text prompts on a single RTX 2080 Ti GPU. Our proposed framework is generalizable to any existing diffusion models and acceleration schedulers, allowing sub-second (0.64 seconds) image content creation application upon well-established image diffusion models. The demo application can be found in Supplementary Material.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "SemanticDraw introduces a novel approach to interactive content creation, enabling high-quality image generation in near real-time from multiple hand-drawn regions with specified semantic meanings. The framework addresses the limitations of existing diffusion models, which are slow and incompatible with acceleration techniques, by establishing compatibility between region-based controls and acceleration methods. This compatibility reduces the number of inference steps by a factor of 10 while maintaining high fidelity. Additionally, the framework enhances generation throughput with a multi-prompt stream batch pipeline, achieving sub-second image generation on a single RTX 2080 Ti GPU. The solution is generalizable to existing diffusion models and acceleration schedulers, making it suitable for real-time interactive content creation.",
        "Tags": [
            "Diffusion Models",
            "Image Generation",
            "Real-Time Generation",
            "Region-Based Control",
            "Multi-Prompt Generation"
        ]
    },
    {
        "Title": "Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose Interaction",
        "Authors": "Dong Li \u00b7 Wenqi Zhong \u00b7 Wei Yu \u00b7 Yingwei Pan \u00b7 Dingwen Zhang \u00b7 Ting Yao \u00b7 Junwei Han \u00b7 Tao Mei",
        "Abstract": "Video virtual try-on aims to seamlessly dress a subject in a video with a specific garment. The primary challenge involves preserving the visual authenticity of the garment while dynamically adapting to the pose and physique of the subject. While existing methods have predominantly focused on image-based virtual try-on, extending these techniques directly to videos often results in temporal inconsistencies. Most current video virtual try-on approaches alleviate this challenge by incorporating temporal modules, yet still overlook the critical spatiotemporal pose interactions between human and garment. Effective pose interactions in videos should not only consider spatial alignment between human and garment poses in each frame but also account for the temporal dynamics of human poses throughout the entire video. With such motivation, we propose a new framework, namely Dynamic Pose Interaction Diffusion Models (DPIDM), to leverage diffusion models to delve into dynamic pose interactions for video virtual try-on. Technically, DPIDM introduces a skeleton-based pose adapter to integrate synchronized human and garment poses into the denoising network. A hierarchical attention module is then exquisitely designed to model intra-frame human-garment pose interactions and long-term human pose dynamics across frames through pose-aware spatial and temporal attention mechanisms. Moreover, DPIDM capitalizes on a temporal regularized attention loss between consecutive frames to enhance temporal consistency. Extensive experiments conducted on VITON-HD, VVT and ViViD datasets demonstrate the superiority of our DPIDM against the baseline methods. Notably, DPIDM achieves VFID score of 0.506 on VVT dataset, leading to 60.5% improvement over the state-of-the-art GPD-VVTO approach.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Video virtual try-on aims to dress a subject in a video with a specific garment while maintaining visual authenticity and adapting to the subject's pose and physique. Existing methods often result in temporal inconsistencies when applied to videos. To address this, the paper introduces Dynamic Pose Interaction Diffusion Models (DPIDM), a framework that leverages diffusion models to explore dynamic pose interactions. DPIDM incorporates a skeleton-based pose adapter and a hierarchical attention module to model intra-frame human-garment pose interactions and long-term human pose dynamics. It also uses a temporal regularized attention loss to enhance temporal consistency. The framework demonstrates superior performance on benchmark datasets compared to existing methods.",
        "Tags": [
            "Video Generation",
            "Diffusion Models",
            "Temporal Consistency",
            "Pose Interaction",
            "Hierarchical Attention"
        ]
    },
    {
        "Title": "PromptHash:Affinity-Prompted Collaborative Cross-Modal Learning for Adaptive Hashing Retrieval",
        "Authors": "Qiang Zou \u00b7 Qiang Zou \u00b7 Shuli Cheng \u00b7 Jiayi Chen",
        "Abstract": "Cross-modal hashing is a promising approach for efficient data retrieval and storage optimization. However, contemporary methods exhibit significant limitations in semantic preservation, contextual integrity, and information redundancy, which constrains retrieval efficacy. We present PromptHash, an innovative framework leveraging affinity prompt-aware collaborative learning for adaptive cross-modal hashing. We propose an end-to-end framework for affinity-prompted collaborative hashing, with the following fundamental technical contributions: (i) a text affinity prompt learning mechanism that preserves contextual information while maintaining parameter efficiency, (ii) an adaptive gated selection fusion architecture that synthesizes State Space Model with Transformer network for precise cross-modal feature integration, and (iii) a prompt affinity alignment strategy that bridges modal heterogeneity through hierarchical contrastive learning. To the best of our knowledge, this study presents the first investigation into affinity prompt awareness within collaborative cross-modal adaptive hash learning, establishing a paradigm for enhanced semantic consistency across modalities. Through comprehensive evaluation on three benchmark multi-label datasets, PromptHash demonstrates substantial performance improvements over existing approaches. Notably, on the NUS-WIDE dataset, our method achieves significant gains of 18.22% and 18.65% in image-to-text and text-to-image retrieval tasks, respectively. The code is publicly available at https://anonymous.4open.science/r/PromptHash-8ED3.",
        "Link": "https://anonymous.4open.science/r/PromptHash-8ED3",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "PromptHash introduces a novel framework for adaptive cross-modal hashing, addressing limitations in semantic preservation and contextual integrity. The framework features a text affinity prompt learning mechanism, an adaptive gated selection fusion architecture combining State Space Model with Transformer, and a prompt affinity alignment strategy using hierarchical contrastive learning. This approach enhances semantic consistency across modalities and demonstrates significant performance improvements on benchmark datasets, notably achieving gains of 18.22% and 18.65% in image-to-text and text-to-image retrieval tasks on the NUS-WIDE dataset.",
        "Tags": [
            "Cross-Modal Hashing",
            "Affinity Prompt Learning",
            "Transformer",
            "State Space Model",
            "Hierarchical Contrastive Learning",
            "Adaptive Gated Selection Fusion",
            "Hierarchical Contrastive Learning",
            "Affinity Prompt Alignment"
        ]
    },
    {
        "Title": "Classifier-Free Guidance inside the Attraction Basin May Cause Memorization",
        "Authors": "Anubhav Jain \u00b7 Yuya Kobayashi \u00b7 Takashi Shibuya \u00b7 Yuhta Takida \u00b7 Nasir Memon \u00b7 Julian Togelius \u00b7 Yuki Mitsufuji",
        "Abstract": "Diffusion models are prone to exactly reproduce images from the training data. This exact reproduction of the training data is concerning as it can lead to copyright infringement and/or leakage of privacy-sensitive information. In this paper, we present a novel way to understand the memorization phenomenon, and propose a simple yet effective approach to mitigate memorization. We argue that memorization occurs because of an attraction basin in the denoising process which steers the diffusion trajectory towards a memorized image. However,  this can be mitigated by guiding the diffusion trajectory away from the attraction basin by not applying classifier-free guidance until an ideal transition point occurs. This leads to the generation of non-memorized images that are high in image quality and well aligned with the conditioning mechanism. To further improve on this, we present a new guidance technique, opposite guidance, that escapes the attraction basin sooner in the denoising process. We demonstrate the existence of attraction basins in various scenarios in which memorization occurs, and we show that our proposed approach successfully mitigates memorization.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Diffusion models often reproduce training data exactly, raising concerns about copyright infringement and privacy breaches. This paper explains memorization as a result of an attraction basin in the denoising process, which directs the diffusion trajectory toward memorized images. To mitigate this, the authors propose avoiding classifier-free guidance until a transition point is reached, thereby generating high-quality, non-memorized images aligned with the conditioning mechanism. Additionally, they introduce 'opposite guidance' to escape the attraction basin earlier in the denoising process. The study demonstrates the existence of attraction basins in memorization scenarios and validates the effectiveness of the proposed approach.",
        "Tags": [
            "Diffusion Models",
            "Memorization",
            "Attraction Basin",
            "Classifier-Free Guidance",
            "Opposite Guidance"
        ]
    },
    {
        "Title": "MirrorVerse: Pushing Diffusion Models to Realistically Reflect the World",
        "Authors": "Ankit Dhiman \u00b7 Manan Shah \u00b7 R. Venkatesh Babu",
        "Abstract": "Diffusion models have become central to various image editing tasks, yet they often fail to fully adhere to physical laws, particularly with effects like shadows, reflections, and occlusions. In this work, we address the challenge of generating photorealistic mirror reflections using diffusion-based generative models. Despite extensive training data, existing diffusion models frequently overlook the nuanced details crucial to authentic mirror reflections. Recent approaches have attempted to resolve this by creating synthetic datasets and framing reflection generation as an inpainting task; however, they struggle to generalize across different object orientations and positions relative to the mirror.Our method overcomes these limitations by introducing key augmentations into the synthetic data pipeline: (1) random object positioning, (2) randomized rotations, and (3) grounding of objects, significantly enhancing generalization across poses and placements. To further address spatial relationships and occlusions in scenes with multiple objects, we implement a strategy to pair objects during dataset generation, resulting in a dataset robust enough to handle these complex scenarios. Achieving generalization to real-world scenes remains a challenge, so we introduce a three-stage training curriculum to train a conditional generative model, aimed at improving real-world performance. We provide extensive qualitative and quantitative evaluations to support our approach, and the code and data will be released for research purposes.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the limitations of diffusion models in generating photorealistic mirror reflections, which often fail to adhere to physical laws. The authors propose a method to enhance the realism of mirror reflections by introducing key augmentations in the synthetic data pipeline, including random object positioning, randomized rotations, and grounding of objects. These augmentations improve generalization across different object orientations and positions relative to the mirror. Additionally, the paper introduces a strategy to handle spatial relationships and occlusions in scenes with multiple objects, and a three-stage training curriculum to improve real-world performance. The approach is supported by extensive qualitative and quantitative evaluations.",
        "Tags": [
            "Diffusion Models",
            "Image Editing",
            "Photorealistic Reflections",
            "Synthetic Data Augmentation",
            "Conditional Generative Models"
        ]
    },
    {
        "Title": "Controllable Human Image Generation with Personalized Multi-Garments",
        "Authors": "Yisol Choi \u00b7 Sangkyung Kwak \u00b7 Sihyun Yu \u00b7 Hyungwon Choi \u00b7 Jinwoo Shin",
        "Abstract": "We present BootControl, a novel framework based on text-to-image diffusion models for controllable human image generation with multiple reference garments.Here, the main bottleneck is data acquisition for training: collecting a large-scale dataset of high-quality reference garment images per human subject is quite challenging, i.e., ideally, one needs to manually gather every single garment photograph worn by each human.To address this, we propose a data generation pipeline to construct a large synthetic dataset, consisting of human and multiple-garment pairs, by introducing a model to extract any reference garment images from each human image.To ensure data quality, we also propose a filtering strategy to remove undesirable generated data based on measuring perceptual similarities between the garment presented in human image and extracted garment.Finally, by utilizing the constructed synthetic dataset, we train a diffusion model having two parallel denoising paths that use multiple garment images as conditions to generate human images while preserving their fine-grained details.We further show the wide-applicability of our framework by adapting it to different types of reference-based generation in the fashion domain, including virtual try-on, and controllable human image generation with other conditions, e.g., pose, face, etc.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces BootControl, a framework leveraging text-to-image diffusion models for controllable human image generation using multiple reference garments. A key challenge addressed is the difficulty in acquiring large-scale, high-quality datasets of garments worn by individuals. The authors propose a synthetic data generation pipeline that extracts garment images from human photos and filters them based on perceptual similarity to ensure quality. This dataset is used to train a diffusion model with dual denoising paths, enabling the generation of detailed human images conditioned on multiple garments. The framework's versatility is demonstrated through applications in virtual try-on and other conditional human image generation tasks in the fashion domain.",
        "Tags": [
            "Diffusion Models",
            "Image Generation",
            "Data Augmentation",
            "Fashion",
            "Virtual Try-On",
            "Text-to-Image",
            "Synthetic Data Generation",
            "Perceptual Similarity"
        ]
    },
    {
        "Title": "Vision-Language Embodiment for Monocular Depth Estimation",
        "Authors": "Jinchang Zhang \u00b7 Guoyu Lu",
        "Abstract": "Depth estimation is a core problem in robotic perception and vision tasks, but 3D reconstruction from a single image presents inherent uncertainties. With the development of deep learning, current methods primarily rely on inter-image relationships to train supervised models, often overlooking intrinsic information provided by the camera itself. From the perspective of embodied intelligence, perception and understanding are not only based on external data inputs but are also closely linked to the physical environment in which the model is embedded. Following this concept, we propose a method that embeds the camera model and its physical characteristics into a deep learning model to compute Embodied Scene Depth through interactions with road environments. This approach leverages the intrinsic properties of the camera and provides robust depth priors without the need for additional equipment.By combining Embodied Scene Depth with RGB image features, the model gains a comprehensive perspective of both geometric and visual details. Additionally, we incorporate text descriptions containing environmental content and depth information as another dimension of embodied intelligence, embedding them as scale priors for scene understanding, thus enriching the model\u2019s perception of the scene. This integration of image and language \u2014 two inherently ambiguous modalities \u2014 leverages their complementary strengths for monocular depth estimation, ensuring a more realistic understanding of scenes in diverse environments. We validated this method on outdoor datasets KITTI and CityScapes, with experimental results demonstrating that this embodied intelligence-based depth estimation method consistently enhances model performance across different scenes.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of monocular depth estimation by integrating embodied intelligence principles into a deep learning model. The proposed method incorporates the camera's intrinsic properties and physical characteristics to compute Embodied Scene Depth, which interacts with road environments to provide robust depth priors without additional equipment. By combining this with RGB image features and text descriptions containing environmental and depth information, the model achieves a comprehensive understanding of both geometric and visual details. This integration of image and language modalities leverages their complementary strengths, enhancing the model's performance in diverse environments. The method was validated on the KITTI and CityScapes datasets, demonstrating consistent improvements in depth estimation accuracy.",
        "Tags": [
            "Depth Estimation",
            "Embodied AI",
            "Vision-Language Models (VLMs)",
            "Monocular Depth Estimation",
            "Camera Intrinsic Properties",
            "Text-Image Integration"
        ]
    },
    {
        "Title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model",
        "Authors": "Zongjian Li \u00b7 Bin Lin \u00b7 Yang Ye \u00b7 Liuhan Chen \u00b7 Xinhua Cheng \u00b7 Shenghai Yuan \u00b7 Li Yuan",
        "Abstract": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional latent space, becoming a key component of most Latent Video Diffusion Models (LVDMs) to reduce model training costs. However, as the resolution and duration of generated videos increase, the encoding cost of Video VAEs becomes a limiting bottleneck in training LVDMs. Moreover, the block-wise inference method adopted by most LVDMs can lead to discontinuities of latent space when processing long-duration videos. The key to addressing the computational bottleneck lies in decomposing videos into distinct components and efficiently encoding the critical information. Wavelet transform can decompose videos into multiple frequency-domain components and improve the efficiency significantly, we thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages multi-level wavelet transform to facilitate low-frequency energy flow into latent representation. Furthermore, we introduce a method called Causal Cache, which maintains the integrity of latent space during block-wise inference. Compared to state-of-the-art video VAEs, WF-VAE demonstrates superior performance in both PSNR and LPIPS metrics, achieving 2\u00d7 higher throughput and 4\u00d7 lower memory consumption while maintaining competitive reconstruction quality. Our code and models will be released to inspire further research.",
        "Link": "https://github.com/PKU-YuanGroup/WF-VAE",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Wavelet Flow VAE (WF-VAE), a novel Video Variational Autoencoder that leverages multi-level wavelet transform to enhance the encoding of videos into a low-dimensional latent space, addressing the computational bottleneck in Latent Video Diffusion Models (LVDMs). WF-VAE improves efficiency by decomposing videos into frequency-domain components, facilitating low-frequency energy flow into latent representations. Additionally, the Causal Cache method is proposed to maintain latent space integrity during block-wise inference. WF-VAE outperforms state-of-the-art video VAEs in PSNR and LPIPS metrics, achieving significantly higher throughput and lower memory consumption while maintaining competitive reconstruction quality.",
        "Tags": [
            "Video Generation",
            "Diffusion Models",
            "Wavelet Transform",
            "Latent Space Integrity",
            "Causal Cache",
            "Frequency-Domain Decomposition"
        ]
    },
    {
        "Title": "ART: Anonymous Region Transformer for Variable Multi-Layer Transparent Image Generation",
        "Authors": "Yifan Pu \u00b7 Yiming Zhao \u00b7 Zhicong Tang \u00b7 Ruihong Yin \u00b7 Haoxing Ye \u00b7 Yuhui Yuan \u00b7 Dong Chen \u00b7 Jianmin Bao \u00b7 Sirui Zhang \u00b7 Yanbin Wang \u00b7 Lin Liang \u00b7 Lijuan Wang \u00b7 Ji Li \u00b7 Xiu Li \u00b7 Zhouhui Lian \u00b7 Gao Huang \u00b7 Baining Guo",
        "Abstract": "Multi-layer image generation is a fundamental task that enables users to isolate, select, and edit specific image layers, thereby revolutionizing interactions with generative models. In this paper, we introduce the Anonymous Region Transformer (ART), which facilitates the direct generation of variable multi-layer transparent images based on a global text prompt and an anonymous region layout. Inspired by Schema theory, this anonymous region layout allows the generative model to autonomously determine which set of visual tokens should align with which text tokens, which is in contrast to the previously dominant semantic layout for the image generation task. In addition, the layer-wise region crop mechanism, which only selects the visual tokens belonging to each anonymous region, significantly reduces attention computation costs and enables the efficient generation of images with numerous distinct layers (e.g., 50+). When compared to the full attention approach, our method is over 12 times faster and exhibits fewer layer conflicts. Furthermore, we propose a high-quality multi-layer transparent image autoencoder that supports the direct encoding and decoding of the transparency of variable multi-layer images in a joint manner. By enabling precise control and scalable layer generation, ART establishes a new paradigm for interactive content creation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces the Anonymous Region Transformer (ART), a novel approach for generating variable multi-layer transparent images using a global text prompt and an anonymous region layout. Inspired by Schema theory, ART allows the model to autonomously align visual tokens with text tokens, diverging from traditional semantic layouts. A layer-wise region crop mechanism reduces attention computation costs, enabling efficient generation of images with numerous layers (e.g., 50+), making it over 12 times faster than full attention approaches with fewer layer conflicts. Additionally, ART includes a high-quality multi-layer transparent image autoencoder for joint encoding and decoding of transparency, offering precise control and scalable layer generation for interactive content creation.",
        "Tags": [
            "Image Generation",
            "Multimodal Learning",
            "Anonymous Region Layout",
            "Layer-wise Region Crop",
            "Transparency Autoencoder"
        ]
    },
    {
        "Title": "CTRL-O: Language-Controllable Object-Centric Visual Representation Learning",
        "Authors": "Aniket Rajiv Didolkar \u00b7 Andrii Zadaianchuk \u00b7 Rabiul Awal \u00b7 Maximilian Seitzer \u00b7 Efstratios Gavves \u00b7 Aishwarya Agrawal",
        "Abstract": "Object-centric representation learning aims to decompose visual scenes into fixed-size vectors called slots'' orobject files'', where each slot captures a distinct object. Current state-of-the-art object-centric models have shown remarkable success in object discovery in diverse domains including complex real-world scenes. However, these models suffer from a key limitation: they lack controllability. Specifically, current object-centric models learn representations based on their preconceived understanding of objects and parts, without allowing user input to guide which objects are represented. Introducing controllability into object-centric models could unlock a range of useful capabilities, such as the ability to extract instance-specific representations from a scene. In this work, we propose a novel approach for user-directed control over slot representations by conditioning slots on language descriptions. The proposed ConTRoLlable Object-centric representation learning approach, which we term CTRL-O, achieves targeted object-language binding in complex real-world scenes without requiring mask supervision. Next, we apply these controllable slot representations on two downstream vision language tasks: text-to-image generation and visual question answering. We find that the proposed approach enables instance-specific text-to-image generation and also achieves strong performance on visual question answering.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Object-centric representation learning decomposes visual scenes into fixed-size vectors, or 'slots', each capturing a distinct object. Current models excel in object discovery but lack controllability, as they do not allow user input to guide object representation. This paper introduces CTRL-O, a novel approach enabling user-directed control over slot representations by conditioning them on language descriptions. CTRL-O achieves targeted object-language binding in complex scenes without mask supervision and demonstrates its utility in text-to-image generation and visual question answering, enabling instance-specific generation and strong performance on downstream tasks.",
        "Tags": [
            "Object Detection",
            "Vision-Language Models (VLMs)",
            "Language-Controllable Learning",
            "Text-to-Image Generation",
            "Visual Question Answering"
        ]
    },
    {
        "Title": "ReconDreamer: Crafting World Models for Driving Scene Reconstruction  via Online Restoration",
        "Authors": "Chaojun Ni \u00b7 Guosheng Zhao \u00b7 Xiaofeng Wang \u00b7 Zheng Zhu \u00b7 Wenkang Qin \u00b7 Guan Huang \u00b7 Chen Liu \u00b7 Yuyin Chen \u00b7 Yida Wang \u00b7 Xueyang Zhang \u00b7 Yifei Zhan \u00b7 Kun Zhan \u00b7 Peng Jia \u00b7 XianPeng Lang \u00b7 Xingang Wang \u00b7 Wenjun Mei",
        "Abstract": "Closed-loop simulation is crucial for end-to-end autonomous driving. Existing sensor simulation methods (e.g., NeRF and 3DGS) reconstruct driving scenes based on conditions that closely mirror training data distributions. However, these methods struggle with rendering novel trajectories, such as lane changes. Recent work, DriveDreamer4D, has demonstrated that integrating world model knowledge alleviates these issues. Although the training-free integration approach is efficient, it still struggles to render larger maneuvers, such as multi-lane shifts.Therefore, we introduce ReconDreamer, which enhances driving scene reconstruction through incremental integration of world model knowledge. Specifically, based on the world model, DriveRestorer is proposed to mitigate ghosting artifacts via online restoration. Additionally,  we propose the progressive data update strategy to ensure high-quality rendering for larger maneuvers. Notably, ReconDreamer is the first method to effectively render in large maneuvers (e.g., across multiple lanes, spanning up to 6 meters). Additionally, experimental results demonstrate that ReconDreamer outperforms Street Gaussians in the NTA-IoU, NTL-IoU, and FID, with a relative improvement by 24.87\\%, 6.72\\%, and 29.97\\%. Furthermore, ReconDreamer surpasses DriveDreamer4D with PVG during large maneuver rendering, as verified by a relative improvement of 195.87\\% in the NTA-IoU metric and a comprehensive user study.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "ReconDreamer introduces a novel approach to enhance driving scene reconstruction by incrementally integrating world model knowledge, addressing limitations in rendering novel trajectories like lane changes. The method includes DriveRestorer for mitigating ghosting artifacts through online restoration and a progressive data update strategy to ensure high-quality rendering for larger maneuvers. ReconDreamer is the first to effectively render large maneuvers, such as multi-lane shifts, and outperforms existing methods like Street Gaussians and DriveDreamer4D in key metrics and user studies.",
        "Tags": [
            "Autonomous Driving",
            "3DGS (Gaussian Splatting)",
            "World Model Integration",
            "Online Restoration",
            "Progressive Data Update"
        ]
    },
    {
        "Title": "Exploring Intrinsic Normal Prototypes within a Single Image for Universal Anomaly Detection",
        "Authors": "Wei Luo \u00b7 Yunkang Cao \u00b7 Haiming Yao \u00b7 Xiaotian Zhang \u00b7 Jianan Lou \u00b7 Yuqi Cheng \u00b7 Weiming Shen \u00b7 Wenyong Yu",
        "Abstract": "Anomaly detection (AD) is essential for industrial inspection, yet existing methods typically rely on \"comparing'' test images to normal references from a training set. However, variations in appearance and positioning often complicate the alignment of these references with the test image, limiting detection accuracy. We observe that most anomalies manifest as local variations, meaning that even within anomalous images, valuable normal information remains. We argue that this information is useful and may be more aligned with the anomalies since both the anomalies and the normal information originate from the same image. Therefore, rather than relying on external normality from the training set, we propose INP-Former, a novel method that extracts Intrinsic Normal Prototypes (INPs) directly from the test image. Specifically, we introduce the INP Extractor, which linearly combines normal tokens to represent INPs. We further propose an INP Coherence Loss to ensure INPs can faithfully represent normality for the testing image. These INPs then guide the INP-Guided Decoder to reconstruct only normal tokens, with reconstruction errors serving as anomaly scores. Additionally, we propose a Soft Mining Loss to prioritize hard-to-optimize samples during training. INP-Former achieves state-of-the-art performance in single-class, multi-class, and few-shot AD tasks across MVTec-AD, VisA, and Real-IAD, positioning it as a versatile and universal solution for AD. Remarkably, INP-Former also demonstrates some zero-shot AD capability. Code will be made available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Anomaly detection (AD) is crucial for industrial inspection, but existing methods often struggle with aligning test images to normal references due to variations in appearance and positioning. This paper introduces INP-Former, a novel approach that extracts Intrinsic Normal Prototypes (INPs) directly from the test image, leveraging the observation that anomalies are typically local variations and that normal information within the same image can be more aligned with anomalies. The method includes an INP Extractor for generating INPs, an INP Coherence Loss to ensure accurate representation of normality, and an INP-Guided Decoder for reconstructing normal tokens, with reconstruction errors used as anomaly scores. A Soft Mining Loss is also proposed to focus on challenging samples during training. INP-Former achieves state-of-the-art performance across various AD tasks and demonstrates zero-shot AD capabilities.",
        "Tags": [
            "Anomaly Detection",
            "Self-Supervised Learning",
            "Intrinsic Normal Prototypes",
            "Reconstruction-based Anomaly Detection",
            "Zero-Shot Anomaly Detection"
        ]
    },
    {
        "Title": "Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction",
        "Authors": "Xia Wenke \u00b7 Ruoxuan Feng \u00b7 Dong Wang \u00b7 Di Hu",
        "Abstract": "Building a generalizable self-correction system as human cognition is crucial for robots to recover from failures.Despite advancements in Multimodal Large Language Models (MLLMs) that empower robots with semantic reflection ability for failure, translating this semantic reflection into \"how to correct\" fine-grained robotic actions remains a significant challenge.To address this gap, we build the Phoenix framework, which leverages motion instruction as a bridge to connect high-level semantic reflection with low-level robotic action correction. In this motion-based self-reflection framework,we start with a dual-process motion adjustment mechanism with MLLMs to translate the semantic reflection into coarse-grained motion instruction adjustment. To leverage this motion instruction for guiding \"how to correct\" fine-grained robotic actions, a multi-task motion-conditioned diffusion policy is proposed to integrate visual observations for high-frequency robotic action correction.By combining these two models, we could shift the demand for generalization capability from the low-level manipulation policy to the MLLMs-driven motion refinement model and facilitate precise, fine-grained robotic action correction.Utilizing this framework, we further develop a continual learning method to automatically improve the model's capability from interactions with dynamic environments.The experiments conducted in both the RoboMimic simulation and real-world scenarios prove the superior generalization and robustness of our framework across a variety of manipulation tasks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The Phoenix framework introduces a motion-based self-reflection system for fine-grained robotic action correction, addressing the challenge of translating semantic reflection into actionable corrections. It employs a dual-process motion adjustment mechanism using Multimodal Large Language Models (MLLMs) to convert semantic reflections into coarse-grained motion instructions. These instructions guide a multi-task motion-conditioned diffusion policy that integrates visual observations for precise action correction. The framework also includes a continual learning method to enhance the model's adaptability in dynamic environments, demonstrating superior generalization and robustness in both simulated and real-world scenarios.",
        "Tags": [
            "Embodied AI",
            "Multimodal Large Language Models (MLLs)",
            "Motion-based Correction",
            "Continual Learning",
            "Diffusion Policy"
        ]
    },
    {
        "Title": "Schedule On the Fly: Diffusion Time Prediction for Faster and Better Image Generation",
        "Authors": "Zilyu Ye \u00b7 Zhiyang Chen \u00b7 Tiancheng Li \u00b7 Zemin Huang \u00b7 Weijian Luo \u00b7 Guo-Jun Qi",
        "Abstract": "Diffusion and flow models have achieved remarkable successes in various applications such as text-to-image generation. However, these models typically rely on the same predetermined denoising schedules during inference for each prompt, which potentially limits the inference efficiency as well as the flexibility when handling different prompts.In this paper, we argue that the optimal noise schedule should adapt to each inference instance, and introduce the Time Prediction Diffusion Model (TPDM) to accomplish this. TPDM employs a plug-and-play Time Prediction Module (TPM) that predicts the next noise level based on current latent features at each denoising step. We train the TPM using reinforcement learning to maximize the final image quality while discounting the number of denoising steps.With such an adaptive scheduler, TPDM not only generates high-quality images that are aligned closely with human preferences but also adjusts the number of denoising steps and time on the fly, enhancing both performance and efficiency. We train TPDMs on multiple diffusion model benchmarks. With Stable Diffusion 3 Medium architecture, TPDM achieves an aesthetic score of 5.44 and a human preference score (HPS) of 29.59, while using 50% fewer denoising steps to achieve better performance. We will release our best model alongside this paper.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces the Time Prediction Diffusion Model (TPDM), which enhances the efficiency and flexibility of diffusion models for image generation by adapting the noise schedule to each inference instance. TPDM incorporates a Time Prediction Module (TPM) that dynamically predicts the next noise level based on current latent features, trained via reinforcement learning to optimize image quality and reduce denoising steps. This adaptive approach allows TPDM to generate high-quality images aligned with human preferences while significantly improving computational efficiency. The model demonstrates superior performance on benchmarks, achieving high aesthetic and human preference scores with fewer denoising steps.",
        "Tags": [
            "Diffusion Models",
            "Image Generation",
            "Adaptive Noise Scheduling",
            "Reinforcement Learning",
            "Computational Efficiency"
        ]
    },
    {
        "Title": "SemAlign3D: Semantic Correspondence between RGB-Images through Aligning 3D Object-Class Representations",
        "Authors": "Krispin Wandel \u00b7 Hesheng Wang",
        "Abstract": "Semantic correspondence made tremendous progress through the recent advancements of large vision models (LVM). While these LVMs have been shown to reliably capture local semantics, the same can currently not be said for capturing global geometric relationships between semantic object regions. This problem leads to unreliable performance for semantic correspondence between images with extreme view variation. In this work, we aim to leverage monocular depth estimates to capture these geometric relationships for more robust and data-efficient semantic correspondence. First, we introduce a simple but effective method to build 3D object-class representations from monocular depth estimates and LVM features using a sparsely annotated image correspondence dataset. Second, we formulate an alignment energy that can be minimized using gradient descent to obtain an alignment between the 3D object-class representation and the object-class instance in the input RGB-image. Our method achieves state-of-the-art matching accuracy in multiple categories on the challenging SPair-71k dataset, increasing the PCK@0.1 score by more than 10 points on three categories and overall by 3.3 points from 85.6% to 88.9%.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the challenge of semantic correspondence between RGB images with extreme view variations by leveraging monocular depth estimates to capture global geometric relationships. The authors propose a method to construct 3D object-class representations using monocular depth estimates and features from large vision models (LVMs), combined with a sparsely annotated image correspondence dataset. They introduce an alignment energy formulation, optimized via gradient descent, to align 3D object-class representations with object-class instances in input RGB images. This approach achieves state-of-the-art matching accuracy on the SPair-71k dataset, significantly improving performance across multiple categories.",
        "Tags": [
            "Semantic Segmentation",
            "3D Reconstruction",
            "Vision-Language Models (VLMs)",
            "Depth Estimation",
            "Monocular Depth Estimation",
            "Alignment Energy Optimization",
            "Sparse Annotation"
        ]
    },
    {
        "Title": "Hypergraph Vision Transformers: Images are More than Nodes, More than Edges",
        "Authors": "Joshua Fixelle",
        "Abstract": "Recent advancements in computer vision have highlighted the scalability of Vision Transformers (ViTs) across various tasks, yet challenges remain in balancing adaptability, computational efficiency, and the ability to model higher-order relationships. Vision Graph Neural Networks (ViGs) offer an alternative by leveraging graph-based methodologies but are hindered by the computational bottlenecks of clustering algorithms used for edge generation. To address these issues, we propose the Hypergraph Vision Transformer (HgVT), which incorporates a hierarchical bipartite hypergraph structure into the vision transformer framework to capture higher-order semantic relationships while maintaining computational efficiency. HgVT leverages population and diversity regularization for dynamic hypergraph construction without clustering, and expert edge pooling to enhance semantic extraction and facilitate graph-based image retrieval. Empirical results demonstrate that HgVT achieves state-of-the-art performance on image classification and competitive performance on image retrieval, positioning it as an efficient and adaptable framework for semantic-based vision tasks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces the Hypergraph Vision Transformer (HgVT), a novel framework that integrates a hierarchical bipartite hypergraph structure into Vision Transformers to model higher-order semantic relationships efficiently. Unlike traditional Vision Graph Neural Networks (ViGs), which face computational bottlenecks due to clustering algorithms, HgVT employs population and diversity regularization for dynamic hypergraph construction and expert edge pooling for improved semantic extraction. This approach enables HgVT to achieve state-of-the-art performance in image classification and competitive results in image retrieval, making it a versatile and efficient solution for semantic-based vision tasks.",
        "Tags": [
            "Vision Transformer (ViT)",
            "Graph Neural Networks (GNNs)",
            "Hypergraph Learning",
            "Semantic Relationship Modeling",
            "Dynamic Graph Construction"
        ]
    },
    {
        "Title": "ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in Hour-Long Videos",
        "Authors": "Tanveer Hannan \u00b7 Md Mohaiminul Islam \u00b7 Jindong Gu \u00b7 Thomas Seidl \u00b7 Gedas Bertasius",
        "Abstract": "Large language models (LLMs) excel at retrieving information from lengthy text, but their vision-language counterparts (VLMs) face difficulties with hour-long videos, especially for temporal grounding. Specifically, these VLMs are constrained by frame limitations, often losing essential temporal details needed for accurate event localization in extended video content. We propose ReVisionLLM, a recursive vision-language model designed to locate events in hour-long videos. Inspired by human search strategies, our model initially targets broad segments of interest, progressively revising its focus to pinpoint exact temporal boundaries. Our model can seamlessly handle videos of vastly different lengths\u2014from minutes to hours. We also introduce a hierarchical training strategy that starts with short clips to capture distinct events and progressively extends to longer videos. To our knowledge, ReVisionLLM is the first VLM capable of temporal grounding in hour-long videos, outperforming previous state-of-the-art methods across multiple datasets by a significant margin (e.g., +2.6\\% R1@0.1 on MAD). The code is available in the supplementary and will be released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "ReVisionLLM is a recursive vision-language model designed to address the challenge of temporal grounding in hour-long videos, a task where existing vision-language models (VLMs) struggle due to frame limitations and loss of temporal details. Inspired by human search strategies, ReVisionLLM employs a recursive approach that first identifies broad segments of interest and then refines its focus to locate precise temporal boundaries. The model is capable of handling videos of varying lengths, from minutes to hours, and is trained using a hierarchical strategy that progresses from short clips to longer videos. ReVisionLLM achieves state-of-the-art performance on multiple datasets, significantly outperforming previous methods.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Video Understanding",
            "Temporal Grounding",
            "Recursive Modeling",
            "Hierarchical Training"
        ]
    },
    {
        "Title": "iG-6DoF: Model-free 6DoF Pose Estimation for Unseen Object via Iterative 3D Gaussian Splatting",
        "Authors": "Tuo Cao \u00b7 Fei LUO \u00b7 Jiongming Qin \u00b7 Yu Jiang \u00b7 Yusen Wang \u00b7 Chunxia Xiao",
        "Abstract": "Traditional methods in pose estimation often rely on precise 3D models or additional data such as depth and normals, limiting their generalization, especially when objects undergo large translations or rotations. We propose iG-6DoF, a novel model-free 6D pose estimation method iterative 3D Gaussian Splatting to estimate the pose of unseen objects. We first estimates an initial pose by leveraging multi-scale data augmentation and the rotation-equivariant features to create a better pose hypothesis from a set of candidates. Then, we propose an iterative 3DGS approach through iteratively rendering and comparing the rendered image with the input image to further progressively improve pose estimation accuracy. The proposed end-to-end network consists of an object detector, a multi-scale rotation-equivariant feature based initial pose estimator, and a coarse-to-fine pose refiner. Such combination allows our method to focus on the target object in a complex scene and deal with large movement and weak textures.Such framework allows to deal with large movement and weak texture and complex scene. We conduct extensive experiments on benchmark and real datasets. Our method achieves state-of-the-art results on the LINEMOD, OnePose-LowTexture, and GenMOP datasets, demonstrating its strong generalization to unseen objects and robustness across various scenes.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces iG-6DoF, a model-free 6D pose estimation method for unseen objects using iterative 3D Gaussian Splatting. This approach does not rely on precise 3D models or additional data like depth and normals, enhancing its generalization capabilities. The method begins with an initial pose estimation using multi-scale data augmentation and rotation-equivariant features, followed by an iterative refinement process that compares rendered images with input images to improve accuracy. The end-to-end network includes an object detector, a multi-scale rotation-equivariant feature-based initial pose estimator, and a coarse-to-fine pose refiner, enabling effective handling of large movements, weak textures, and complex scenes. The method demonstrates superior performance on benchmark datasets, showcasing its robustness and generalization to unseen objects.",
        "Tags": [
            "6D Object Pose Estimation",
            "3DGS (Gaussian Splatting)",
            "Rotation-Equivariant Features",
            "Coarse-to-Fine Pose Refinement",
            "Model-Free Pose Estimation"
        ]
    },
    {
        "Title": "Compositional Caching for Training-free Open-vocabulary Attribute Detection",
        "Authors": "Marco Garosi \u00b7 Alessandro Conti \u00b7 Gaowen Liu \u00b7 Elisa Ricci \u00b7 Massimiliano Mancini",
        "Abstract": "Attribute detection is crucial for many computer vision tasks, as it enables systems to describe properties such as color, texture, and material. Current approaches often rely on labor-intensive annotation processes which are inherently limited: objects can be described at an arbitrary level of detail (e.g., color vs. color shades), leading to ambiguities when the annotators are not instructed carefully. Furthermore, they operate within a predefined set of attributes, reducing scalability and adaptability to unforeseen downstream applications. We present Compositional Caching (ComCa), a training-free method for open-vocabulary attribute detection that overcomes these constraints. ComCa requires only the list of target attributes and objects as input, using them to populate an auxiliary cache of images by leveraging web-scale databases and Large Language Models to determine attribute-object compatibility. To account for the compositional nature of attributes, cache images receive soft attribute labels. Those are aggregated at inference time based on the similarity between the input and cache images, refining the predictions of underlying Vision-Language Models (VLMs). Importantly, our approach is model-agnostic, compatible with various VLMs. Experiments on public datasets demonstrate that ComCa significantly outperforms zero-shot and cache-based baselines, competing with recent training-based methods, proving that a carefully designed training-free approach can successfully address open-vocabulary attribute detection.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Compositional Caching (ComCa), a training-free method for open-vocabulary attribute detection that addresses limitations in current approaches, such as labor-intensive annotation processes and predefined attribute sets. ComCa leverages web-scale databases and Large Language Models to populate an auxiliary cache of images with soft attribute labels, which are used to refine predictions of underlying Vision-Language Models (VLMs) at inference time. The method is model-agnostic and compatible with various VLMs, demonstrating superior performance over zero-shot and cache-based baselines on public datasets, and competing with recent training-based methods.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Open-vocabulary Attribute Detection",
            "Training-free Methods",
            "Web-scale Databases",
            "Soft Attribute Labeling"
        ]
    },
    {
        "Title": "Camouflage Anything: Learning to Hide using Controlled Out-painting and Representation Engineering",
        "Authors": "Biplab Das \u00b7 Viswanath Gopalakrishnan",
        "Abstract": "In this work, we introduce Camouflage Anything, a novel and robust approach to generate camouflaged datasets. To the best of our knowledge, we are the first to apply Controlled Out-painting and Representation Engineering (CO + RE) for generating realistic camouflaged images with an objective to hide any segmented object coming from a generic or salient database. Our proposed method uses a novel control design to out-paint a given segmented object, with a camouflaged background. We also uncover the role of representation engineering in enhancing the quality of generated camouflage datasets. We address the limitations of existing metrics FID and KID in capturing the 'camouflage quality',by proposing a novel metric namely, CamOT.  CamOT uses Optimal Transport between foreground & background (boundary) Gaussian Mixture Models (GMM) of concerned camouflaged object to assign an image quality score. Furthermore, we conduct LoRA-based fine-tuning of the robust BiRefNet baseline with our generated camouflaged datasets, leading to notable improvements in camouflaged object segmentation accuracy. The experimental results showcase the efficacy and potential of Camouflage Anything, outperforming existing methods in camouflaged generation tasks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Camouflage Anything, a novel approach for generating camouflaged datasets using Controlled Out-painting and Representation Engineering (CO + RE). The method employs a unique control design to out-paint segmented objects with camouflaged backgrounds, enhancing dataset quality through representation engineering. A new metric, CamOT, is proposed to evaluate camouflage quality by leveraging Optimal Transport between Gaussian Mixture Models (GMM) of foreground and background boundaries. The approach also involves fine-tuning the BiRefNet baseline using LoRA, resulting in improved camouflaged object segmentation accuracy. The method demonstrates superior performance in camouflaged generation tasks compared to existing techniques.",
        "Tags": [
            "Image Editing",
            "Data Augmentation",
            "Camouflage Generation",
            "Optimal Transport",
            "Gaussian Mixture Models",
            "LoRA Fine-tuning"
        ]
    },
    {
        "Title": "FlexDrive: Toward Trajectory Flexibility in Driving Scene Reconstruction and Rendering",
        "Authors": "Jingqiu Zhou \u00b7 Lue Fan \u00b7 Linjiang Huang \u00b7 Zhaoxiang Zhang \u00b7 Xiaoyu Shi \u00b7 Si Liu \u00b7 Hongsheng Li",
        "Abstract": "Driving scene reconstruction and rendering have advanced significantly using the 3D Gaussian Splatting.However, most prior research has focused on the rendering quality along a pre-recorded vehicle path and struggles to generalize to out-of-path viewpoints, which is caused by the lack of high-quality supervision in those out-of-path views. To address this issue, we introduce an Inverse View Warping technique to create compact and high-quality images as supervision for the reconstruction of the out-of-path views, enabling high-quality rendering results for those views.For accurate and robust inverse view warping, a depth bootstrap strategy is proposed to obtain on-the-fly dense depth maps during the optimization process, overcoming the sparsity and incompleteness of LiDAR depth data.Our method achieves superior in-path and out-of-path reconstruction and rendering performance on the widely used Waymo Open dataset.In addition, a simulator-based benchmark is proposed to obtain the out-of-path ground truth and quantitatively evaluate the performance of out-of-path rendering, where our method outperforms previous methods by a significant margin.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces FlexDrive, a method for improving driving scene reconstruction and rendering by addressing the challenge of generalizing to out-of-path viewpoints. Traditional methods, which rely on 3D Gaussian Splatting, often fail to render high-quality views outside the pre-recorded vehicle path due to insufficient supervision. FlexDrive proposes an Inverse View Warping technique to generate high-quality supervision for out-of-path views, supported by a depth bootstrap strategy that enhances the accuracy of dense depth maps during optimization. This approach significantly improves both in-path and out-of-path rendering on the Waymo Open dataset and introduces a simulator-based benchmark for evaluating out-of-path rendering performance, where it surpasses existing methods.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Autonomous Driving",
            "Inverse View Warping",
            "Depth Bootstrap Strategy",
            "Simulator-based Benchmark"
        ]
    },
    {
        "Title": "Mixture of Submodule for Domain Adaptive Person Search",
        "Authors": "Minsu Kim \u00b7 Seungryong Kim \u00b7 Kwanghoon Sohn",
        "Abstract": "Existing technique on domain adaptive person search commonly utilizes the unified framework for jointly localizing and identifying the person across domains. This framework, however, inevitably results in the gradient conflict problem, particularly in cross-domain scenarios with contradictory objectives, as the unified framework employs shared parameters to simultaneously address person detection and re-identification tasks across the domains. To overcome this, we present a novel mixture of submodules framework, dubbed MoS, that dynamically modulates the combination of submodules depending on the specific task to perform person detection and re-identification, separately. We further design the mixtures of submodules that vary depending on the domain, enabling domain-specific knowledge transfer. Especially, we decompose the main model into several submodules and employ diverse mixtures of submodules that vary depending on the tasks and domains through the conditional routing policy. In addition, we also present counterpart domain sample generation that synthesizes the augmented sample and uses them to learn domain invariant representation for person re-identification through the contrastive domain alignment. We conduct experiments to demonstrate the effectiveness of our MoS over the existing domain adaptive person search method and provide ablation studies.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a novel framework, Mixture of Submodules (MoS), for domain adaptive person search, addressing the gradient conflict problem inherent in unified frameworks that use shared parameters for both person detection and re-identification across domains. The MoS framework dynamically adjusts the combination of submodules based on the task and domain, facilitating domain-specific knowledge transfer. It decomposes the main model into several submodules and employs a conditional routing policy to vary the mixtures of submodules according to tasks and domains. Additionally, the framework incorporates counterpart domain sample generation to synthesize augmented samples, aiding in learning domain-invariant representations for person re-identification through contrastive domain alignment. The effectiveness of MoS is demonstrated through experiments and ablation studies.",
        "Tags": [
            "ReID (Person Re-identification)",
            "Domain Adaptation",
            "Conditional Routing Policy",
            "Contrastive Domain Alignment",
            "Domain-Specific Knowledge Transfer"
        ]
    },
    {
        "Title": "Rethinking Temporal Fusion with A Unified Gradient Descent View for 3D Semantic Occupancy Prediction",
        "Authors": "Dubing Chen \u00b7 Huan Zheng \u00b7 Jin Fang \u00b7 Xingping Dong \u00b7 Xianfei Li \u00b7 Wenlong Liao \u00b7 Tao He \u00b7 Pai Peng \u00b7 Jianbing Shen",
        "Abstract": "We present GDFusion, a temporal fusion method for vision-based 3D semantic occupancy prediction (VisionOcc). GDFusion opens up the underexplored aspects of temporal fusion within the VisionOcc framework, with a focus on both temporal cues and fusion strategies. It systematically examines the entire VisionOcc pipeline, identifying three fundamental yet previously overlooked temporal cues: scene-level consistencies, motion calibration, and geometric complementation. These cues capture diverse facets of temporal evolution and provide distinctive contributions across various modules in the general VisionOcc framework.To effectively fuse temporal signals of different representations, we introduce a novel fusion strategy by reinterpreting vanilla RNNs. This approach utilizes gradient descent on features to unify the integration of diverse temporal information. Extensive experiments on NuScenes demonstrate that GDFusion significantly outperforms established baselines, delivering a consistent increase in mIoU between 2.2\\% to 4.7\\% with less memory consumption. Codes will be made publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces GDFusion, a novel temporal fusion method for vision-based 3D semantic occupancy prediction (VisionOcc). GDFusion addresses underexplored aspects of temporal fusion by focusing on temporal cues and fusion strategies. It identifies three key temporal cues\u2014scene-level consistencies, motion calibration, and geometric complementation\u2014that capture diverse facets of temporal evolution and enhance the VisionOcc framework. A unique fusion strategy is proposed, reinterpreting vanilla RNNs through gradient descent on features to unify the integration of temporal information. GDFusion demonstrates significant performance improvements over existing baselines, achieving consistent gains in mIoU with reduced memory consumption.",
        "Tags": [
            "3D Semantic Segmentation",
            "Temporal Fusion",
            "Autonomous Driving",
            "Gradient Descent Fusion",
            "Temporal Cues",
            "Memory Efficiency"
        ]
    },
    {
        "Title": "Samba: A Unified Mamba-based Framework for General Salient Object Detection",
        "Authors": "Jiahao He \u00b7 Keren Fu \u00b7 Xiaohong Liu \u00b7 Qijun Zhao",
        "Abstract": "Existing salient object detection (SOD) models primarily resort to convolutional neural networks (CNNs) and Transformers. However, the limited receptive fields of CNNs and quadratic computational complexity of transformers both constrain the performance of current models on discovering attention-grabbing objects. The emerging state space model, namely Mamba, has demonstrated its potential to balance global receptive fields and computational complexity. Therefore, we propose a novel unified framework based on the pure Mamba architecture, dubbed saliency Mamba (Samba), to flexibly handle general SOD tasks, including RGB/RGB-D/RGB-T SOD, video SOD (VSOD), and RGB-D VSOD. Specifically, we rethink Mamba's scanning strategy from the perspective of SOD, and identify the importance of maintaining spatial continuity of salient patches within scanning sequences. Based on this, we propose a saliency-guided Mamba block (SGMB), incorporating a spatial neighboring scanning (SNS) algorithm to preserve spatial continuity of salient patches. Additionally, we propose a context-aware upsampling (CAU) method to promote hierarchical feature alignment and aggregations by modeling contextual dependencies. Experimental results show that our Samba outperforms existing methods across five SOD tasks on 21 datasets with lower computational cost, confirming the superiority of introducing Mamba to the SOD areas. Our code will be made publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Samba, a novel unified framework for general salient object detection (SOD) based on the Mamba architecture. Unlike traditional models that rely on CNNs and Transformers, which have limitations in receptive fields and computational complexity, Samba leverages Mamba's ability to balance global receptive fields and computational efficiency. The framework includes a saliency-guided Mamba block (SGMB) with a spatial neighboring scanning (SNS) algorithm to maintain spatial continuity of salient patches, and a context-aware upsampling (CAU) method to enhance hierarchical feature alignment and aggregation. Samba demonstrates superior performance across five SOD tasks on 21 datasets, offering a more efficient solution for SOD challenges.",
        "Tags": [
            "Mamba",
            "Salient Object Detection",
            "Spatial Neighboring Scanning",
            "Context-Aware Upsampling",
            "Saliency-Guided Mamba Block"
        ]
    },
    {
        "Title": "STEREO: A Two-Stage Framework for Adversarially Robust Concept Erasing from Text-to-Image Diffusion Models",
        "Authors": "Koushik Srivatsan \u00b7 Fahad Shamshad \u00b7 Muzammal Naseer \u00b7 Vishal M. Patel \u00b7 Karthik Nandakumar",
        "Abstract": "The rapid proliferation of large-scale text-to-image diffusion (T2ID) models has raised serious concerns about their potential misuse in generating harmful content. Although numerous methods have been proposed for erasing undesired concepts from T2ID models, they often provide a false sense of security, because concept-erased models (CEMs) can be easily deceived through adversarial attacks to generate the erased concept. Though some robust concept erasure methods based on adversarial training have emerged recently, they compromise on utility (generation quality for benign concepts) to achieve robustness and/or remain vulnerable to advanced embedding-space attacks. These limitations stem from the failure of robust CEMs to search for \u201cblind spots\u201d in the embedding space thoroughly. To bridge this gap, we propose STEREO, a novel two-stage framework that employs adversarial training as a first step rather than the only step for robust concept erasure. In the first stage, STEREO employs adversarial training as a vulnerability identification mechanism to search thoroughly enough. In the second robustly erase once stage, STEREO introduces an anchor-concept-based compositional objective to robustly erase the target concept at one go while attempting to minimize the degradation on model utility. We benchmark STEREO against 7 state-of-the-art concept erasure methods, demonstrating its enhanced robustness against whitebox, black-box, and advanced embedding-space attacks and its ability to preserve utility to a large extent.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces STEREO, a two-stage framework designed to robustly erase undesired concepts from text-to-image diffusion (T2ID) models while minimizing the degradation of model utility. Unlike existing methods that either compromise utility for robustness or remain vulnerable to advanced attacks, STEREO first uses adversarial training to identify vulnerabilities thoroughly. In the second stage, it employs an anchor-concept-based compositional objective to robustly erase the target concept in one go. The framework demonstrates superior robustness against various attacks and maintains high utility compared to seven state-of-the-art methods.",
        "Tags": [
            "Text-to-Image Generation",
            "Adversarial Training",
            "Concept Erasure",
            "Embedding-Space Attacks",
            "Model Utility Preservation"
        ]
    },
    {
        "Title": "HomoGen: Enhanced Video Inpainting via Homography Propagation and Diffusion",
        "Authors": "Ding Ding \u00b7 Yueming Pan \u00b7 Ruoyu Feng \u00b7 Qi Dai \u00b7 Kai Qiu \u00b7 Jianmin Bao \u00b7 Chong Luo \u00b7 Zhenzhong Chen",
        "Abstract": "In this paper, we present HomoGen, an enhanced video inpainting method based on homography propagation and diffusion models. HomoGen leverages homography registration to propagate contextual pixels as priors for generating missing content in corrupted videos. Unlike previous flow-based propagation methods, which introduce local distortions due to point-to-point optical flows, homography-induced artifacts are typically global structural distortions that preserve semantic integrity. To effectively utilize these priors for generation, we employ a video diffusion model that inherently prioritizes semantic information within the priors over pixel-level details. A content-adaptive control mechanism is proposed to scale and inject the priors into intermediate video latents during iterative denoising. In contrast to existing transformer-based networks that often suffer from artifacts within priors, leading to error accumulation and unrealistic results, our denoising diffusion network can smooth out artifacts and ensure natural output. Extensive experiments demonstrate the effectiveness of the proposed method qualitatively and quantitatively.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "HomoGen introduces an enhanced video inpainting method using homography propagation and diffusion models. It employs homography registration to propagate contextual pixels as priors for generating missing content in corrupted videos, addressing the limitations of flow-based methods that cause local distortions. A video diffusion model is utilized to prioritize semantic information over pixel-level details, with a content-adaptive control mechanism to inject priors into intermediate video latents during iterative denoising. This approach avoids the artifacts and error accumulation common in transformer-based networks, ensuring natural and realistic outputs.",
        "Tags": [
            "Video Inpainting",
            "Diffusion Models",
            "Homography Propagation",
            "Content-Adaptive Control",
            "Iterative Denoising"
        ]
    },
    {
        "Title": "Interpretable Generative Models through Post-hoc Concept Bottlenecks",
        "Authors": "Akshay R. Kulkarni \u00b7 Ge Yan \u00b7 Chung-En Sun \u00b7 Tuomas Oikarinen \u00b7 Tsui-Wei Weng",
        "Abstract": "Concept bottleneck models (CBM) aim to produce inherently interpretable models that rely on human-understandable concepts for their predictions. However, the existing approach to design interpretable generative models based on CBMs is not efficient and scalable, as it requires expensive generative model training from scratch as well as real images with labor-intensive concept supervision. To address these challenges, we present two novel and low-cost methods to build interpretable generative models through post-hoc interpretability and we name them concept-bottleneck autoencoder (CB-AE) and concept controller (CC). Our approach enables efficient and scalable training by using generated images and our method can work with minimal to no concept supervision. Our proposed methods generalize across modern generative model families including generative adversarial networks and diffusion models. We demonstrate the superior interpretability and steerability of our methods on numerous standard datasets like CelebA, CelebA-HQ, and CUB with large improvements (average $\\sim$25\\%) over the prior work, while being 4-15$\\times$ faster to train. Finally, we also perform a large-scale user study to validate the interpretability and steerability of our methods.",
        "Link": "https://lilywenglab.github.io/posthoc-generative-cbm/",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces two novel methods, concept-bottleneck autoencoder (CB-AE) and concept controller (CC), to create interpretable generative models through post-hoc interpretability. These methods address the inefficiency and scalability issues of existing concept bottleneck models (CBM) by utilizing generated images and requiring minimal to no concept supervision. The proposed techniques are compatible with various generative model families, including GANs and diffusion models, and demonstrate significant improvements in interpretability and steerability across standard datasets like CelebA, CelebA-HQ, and CUB. The methods also achieve faster training times compared to prior work and are validated through a large-scale user study.",
        "Tags": [
            "Generative Adversarial Networks (GANs)",
            "Diffusion Models",
            "Post-hoc Interpretability",
            "Concept Bottleneck Models",
            "Steerability"
        ]
    },
    {
        "Title": "ResCLIP: Residual Attention for Training-free Dense Vision-language Inference",
        "Authors": "Jinhong Deng \u00b7 Yuhang Yang \u00b7 Wen Li \u00b7 Lixin Duan",
        "Abstract": "While vision-language models like CLIP have shown remarkable success in open-vocabulary tasks, their application is currently confined to image-level tasks, and they still struggle with dense predictions. Recent works often attribute such deficiency in dense predictions to the self-attention layers in the final block, and have achieved commendable results by modifying the original query-key attention to self-correlation attention, (e.g., query-query and key-key attention). However, these methods overlook the cross-correlation attention (query-key) properties, which capture the rich spatial correspondence. In this paper, we reveal that the cross-correlation of the self-attention in CLIP's non-final layers also exhibits localization properties. Therefore, we propose the Residual Cross-correlation Self-attention (RCS) module, which leverages the cross-correlation self-attention from intermediate layers to remold the attention in the final block. The RCS module effectively reorganizes spatial information, unleashing the localization potential within CLIP for dense vision-language inference. Furthermore, to enhance the focus on regions of the same categories and local consistency, we propose the Semantic Feedback Refinement (SFR) module, which utilizes semantic segmentation maps to further adjust the attention scores. By integrating these two strategies, our method, termed ResCLIP, can be easily incorporated into existing approaches as a plug-and-play module, significantly boosting their performance in dense vision-language inference. Extensive experiments across multiple standard benchmarks demonstrate that our method surpasses state-of-the-art training-free methods, validating the effectiveness of the proposed approach.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "ResCLIP introduces a Residual Cross-correlation Self-attention (RCS) module and a Semantic Feedback Refinement (SFR) module to enhance dense vision-language inference in CLIP-based models. The RCS module leverages cross-correlation self-attention from intermediate layers to improve spatial information organization, while the SFR module uses semantic segmentation maps to refine attention scores. These innovations enable ResCLIP to significantly outperform state-of-the-art training-free methods on dense vision-language tasks.",
        "Tags": [
            "CLIP",
            "Vision-Language Models (VLMs)",
            "Residual Attention",
            "Semantic Feedback Refinement",
            "Dense Vision-Language Inference"
        ]
    },
    {
        "Title": "Gaussian World Model for Streaming 3D Occupancy Prediction",
        "Authors": "Sicheng Zuo \u00b7 Wenzhao Zheng \u00b7 Yuanhui Huang \u00b7 Jie Zhou \u00b7 Jiwen Lu",
        "Abstract": "3D occupancy prediction is important for autonomous driving due to its comprehensive perception of the surroundings. To incorporate sequential inputs, most existing methods fuse representations from previous frames to infer the current 3D occupancy. However, they fail to consider the continuity of driving scenarios and ignore the strong prior provided by the evolution of 3D scenes (e.g., only dynamic objects move). In this paper, we propose a world-model-based framework to exploit scene evolutions for perception. We reformulate 3D occupancy prediction as a 4D occupancy forecasting problem conditioned on the current sensor input. We decompose the scene evolution into three factors: 1) Ego motion alignment of static scenes; 2) local movements of dynamic objects; and 3) completion of newly-observed scenes. We then employ a Gaussian world model (GaussianWorld) to explicitly exploit these priors and infer the scene evolutions in the 3D Gaussian space considering the current RGB observation. We evaluate the effectiveness of our framework on the widely used nuScenes datasets. Our GaussianWorld improves the performance of the single-frame counterpart by over 2\\% in mIoU without introducing additional computations.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces a world-model-based framework for 3D occupancy prediction, reformulating it as a 4D occupancy forecasting problem. The approach leverages scene evolution by decomposing it into ego motion alignment, local movements of dynamic objects, and completion of newly-observed scenes. A Gaussian world model (GaussianWorld) is employed to exploit these priors and infer scene evolutions in 3D Gaussian space, improving performance by over 2% in mIoU on the nuScenes dataset without additional computational costs.",
        "Tags": [
            "3D Semantic Scene Completion",
            "Autonomous Driving",
            "4D Occupancy Forecasting",
            "Gaussian World Model",
            "Scene Evolution"
        ]
    },
    {
        "Title": "ESCAPE: Equivariant Shape Completion via Anchor Point Encoding",
        "Authors": "Burak Bekci \u00b7 Nassir Navab \u00b7 Federico Tombari \u00b7 Saleh",
        "Abstract": "Shape completion, a crucial task in 3D computer vision, involves predicting and filling the missing regions of scanned or partially observed objects. Current methods expect known pose or canonical coordinates and do not perform well under varying rotations, limiting their real-world applicability. We introduce \\textbf{ESCAPE} (Equivariant Shape Completion via Anchor Point Encoding), a novel framework designed to achieve rotation-equivariant shape completion. Our approach employs a distinctive encoding strategy by selecting anchor points from a shape and representing all points as a distance to all anchor points. This enables the model to capture a consistent, rotation-equivariant understanding of the object\u2019s geometry. ESCAPE leverages a transformer architecture to encode and decode the distance transformations, ensuring that generated shape completions remain accurate and equivariant under rotational transformations. Subsequently, we perform optimization to calculate the predicted shapes from the encodings. Experimental evaluations demonstrate that ESCAPE achieves robust, high-quality reconstructions across arbitrary rotations and translations, showcasing its effectiveness in real-world applications without additional pose estimation modules.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "ESCAPE (Equivariant Shape Completion via Anchor Point Encoding) is a novel framework designed for rotation-equivariant shape completion in 3D computer vision. Unlike existing methods that require known poses or canonical coordinates and struggle with varying rotations, ESCAPE introduces a unique encoding strategy using anchor points to represent all points as distances to these anchors. This approach ensures a consistent, rotation-equivariant understanding of object geometry. The framework employs a transformer architecture to encode and decode distance transformations, enabling accurate and equivariant shape completions under rotational transformations. ESCAPE demonstrates robust, high-quality reconstructions across arbitrary rotations and translations, making it highly effective for real-world applications without the need for additional pose estimation modules.",
        "Tags": [
            "3D Reconstruction",
            "Shape Completion",
            "Transformer",
            "Rotation-Equivariant Learning",
            "Anchor Point Encoding",
            "3D Geometry Understanding"
        ]
    },
    {
        "Title": "LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary Semantic Segmentation",
        "Authors": "Vladan Stojni\u0107 \u00b7 Yannis Kalantidis \u00b7 Jiri Matas \u00b7 Giorgos Tolias",
        "Abstract": "We propose a training-free method for open-vocabulary semantic segmentation using Vision-and-Language Models (VLMs). Our approach enhances the initial per-patch predictions of VLMs through label propagation, which jointly optimizes predictions by incorporating patch-to-patch relationships. Since VLMs are primarily optimized for cross-modal alignment and not for intra-modal similarity, we use a Vision Model (VM) that is observed to better captures these relationships. We address resolution limitations inherent to patch-based encoders by applying label propagation at the pixel level as a refinement step, significantly improving segmentation accuracy near class boundaries. Our method called LPOSS+, performs inference over the entire image, avoiding window-based processing and thereby capturing contextual interactions across the full image. LPOSS+ achieves state-of-the-art performance across a diverse set of datasets.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces LPOSS+, a training-free method for open-vocabulary semantic segmentation that leverages Vision-and-Language Models (VLMs). The approach improves initial per-patch predictions from VLMs through label propagation, which integrates patch-to-patch relationships using a Vision Model (VM) for better intra-modal similarity. To overcome resolution limitations of patch-based encoders, label propagation is also applied at the pixel level, enhancing segmentation accuracy near class boundaries. LPOSS+ processes the entire image at once, capturing full-image contextual interactions, and achieves state-of-the-art performance across various datasets.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Semantic Segmentation",
            "Label Propagation",
            "Open-vocabulary Learning",
            "Pixel-level Refinement"
        ]
    },
    {
        "Title": "CoSpace: Benchmarking Continuous Space Perception Ability for Vision-Language Models",
        "Authors": "Yiqi Zhu \u00b7 Ziyue Wang \u00b7 Can Zhang \u00b7 Peng Li \u00b7 Yang Liu",
        "Abstract": "Vision-Language Models (VLMs) have recently witnessed significant progress in visual comprehension. As the permitting length of image context grows, VLMs can now comprehend a broader range of views and spaces. Current benchmarks provide insightful analysis of VLMs in tasks involving complex visual instructions following, multi-image understanding and spatial reasoning. However, they usually focus on spatially irrelevant images or discrete images captured from varied viewpoints. The compositional characteristic of images captured from a static viewpoint remains underestimated. We term this characteristic as $\\textbf{Continuous Space Perception}$. When observing a scene from a static viewpoint while shifting orientations, it produces a series of spatially continuous images, enabling the reconstruction of the entire space. In this paper, we present CoSpace, a multi-image visual understanding benchmark designed to assess the $\\textbf{Co}$ntinuous $\\textbf{Space}$ perception ability for VLMs. CoSpace contains 2,918 images and 1,626 question-answer pairs, covering seven types of tasks. We conduct evaluation across 16 proprietary and open-source VLMs. Results reveal that there exist pitfalls on the continuous space perception ability for most of the evaluated models, including proprietary ones. Interestingly, we find that the main discrepancy between open-source and proprietary models lies not in accuracy but in the consistency of responses. We believe that enhancing the ability of continuous space perception is essential for VLMs to perform effectively in real-world tasks and encourage further research to advance this capability.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces CoSpace, a benchmark designed to evaluate the continuous space perception ability of Vision-Language Models (VLMs). Continuous space perception refers to the ability to understand and reconstruct a scene from a series of spatially continuous images captured from a static viewpoint. CoSpace includes 2,918 images and 1,626 question-answer pairs across seven task types. Evaluations of 16 proprietary and open-source VLMs reveal significant challenges in continuous space perception, with proprietary models showing better consistency in responses compared to open-source ones. The study underscores the importance of enhancing this capability for real-world applications.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Continuous Space Perception",
            "Multi-Image Understanding",
            "Spatial Reasoning",
            "Benchmark Evaluation"
        ]
    },
    {
        "Title": "Progress-Aware Video Frame Captioning",
        "Authors": "Zihui Xue \u00b7 Joungbin An \u00b7 Xitong Yang \u00b7 Kristen Grauman",
        "Abstract": "While image captioning provides isolated descriptions for individual images, and video captioning offers one single narrative for an entire video clip, our work explores an important middle ground: progress-aware video captioning at the frame level. This novel task aims to generate temporally fine-grained captions that not only accurately describe each frame but also capture the subtle progression of actions throughout a video sequence. Despite the strong capabilities of existing leading vision language models, they often struggle to discern the nuances of frame-wise differences. To address this, we propose ProgressCaptioner, a captioning model designed to capture the fine-grained temporal dynamics within an action sequence. Alongside, we develop the FrameCap dataset to support training and the FrameCapEval benchmark to assess caption quality. The results demonstrate that ProgressCaptioner significantly surpasses leading captioning models, producing precise captions that accurately capture action progression and set a new standard for temporal precision in video captioning. Finally, we showcase practical applications of our approach, specifically in aiding keyframe selection and advancing video understanding, highlighting its broad utility.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces progress-aware video frame captioning, a novel task that generates temporally fine-grained captions for each frame while capturing the progression of actions throughout a video. Existing vision-language models often fail to discern frame-wise differences, prompting the development of ProgressCaptioner, a model designed to capture fine-grained temporal dynamics. The authors also introduce the FrameCap dataset for training and the FrameCapEval benchmark for evaluation. ProgressCaptioner outperforms existing models, setting a new standard for temporal precision in video captioning. The approach has practical applications in keyframe selection and video understanding.",
        "Tags": [
            "Video Understanding",
            "Image Captioning",
            "Temporal Dynamics",
            "Frame-Level Captioning",
            "Action Progression"
        ]
    },
    {
        "Title": "HumanMM: Global Human Motion Recovery from Multi-shot Videos",
        "Authors": "Yuhong Zhang \u00b7 Guanlin Wu \u00b7 Ling-Hao Chen \u00b7 Zhuokai Zhao \u00b7 Jing Lin \u00b7 Xiaoke Jiang \u00b7 Jiamin WU \u00b7 Zhuoheng Li \u00b7 Hao Frank Yang \u00b7 Haoqian Wang \u00b7 Lei Zhang",
        "Abstract": "In this paper, we present a novel framework designed to reconstruct long-sequence 3D human motion in the world coordinates from in-the-wild videos with multiple shot transitions. Such long-sequence in-the-wild motions are highly valuable to applications such as motion generation and motion understanding, but are of great challenge to be recovered due to abrupt shot transitions, partial occlusions, and dynamic backgrounds presented in such videos. Existing methods primarily focus on single-shot videos, where continuity is maintained within a single camera view, or simplify multi-shot alignment in camera space only. In this work, we tackle the challenges by integrating an enhanced camera pose estimation with Human Motion Recovery (HMR) by incorporating a shot transition detector and a robust alignment module for accurate pose and orientation continuity across shots. By leveraging a custom motion integrator, we effectively mitigate the problem of foot sliding and ensure temporal consistency in human pose. Extensive evaluations on our created multi-shot dataset from public 3D human datasets demonstrate the robustness of our method in reconstructing realistic human motion in world coordinates.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a novel framework for reconstructing long-sequence 3D human motion in world coordinates from multi-shot in-the-wild videos, addressing challenges such as abrupt shot transitions, partial occlusions, and dynamic backgrounds. The framework integrates enhanced camera pose estimation with Human Motion Recovery (HMR), utilizing a shot transition detector and a robust alignment module to ensure accurate pose and orientation continuity across shots. A custom motion integrator is employed to mitigate foot sliding and maintain temporal consistency in human pose. The method's effectiveness is demonstrated through evaluations on a newly created multi-shot dataset derived from public 3D human datasets.",
        "Tags": [
            "3D Human Pose Estimation",
            "3D Reconstruction",
            "Shot Transition Detection",
            "Temporal Consistency",
            "Foot Sliding Mitigation"
        ]
    },
    {
        "Title": "Hybrid Reciprocal Transformer with Triplet Feature Alignment for Scene Graph Generation",
        "Authors": "Jiawei Fu \u00b7 ZHANG Tiantian \u00b7 Kai Chen \u00b7 Qi Dou",
        "Abstract": "Scene graph generation is a pivotal task in computer vision, aiming to identify all visual relation tuples within an image. The advancement of methods involving triplets has sought to enhance task performance by integrating triplets as contextual features for more precise predicate identification from component level. However, challenges remain due to interference from multi-role objects in overlapping tuples within complex environments, which impairs the model's ability to distinguish and align specific triplet features for reasoning diverse semantics of multi-role objects.To address these issues, we introduce a novel framework that incorporates a triplet alignment model into a hybrid reciprocal transformer architecture, starting from using triplet mask features to guide the learning of component-level relation graphs. To effectively distinguish multi-role objects characterized by overlapping visual relation tuples, we introduce a triplet alignment loss, which provides multi-role objects with aligned features from triplet and helps customize them.Additionally, we explore the inherent connectivity between hybrid aligned triplet and component features through a bidirectional refinement module, which enhances feature interaction and reciprocal reinforcement. Experimental results demonstrate that our model achieves state-of-the-art performance on the Visual Genome and Action Genome datasets, underscoring its effectiveness and adaptability.The code will be available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces a novel framework for scene graph generation, addressing challenges in identifying visual relation tuples within images, particularly in complex environments with multi-role objects. The proposed method integrates a triplet alignment model into a hybrid reciprocal transformer architecture, utilizing triplet mask features to guide the learning of component-level relation graphs. A triplet alignment loss is introduced to distinguish multi-role objects by aligning their features, and a bidirectional refinement module enhances feature interaction and reciprocal reinforcement. The model demonstrates state-of-the-art performance on the Visual Genome and Action Genome datasets.",
        "Tags": [
            "Scene Graph Generation",
            "Vision Transformer (ViT)",
            "Triplet Feature Alignment",
            "Hybrid Reciprocal Transformer",
            "Bidirectional Refinement Module"
        ]
    },
    {
        "Title": "Retrieval-Augmented Personalization for Multimodal Large Language Models",
        "Authors": "Haoran Hao \u00b7 Jiaming Han \u00b7 Changsheng Li \u00b7 Yu-Feng Li \u00b7 Xiangyu Yue",
        "Abstract": "The development of large language models (LLMs) has significantly enhanced the capabilities of multimodal LLMs (MLLMs) as general assistants. However, lack of user-specific knowledge still restricts their application in human's daily life. In this paper, we introduce the Retrieval Augmented Personalization (RAP) framework for MLLMs' personalization. Starting from a general MLLM, we turn it into a personalized assistant in three steps. (a) Remember: We design a key-value database to store user-related information, e.g., user's name, avatar and other attributes. (b) Retrieve: When the user initiates a conversation, RAP will retrieve relevant information from the database using a multimodal retriever. (c) Generate: The input query and retrieved concepts' information are fed into MLLMs to generate personalized, knowledge-augmented responses. Unlike previous methods, RAP allows real-time concept editing via updating the external database. To further improve generation quality and alignment with user-specific information, we design a pipeline for data collection and create a specialized dataset for personalized training of MLLMs. Based on the dataset, we train a series of MLLMs as personalized multimodal assistants. By pretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual concepts without additional finetuning. Our models demonstrate outstanding flexibility and generation quality across a variety of tasks, such as personalized image captioning, question answering and visual recognition. The code, data and models will be publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The Retrieval Augmented Personalization (RAP) framework enhances multimodal large language models (MLLMs) by enabling personalization through a three-step process: remembering user-specific information in a key-value database, retrieving relevant data during conversations, and generating personalized responses. RAP supports real-time concept editing and improves generation quality by training on a specialized dataset. The resulting RAP-MLLMs exhibit flexibility and high-quality performance in tasks like personalized image captioning, question answering, and visual recognition.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Personalization",
            "Key-Value Database",
            "Real-Time Concept Editing",
            "Personalized Multimodal Assistants"
        ]
    },
    {
        "Title": "ITA-MDT: Image-Timestep-Adaptive Masked Diffusion Transformer Framework for Image-Based Virtual Try-On",
        "Authors": "Ji Woo Hong \u00b7 Tri Ton \u00b7 Trung X. Pham \u00b7 Gwanhyeong Koo \u00b7 Sunjae Yoon \u00b7 Chang D. Yoo",
        "Abstract": "This paper introduces ITA-MDT, the Image-Timestep-Adaptive Masked Diffusion Transformer Framework for Image-Based Virtual Try-On (IVTON), designed to overcome the limitations of previous approaches by leveraging the Masked Diffusion Transformer (MDT) for improved handling of both global garment context and fine-grained details. The IVTON task involves seamlessly superimposing a garment from one image onto a person in another, creating a realistic depiction of the person wearing the specified garment. Unlike conventional diffusion-based virtual try-on models that depend on large pre-trained U-Net architectures, ITA-MDT leverages a lightweight, scalable transformer-based denoising diffusion model with a mask latent modeling scheme, achieving competitive results while reducing computational overhead.A key component of ITA-MDT is the Image-Timestep Adaptive Feature Aggregator (ITAFA), a dynamic feature aggregator that combines all of the features from the image encoder into a unified feature of the same size, guided by diffusion timestep and garment image complexity. This enables adaptive weighting of features, allowing the model to emphasize either global information or fine-grained details based on the requirements of the denoising stage. Additionally, the Salient Region Extractor (SRE) module is presented to identify complex region of the garment to provide high-resolution local information to the denoising model as an additional condition alongside the global information of the full garment image. This targeted conditioning strategy enhances detail preservation of fine details in highly salient garment regions, optimizing computational resources by avoiding unnecessarily processing entire garment image.Comparative evaluations confirms that ITA-MDT improves efficiency while maintaining strong performance, reaching state-of-the-art results in several metrics.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper presents ITA-MDT, an Image-Timestep-Adaptive Masked Diffusion Transformer Framework for Image-Based Virtual Try-On (IVTON). The framework addresses the limitations of previous methods by utilizing a Masked Diffusion Transformer (MDT) to better manage global garment context and fine-grained details. ITA-MDT introduces a lightweight, scalable transformer-based denoising diffusion model with a mask latent modeling scheme, reducing computational overhead while maintaining competitive performance. Key innovations include the Image-Timestep Adaptive Feature Aggregator (ITAFA), which dynamically combines features based on diffusion timestep and garment complexity, and the Salient Region Extractor (SRE), which identifies complex garment regions to provide high-resolution local information. These components enable adaptive feature weighting and targeted conditioning, enhancing detail preservation and computational efficiency. ITA-MDT achieves state-of-the-art results in multiple metrics.",
        "Tags": [
            "Image Editing",
            "Diffusion Models",
            "Masked Diffusion Transformer",
            "Image-Timestep Adaptive Feature Aggregator",
            "Salient Region Extractor"
        ]
    },
    {
        "Title": "What Makes a Good Dataset for Knowledge Distillation?",
        "Authors": "Logan Frank \u00b7 Jim Davis",
        "Abstract": "Knowledge distillation (KD) has been a popular and effective method for model compression. One important assumption of KD is that the teacher's original dataset will also be available when training the student. However, in situations such as continual learning and distilling large models trained on company-withheld datasets, having access to the original data may not always be possible. This leads practitioners towards utilizing other sources of supplemental data, which could yield mixed results. One must then ask: \"what makes a good dataset for transferring knowledge from teacher to student?\" Many would assume that only real in-domain imagery is viable, but is that the only option? In this work, we explore multiple possible surrogate distillation datasets and demonstrate that many different datasets, even unnatural synthetic imagery, can serve as a suitable alternative in KD. From examining these alternative datasets, we identify and present various criteria describing what makes a good dataset for distillation. Source code will be available in the future.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Knowledge distillation (KD) is a widely used method for model compression, typically relying on the teacher model's original dataset for training the student. However, access to the original data is not always feasible, especially in scenarios like continual learning or when dealing with proprietary datasets. This paper investigates what constitutes a good dataset for KD, challenging the assumption that only real in-domain imagery is effective. The authors explore various surrogate datasets, including synthetic imagery, and identify criteria that define a suitable dataset for distillation. Their findings suggest that diverse datasets, even those with unnatural synthetic imagery, can effectively transfer knowledge from teacher to student models.",
        "Tags": [
            "Knowledge Distillation",
            "Datasets and Benchmarks",
            "Synthetic Data",
            "Model Compression",
            "Surrogate Datasets"
        ]
    },
    {
        "Title": "Visual Agentic AI for Spatial Reasoning with a Dynamic API",
        "Authors": "Damiano Marsili \u00b7 Rohun Agrawal \u00b7 Yisong Yue \u00b7 Georgia Gkioxari",
        "Abstract": "Visual reasoning -- the ability to interpret the visual world -- is crucial for embodied agents that operate within three-dimensional scenes. Recent progress in AI has led to vision and language models capable of answering questions from images. However, their performance declines when tasked with 3D spatial reasoning. To tackle the complexity of such reasoning problems, we introduce an agentic program synthesis approach where LLM agents collaboratively generate a Pythonic API with new functions to solve common subproblems. Our method overcomes limitations of prior approaches that rely on a static, human-defined API, allowing it to handle a wider range of queries. To better assess AI capabilities for 3D understanding, we introduce a new benchmark of queries involving multiple steps of grounding and inference.We show that our method outperforms prior zero-shot models for visual reasoning in 3D and empirically validate the effectiveness of our agentic framework for 3D spatial reasoning tasks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the challenge of 3D spatial reasoning in visual reasoning tasks, which is crucial for embodied agents operating in three-dimensional environments. While current vision and language models excel at answering questions from images, their performance drops significantly when dealing with 3D spatial reasoning. To overcome this, the authors propose an agentic program synthesis approach where LLM agents collaboratively generate a dynamic Pythonic API with new functions to solve common subproblems. This method surpasses prior approaches that rely on static, human-defined APIs, enabling it to handle a broader range of queries. A new benchmark is introduced to better evaluate AI capabilities in 3D understanding, involving multi-step grounding and inference. The results demonstrate that the proposed method outperforms existing zero-shot models for 3D visual reasoning, validating the effectiveness of the agentic framework.",
        "Tags": [
            "Embodied AI",
            "Large Language Models (LLMs)",
            "3D Spatial Reasoning",
            "Dynamic API",
            "Program Synthesis"
        ]
    },
    {
        "Title": "Exploring Timeline Control for Facial Motion Generation",
        "Authors": "Yifeng Ma \u00b7 Jinwei Qi \u00b7 Chaonan Ji \u00b7 Peng Zhang \u00b7 Bang Zhang \u00b7 Zhidong Deng \u00b7 Liefeng Bo",
        "Abstract": "This paper introduces a new control signal for facial motion generation: timeline control. Compared to audio and text signals, timelines provide more fine-grained control, such as generating specific facial motions with precise timing. Users can specify a multi-track timeline of facial actions arranged in temporal intervals, allowing precise control over the timing of each action. To model the timeline control capability, We first annotate the time intervals of facial actions in natural facial motion sequences at a frame-level granularity. This process is facilitated by Toeplitz Inverse Covariance-based Clustering to minimize human labor. Based on the annotations, we propose a diffusion-based generation model capable of generating facial motions that are natural and accurately aligned with input timelines. Our method supports text-guided motion generation by using ChatGPT to convert text into timelines. Experimental results show that our method can annotate facial action intervals with satisfactory accuracy, and produces natural facial motions accurately aligned with timelines.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces timeline control as a novel control signal for facial motion generation, offering more precise and fine-grained control compared to audio and text signals. Users can specify multi-track timelines of facial actions with exact timing. The authors annotate facial action intervals in natural motion sequences using Toeplitz Inverse Covariance-based Clustering to reduce manual effort. A diffusion-based model is proposed to generate natural facial motions aligned with input timelines. The method also supports text-guided motion generation by converting text into timelines using ChatGPT. Results demonstrate accurate annotation of facial action intervals and natural motion generation aligned with timelines.",
        "Tags": [
            "Facial Motion Generation",
            "Timeline Control",
            "Diffusion Models",
            "Toeplitz Inverse Covariance-based Clustering",
            "Text-to-Timeline Conversion",
            "Precision Timing Control"
        ]
    },
    {
        "Title": "Geometric Knowledge-Guided Localized Global Distribution Alignment for Federated Learning",
        "Authors": "Yanbiao Ma \u00b7 Wei Dai \u00b7 Wenke Huang \u00b7 Jiayi Chen",
        "Abstract": "Data heterogeneity in federated learning, characterized by a significant misalignment between local and global distributions, leads to divergent local optimization directions and hinders global model training. Existing studies mainly focus on optimizing local updates or global aggregation, but these indirect approaches demonstrate instability when handling highly heterogeneous data distributions, especially in scenarios where label skew and domain skew coexist. To address this, we propose a geometry-guided data generation method that centers on simulating the global embedding distribution locally. We first introduce the concept of the geometric shape of an embedding distribution and then address the challenge of obtaining global geometric shapes under privacy constraints. Subsequently, we propose GGEUR, which leverages global geometric shapes to guide the generation of new samples, enabling a closer approximation to the ideal global distribution. In single-domain scenarios, we augment samples based on global geometric shapes to enhance model generalization; in multi-domain scenarios, we further employ class prototypes to simulate the global distribution across domains. Extensive experimental results demonstrate that our method significantly enhances the performance of existing approaches in handling highly heterogeneous data, including scenarios with label skew, domain skew, and their coexistence.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of data heterogeneity in federated learning, where misalignment between local and global distributions hinders global model training. Existing methods, which focus on local updates or global aggregation, struggle with highly heterogeneous data, especially when label skew and domain skew coexist. The authors propose a geometry-guided data generation method that simulates the global embedding distribution locally. They introduce the concept of the geometric shape of an embedding distribution and tackle the challenge of obtaining global geometric shapes under privacy constraints. The proposed method, GGEUR, uses global geometric shapes to generate new samples, approximating the ideal global distribution. In single-domain scenarios, samples are augmented based on global geometric shapes to improve model generalization. In multi-domain scenarios, class prototypes are used to simulate the global distribution across domains. The method significantly enhances the performance of existing approaches in handling highly heterogeneous data, including scenarios with label skew, domain skew, and their coexistence.",
        "Tags": [
            "Federated Learning",
            "Data Augmentation",
            "Geometric Shape",
            "Global Embedding Distribution",
            "Class Prototypes"
        ]
    },
    {
        "Title": "SOLVE: Synergy of Language-Vision and End-to-End Networks for Autonomous Driving",
        "Authors": "Xuesong Chen \u00b7 Linjiang Huang \u00b7 Tao Ma \u00b7 Rongyao Fang \u00b7 Shaoshuai Shi \u00b7 Hongsheng Li",
        "Abstract": "The integration of Vision-Language Models (VLMs) into autonomous driving systems has shown promise in addressing key challenges such as learning complexity, interpretability, and common-sense reasoning. However, existing approaches often struggle with efficient integration and real-time decision-making due to computational demands. In this paper, we introduce SOLVE, an innovative framework that synergizes VLMs with end-to-end (E2E) models to enhance autonomous vehicle planning. Our approach emphasizes knowledge sharing at the feature level through a shared visual encoder, enabling comprehensive interaction between VLM and E2E components. We propose a Trajectory Chain-of-Thought (T-CoT) paradigm, which progressively refines trajectory predictions, reducing uncertainty and improving accuracy. By employing a temporal decoupling strategy, SOLVE achieves efficient asynchronous cooperation, aligning high-quality VLM outputs with E2E real-time performance. Evaluated on the nuScenes dataset, our method demonstrates significant improvements in trajectory prediction accuracy, paving the way for more robust and interpretable autonomous driving systems.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces SOLVE, a framework that integrates Vision-Language Models (VLMs) with end-to-end (E2E) models to enhance autonomous vehicle planning. The approach leverages a shared visual encoder for feature-level knowledge sharing and introduces a Trajectory Chain-of-Thought (T-CoT) paradigm to refine trajectory predictions, reducing uncertainty and improving accuracy. A temporal decoupling strategy enables efficient asynchronous cooperation between VLM and E2E components, ensuring real-time performance. The method demonstrates significant improvements in trajectory prediction accuracy on the nuScenes dataset, contributing to more robust and interpretable autonomous driving systems.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Autonomous Driving",
            "Trajectory Chain-of-Thought (T-CoT)",
            "Temporal Decoupling",
            "Feature-Level Knowledge Sharing"
        ]
    },
    {
        "Title": "GliaNet: Adaptive Neural Network Structure Learning with Glia-Driven",
        "Authors": "Mengqiao Han \u00b7 Liyuan Pan \u00b7 Xiabi Liu",
        "Abstract": "Neural networks derived from the M-P model have excelled in various visual tasks. However, as a simplified simulation version of the brain neural pathway, their structures are locked during training, causing over-fitting and over-parameterization. Although recent models have begun using the biomimetic concept and empirical pruning, they still result in irrational pruning, potentially affecting the accuracy of the model. In this paper, we introduce the Glia unit, composed of oligodendrocytes (Oli) and astrocytes (Ast), to emulate the exact workflow of the mammalian brain, thereby enhancing the biological plausibility of neural functions. Oli selects neurons involved in signal transmission during neural communication and, together with Ast, adaptively optimizes the neural structure. Specifically, we first construct the artificial Glia-Neuron (G-N) model, which is formulated at the instance, group, and interaction levels with adaptive and collaborative mechanisms. Then, we construct GliaNet based on our G-N model, whose structure and connections can be continuously optimized during training. Experiments show that our GliaNet advances state-of-the-art on multiple tasks while significantly reducing its parameters.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces GliaNet, a novel neural network architecture inspired by the mammalian brain's glial cells, specifically oligodendrocytes (Oli) and astrocytes (Ast). The Glia unit, composed of Oli and Ast, emulates the brain's adaptive neural communication and structure optimization. The authors propose the artificial Glia-Neuron (G-N) model, which operates at instance, group, and interaction levels with adaptive and collaborative mechanisms. GliaNet, built on the G-N model, dynamically optimizes its structure and connections during training, achieving state-of-the-art performance on multiple tasks while significantly reducing model parameters.",
        "Tags": [
            "Neural Architecture Search (NAS)",
            "Self-Supervised Learning",
            "Biomimetic Neural Networks",
            "Adaptive Neural Structure Optimization",
            "Glial Cell-Inspired Learning"
        ]
    },
    {
        "Title": "Enhancing Dance-to-Music Generation via Negative Conditioning Latent Diffusion Model",
        "Authors": "Changchang Sun \u00b7 Gaowen Liu \u00b7 Charles Fleming \u00b7 Yan Yan",
        "Abstract": "Recently, conditional diffusion models have gained increasing attention due to their impressive results for cross-modal synthesis.Typically, existing methods target at achieving strong alignment between conditioning input and generated output by training a time-conditioned U-Net augmented with cross-attention mechanism. In this paper, we focus on the problem of generating music synchronized with rhythmic visual cues of the given dance video. Considering that bi-directional guidance is more beneficial for training a diffusion model, we propose to improve the quality of generated music and its synchronization with dance videos by adopting both positive rhythmic information and negative ones (PN-Diffusion) as conditions, where a dual diffusion and reverse processes is devised. Different from existing dance-to-music diffusion models, PN-Diffusion consists of a noise prediction objective for positive conditioning and an additional noise prediction objective for negative conditioning to train a sequential multi-modal U-Net structure. To ensure the accurate definition and selection of negative conditioning, we ingeniously leverage temporal correlations between music and dance videos, where positive and negative rhythmic visual cues and motion information are captured by playing dance videos forward and backward, respectively. By subjectively and objectively evaluating input-output correspondence in terms of dance-music beats and the quality of generated music, experimental results on dance video datasets AIST++ and TikTok demonstrate the superiority of our model over SOTA dance-to-music generation models.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a novel approach to generating music synchronized with dance videos using a negative conditioning latent diffusion model (PN-Diffusion). Unlike traditional methods that focus solely on positive rhythmic information, PN-Diffusion incorporates both positive and negative rhythmic cues to enhance the quality and synchronization of the generated music. The model employs a dual diffusion and reverse process within a sequential multi-modal U-Net structure, leveraging temporal correlations between music and dance videos to define and select negative conditioning effectively. Evaluations on the AIST++ and TikTok datasets demonstrate that PN-Diffusion outperforms state-of-the-art dance-to-music generation models in terms of both subjective and objective metrics.",
        "Tags": [
            "Diffusion Models",
            "Multimodal Learning",
            "Negative Conditioning",
            "Dance-to-Music Synchronization",
            "Temporal Correlation Analysis"
        ]
    },
    {
        "Title": "Shadow Generation Using Diffusion Model with Geometry Prior",
        "Authors": "Haonan Zhao \u00b7 Qingyang Liu \u00b7 Xinhao Tao \u00b7 Li Niu \u00b7 Guangtao Zhai",
        "Abstract": "Image composition involves integrating foreground object into background image to obtain a composite image. One of the key challenges is to produce realistic shadow for the inserted foreground object. Recently, diffusion-based methods have shown superior performance compared to GAN-based methods in shadow generation. However, they are still struggling to generate shadows with plausible geometry in complex cases. In this paper, we focus on promoting diffusion-based methods by leveraging geometry priors. Specifically, we first predict the rotated bounding box and matched shadow shapes for the foreground shadow. Then, the geometry information of rotated bounding box and matched shadow shapes is injected into ControlNet to facilitate shadow generation. Extensive experiments on both DESOBAv2 dataset and real composite images validate the effectiveness of our proposed method.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of generating realistic shadows for foreground objects in image composition tasks. While diffusion-based methods have outperformed GAN-based approaches, they often struggle with producing geometrically plausible shadows in complex scenarios. The authors propose a method that leverages geometry priors to enhance diffusion-based shadow generation. Specifically, they predict rotated bounding boxes and matched shadow shapes for the foreground object and integrate this geometry information into ControlNet to improve shadow generation. The effectiveness of the proposed method is validated through experiments on the DESOBAv2 dataset and real composite images.",
        "Tags": [
            "Diffusion Models",
            "Image Editing",
            "ControlNet",
            "Geometry Priors",
            "Shadow Generation"
        ]
    },
    {
        "Title": "AniDoc: Animation Creation Made Easier",
        "Authors": "Yihao Meng \u00b7 Hao Ouyang \u00b7 Hanlin Wang \u00b7 Qiuyu Wang \u00b7 Wen Wang \u00b7 Ka Leong Cheng \u00b7 Zhiheng Liu \u00b7 Yujun Shen \u00b7 Huamin Qu",
        "Abstract": "The production of 2D animation follows an industry-standard workflow, encompassing four essential stages: character design, keyframe animation, in-betweening, and coloring. Our research focuses on reducing the labor costs in the above process by harnessing the potential of increasingly powerful generative AI. Using video diffusion models as the foundation, Anidoc emerges as a video line art colorization tool, which automatically converts sketch sequences into colored animations following the reference character specification. Our model exploits correspondence matching as an explicit guidance, yielding strong robustness to the variations (e.g., posture) between the reference character and each line art frame. In addition, our model could even automate the in-betweening process, such that users can easily create a temporally consistent animation by simply providing a character image as well as the start and end sketches. We will make the model public to facility the community.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces AniDoc, a tool designed to streamline the production of 2D animation by leveraging generative AI, specifically video diffusion models. AniDoc automates the conversion of sketch sequences into colored animations based on a reference character, utilizing correspondence matching to ensure robustness against variations like posture. Additionally, it automates the in-betweening process, allowing users to create temporally consistent animations with minimal input\u2014just a character image and start and end sketches. This approach significantly reduces labor costs in animation production.",
        "Tags": [
            "Video Generation",
            "Diffusion Models",
            "Character Design Automation",
            "Temporal Consistency",
            "Correspondence Matching"
        ]
    },
    {
        "Title": "Structured 3D Latents for Scalable and Versatile 3D Generation",
        "Authors": "Jianfeng XIANG \u00b7 Zelong Lv \u00b7 Sicheng Xu \u00b7 Yu Deng \u00b7 Ruicheng Wang \u00b7 Bowen Zhang \u00b7 Dong Chen \u00b7 Xin Tong \u00b7 Jiaolong Yang",
        "Abstract": "We introduce a novel 3D generation method for versatile and high-quality 3D asset creation.The cornerstone is a unified Structured LATent (SLAT) representation which allows decoding to different output formats, such as Radiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a sparsely-populated 3D grid with dense multiview visual features extracted from a powerful vision foundation model, comprehensively capturing both structural (geometry) and textural (appearance) information while maintaining flexibility during decoding.We employ rectified flow transformers tailored for SLAT as our 3D generation models and train models with up to 2 billion parameters on a large 3D asset dataset of 500K diverse objects. Our model generates high-quality results with text or image conditions, significantly surpassing existing methods, including recent ones at similar scales. We showcase flexible output format selection and local 3D editing capabilities which were not offered by previous models. Code, model, and data will be released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper presents a novel 3D generation method using a Structured LATent (SLAT) representation, which enables decoding into various formats like Radiance Fields, 3D Gaussians, and meshes. The method integrates a sparsely-populated 3D grid with dense multiview visual features from a vision foundation model, capturing both structural and textural details. The authors employ rectified flow transformers for SLAT and train models with up to 2 billion parameters on a dataset of 500K diverse 3D objects. The model excels in generating high-quality 3D assets from text or image inputs, outperforming existing methods, and supports flexible output formats and local 3D editing.",
        "Tags": [
            "3D Generation",
            "3DGS (Gaussian Splatting)",
            "NeRF (Neural Radiance Fields)",
            "Structured Latent Representation",
            "Rectified Flow Transformers",
            "Local 3D Editing"
        ]
    },
    {
        "Title": "EZSR: Event-based Zero-Shot Recognition",
        "Authors": "Yan Yang \u00b7 Liyuan Pan \u00b7 Dongxu Li \u00b7 Liu Liu",
        "Abstract": "This paper studies zero-shot object recognition using event camera data. Guided by CLIP, which is pre-trained on RGB images, existing approaches achieve zero-shot object recognition by optimizing embedding similarities between event data and RGB images respectively encoded by an event encoder and the CLIP image encoder. Alternatively, several methods learn RGB frame reconstructions from event data for the CLIP image encoder. However, they often result in suboptimal zero-shot performance.This study develops an event encoder without relying on additional reconstruction networks. We theoretically analyze the performance bottlenecks of previous approaches: the embedding optimization objectives are prone to suffer from the spatial sparsity of event data, causing semantic misalignments between the learned event embedding space and the CLIP text embedding space. To mitigate the issue, we explore a scalar-wise modulation strategy. Furthermore, to scale up the number of events and RGB data pairs for training, we also study a pipeline for synthesizing event data from static RGB images in mass.Experimentally, we demonstrate an attractive scaling property in the number of parameters and synthesized data. We achieve superior zero-shot object recognition performance on extensive standard benchmark datasets, even compared with past supervised learning approaches. For example, our model with a ViT/B-16 backbone achieves 47.84% zero-shot accuracy on the N-ImageNet dataset.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces EZSR, a novel approach for zero-shot object recognition using event camera data. Unlike existing methods that rely on reconstructing RGB frames from event data or optimizing embedding similarities, EZSR develops an event encoder that avoids additional reconstruction networks. The study identifies performance bottlenecks in previous approaches, such as semantic misalignments caused by the spatial sparsity of event data, and proposes a scalar-wise modulation strategy to address these issues. Additionally, a pipeline for synthesizing event data from static RGB images is explored to scale up training data. The proposed method demonstrates superior zero-shot recognition performance on benchmark datasets, outperforming even supervised learning approaches, with a ViT/B-16 backbone achieving 47.84% accuracy on the N-ImageNet dataset.",
        "Tags": [
            "Zero-Shot Learning",
            "CLIP",
            "Event-based Vision",
            "Event Encoder",
            "Scalar-wise Modulation",
            "Synthetic Event Data"
        ]
    },
    {
        "Title": "ImPortrait: Implicit Condition Control for Enhanced Portrait Animation",
        "Authors": "Zunnan Xu \u00b7 Zhentao Yu \u00b7 Zixiang Zhou \u00b7 Jun Zhou \u00b7 Xiaoyu Jin \u00b7 Fa-Ting Hong \u00b7 Xiaozhong Ji \u00b7 Junwei Zhu \u00b7 Chengfei Cai \u00b7 Shiyu Tang \u00b7 Qin Lin \u00b7 Xiu Li \u00b7 qinglin lu",
        "Abstract": "We introduce ImPortrait, a diffusion-based condition control method that employs implicit representations for highly controllable and lifelike portrait animation. Given a single portrait image as an appearance reference and video clips as driving templates, ImPortrait can animate the character in the reference image by the facial expression and head pose of the driving videos. In our framework, we utilize pre-trained encoders to achieve the decoupling of portrait motion information and identity in videos. To do so, implicit representation is adopted to encode motion information and is employed as control signals in the animation phase. By leveraging the power of stable video diffusion (SVD) as the main building block, we carefully design adapter layers to inject control signals into denoising unet through attention mechanisms. These bring spatial richness of details and temporal consistency. ImPortrait also exhibits strong generalization performance, which can effectively disentangle appearance and motion under different image styles. Our framework outperforms existing methods, demonstrating superior temporal consistency and controllability.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "ImPortrait is a diffusion-based condition control method designed for highly controllable and lifelike portrait animation. It uses a single portrait image as an appearance reference and video clips as driving templates to animate the character by mimicking the facial expressions and head poses from the driving videos. The framework employs pre-trained encoders to decouple portrait motion information and identity, utilizing implicit representations to encode motion information as control signals. By integrating stable video diffusion (SVD) and designing adapter layers to inject control signals into the denoising process, ImPortrait achieves spatial richness and temporal consistency. The method demonstrates strong generalization, effectively disentangling appearance and motion across different image styles, and outperforms existing methods in temporal consistency and controllability.",
        "Tags": [
            "Diffusion Models",
            "Image Editing",
            "Implicit Representations",
            "Attention Mechanisms",
            "Temporal Consistency"
        ]
    },
    {
        "Title": "VISCO: Benchmarking Fine-Grained Critique and Correction Towards Self-Improvement in Visual Reasoning",
        "Authors": "Xueqing Wu \u00b7 Yuheng Ding \u00b7 Bingxuan Li \u00b7 Pan Lu \u00b7 Da Yin \u00b7 Kai-Wei Chang \u00b7 Nanyun Peng",
        "Abstract": "The ability of large vision-language models (LVLMs) to critique and correct their reasoning is an essential building block towards their self-improvement. However, a systematic analysis of such capabilities in LVLMs is still lacking. We propose VISCO, the first benchmark to extensively analyze the fine-grained critique and correction capabilities of LVLMs. Compared to existing work that uses a single scalar value to critique the entire reasoning [4], VISCO features dense and fine-grained critique, requiring LVLMs to evaluate the correctness of each step in the chain-of-thought and provide natural language explanations to support their judgments. Extensive evaluation of 24 LVLMs demonstrates that human-written critiques significantly enhance the performance after correction, showcasing the potential of the self-improvement strategy. However, the model-generated critiques are less helpful and sometimes detrimental to the performance, suggesting that critique is the crucial bottleneck. We identified three common patterns in critique failures: failure to critique visual perception, reluctance to \"say no\", and exaggerated assumption of error propagation. To address these issues, we propose an effective LookBack strategy that revisits the image to verify each piece of information in the initial reasoning. \\ourscritic{} significantly improves critique and correction performance by up to 13.5%.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces VISCO, a benchmark designed to analyze the fine-grained critique and correction capabilities of large vision-language models (LVLMs) towards self-improvement in visual reasoning. Unlike previous approaches that use a single scalar value for critique, VISCO requires LVLMs to evaluate each step in the reasoning chain and provide natural language explanations. Evaluations of 24 LVLMs reveal that human-written critiques significantly enhance performance after correction, whereas model-generated critiques are less effective and sometimes harmful. The study identifies three common critique failure patterns and proposes a LookBack strategy to improve critique and correction performance by up to 13.5%.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Self-Supervised Learning",
            "Fine-Grained Critique",
            "Chain-of-Thought Analysis",
            "LookBack Strategy"
        ]
    },
    {
        "Title": "H-MoRe: Learning Human-centric Motion Representation for Action Analysis",
        "Authors": "Zhanbo Huang \u00b7 Xiaoming Liu \u00b7 Yu Kong",
        "Abstract": "In this paper, we propose H-MoRe, a novel pipeline for learning precise human-centric motion representation. Our approach dynamically preserves relevant human motion while filtering out background movement. Notably, unlike previous methods relying on fully supervised learning from synthetic data, H-MoRe learns directly from real-world scenarios in a self-supervised manner, incorporating both human pose and body shape information. Inspired by kinematics, H-MoRe represents absolute and relative movements of each body point in a matrix format that captures nuanced motion details, termed world-local flows. H-MoRe offers refined insights into human motion, which can be integrated seamlessly into various action-related applications. Experimental results demonstrate that H-MoRe brings substantial improvements across various downstream tasks, including gait recognition(CL@R1: +16.01%), action recognition(Acc@1: +8.92%), and video generation(FVD: -67.07%). Additionally, H-MoRe exhibits high inference efficiency (34 fps), making it suitable for most real-time scenarios. Models and code will be released upon publication.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces H-MoRe, a novel pipeline for learning human-centric motion representation that dynamically preserves relevant human motion while filtering out background movement. Unlike previous methods that rely on fully supervised learning from synthetic data, H-MoRe learns directly from real-world scenarios in a self-supervised manner, incorporating both human pose and body shape information. The approach represents absolute and relative movements of each body point in a matrix format, termed world-local flows, capturing nuanced motion details. H-MoRe provides refined insights into human motion, applicable to various action-related tasks, and demonstrates significant improvements in gait recognition, action recognition, and video generation, with high inference efficiency suitable for real-time scenarios.",
        "Tags": [
            "Human Action Prediction",
            "Self-Supervised Learning",
            "Video Understanding",
            "World-Local Flows",
            "Real-Time Inference",
            "Kinematics-Inspired Representation"
        ]
    },
    {
        "Title": "DIO: Decomposable Implicit 4D Occupancy-Flow World Model",
        "Authors": "Christopher Diehl \u00b7 Quinlan Sykora \u00b7 Ben Agro \u00b7 Thomas Gilles \u00b7 Sergio Casas \u00b7 Raquel Urtasun",
        "Abstract": "We present DIO, a flexible world model that can estimate the scene occupancy-flow from a sparse set of LiDAR observations, and decompose it into individual instances. DIO can not only complete instance shapes at the present time, but also forecast their occupancy-flow evolution over a future horizon. Thanks to its flexible prompt representation, DIO can take instance prompts from off-the-shelf models like 3D detectors, achieving state-of-the-art performance in the task of 4D semantic occupancy completion and forecasting on the Argoverse 2 dataset. Moreover, our world model can easily and effectively be transferred to downstream tasks like LiDAR point cloud forecasting, ranking first compared to all baselines in the Argoverse 4D occupancy forecasting challenge.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "DIO is a flexible world model designed to estimate scene occupancy-flow from sparse LiDAR observations and decompose it into individual instances. It can complete instance shapes at the current time and forecast their occupancy-flow evolution over a future horizon. Leveraging instance prompts from off-the-shelf models like 3D detectors, DIO achieves state-of-the-art performance in 4D semantic occupancy completion and forecasting on the Argoverse 2 dataset. Additionally, DIO excels in downstream tasks such as LiDAR point cloud forecasting, outperforming all baselines in the Argoverse 4D occupancy forecasting challenge.",
        "Tags": [
            "3D Point Cloud",
            "4D Semantic Occupancy Completion",
            "LiDAR Forecasting",
            "Instance Decomposition",
            "Occupancy-Flow Forecasting",
            "Prompt Representation"
        ]
    },
    {
        "Title": "AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers",
        "Authors": "Sherwin Bahmani \u00b7 Ivan Skorokhodov \u00b7 Guocheng Qian \u00b7 Aliaksandr Siarohin \u00b7 Willi Menapace \u00b7 Andrea Tagliasacchi \u00b7 David B. Lindell \u00b7 Sergey Tulyakov",
        "Abstract": "Numerous works have recently integrated 3D camera control into foundational text-to-video models, but the resulting camera control is often imprecise, and video generation quality suffers. In this work, we analyze camera motion from a first principles perspective, uncovering insights that enable precise 3D camera manipulation without compromising synthesis quality. First, we determine that motion induced by camera movements in videos is low-frequency in nature. This motivates us to adjust train and test pose conditioning schedules, accelerating training convergence while improving visual and motion quality. Then, by probing the representations of an unconditional video diffusion transformer, we observe that they implicitly perform camera pose estimation under the hood, and only a sub-portion of their layers contain the camera information. This suggested us to limit the injection of camera conditioning to a subset of the architecture to prevent interference with other video features, leading to $\\approx{4\\times}$ reduction of training parameters, improved training speed and $\\approx{10}\\%$ higher visual quality. Finally, we complement the typical dataset for camera control learning with a curated dataset of $20k$ in-the-wild dynamic videos with stationary cameras. This helps the model disambiguate the difference between camera and scene motion, and improves the dynamics of generated pose-conditioned videos. We compound these findings to design the Advanced 3D Camera Control (AC3D) architecture, architecture, the new state-of-the-art model for generative video modeling with camera control.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of integrating precise 3D camera control into text-to-video models without compromising video quality. The authors analyze camera motion from first principles, identifying that such motion is low-frequency, which informs adjustments to pose conditioning schedules to enhance training convergence and visual quality. They also discover that a subset of layers in an unconditional video diffusion transformer implicitly performs camera pose estimation, leading to a strategy that limits camera conditioning to these layers, reducing training parameters by approximately 4 times and improving visual quality by about 10%. Additionally, the introduction of a curated dataset of 20,000 dynamic videos with stationary cameras helps the model distinguish between camera and scene motion, improving the dynamics of generated videos. These insights culminate in the development of the Advanced 3D Camera Control (AC3D) architecture, setting a new benchmark in generative video modeling with camera control.",
        "Tags": [
            "3D Generation",
            "Video Generation",
            "Camera Pose Estimation",
            "Low-Frequency Motion Analysis",
            "Pose Conditioning"
        ]
    },
    {
        "Title": "Adv-CPG: A Customized Portrait Generation Framework with Facial Adversarial Attacks",
        "Authors": "Junying Wang \u00b7 Hongyuan Zhang \u00b7 Yuan Yuan",
        "Abstract": "Recent personalized portrait generation methods, taking a facial image and a textual prompt as inputs, have attracted substantial attention. Although these methods generate high-fidelity portraits, they fail to prevent the generated portraits from being tracked and misused by malicious face recognition systems. To address this, this paper proposes a Customized Portrait Generation framework with facial Adversarial attacks (Adv-CPG). Specifically, to achieve facial privacy protection, we devise a lightweight local ID encryptor and an encryption enhancer. They implement progressive double-layer encryption protection by directly injecting the target identity and adding additional identity guidance, respectively. Furthermore, to accomplish fine-grained and customized portrait generation, we develop a multi-modal image customizer capable of generating controllable fine-grained facial features. To the best of our knowledge, Adv-CPG is the first study that introduces facial adversarial attacks into customized portrait generation. Extensive experiments demonstrate the superiority of Adv-CPG, e.g., the average attack success rate of the proposed Adv-CPG is 28.1% and 2.86% higher compared to the SOTA noise-based attack methods and unconstrained attack methods, respectively.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Adv-CPG, a Customized Portrait Generation framework that incorporates facial adversarial attacks to protect against misuse by malicious face recognition systems. The framework features a lightweight local ID encryptor and an encryption enhancer for progressive double-layer encryption, alongside a multi-modal image customizer for generating fine-grained facial features. Adv-CPG is the first to integrate facial adversarial attacks into customized portrait generation, demonstrating superior attack success rates compared to state-of-the-art methods.",
        "Tags": [
            "Image Generation",
            "Adversarial Attacks",
            "Facial Privacy Protection",
            "Multi-modal Customization",
            "Progressive Encryption"
        ]
    },
    {
        "Title": "Scene Map-based Prompt Tuning for Navigation Instruction Generation",
        "Authors": "Sheng Fan \u00b7 Rui Liu \u00b7 Wenguan Wang \u00b7 Yi Yang",
        "Abstract": "Navigation instruction generation (NIG), which offers interactive feedback and guidance to humans along a trajectory, is essential for developing embodied agents capable of human-machine communication and collaboration through natural language. Early data-driven methods directly map sequences of RGB frames to route descriptions on limited datasets. While recent approaches leverage Large Language Models (LLMs) to improve NIG, they often overlook the map representation of the navigation environment, which encodes multi-view semantic and topological information along the trajectory. Instead of solely inputting textual descriptions of the map into LLMs, we propose a scene map-based prompt tuning framework for NIG, \\textsc{MAPInstructor}, which incorporates map priors for parameter-efficient updating of LLMs. \\textsc{MAPInstructor} consists of (i) scene representation encoding, where egocentric observations are projected into 3D voxels for finer-grained scene understanding; (ii) mapping prompt tuning, which integrates the topological map representation of the entire route into the LLM-based decoder; and (iii) landmark uncertainty assessment, which reduces hallucinations in landmark prediction and further enhances instruction generation. Extensive experiments on three navigation datasets (i.e., R2R, REVERIE, RxR) confirm the generalization and effectiveness of our framework. Our code will be released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces a novel framework, MAPInstructor, for navigation instruction generation (NIG) that enhances human-machine communication by incorporating map representations into Large Language Models (LLMs). Unlike previous methods that rely solely on textual descriptions, MAPInstructor utilizes a scene map-based prompt tuning approach, which includes scene representation encoding into 3D voxels, mapping prompt tuning that integrates topological map representations, and landmark uncertainty assessment to improve instruction accuracy. This framework demonstrates superior generalization and effectiveness across multiple navigation datasets.",
        "Tags": [
            "Embodied AI",
            "Large Language Models (LLMs)",
            "3D Voxel Encoding",
            "Topological Map Integration",
            "Landmark Uncertainty Assessment"
        ]
    },
    {
        "Title": "Improving Adversarial Transferability on Vision Transformers via Forward Propagation Refinement",
        "Authors": "Yuchen Ren \u00b7 Zhengyu Zhao \u00b7 Chenhao Lin \u00b7 Bo Yang \u00b7 Lu Zhou \u00b7 Zhe Liu \u00b7 Chao Shen",
        "Abstract": "Vision Transformers (ViTs) have been widely applied in various computer vision and vision-language tasks. To gain insights into their robustness in practical scenarios, transferable adversarial examples on ViTs have been extensively studied. A typical approach to improving adversarial transferability is by refining the surrogate model. However, existing work on ViTs has restricted their surrogate refinement to backward propagation. In this work, we instead focus on Forward Propagation Refinement (FPR) and specifically refine two key modules of ViTs: attention maps and token embeddings. For attention maps, we propose Attention Map Diversification (AMD), which diversifies certain attention maps and also implicitly imposes beneficial gradient vanishing during backward propagation. For token embeddings, we propose Momentum Token Embedding (MTE), which accumulates historical token embeddings to stabilize the forward updates in both the Attention and MLP blocks. We conduct extensive experiments with adversarial examples transferred from ViTs to various CNNs and ViTs, demonstrating that our FPR outperforms the current best (backward) surrogate refinement method by up to 7.0\\% on average.We also validate its superior against popular defenses and its compatibility with other transfer methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Forward Propagation Refinement (FPR) to enhance the adversarial transferability of Vision Transformers (ViTs). Unlike existing methods that focus on backward propagation, FPR refines two key components of ViTs: attention maps and token embeddings. For attention maps, the authors propose Attention Map Diversification (AMD), which diversifies attention maps and induces beneficial gradient vanishing during backward propagation. For token embeddings, they introduce Momentum Token Embedding (MTE), which stabilizes forward updates by accumulating historical token embeddings. Extensive experiments show that FPR outperforms current backward propagation-based methods by up to 7.0% on average, demonstrating superior performance against defenses and compatibility with other transfer methods.",
        "Tags": [
            "Vision Transformer (ViT)",
            "Adversarial Transferability",
            "Attention Map Diversification",
            "Momentum Token Embedding",
            "Forward Propagation Refinement"
        ]
    },
    {
        "Title": "HOIGPT: Learning Long Sequence Hand-Object Interaction with Language Models",
        "Authors": "Mingzhen Huang \u00b7 Fu-Jen Chu \u00b7 Bugra Tekin \u00b7 Kevin Liang \u00b7 Haoyu Ma \u00b7 Weiyao Wang \u00b7 Xingyu Chen \u00b7 Pierre Gleize \u00b7 Hongfei Xue \u00b7 Siwei Lyu \u00b7 Kris Kitani \u00b7 Matt Feiszli \u00b7 Hao Tang",
        "Abstract": "We introduce HOIGPT, a token-based generative method that unifies 3D hand-object interactions (HOI) perception and generation, offering the first comprehensive solution for captioning and generating high-quality 3D HOI sequences from a diverse range of conditional signals (\\eg text, objects, partial sequences). At its core, HOIGPT utilizes a large language model to predict the bidrectional transformation between HOI sequences and natural language descriptions. Given text inputs, HOIGPT generates a sequence of hand and object meshes; given (partial) HOI sequences, HOIGPT generates text descriptions and completes the sequences. To facilitate HOI understanding with a large language model, this paper introduces two key innovations: (1) a novel physically grounded HOI tokenizer, the hand-object decomposed VQ-VAE, for discretizing HOI sequences, and (2) a motion-aware language model trained to process and generate both text and HOI tokens. Extensive experiments demonstrate that HOIGPT sets new state-of-the-art performance on both text generation (+2.01% R Precision) and HOI generation (-2.56 FID) across multiple tasks and benchmarks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "HOIGPT is a token-based generative method that integrates 3D hand-object interaction (HOI) perception and generation, providing a unified solution for captioning and generating high-quality 3D HOI sequences from various conditional inputs such as text, objects, and partial sequences. The method leverages a large language model to predict bidirectional transformations between HOI sequences and natural language descriptions. Key innovations include a physically grounded HOI tokenizer (hand-object decomposed VQ-VAE) for discretizing HOI sequences and a motion-aware language model capable of processing and generating both text and HOI tokens. HOIGPT achieves state-of-the-art performance in text and HOI generation across multiple benchmarks.",
        "Tags": [
            "3D Generation",
            "Vision-Language Models (VLMs)",
            "Hand-Object Interaction",
            "Token-Based Generation",
            "Motion-Aware Language Model"
        ]
    },
    {
        "Title": "PHGC: Procedural Heterogeneous Graph Completion for Natural Language Task Verification in Egocentric Videos",
        "Authors": "Xun Jiang \u00b7 Zhiyi Huang \u00b7 Xing Xu \u00b7 Jingkuan Song \u00b7 Fumin Shen \u00b7 Heng Tao Shen",
        "Abstract": "Natural Language-based Egocentric Task Verification (NLETV) aims to equip agents with the ability to determine if operation flows of procedural tasks in egocentric videos align with natural language instructions. Describing rules with natural language provides generalizable applications, but also raises cross-modal heterogeneity and hierarchical misalignment challenges. In this paper, we proposed a novel approach termed Procedural Heterogeneous Graph Completion (PHGC), which addresses these challenges with heterogeneous graphs representing the logic in rules and operation flows. Specifically, our PHGC method mainly consists of three key components: (1) Heterogeneous Graph Construction module that defines objective states and operation flows as vertices, with temporal and sequential relations as edges. (2) Cross-Modal Path Finding module that aligns semantic relations between hierarchical video and text elements. (3) Discriminative Entity Representation module excavating hidden entities that integrate the general logical relations and discriminative cues to reveal final verification results. Additionally, we further constructed a new dataset called CSV-NL comprised of realistic videos. Extensive experiments on the two benchmark datasets covering both digital and physical scenarios, i.e., EgoTV and CSV-NL, demonstrate that our proposed PHGC establishes state-of-the-art performance across different settings. Our implementation is available at https://anonymous.4open.science/r/PHGC-7A1B.",
        "Link": "https://anonymous.4open.science/r/PHGC-7A1B",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Procedural Heterogeneous Graph Completion (PHGC), a novel approach for Natural Language-based Egocentric Task Verification (NLETV). PHGC addresses challenges of cross-modal heterogeneity and hierarchical misalignment by using heterogeneous graphs to represent the logic in rules and operation flows. The method includes three key components: Heterogeneous Graph Construction, Cross-Modal Path Finding, and Discriminative Entity Representation. A new dataset, CSV-NL, was also constructed for realistic video scenarios. PHGC demonstrates state-of-the-art performance on benchmark datasets, EgoTV and CSV-NL, across various settings.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Video Understanding",
            "Heterogeneous Graphs",
            "Cross-Modal Alignment",
            "Egocentric Videos",
            "Natural Language Processing",
            "Task Verification",
            "Hierarchical Alignment"
        ]
    },
    {
        "Title": "Explicit Depth-Aware Blurry Video Frame Interpolation Guided by Differential Curves",
        "Authors": "yan zaoming \u00b7 pengcheng lei \u00b7 Tingting Wang \u00b7 Faming Fang \u00b7 Junkang Zhang \u00b7 Yaomin Huang \u00b7 Haichuan Song",
        "Abstract": "Blurry video frame interpolation (BVFI), which aims to generate high-frame-rate clear videos from low-frame-rate blurry inputs, is a challenging yet significant task in computer vision. Current state-of-the-art approaches typically rely on linear or quadratic models to estimate intermediate motion. However, these methods often overlook depth-related changes, such as object size and viewing angle variations, which occur as objects move through the scene. This paper proposes a novel approach to addressing this challenge by leveraging differential curves that can describe motion velocity and depth changes.Specifically, we introduce an explicit framework, termed Differential Curve-guided Blurry Video Multi-Frame Interpolation (DC-BMFI), to eliminate the effects of depth variation caused by object motion on BVFI task. In contrast to existing methods that utilize optical flow for 2D awareness, we introduce an MPNet submodule within the DC-BMFI framework to estimate 3D scene flow, thereby enhancing the awareness of depth and velocity changes.To estimate the 3D scene flow from video frames, we propose a submodule termed UBNet to transform video frames into 3D camera space point maps and then estimate scene flow between point maps.Extensive experiments demonstrate that the proposed DC-BMFI surpasses state-of-the-art performance in simulated and real-world datasets.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of blurry video frame interpolation (BVFI) by proposing a novel approach that leverages differential curves to account for depth-related changes caused by object motion. The authors introduce the Differential Curve-guided Blurry Video Multi-Frame Interpolation (DC-BMFI) framework, which enhances depth and velocity awareness through a 3D scene flow estimation submodule called MPNet. Additionally, a UBNet submodule is proposed to transform video frames into 3D camera space point maps for accurate scene flow estimation. The proposed method demonstrates superior performance compared to existing state-of-the-art approaches on both simulated and real-world datasets.",
        "Tags": [
            "Video Generation",
            "3D Reconstruction",
            "Depth Estimation",
            "Differential Curves",
            "3D Scene Flow",
            "Depth-Aware Interpolation"
        ]
    },
    {
        "Title": "Gazing Into Missteps: Leveraging Eye-Gaze for Unsupervised Mistake Detection in Egocentric Videos of Skilled Human Activities",
        "Authors": "Michele Mazzamuto \u00b7 Antonino Furnari \u00b7 Yoichi Sato \u00b7 Giovanni Maria Farinella",
        "Abstract": "We address the challenge of unsupervised mistake detection in egocentric video of skilled human activities through the analysis of gaze signals. While traditional methods rely on manually labeled mistakes, our approach does not require mistake annotations, hence overcoming the need of domain-specific labeled data. Based on the observation that eye movements closely follow object manipulation activities, we assess to what extent eye-gaze signals can support mistake detection, proposing to identify deviations in attention patterns measured through a gaze tracker with respect to those estimated by a gaze prediction model. Since predicting gaze in video is characterized by high uncertainty, we propose a novel gaze completion task, where eye fixations are predicted from visual observations and partial gaze trajectories, and contribute a novel gaze completion approach which explicitly models correlations between gaze information and local visual tokens. Inconsistencies between predicted and observed gaze trajectories act as an indicator to identify mistakes. Experiments highlight the effectiveness of the proposed approach in different settings, with relative gains up to  +14%, +11%, and +5% in EPIC-Tent, HoloAssist and IndustReal respectively, remarkably matching results of supervised approaches without seeing any labels. We further show that gaze-based analysis is particularly useful in the presence of skilled actions, low action execution confidence, and actions requiring hand-eye coordination and object manipulation skills. Our method is ranked first on the HoloAssist Mistake Detection challenge.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces an unsupervised method for detecting mistakes in egocentric videos of skilled human activities by analyzing eye-gaze signals. Unlike traditional approaches that depend on labeled mistake data, this method leverages gaze patterns to identify deviations without requiring domain-specific annotations. The authors propose a gaze completion task where eye fixations are predicted from visual observations and partial gaze trajectories, using a novel approach that models correlations between gaze information and local visual tokens. Discrepancies between predicted and observed gaze trajectories serve as indicators of mistakes. The method demonstrates significant effectiveness across various datasets, achieving performance comparable to supervised approaches without using labeled data. It is particularly effective for skilled actions, low-confidence executions, and tasks requiring hand-eye coordination and object manipulation.",
        "Tags": [
            "Unsupervised Learning",
            "Egocentric Video Analysis",
            "Eye-Gaze Tracking",
            "Gaze Prediction Model",
            "Attention Pattern Analysis",
            "Hand-Eye Coordination"
        ]
    },
    {
        "Title": "From Sparse Signal to Smooth Motion: Real-Time Motion Generation with Rolling Prediction Models",
        "Authors": "German Barquero \u00b7 Nadine Bertsch \u00b7 Manojkumar Marramreddy \u00b7 Carlos Chac\u00f3n \u00b7 Filippo Arcadu \u00b7 Ferran Rigual \u00b7 Nicky Sijia He \u00b7 Cristina Palmero \u00b7 Sergio Escalera \u00b7 Yuting Ye \u00b7 Robin Kips",
        "Abstract": "In extended reality (XR), generating full-body motion of the users is important to understand their actions, drive their virtual avatars for social interaction, and convey a realistic sense of presence. While prior works focused on spatially sparse and always-on input signals from motion controllers, many XR applications opt for vision-based hand tracking for reduced user friction and better immersion. Compared to controllers, hand tracking signals are less accurate and can even be missing for an extended period of time. To handle such unreliable inputs, we present Rolling Prediction Model (RPM), an online and real-time approach that generates smooth full-body motion from temporally and spatially sparse input signals. Our model generates 1) accurate motion that matches the inputs (i.e., tracking mode) and 2) plausible motion when inputs are missing (i.e., synthesis mode). More importantly, RPM generates seamless transitions from tracking to synthesis, and vice versa. To demonstrate the practical importance of handling noisy and missing inputs, we present GORP, the first dataset of realistic sparse inputs from a commercial virtual reality (VR) headset with paired high quality body motion ground truth. GORP provides >14 hours of VR gameplay data from 28 people using motion controllers (spatially sparse) and hand tracking (spatially and temporally sparse). We benchmark RPM against the state of the art on both synthetic data and GORP to highlight how we can bridge the gap for real-world applications with a realistic dataset and by handling unreliable input signals. Our code, pretrained models, and GORP dataset are available in BLINDED FOR REVIEW.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces the Rolling Prediction Model (RPM), a real-time approach for generating smooth full-body motion in extended reality (XR) applications from sparse and unreliable input signals, such as those from vision-based hand tracking. RPM operates in two modes: tracking mode, which accurately matches the input signals, and synthesis mode, which generates plausible motion when inputs are missing. The model ensures seamless transitions between these modes. The authors also present GORP, a novel dataset comprising realistic sparse inputs from a commercial VR headset paired with high-quality body motion ground truth, to demonstrate the practical importance of handling noisy and missing inputs. RPM's effectiveness is validated against state-of-the-art methods on both synthetic data and the GORP dataset, showcasing its potential for real-world applications.",
        "Tags": [
            "Avatars",
            "3D Reconstruction",
            "Real-Time Motion Generation",
            "Hand Tracking",
            "Dataset Creation"
        ]
    },
    {
        "Title": "FineLIP: Extending CLIP\u2019s Reach via Fine-Grained Alignment with Longer Text Inputs",
        "Authors": "Mothilal Asokan \u00b7 Kebin wu \u00b7 Fatima Albreiki",
        "Abstract": "As a pioneering vision-language model, CLIP (Contrastive Language-Image Pre-training) has achieved significant success across various domains and a wide range of downstream vision-language tasks. However, the text encoders in popular CLIP models are limited to processing only 77 text tokens, which constrains their ability to effectively handle longer, detail-rich captions. Additionally, CLIP models often struggle to effectively capture detailed visual and textual information, which hampers their performance on tasks that require fine-grained analysis. To address these limitations, we present a novel approach, FineLIP, that extends the capabilities of CLIP. FineLIP enhances cross-modal text-image mapping by incorporating Fine-grained alignment with Longer text input within the CLIP-style framework. FineLIP first extends the positional embeddings to handle longer text, followed by the dynamic aggregation of local image and text tokens. The aggregated results are then used to enforce fine-grained token-to-token cross-modal alignment. We validate our model on datasets with long, detailed captions across two tasks: zero-shot cross-modal retrieval and text-to-image generation. Quantitative and qualitative experimental results demonstrate the effectiveness of FineLIP, outperforming existing state-of-the-art approaches. Furthermore, comprehensive ablation studies validate the benefits of key design elements within FineLIP.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "FineLIP extends the capabilities of CLIP by addressing its limitations in handling longer, detail-rich text inputs and capturing fine-grained visual-textual information. The proposed approach enhances cross-modal text-image mapping through extended positional embeddings and dynamic aggregation of local image and text tokens, enabling fine-grained token-to-token alignment. FineLIP demonstrates superior performance in zero-shot cross-modal retrieval and text-to-image generation tasks, validated through quantitative and qualitative experiments, as well as ablation studies.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Zero-Shot Learning",
            "Fine-Grained Alignment",
            "Longer Text Inputs",
            "Dynamic Token Aggregation"
        ]
    },
    {
        "Title": "Gyro-based Neural Single Image Deblurring",
        "Authors": "Heemin Yang \u00b7 Jaesung Rim \u00b7 Seungyong Lee \u00b7 Seung-Hwan Baek \u00b7 Sunghyun Cho",
        "Abstract": "In this paper, we present GyroDeblurNet, a novel single image deblurring method that utilizes a gyro sensor to effectively resolve the ill-posedness of image deblurring.The gyro sensor provides valuable information about camera motion that can significantly improve deblurring quality.However, effectively exploiting real-world gyro data is challenging due to significant errors from various sources.To handle these errors, GyroDeblurNet is equipped with two novel neural network blocks: a gyro refinement block and a gyro deblurring block.The gyro refinement block refines the erroneous gyro data using the blur information from the input image.The gyro deblurring block removes blur from the input image using the refined gyro data and further compensates for gyro error by leveraging the blur information from the input image.For training a neural network with erroneous gyro data, we propose a training strategy based on the curriculum learning.We also introduce a novel gyro data embedding scheme to represent real-world intricate camera shakes.Finally, we present both synthetic and real-world datasets for training and evaluating gyro-based single image deblurring.Our experiments demonstrate that our approach achieves state-of-the-art deblurring quality by effectively utilizing erroneous gyro data.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces GyroDeblurNet, a novel single image deblurring method that leverages gyro sensor data to address the ill-posed nature of image deblurring. The method incorporates two key neural network blocks: a gyro refinement block, which corrects errors in gyro data using blur information from the input image, and a gyro deblurring block, which removes blur using the refined gyro data and compensates for residual errors. A curriculum learning-based training strategy is proposed to handle erroneous gyro data, along with a novel gyro data embedding scheme to represent complex camera shakes. The approach is validated on both synthetic and real-world datasets, demonstrating state-of-the-art deblurring performance.",
        "Tags": [
            "Deblur",
            "Low-Level Vision",
            "Gyro Sensor Integration",
            "Curriculum Learning",
            "Camera Motion Estimation"
        ]
    },
    {
        "Title": "Devil is in the Detail: Towards Injecting Fine Details of Image Prompt in Image Generation via Conflict-free Guidance and Stratified Attention",
        "Authors": "Kyungmin Jo \u00b7 Jooyeol Yun \u00b7 Jaegul Choo",
        "Abstract": "While large-scale text-to-image diffusion models enable the generation of high-quality, diverse images from text prompts, these prompts struggle to capture intricate details, such as textures, preventing the user intent from being reflected. This limitation has led to efforts to generate images conditioned on user-provided images, referred to as image prompts. Recent work modifies the self-attention mechanism to impose image conditions in generated images by replacing or concatenating the keys and values from the image prompt. This enables the self-attention layer to work like a cross-attention layer, generally used to incorporate text prompts.In this paper, we identify two common issues in existing methods of modifying self-attention that hinder diffusion models from reflecting the image prompt. By addressing these issues, we propose a novel method that generates images that properly reflect the details of image prompts. First, existing approaches often neglect the importance of image prompts in classifier-free guidance, which directs the model towards the intended conditions and away from those undesirable. Specifically, current methods use image prompts as both desired and undesired conditions, causing conflicting signals. To resolve this, we propose conflict-free guidance by using image prompts only as desired conditions, ensuring that the generated image faithfully reflects the image prompt.In addition, we observe that the two most common self-attention modifications involve a trade-off between the realism of the generated image and alignment with the image prompt, achieved by selectively using keys and values from both images. Specifically, selecting more keys and values from the image prompt improves alignment, while selecting more from the generated image enhances realism. To balance both, we propose an alternative self-attention modification method, Stratified Attention, which jointly uses keys and values from both images rather than selecting between them.Through extensive experiments across three distinct image generation tasks, we demonstrate that the proposed method outperforms existing image-prompting models in faithfully reflecting the image prompt.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of incorporating intricate details from image prompts into generated images using diffusion models. Existing methods often fail to reflect fine details due to conflicting signals in classifier-free guidance and trade-offs between realism and alignment in self-attention modifications. The authors propose two key solutions: (1) conflict-free guidance, which uses image prompts exclusively as desired conditions to avoid conflicting signals, and (2) stratified attention, a novel self-attention modification that jointly utilizes keys and values from both the image prompt and the generated image to balance realism and alignment. The proposed method demonstrates superior performance in reflecting image prompts across multiple image generation tasks.",
        "Tags": [
            "Diffusion Models",
            "Image Generation",
            "Conflict-Free Guidance",
            "Stratified Attention",
            "Image-Prompted Generation"
        ]
    },
    {
        "Title": "Dual Prompting for Image Restoration across Full-Scene with Diffusion Transformers",
        "Authors": "Dehong Kong \u00b7 Fan Li \u00b7 Zhixin Wang \u00b7 Jiaqi Xu \u00b7 Renjing Pei \u00b7 Wenbo Li \u00b7 Wenqi Ren",
        "Abstract": "Recent state-of-the-art image restoration methods mostly adopt latent diffusion models with U-Net backbones, yet still facing challenges in achieving high-quality restoration due to their limited capabilities. Diffusion transformers (DiTs), like SD3, are emerging as a promising alternative because of their better quality with scalability. However, previous conditional control methods for U-Net-based diffusion models, such as ControlNet, are not well-suited for DiTs. In this paper, we introduce DPIR (Dual Prompting Image Restoration), a novel DiT-based image restoration method that effectivly extracts conditional information of low-quality images from multiple perspectives. Specifically, DPIR consits of two branches: a low-quality image prior conditioning branch and a dual prompting control branch. into the DiT with high training efficiency. More importantly, we believe that in image restoration, the image's textual description alone cannot fully capture its rich visual characteristics. Therefore, a dual prompting module is designed to provide DiT with additional visual cues, capturing both global context and local appearance. The extracted global-local visual prompts as extra conditional control, together with text prompts, greatly enhance the quality and fidelity of the restoration. Extensive experimental results demonstrate that DPIR delivers superior image restoration performance with broad applicability.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces DPIR (Dual Prompting Image Restoration), a novel image restoration method based on Diffusion Transformers (DiTs). Unlike traditional U-Net-based approaches, DPIR leverages a dual-branch architecture: one branch extracts conditional information from low-quality images, while the other employs a dual prompting module to provide additional visual cues, capturing both global context and local appearance. This approach enhances the quality and fidelity of image restoration by combining textual and visual prompts, addressing the limitations of existing methods. DPIR demonstrates superior performance and broad applicability in image restoration tasks.",
        "Tags": [
            "Diffusion Models",
            "Image Restoration",
            "Vision Transformer (ViT)",
            "Dual Prompting",
            "Global-Local Visual Cues",
            "Conditional Control"
        ]
    },
    {
        "Title": "ExpertAF: Expert Actionable Feedback from Video",
        "Authors": "Kumar Ashutosh \u00b7 Tushar Nagarajan \u00b7 Georgios Pavlakos \u00b7 Kris Kitani \u00b7 Kristen Grauman",
        "Abstract": "Feedback is essential for learning a new skill or improving one's current skill-level. However, current methods for skill-assessment from video only provide scores or compare demonstrations, leaving the burden of knowing what to do differently on the user. We introduce a novel method to generate actionable feedback from video of a person doing a physical activity, such as basketball or soccer.  Our method takes a video demonstration and its accompanying 3D body pose and generates (1) free-form expert commentary describing what the person is doing well and what they could improve, and (2) a visual expert demonstration that incorporates the required corrections. We show how to leverage Ego-Exo4D's videos of skilled activity and expert commentary together with a strong language model to create a weakly-supervised training dataset for this task, and we devise a multimodal video-language model to infer coaching feedback. Our method is able to reason across multi-modal input combinations to output full-spectrum, actionable coaching---expert commentary, expert video retrieval, and expert pose generation---outperforming strong vision-language models on both established metrics and human preference studies. Code and data will be publicly released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces ExpertAF, a novel method for generating actionable feedback from videos of physical activities like basketball or soccer. The method uses a video demonstration and its 3D body pose to produce expert commentary and visual demonstrations that highlight areas for improvement. It leverages the Ego-Exo4D dataset and a strong language model to create a weakly-supervised training dataset, and employs a multimodal video-language model to infer coaching feedback. The approach outperforms existing vision-language models in generating comprehensive coaching feedback, including expert commentary, video retrieval, and pose generation.",
        "Tags": [
            "Avatars",
            "Vision-Language Models (VLMs)",
            "3D Pose Estimation",
            "Multimodal Learning",
            "Weakly-Supervised Learning"
        ]
    },
    {
        "Title": "SeqMvRL: A Sequential Fusion Framework for Multi-view Representation Learning",
        "Authors": "Ren Wang \u00b7 Haoliang Sun \u00b7 Yuxiu Lin \u00b7 Chuanhui Zuo \u00b7 Yongshun Gong \u00b7 Yilong Yin \u00b7 Wenjia Meng",
        "Abstract": "Multi-view representation learning integrates multiple observable views of an entity into a unified representation to facilitate downstream tasks. Current methods predominantly focus on distinguishing compatible components across views, followed by a single-step parallel fusion process. However, this parallel fusion is static in essence, overlooking potential conflicts among views and compromising representation ability. To address this issue, this paper proposes a novel \\textbf{Seq}uential fusion framework for \\textbf{M}ulti-\\textbf{v}iew \\textbf{R}epresentation \\textbf{L}earning, termed \\textbf{SeqMvRL}. Specifically, we model multi-view fusion as a sequential decision-making problem and construct a pairwise integrator (PI) and a next-view selector (NVS), which represent the \\textit{environment} and \\textit{agent} in reinforcement learning, respectively. PI merges the current fused feature with the selected view, while NVS is introduced to determine which view to fuse subsequently. By adaptively selecting the next optimal view for fusion based on the current fusion state, SeqMvRL thereby effectively reduces conflicts and enhances unified representation quality. Additionally, an elaborate novel reward function encourages the model to prioritize views that enhance the discriminability of the fused features. Experimental results demonstrate that SeqMvRL outperforms parallel fusion approaches in classification and clustering tasks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces SeqMvRL, a sequential fusion framework for multi-view representation learning, addressing the limitations of static parallel fusion methods. SeqMvRL models the fusion process as a sequential decision-making problem, utilizing a pairwise integrator (PI) to merge features and a next-view selector (NVS) to determine the subsequent view for fusion. This adaptive approach reduces conflicts among views and improves the quality of unified representations. A novel reward function is also introduced to prioritize views that enhance the discriminability of the fused features. The framework demonstrates superior performance in classification and clustering tasks compared to traditional parallel fusion methods.",
        "Tags": [
            "Multimodal Learning",
            "Reinforcement Learning",
            "Sequential Fusion",
            "Adaptive View Selection",
            "Discriminability Enhancement"
        ]
    },
    {
        "Title": "RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied Navigation",
        "Authors": "Mingfei Han \u00b7 Liang Ma \u00b7 Kamila Zhumakhanova \u00b7 Ekaterina Radionova \u00b7 Jingyi Zhang \u00b7 Xiaojun Chang \u00b7 Xiaodan Liang \u00b7 Ivan Laptev",
        "Abstract": "Vision-and-Language Navigation (VLN) suffers from the limited diversity and scale of training data, primarily constrained by the manual curation of existing simulators.To address this, we introduce RoomTour3D, a video-instruction dataset derived from web-based room tour videos that capture real-world indoor spaces and human walking demonstrations. Unlike existing VLN datasets, RoomTour3D leverages the scale and diversity of online videos to generate open-ended human walking trajectories and open-world navigable instructions. To compensate for the lack of navigation data in online videos, we perform 3D reconstruction and obtain 3D trajectories of walking paths augmented with additional information on the room types, object locations and 3D shape of surrounding scenes. Our dataset includes $\\sim$100K open-ended description-enriched trajectories with $\\sim$200K instructions, and 17K action-enriched trajectories from 1847 room tour environments.We demonstrate experimentally that RoomTour3D enables significant improvements across multiple VLN tasks including CVDN, SOON, R2R, and REVERIE.Moreover, RoomTour3D facilitates the development of trainable zero-shot VLN agents, showcasing the potential and challenges of advancing towards open-world navigation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "RoomTour3D addresses the limitations of Vision-and-Language Navigation (VLN) by introducing a dataset derived from web-based room tour videos, capturing real-world indoor spaces and human walking demonstrations. This dataset leverages the scale and diversity of online videos to generate open-ended human walking trajectories and navigable instructions. To enhance the dataset, 3D reconstruction is performed to obtain 3D trajectories of walking paths, augmented with room types, object locations, and 3D scene shapes. The dataset includes approximately 100K open-ended description-enriched trajectories with around 200K instructions, and 17K action-enriched trajectories from 1847 room tour environments. Experimental results show that RoomTour3D significantly improves performance across multiple VLN tasks and facilitates the development of zero-shot VLN agents, highlighting the potential and challenges of open-world navigation.",
        "Tags": [
            "Embodied AI",
            "3D Reconstruction",
            "Video-Instruction Tuning",
            "Open-World Navigation",
            "Zero-Shot Learning"
        ]
    },
    {
        "Title": "MoEE: Mixture of Emotion Experts for Audio-Driven Portrait Animation",
        "Authors": "Huaize Liu \u00b7 WenZhang Sun \u00b7 Donglin Di \u00b7 Shibo Sun \u00b7 Jiahui Yang \u00b7 Hujun Bao \u00b7 Changqing Zou",
        "Abstract": "The generation of talking avatars has achieved significant advancements in precise audio synchronization. However, crafting lifelike talking head videos requires capturing a broad spectrum of emotions and subtle facial expressions. Current methods face fundamental challenges: a) the absence of frameworks for modeling single basic emotional expressions, which restricts the generation of complex emotions such as compound emotions; b) the lack of comprehensive datasets rich in human emotional expressions, which limits the potential of models. To address these challenges, we propose the following innovations: 1) the Mixture of Emotion Experts (MoEE) model, which decouples six fundamental emotions to enable the precise synthesis of both singular and compound emotional states; 2) the DH-FaceEmoVid-150 dataset, specifically curated to include six prevalent human emotional expressions as well as four types of compound emotions, thereby expanding the training potential of emotion-driven models; 3) an emotion-to-latents module that leverages multimodal inputs, aligning diverse control signals\u2014such as audio, text, and labels\u2014to enhance audio-driven emotion control. Through extensive quantitative and qualitative evaluations, we demonstrate that the MoEE framework, in conjunction with the DH-FaceEmoVid-150 dataset, excels in generating complex emotional expressions and nuanced facial details, setting a new benchmark in the field. These datasets will be publicly released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces the Mixture of Emotion Experts (MoEE) model to address challenges in generating lifelike talking avatars with complex emotional expressions. The MoEE model decouples six fundamental emotions to synthesize both singular and compound emotional states. Additionally, the authors present the DH-FaceEmoVid-150 dataset, which includes six basic and four compound emotional expressions, enhancing the training potential of emotion-driven models. The paper also proposes an emotion-to-latents module that aligns multimodal inputs like audio, text, and labels to improve audio-driven emotion control. The MoEE framework, combined with the DH-FaceEmoVid-150 dataset, demonstrates superior performance in generating nuanced facial details and complex emotional expressions, setting a new benchmark in the field.",
        "Tags": [
            "Avatars",
            "Multimodal Learning",
            "Emotion Synthesis",
            "Compound Emotions",
            "Multimodal Input Alignment"
        ]
    },
    {
        "Title": "An Image-like Diffusion Method for Human-Object Interaction Detection",
        "Authors": "Xiaofei Hui \u00b7 Haoxuan Qu \u00b7 Hossein Rahmani \u00b7 Jun Liu",
        "Abstract": "Human-object interaction (HOI) detection often faces high levels of ambiguity and indeterminacy, as the same interaction can appear vastly different across different human-object pairs. Additionally, the indeterminacy can be further exacerbated by issues such as occlusions and cluttered backgrounds. To handle such a challenging task, in this work, we begin with a key observation: the output of HOI detection for each human-object pair can be recast as an image. Thus, inspired by the strong image generation capabilities of image diffusion models, we propose a new framework, HOI-IDiff. In HOI-IDiff, we tackle HOI detection from a novel perspective, using an Image-like Diffusion process to generate HOI detection outputs as images. Furthermore, recognizing that our recast images differ in certain properties from natural images, we enhance our framework with a customized HOI diffusion process and a slice patchification model architecture, which are specifically tailored to generate our recast ``HOI images''. Extensive experiments demonstrate the efficacy of our framework.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Human-object interaction (HOI) detection is challenging due to ambiguity, indeterminacy, and issues like occlusions and cluttered backgrounds. This work introduces HOI-IDiff, a novel framework that recasts HOI detection outputs as images and leverages image diffusion models for generation. The framework incorporates a customized HOI diffusion process and a slice patchification model architecture tailored for generating recast 'HOI images.' The approach demonstrates significant efficacy in addressing the complexities of HOI detection.",
        "Tags": [
            "Object Detection",
            "Diffusion Models",
            "Human-Object Interaction",
            "Image-like Diffusion",
            "Customized Diffusion Process"
        ]
    },
    {
        "Title": "Activating Sparse Part Concepts for 3D Class Incremental Learning",
        "Authors": "Zhenya Tian \u00b7 Jun Xiao \u00b7 Liu lupeng \u00b7 Haiyong Jiang",
        "Abstract": "This work tackles the challenge of 3D Class-Incremental Learning (CIL), where a model must learn to classify new 3D objects while retaining knowledge of previously learned classes. Existing methods often struggle with catastrophic forgetting, misclassifying old objects due to overreliance on shortcut local features. Our approach addresses this issue by learning a set of part concepts for part-aware features. Particularly, we only activate a small subset of part concepts for the feature representation of each part-aware feature. This facilitates better generalization across categories and mitigates catastrophic forgetting. We further improve the task-wise classification through a part relation-aware Transformer design. At last, we devise learnable affinities to fuse task-wise classification heads and avoid confusion among different tasks. We evaluate our method on three 3D CIL benchmarks, achieving state-of-the-art performance. (Code and data will be released)",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This work addresses the challenge of 3D Class-Incremental Learning (CIL), where a model must classify new 3D objects while retaining knowledge of previously learned classes. The proposed method mitigates catastrophic forgetting by learning a set of part concepts for part-aware features, activating only a small subset for each feature representation. This approach enhances generalization across categories and reduces misclassification of old objects. Additionally, a part relation-aware Transformer design improves task-wise classification, and learnable affinities are introduced to fuse task-wise classification heads, preventing confusion among tasks. The method achieves state-of-the-art performance on three 3D CIL benchmarks.",
        "Tags": [
            "3D Object Detection",
            "Class-Incremental Learning",
            "Transformer",
            "Part-Aware Features",
            "Catastrophic Forgetting Mitigation",
            "Task-Wise Classification Fusion"
        ]
    },
    {
        "Title": "Do computer vision foundation models learn the low-level characteristics of the human visual system?",
        "Authors": "Yancheng Cai \u00b7 Fei Yin \u00b7 Dounia Hammou \u00b7 Rafal Mantiuk",
        "Abstract": "Computer vision foundation models, such as DINO or OpenCLIP, are trained in a self-supervised manner on large image datasets. Analogously, substantial evidence suggests that the human visual system (HVS) is influenced by the statistical distribution of colors and patterns in the natural world, characteristics also present in the training data of foundation models. The question we address in this paper is whether foundation models trained on natural images mimic some of the low-level characteristics of the human visual system, such as contrast detection, contrast masking, and contrast constancy. Specifically, we designed a protocol comprising nine test types to evaluate the image encoders of 45 foundation and generative models. Our results indicate that some foundation models (e.g., DINO, DINOv2, and OpenCLIP), share some of the characteristics of human vision, but other models show little resemblance. Foundation models tend to show smaller sensitivity to low contrast and rather irregular responses to contrast across frequencies. The foundation models show the best agreement with human data in terms of contrast masking. Our findings suggest that human vision and computer vision may take both similar and different paths when learning to interpret images of the real world. Overall, while differences remain, foundation models trained on vision tasks start to align with low-level human vision, with DINOv2 showing the closest resemblance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper investigates whether computer vision foundation models, such as DINO and OpenCLIP, trained on large image datasets in a self-supervised manner, mimic low-level characteristics of the human visual system (HVS), including contrast detection, masking, and constancy. A protocol with nine test types was designed to evaluate 45 foundation and generative models. Results show that some models, particularly DINO, DINOv2, and OpenCLIP, share certain characteristics with human vision, such as contrast masking, but exhibit less sensitivity to low contrast and irregular responses to contrast across frequencies. While differences persist, foundation models trained on vision tasks begin to align with low-level human vision, with DINOv2 showing the closest resemblance.",
        "Tags": [
            "Self-Supervised Learning",
            "Vision-Language Models (VLMs)",
            "Contrast Detection",
            "Contrast Masking",
            "Human Visual System (HVS)"
        ]
    },
    {
        "Title": "VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM",
        "Authors": "Yuqian Yuan \u00b7 Hang Zhang \u00b7 Wentong Li \u00b7 Zesen Cheng \u00b7 Boqiang Zhang \u00b7 Long Li \u00b7 Xin Li \u00b7 Deli Zhao \u00b7 Wenqiao Zhang \u00b7 Yueting Zhuang \u00b7 Jianke Zhu \u00b7 Lidong Bing",
        "Abstract": "Video Large Language Models (Video LLMs) have recently exhibited remarkable capabilities in general video understanding.However, they mainly focus on holistic comprehension and struggle with capturing fine-grained spatial and temporal details. Besides, the lack of high-quality object-level video instruction data and a comprehensive benchmark further hinders their advancements. To tackle these challenges, we introduce the VideoRefer Suite to empower Video LLM for finer-level spatial-temporal video understanding, i.e., enabling perception and reasoning on any objects throughout the video. Specially, we thoroughly develop VideoRefer Suite across three essential aspects: dataset, model, and benchmark. Firstly, we introduce a multi-agent data engine to meticulously curate a large-scale, high-quality object-level video instruction dataset, termed VideoRefer-700K. Next, we present the VideoRefer model, which equips a versatile spatial-temporal object encoder to capture precise regional and sequential representations. Finally, we meticulously create a VideoRefer-Bench to comprehensively assess the spatial-temporal understanding capability of a Video LLM, evaluating it across various aspects. Extensive experiments and analyses demonstrate that our VideoRefer model not only achieves promising performance on video referring benchmarks but also facilitates general video understanding capabilities. Our codes, models, and dataset will be made publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Video Large Language Models (Video LLMs) have shown significant progress in general video understanding but often fail to capture fine-grained spatial and temporal details. To address this, the authors introduce the VideoRefer Suite, which enhances Video LLMs for detailed spatial-temporal object understanding. The suite includes three key components: a large-scale, high-quality object-level video instruction dataset (VideoRefer-700K), the VideoRefer model with a spatial-temporal object encoder for precise regional and sequential representations, and the VideoRefer-Bench benchmark for evaluating spatial-temporal understanding. The VideoRefer model demonstrates strong performance on video referring benchmarks and improves general video understanding capabilities.",
        "Tags": [
            "Video Understanding",
            "Large Language Models (LLMs)",
            "Spatial-Temporal Object Understanding",
            "Video Instruction Dataset",
            "Video LLM Benchmark"
        ]
    },
    {
        "Title": "FINECAPTION: Compositional Image Captioning Focusing on Wherever You Want at Any Granularity",
        "Authors": "Hang Hua \u00b7 Qing Liu \u00b7 Lingzhi Zhang \u00b7 Jing Shi \u00b7 Soo Ye Kim \u00b7 Zhifei Zhang \u00b7 Yilin Wang \u00b7 Jianming Zhang \u00b7 Zhe Lin \u00b7 Jiebo Luo",
        "Abstract": "The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal tasks, enabling more sophisticated and accurate integration of visual and textual information across various applications, including image and video captioning, visual question answering, and cross-modal retrieval.Despite their superior capabilities, VLMs still struggle with fine-grained compositional image region descriptions. Specifically, they have difficulty recognizing arbitrary segmentation masks as referential inputs, interpreting compositional aspect instructions for referencing, and precisely describing the compositional aspects of a region. However, compositionality\u2014the ability to understand and generate novel combinations of known visual and textual components\u2014is critical for facilitating coherent reasoning and understanding across modalities in VLMs. To address this issue, we propose OpenCompositionCap, a new dataset for multi-grained region compositional image captioning that distinguishes itself from prior works by introducing the new task of compositional aspect-aware regional image captioning. To support this endeavor, we also introduce a new VLM model, FineCaption. The empirical results illustrate the effectiveness of our proposed model compared with other strong VLMs. In addition, we analyze the capabilities of current VLMs in recognizing various visual prompts for compositional region image captioning, highlighting areas for improvement in VLM design and training.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the limitations of large Vision-Language Models (VLMs) in generating fine-grained compositional descriptions of image regions. It introduces OpenCompositionCap, a dataset designed for multi-grained region compositional image captioning, and proposes FineCaption, a new VLM model tailored for this task. The study demonstrates the effectiveness of FineCaption over existing VLMs and identifies areas for improvement in VLM design and training, particularly in recognizing arbitrary segmentation masks and interpreting compositional aspect instructions.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Image Captioning",
            "Compositional Image Captioning",
            "Fine-Grained Visual Understanding",
            "Segmentation Masks"
        ]
    },
    {
        "Title": "SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding",
        "Authors": "Hao Li \u00b7 Changyao TIAN \u00b7 Jie Shao \u00b7 Xizhou Zhu \u00b7 Zhaokai Wang \u00b7 Jinguo Zhu \u00b7 Wenhan Dou \u00b7 Xiaogang Wang \u00b7 Hongsheng Li \u00b7 Lewei Lu \u00b7 Jifeng Dai",
        "Abstract": "The remarkable success of Large Language Models (LLMs) has extended to the multimodal domain, achieving outstanding performance in image understanding and generation. Recent efforts to develop unified Multimodal Large Language Models (MLLMs) that integrate these capabilities have shown promising results. However, existing approaches often involve complex designs in model architecture or training pipeline, increasing the difficulty of model training and scaling. In this paper, we propose SynerGen-VL, a simple yet powerful encoder-free MLLM capable of both image understanding and generation. To address challenges identified in existing encoder-free unified MLLMs, we introduce the token folding mechanism and the vision-expert-based progressive alignment pretraining strategy, effectively supporting high-resolution image understanding while reducing training complexity. After being trained on large-scale mixed image-text data with a unified next-token prediction objective, SynerGen-VL achieves or surpasses the performance of existing encoder-free unified MLLMs with comparable or smaller parameter sizes, and narrows the gap with task-specific state-of-the-art models, highlighting a promising path toward future unified MLLMs. Our code and models shall be released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "SynerGen-VL is introduced as a simple yet powerful encoder-free Multimodal Large Language Model (MLLM) capable of both image understanding and generation. It addresses the challenges of existing encoder-free unified MLLMs through a token folding mechanism and a vision-expert-based progressive alignment pretraining strategy. These innovations support high-resolution image understanding and reduce training complexity. Trained on large-scale mixed image-text data with a unified next-token prediction objective, SynerGen-VL achieves or surpasses the performance of existing encoder-free unified MLLMs with comparable or smaller parameter sizes, narrowing the gap with task-specific state-of-the-art models.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Image Generation",
            "Token Folding",
            "Progressive Alignment Pretraining",
            "Encoder-Free Model"
        ]
    },
    {
        "Title": "OmniGuard: Hybrid Manipulation Localization via Augmented Versatile Deep Image Watermarking",
        "Authors": "Xuanyu Zhang \u00b7 Zecheng Tang \u00b7 Zhipei Xu \u00b7 Runyi Li \u00b7 Youmin Xu \u00b7 Bin Chen \u00b7 Feng Gao \u00b7 Jian Zhang",
        "Abstract": "With the rapid growth of generative AI and its widespread application in image editing, new risks have emerged regarding the authenticity and integrity of digital content. Existing versatile watermarking approaches suffer from trade-offs between tamper localization precision and visual quality. Constrained by the limited flexibility of previous framework, their localized watermark must remain fixed across all images. Under AIGC-editing, their copyright extraction accuracy is also unsatisfactory. To address these challenges, we propose \\textbf{OmniGuard}, a novel augmented versatile watermarking approach that integrates proactive embedding with passive, blind extraction for robust copyright protection and tamper localization. OmniGuard employs a hybrid forensic framework that enables flexible localization watermark selection and introduces a degradation-aware tamper extraction network for precise localization under challenging conditions. Additionally, a lightweight AIGC-editing simulation layer is designed to enhance robustness across global and local editing. Extensive experiments show that OmniGuard achieves superior fidelity, robustness, and flexibility. Compared to the recent state-of-the-art approach EditGuard, our method outperforms it by 4.25dB in PSNR of the container image, 20.7\\% in F1-Score under noisy conditions, and 14.8\\% in average bit accuracy.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces OmniGuard, a novel augmented versatile watermarking approach designed to address the challenges of tamper localization precision and visual quality in digital content. OmniGuard integrates proactive embedding with passive, blind extraction to enhance copyright protection and tamper localization. It features a hybrid forensic framework for flexible watermark selection and a degradation-aware tamper extraction network for precise localization under challenging conditions. Additionally, a lightweight AIGC-editing simulation layer is included to improve robustness across various editing scenarios. The method demonstrates superior fidelity, robustness, and flexibility, outperforming the state-of-the-art approach EditGuard in several key metrics.",
        "Tags": [
            "Image Editing",
            "Data Augmentation",
            "Hybrid Forensic Framework",
            "Degradation-Aware Network",
            "AIGC-Editing Simulation"
        ]
    },
    {
        "Title": "Stochastic Human Motion Prediction with Memory of  Action Transition and Action Characteristic",
        "Authors": "JIANWEI TANG \u00b7 Hong Yang \u00b7 Tengyue Chen \u00b7 Jian-Fang Hu",
        "Abstract": "Action-driven stochastic human motion prediction aims to generate future motion sequence of a pre-defined target action based on given past observed sequences performing non-traget actions. This task primarily presents two challenges. Firstly, generating smooth transition motions is hard due to the vary transition speed of different actions. Secondly, the action characteristic is difficult to be learned because of the similarity of some actions. These issues cause the predicting results being unreasonable and inconsistent. As a result, we propose two memory banks, the Soft-transition Action Bank (STAB) and Action Characteristic Bank (ACB), to tackle the problems above. The STAB stores the action transition information. It is equiped with the novel soft searching approach, which encourages the model to focus on multiple possible action categories of observed motions. The ACB records action characteristic, which produces more prior information for predicting certain action. To fuse the features retrieved from two banks better, we further propose the Adaptive Attention Adjustment (AAA) strategy. Extensive experiments on four motion prediction datasets demonstrate that our approach consistently outperforms previous state-of-the-art.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenges in stochastic human motion prediction, specifically focusing on generating future motion sequences for a target action based on past observed sequences of non-target actions. The primary challenges include generating smooth transition motions due to varying transition speeds and learning distinct action characteristics despite similarities among actions. To overcome these issues, the authors propose two memory banks: the Soft-transition Action Bank (STAB) and the Action Characteristic Bank (ACB). STAB stores action transition information and employs a soft searching approach to consider multiple possible action categories. ACB records action characteristics to provide prior information for prediction. Additionally, an Adaptive Attention Adjustment (AAA) strategy is introduced to better fuse features from both banks. The proposed method demonstrates superior performance on four motion prediction datasets compared to existing state-of-the-art approaches.",
        "Tags": [
            "Human Action Prediction",
            "Stochastic Modeling",
            "Soft-transition Action Bank",
            "Action Characteristic Bank",
            "Adaptive Attention Adjustment"
        ]
    },
    {
        "Title": "CroCoDL: Cross-device Collaborative Dataset for Localization",
        "Authors": "Hermann Blum \u00b7 Alessandro Mercurio \u00b7 Joshua O&#x27;Reilly \u00b7 Tim Engelbracht \u00b7 Mihai Dusmanu \u00b7 Marc Pollefeys \u00b7 Zuria Bauer",
        "Abstract": "Accurate localization plays a pivotal role in the autonomy of systems operating in unfamiliar environments, particularly when interaction with humans is expected. High-accuracy visual localization systems encompass various components, such as feature extractors, matchers, and pose estimation methods. This complexity translates to the necessity of robust evaluation settings and pipelines. However, existing datasets and benchmarks primarily focus on single-agent scenarios, overlooking the critical issue of cross-device localization. Different agents with different sensors will show their own specific strengths and weaknesses, and the data they have available varies substantially. This work addresses this gap by enhancing an existing augmented reality visual localization benchmark with data from legged robots, and evaluating human-robot, cross-device mapping and localization. Our contributions extend beyond device diversity and include high environment variability, spanning ten distinct locations ranging from disaster sites to art exhibitions. Each scene in our dataset features recordings from robot agents, hand-held and head-mounted devices, and high-accuracy ground truth LiDAR scanners, resulting in a comprehensive multi-agent dataset and benchmark. This work represents a significant advancement in the field of visual localization benchmarking, with key insights into the performance of cross-device localization methods across diverse settings.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces CroCoDL, a cross-device collaborative dataset designed to address the limitations of existing visual localization benchmarks, which primarily focus on single-agent scenarios. CroCoDL enhances an augmented reality visual localization benchmark by incorporating data from legged robots, hand-held, and head-mounted devices, alongside high-accuracy ground truth LiDAR scanners. This dataset spans ten distinct environments, from disaster sites to art exhibitions, providing a comprehensive evaluation setting for cross-device localization methods. The work highlights the importance of device diversity and environmental variability in improving the robustness and applicability of visual localization systems.",
        "Tags": [
            "Visual Tracking",
            "3D Reconstruction",
            "Datasets and Benchmarks",
            "Cross-device Localization",
            "Multi-agent Systems",
            "Environmental Variability"
        ]
    },
    {
        "Title": "Multi-view Reconstruction via SfM-guided Monocular Depth Estimation",
        "Authors": "Haoyu Guo \u00b7 He Zhu \u00b7 Sida Peng \u00b7 Haotong Lin \u00b7 Yunzhi Yan \u00b7 Tao Xie \u00b7 Wenguan Wang \u00b7 Xiaowei Zhou \u00b7 Hujun Bao",
        "Abstract": "This paper aims to reconstruct the scene geometry from multi-view images with strong robustness and high quality. Previous learning-based methods incorporate neural networks into the multi-view stereo matching and have shown impressive reconstruction results. However, due to the reliance on matching across input images, they typically suffer from high GPU memory consumption and tend to fail in sparse view scenarios. To overcome this problem, we develop a new pipeline, named Murre, for multi-view geometry reconstruction of 3D scenes based on SfM-guided monocular depth estimation. For input images, Murre first recover the SfM point cloud that captures the global scene structure, and then use it to guide a conditional diffusion model to produce multi-view metric depth maps for the final TSDF fusion. By predicting the depth map from a single image, Murre bypasses the multi-view matching step and naturally resolves the issues of previous MVS-based methods. In addition, the diffusion-based model can easily leverage the powerful priors of 2D foundation models, achieving good generalization ability across diverse real-world scenes. To obtain multi-view consistent depth maps, our key design is providing effective guidance on the diffusion model through the SfM point cloud, which is a condensed form of multi-view information, highlighting the scene's salient structure, and can be readily transformed into point maps to drive the image-space estimation process. We evaluate the reconstruction quality of Murre in various types of real-world datasets including indoor, streetscapes, and aerial scenes, surpassing state-of-the-art MVS-based and implicit neural reconstruction-based methods. The code will be released for reproducibility.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Murre, a novel pipeline for multi-view geometry reconstruction of 3D scenes using SfM-guided monocular depth estimation. Unlike previous methods that rely on multi-view stereo matching, Murre predicts depth maps from single images, reducing GPU memory consumption and improving performance in sparse view scenarios. The pipeline leverages a conditional diffusion model guided by SfM point clouds to produce multi-view consistent depth maps, which are then used for TSDF fusion. This approach bypasses the limitations of traditional MVS-based methods and achieves superior reconstruction quality across diverse real-world datasets, including indoor, streetscapes, and aerial scenes.",
        "Tags": [
            "3D Reconstruction",
            "Depth Estimation",
            "Diffusion Models",
            "SfM-guided Depth Estimation",
            "TSDF Fusion",
            "Multi-view Consistency"
        ]
    },
    {
        "Title": "CaMuViD: Calibration-Free Multi-View Detection",
        "Authors": "Amir Etefaghi Daryani \u00b7 M. Usman Maqbool Bhutta \u00b7 Byron Hernandez \u00b7 Henry Medeiros",
        "Abstract": "Multi-view object detection in crowded environments presents significant challenges, particularly for occlusion management across multiple camera views. This paper introduces a novel approach that extends conventional multi-view detection to operate directly within each camera's image space. Our method finds objects bounding boxes for images from various perspectives without resorting to a bird\u2019s eye view (BEV) representation. Thus, our approach removes the need for camera calibration by leveraging a learnable architecture that facilitates flexible transformations and improves feature fusion across perspectives to increase detection accuracy. Our model achieves Multi-Object Detection Accuracy (MODA) scores of 95.0% and 96.5% on the Wildtrack and MultiviewX datasets, respectively,  significantly advancing the state of the art in multi-view detection. Furthermore, it demonstrates robust performance even without ground truth annotations, highlighting its resilience and practicality in real-world applications. These results emphasize the effectiveness of our calibration-free, multi-view object detector.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper presents CaMuViD, a novel approach for multi-view object detection in crowded environments that operates directly within each camera's image space, eliminating the need for camera calibration. The method leverages a learnable architecture to enhance feature fusion across perspectives, significantly improving detection accuracy. It achieves state-of-the-art Multi-Object Detection Accuracy (MODA) scores on the Wildtrack and MultiviewX datasets and demonstrates robust performance without ground truth annotations, underscoring its practicality for real-world applications.",
        "Tags": [
            "Object Detection",
            "Multimodal Learning",
            "Calibration-Free Detection",
            "Feature Fusion",
            "Crowded Scene Analysis"
        ]
    },
    {
        "Title": "$\\textit{Early-Bird Diffusion}$: Investigating and Leveraging Timestep-Aware Early-Bird Tickets in Diffusion Models for Efficient Training",
        "Authors": "Lexington Whalen \u00b7 Zhenbang Du \u00b7 Haoran You \u00b7 Chaojian Li \u00b7 Sixu Li \u00b7 Yingyan (Celine) Lin",
        "Abstract": "Training diffusion models (DMs) is highly computationally demanding, necessitating multiple forward and backward passes across numerous timesteps. This challenge has motivated the exploration of various efficient DM training techniques. In this paper, we propose $\\textbf{EB-Diff-Train}$, a new and orthogonal efficient DM training approach by investigating and leveraging Early-Bird (EB) tickets\u2014sparse subnetworks that manifest early in the training process and maintain high generation quality. We first investigate the existence of traditional EB tickets in DMs, enabling competitive generation quality without fully training a dense model. Then, we delve into the concept of diffusion-dedicated EB tickets, which draw on insights from varying importance of different timestep regions/periods. These tickets adapt their sparsity levels according to the importance of corresponding timestep regions, allowing for aggressive sparsity during non-critical regions while conserving computational resources for crucial timestep regions. Building on this, we develop an efficient DM training technique that derives timestep-aware EB tickets, trains them in parallel, and combines them through an ensemble during inference for image generation. This approach can significantly reduce training time both spatially and temporally\u2014achieving 2.9$\\times$$\\sim$5.8$\\times$ speedups over training unpruned dense models, and up to 10.3$\\times$ faster training compared to standard train-prune-finetune pipelines\u2014without compromising generative quality. Extensive experiments and ablation studies validate the existence of both traditional and timestep-aware EB tickets, as well as the effectiveness of our proposed EB-Diff-Train method. Our work not only enhances the understanding of DM training dynamics but also significantly improves training efficiency by exploiting temporal and spatial sparsity. All codes and models will be released upon acceptance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Error_DeepSeek_Response": "```json\n{\n  \"tags\": [\"Diffusion Models\", \"Efficient Training\"],\n  \"abstract\": \"The paper introduces $\\textbf{EB-Diff-Train}$, a novel approach to efficiently train diffusion models (DMs) by leveraging Early-Bird (EB) tickets\u2014sparse subnetworks that emerge early in training and maintain high generation quality. The study first confirms the existence of traditional EB tickets in DMs, which achieve competitive generation quality without full dense model training. It then explores diffusion-dedicated EB tickets, which adjust sparsity levels based on the importance of different timestep regions, enabling aggressive sparsity in non-critical regions and conserving resources for crucial ones. The proposed method trains timestep-aware EB tickets in parallel and combines them during inference, achieving significant training speedups (2.9$\\times$$\\sim$5.8$\\times$ over unpruned dense models and up to 10.3$\\times$ faster than standard pipelines) without compromising generative quality. The work advances the understanding of DM training dynamics and improves efficiency through temporal and spatial sparsity.\",\n  \"extra_tags\": [\"Sparse Subnetworks\", \"Timestep-Aware Sparsity\", \"Parallel Training\"]\n}\n```"
    },
    {
        "Title": "SEC-Prompt:SEmantic Complementary Prompting for Few-Shot Class-Incremental Learning",
        "Authors": "Ye Liu \u00b7 Meng Yang",
        "Abstract": "Few-shot class-incremental learning (FSCIL) presents a significant challenge in machine learning, requiring models to integrate new classes from limited examples while preserving performance on previously learned classes. Recently, prompt-based CIL approaches leverage ample data to train prompts, effectively mitigating catastrophic forgetting. However, these methods do not account for the semantic features embedded in prompts, exacerbating the plasticity-stability dilemma in few-shot incremental learning. In this paper, we propose a novel and simple framework named SEmantic Complementary Prompt(SEC-Prompt), which learns two sets of semantically complementary prompts based on an adaptive query: discriminative prompts(D-Prompt) and non-discriminative prompts(ND-Prompt). D-Prompt enhances the separation of class-specific feature distributions by strengthening key discriminative features, while ND-Prompt balances non-discriminative information to promote generalization to novel classes. To efficiently learn high-quality knowledge from limited samples, we leverage ND-Prompt for data augmentation to increase sample diversity and introduce Prompt Clustering Loss to prevent noise contamination in D-Prompt, ensuring robust discriminative feature learning and improved generalization. Our experimental results showcase state-of-the-art performance across four benchmark datasets, including CIFAR100, ImageNet-R and CUB datasets.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Few-shot class-incremental learning (FSCIL) is a challenging task that requires models to learn new classes from limited examples while maintaining performance on previously learned classes. Existing prompt-based approaches often overlook the semantic features in prompts, worsening the plasticity-stability trade-off. This paper introduces SEC-Prompt, a framework that learns two sets of semantically complementary prompts: discriminative prompts (D-Prompt) and non-discriminative prompts (ND-Prompt). D-Prompt enhances class-specific feature separation, while ND-Prompt balances non-discriminative information to improve generalization. The framework also employs ND-Prompt for data augmentation and introduces Prompt Clustering Loss to ensure robust feature learning. SEC-Prompt achieves state-of-the-art performance on benchmark datasets such as CIFAR100, ImageNet-R, and CUB.",
        "Tags": [
            "Few-Shot Learning",
            "Class-Incremental Learning",
            "Semantic Prompting",
            "Data Augmentation",
            "Prompt Clustering Loss"
        ]
    },
    {
        "Title": "Apollo:  An Exploration of Video Understanding in Large Multi-Modal Models",
        "Authors": "Orr Zohar \u00b7 Xiaohan Wang \u00b7 Yann Dubois \u00b7 Nikhil Mehta \u00b7 Tong Xiao \u00b7 Philippe Hansen-Estruch \u00b7 Licheng Yu \u00b7 Xiaofang Wang \u00b7 Felix Juefei-Xu \u00b7 Ning Zhang \u00b7 Serena Yeung \u00b7 Xide Xia",
        "Abstract": "Despite the rapid integration of video perception capabilities into Large Multi-modal Models (LMMs), what drives their video perception remains poorly understood.Consequently, many design decisions in this domain are made without proper justification or analysis. The high computational cost of training and evaluating such models, coupled with limited open research, hinders the development of video-LMMs. To address this, we present a comprehensive study that uncovers what effectively drives video understanding in LMMs.We begin by critically examining the primary contributors to the high computational requirements associated with video-LMM research and discover *Scaling Consistency*, wherein design and training decisions made on smaller models and datasets (up to a critical size) effectively transfer to larger models. Leveraging these insights, we explored many video-specific aspects of video-LMMs, including video sampling, architectures, data composition, training schedules, and more.Guided by these findings, we introduce **Apollo**, a state-of-the-art family of LMMs that achieve superior performance across different model sizes. Our models process over $1$-hour videos efficiently, with the $3$B parameter variant outperforming most existing $7$B models. **Apollo**-$7$B is state-of-the-art compared to 7B LMMs with a $70.9$ on MLVU, and $63.3$ on Video-MME. Our code and models will be made available at publication.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper investigates the factors driving video understanding in Large Multi-modal Models (LMMs), addressing the lack of understanding and high computational costs in this domain. The study identifies *Scaling Consistency*, where design and training decisions from smaller models and datasets effectively transfer to larger models. Leveraging these insights, the authors explore various video-specific aspects of LMMs, such as video sampling, architectures, and training schedules. They introduce **Apollo**, a family of LMMs that achieve state-of-the-art performance across different model sizes, with the 3B parameter variant outperforming most existing 7B models. **Apollo**-7B sets new benchmarks on MLVU and Video-MME datasets.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Video Understanding",
            "Scaling Consistency",
            "Video-LMMs",
            "Efficient Video Processing"
        ]
    },
    {
        "Title": "SpatialDreamer: Self-supervised Stereo Video Synthesis from Monocular Input",
        "Authors": "Zhen Lv \u00b7 Yangqi Long \u00b7 Congzhentao Huang \u00b7 Cao Li \u00b7 Chengfei Lv \u00b7 Hao Ren \u00b7 Dian Zheng",
        "Abstract": "Stereo video synthesis from a monocular input is a demanding task in the fields of spatial computing and virtual reality. The main challenges of this task lie on the insufficiency of high-quality paired stereo videos for training and the difficulty of maintaining the spatio-temporal consistency between frames. Existing methods mainly handle these problems by directly applying novel view synthesis $(\\textbf{NVS})$ methods to video, which is naturally unsuitable. In this paper, we introduce a novel self-supervised stereo video synthesis paradigm via a video diffusion model, termed $\\textbf{SpatialDreamer}$, which meets the challenges head-on. Firstly, to address the stereo video data insufficiency, we propose a $\\textbf{D}$epth based $\\textbf{V}$ideo $\\textbf{G}$eneration module $\\textbf{DVG}$, which employs a forward-backward rendering mechanism to generate paired videos with geometric and temporal priors. Leveraging data generated by DVG, we propose RefinerNet along with a self-supervised synthetic framework designed to facilitate efficient and dedicated training.More importantly, we devise a consistency control module, which consists of a metric of stereo deviation strength and a $\\textbf{T}$emporal $\\textbf{I}$nteraction $\\textbf{L}$earning module $\\textbf{TIL}$ for geometric and temporal consistency ensurance respectively. We evaluated the proposed method against various benchmark methods, with the results showcasing its superior performance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "SpatialDreamer introduces a self-supervised stereo video synthesis paradigm from monocular input, addressing challenges in spatial computing and virtual reality. The method leverages a Depth-based Video Generation (DVG) module to create paired stereo videos with geometric and temporal priors, overcoming the scarcity of high-quality training data. A RefinerNet and a self-supervised synthetic framework are proposed for efficient training. Additionally, a consistency control module, including a stereo deviation strength metric and a Temporal Interaction Learning (TIL) module, ensures geometric and temporal consistency. The method demonstrates superior performance compared to existing benchmarks.",
        "Tags": [
            "Stereo Matching",
            "Video Generation",
            "Self-Supervised Learning",
            "Depth Estimation",
            "Temporal Consistency"
        ]
    },
    {
        "Title": "BWFormer: Building Wireframe Reconstruction  from airborne LiDAR point clouds with Transformer",
        "Authors": "yuzhou liu \u00b7 Lingjie Zhu \u00b7 Hanqiao Ye \u00b7 Shangfeng Huang \u00b7 Xiang Gao \u00b7 Xianwei Zheng \u00b7 Shuhan Shen",
        "Abstract": "In this paper, we present BWFormer, a novel Transformer-based model for building wireframe reconstruction from airborne LiDAR point cloud.  The problem is solved in a ground-up manner by detecting the building corners in 2D, lifting and connecting them in 3D space afterwards with additional data augmentation.Due to the 2.5D  characteristic of the airborne LiDAR point cloud, we simplify the problem by projecting the points on the ground plane to produce a 2D height map. With the height map, a heat map is first predicted with pixel-wise corner likelihood to predict the possible 2D corners.Then, 3D corners are predicted by a Transformer-based network with extra height embedding initialization.This 2D-to-3D corner detection strategy reduces the search space significantly.To recover the topological connections among the corners, edges are finally predicted from geometrical and visual cues in the height map with the proposed edge attention mechanism, which extracts holistic features and preserves local details simultaneously.In addition, due to the limited datasets in the field and the irregularity of the point clouds, a conditional latent diffusion model for LiDAR scanning simulation is utilized for data augmentation.BWFormer surpasses other state-of-the-art methods, especially in reconstruction completeness. We commit to release all our codes and pre-trained models.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces BWFormer, a Transformer-based model designed for building wireframe reconstruction from airborne LiDAR point clouds. The approach simplifies the problem by projecting 3D points onto a 2D ground plane to create a height map, which is then used to predict 2D corners. These corners are subsequently lifted to 3D using a Transformer-based network with height embedding initialization. The model employs an edge attention mechanism to recover topological connections between corners, leveraging both geometric and visual cues. To address dataset limitations and point cloud irregularity, a conditional latent diffusion model is used for data augmentation. BWFormer demonstrates superior performance in reconstruction completeness compared to existing methods.",
        "Tags": [
            "3D Point Cloud",
            "3D Reconstruction",
            "Transformer-based Network",
            "Edge Attention Mechanism",
            "Conditional Latent Diffusion Model"
        ]
    },
    {
        "Title": "CCIN: Compositional Conflict Identification and Neutralization for Composed Image Retrieval",
        "Authors": "Likai Tian \u00b7 Jian Zhao \u00b7 Zechao Hu \u00b7 Zhengwei Yang \u00b7 Hao Li \u00b7 Lei Jin \u00b7 Zheng Wang \u00b7 Xuelong Li",
        "Abstract": "Composed Image Retrieval (CIR) is a multi-modal task that seeks to retrieve target images by harmonizing a reference image with a modified instruction. The main challenge in CIR lies in compositional conflicts between the reference image (e.g., blue, long sleeve) and the modified instruction (e.g., grey, short sleeve). Previous works attempt to mitigate such conflicts through feature-level manipulation, commonly employing learnable masks to obscure conflicting features within the reference image. However, the inherent complexity of feature spaces poses significant challenges in precise conflict neutralization, thereby leading to uncontrollable results. To this end, this paper proposes the Compositional Conflict Identification and Neutralization (CCIN) framework, which sequentially identifies and neutralizes compositional conflicts for effective CIR. Specifically, CCIN comprises two core modules: 1) Compositional Conflict Identification module, which utilizes LLM-based analysis to identify specific conflicting attributes, and 2) Compositional Conflict Neutralization module, which first generates a kept instruction to preserve non-conflicting attributes, then neutralizes conflicts under collaborative guidance of both the kept and modified instructions. Furthermore, an orthogonal parameter regularization loss is introduced to emphasize the distinction between target and conflicting features. Extensive experiments demonstrate the superiority of CCIN over the state-of-the-arts.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Composed Image Retrieval (CIR) is a multi-modal task that retrieves target images by harmonizing a reference image with a modified instruction. The primary challenge is addressing compositional conflicts between the reference image and the modified instruction. Existing methods use feature-level manipulation, such as learnable masks, but struggle with precise conflict neutralization due to the complexity of feature spaces. This paper introduces the Compositional Conflict Identification and Neutralization (CCIN) framework, which sequentially identifies and neutralizes conflicts. CCIN consists of two modules: 1) a Compositional Conflict Identification module that uses LLM-based analysis to detect conflicting attributes, and 2) a Compositional Conflict Neutralization module that generates a kept instruction to preserve non-conflicting attributes and neutralizes conflicts under the guidance of both kept and modified instructions. An orthogonal parameter regularization loss is also introduced to distinguish target and conflicting features. CCIN demonstrates superior performance over state-of-the-art methods.",
        "Tags": [
            "Composed Image Retrieval",
            "Multimodal Learning",
            "Large Language Models (LLMs)",
            "Conflict Neutralization",
            "Orthogonal Parameter Regularization",
            "LLM-Based Analysis"
        ]
    },
    {
        "Title": "Descriptor-In-Pixel : Point-Feature Tracking For Pixel Processor Arrays",
        "Authors": "Laurie Bose \u00b7 Piotr Dudek \u00b7 Jianing Chen",
        "Abstract": "This paper presents a novel approach for joint point-feature detection and tracking, specifically designed for Pixel Processor Array sensors (PPA). Instead of standard pixels, PPA sensors consists of thousands of \"pixel-processors\", enabling massive parallel computation of visual data at the point of light capture. Our approach performs all computation within these pixel-processors, meaning no raw image data need ever leave the sensor. Instead, sensor output can be reduced to merely the locations of tracked features, and the descriptors of newly initialized features, minimizing data transfer between sensor and external processing. To achieve this we store feature descriptors inside every pixel-processor, adjusting the layout of these descriptors every frame. The PPA's architecture enables us to compute the response of every stored descriptor in parallel. This \"response map\" is utilized for both detection and tracking of point-features across the pixel-processor array. This approach is very fast, our implementation upon the SCAMP-7 PPA prototype runs at over 3000 FPS (Frames Per Second), tracking point-features reliably even under violent motion. This is the first work performing point-feature detection and tracking entirely \"in-pixel\".",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a novel method for joint point-feature detection and tracking tailored for Pixel Processor Array (PPA) sensors. Unlike traditional sensors, PPAs consist of thousands of pixel-processors that enable parallel computation of visual data directly at the point of light capture. The proposed approach performs all computations within these pixel-processors, eliminating the need for raw image data to leave the sensor. Instead, the sensor output is reduced to the locations of tracked features and descriptors of newly initialized features, significantly minimizing data transfer. Feature descriptors are stored within each pixel-processor and adjusted every frame, allowing parallel computation of descriptor responses. This response map is used for both detection and tracking of point-features across the array. The method is highly efficient, achieving over 3000 FPS on the SCAMP-7 PPA prototype, and reliably tracks point-features even under rapid motion. This is the first work to perform point-feature detection and tracking entirely within the pixel-processors.",
        "Tags": [
            "3D Point Cloud",
            "Visual Tracking",
            "In-Pixel Computation",
            "Parallel Processing",
            "Feature Descriptor Storage"
        ]
    },
    {
        "Title": "HotSpot: Screened Poisson Equation for Signed Distance Function Optimization",
        "Authors": "Zimo Wang \u00b7 Cheng Wang \u00b7 Taiki Yoshino \u00b7 Sirui Tao \u00b7 Ziyang Fu \u00b7 Tzu-Mao Li",
        "Abstract": "We propose a method, HotSpot, for optimizing neural signed distance functions, based on a relation between the solution of a screened Poisson equation and the distance function.Existing losses such as the eikonal loss cannot guarantee the recovered implicit function to be a distance function, even when the implicit function satisfies the eikonal equation almost everywhere.Furthermore, the eikonal loss suffers from stability issues in optimization and the remedies that introduce area or divergence minimization can lead to oversmoothing.We address these challenges by designing a loss function that when minimized can converge to the true distance function, is stable, and naturally penalize large surface area.We provide theoretical analysis and experiments on both challenging 2D and 3D datasets and show that our method provide better surface reconstruction and more accurate distance approximation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces HotSpot, a novel method for optimizing neural signed distance functions by leveraging the relationship between the solution of a screened Poisson equation and the distance function. Existing methods, such as the eikonal loss, fail to guarantee that the recovered implicit function is a true distance function and often suffer from stability issues or oversmoothing. HotSpot addresses these limitations by designing a loss function that ensures convergence to the true distance function, improves stability, and naturally penalizes large surface areas. Theoretical analysis and experiments on 2D and 3D datasets demonstrate that HotSpot achieves superior surface reconstruction and more accurate distance approximation.",
        "Tags": [
            "3D Reconstruction",
            "Implicit Neural Representations",
            "Screened Poisson Equation",
            "Eikonal Loss",
            "Surface Reconstruction"
        ]
    },
    {
        "Title": "RipVIS: Rip Currents Video Instance Segmentation Benchmark for Beach Monitoring and Safety",
        "Authors": "Andrei Dumitriu \u00b7 Florin Tatui \u00b7 Florin Miron \u00b7 Aakash Ralhan \u00b7 Radu Tudor Ionescu \u00b7 Radu Timofte",
        "Abstract": "Rip currents are strong, localized and narrow currents of water that flow outwards into the sea causing numerous beach-related injuries and fatalities worldwide. Accurate identification of rip currents remains challenging due to their amorphous nature and a lack of annotated data, which often requires expert knowledge. To address these issues, we present RipVIS, a large-scale video instance segmentation benchmark explicitly designed for rip current segmentation. RipVIS is an order of magnitude larger than previous datasets, featuring 140 videos (204,056 frames), out of which 115 (161,600 frames) videos are with rip currents, collected from various sources, including drones, mobile phones, and fixed beach cameras. Our dataset encompasses diverse visual contexts, such as wave-breaking patterns, sediment flows, and water color variations, across multiple global locations, including the USA, Greece, Portugal, Romania, Costa Rica, Sri Lanka, New Zealand and Australia. Most videos are annotated at $5$ FPS to ensure accuracy in dynamic scenarios, supplemented by an additional 25 videos (42,456 frames) without rip currents. We conduct comprehensive ablation studies on Mask R-CNN, Cascade Mask R-CNN, YOLACT, and YOLO11, fine-tuning these models for the task of rip current segmentation. Baseline results are reported across standard metrics, with a particular focus on the F2 score to prioritize recall and reduce false negatives. To further enhance segmentation quality, we apply a post-processing step using Temporal Confidence Aggregation (TCA). RipVIS aims to set a new standard for rip current segmentation, contributing towards safer beach environments. We also offer a benchmark website to share data, models, and results with the research community, encouraging ongoing collaboration and future contributions at link.hidden.for.review.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Rip currents pose significant risks to beach safety, but their accurate identification is hindered by their amorphous nature and limited annotated data. To address this, the authors introduce RipVIS, a large-scale video instance segmentation benchmark specifically designed for rip current segmentation. RipVIS comprises 140 videos (204,056 frames), including 115 videos (161,600 frames) with rip currents, collected from diverse sources such as drones, mobile phones, and fixed beach cameras. The dataset covers various visual contexts, including wave-breaking patterns and water color variations, across multiple global locations. Most videos are annotated at 5 FPS for accuracy, with an additional 25 videos (42,456 frames) without rip currents. The authors evaluate state-of-the-art models like Mask R-CNN, Cascade Mask R-CNN, YOLACT, and YOLO11, fine-tuning them for rip current segmentation and reporting baseline results with a focus on the F2 score. Temporal Confidence Aggregation (TCA) is applied to improve segmentation quality. RipVIS aims to establish a new standard for rip current segmentation and promote safer beach environments, supported by a benchmark website for sharing data and results.",
        "Tags": [
            "Video Instance Segmentation",
            "Datasets and Benchmarks",
            "Temporal Confidence Aggregation",
            "Rip Current Detection",
            "Beach Safety Monitoring"
        ]
    },
    {
        "Title": "FastVLM: Efficient Vision Encoding for Vision Language Models",
        "Authors": "Pavan Vasu Vasu \u00b7 Fartash Faghri \u00b7 Chun-Liang Li \u00b7 Cem Koc \u00b7 Nate True \u00b7 Gokula Krishnan Santhanam \u00b7 Albert Antony \u00b7 James Gabriel \u00b7 Peter Grasch \u00b7 Oncel Tuzel \u00b7 Hadi Pouransari",
        "Abstract": "Vision Language Models (VLMs) like LLaVA encode images into tokens aligned to the word embedding space of the LLM decoder. Scaling input image resolution is essential for improving performance, especially in text-rich image understanding tasks. However, popular visual encoders such as CLIP-pretrained ViTs become inefficient at high resolutions due to the large number of tokens and high encoding latency caused by stacked self-attention layers. At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency. In this work, we introduce FastVLM, which achieves an optimized trade-off between resolution, latency, and accuracy by incorporating FastViTHD\u2014a new hybrid vision encoder that outputs fewer tokens and significantly reduces encoding time while processing high-resolution images. We provide a comprehensive efficiency analysis of the interplay between image resolution, vision latency, number of visual tokens, and LLM size. In the LLaVA-1.5 setup, we achieve 3.2$\\times$ improvement in overall time-to-first-token (TTFT) while maintaining similar performance on VLM benchmarks compared to prior works. On text-rich evaluations like TextVQA and DocVQA, FastVLM obtains +8.4\\% and +12.5\\% better accuracy than ConvLLaVA at a similar operating point of 144 visual tokens. Compared to LLaVa-OneVision at the highest resolution (1152$\\times$1152), FastVLM achieves comparable performance on key benchmarks like SeedBench and MMMU, using the same LLM, but with 85$\\times$ faster TTFT, 3$\\times$ less vision instruction tuning data, and a vision encoder that is 3.4$\\times$ smaller.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "FastVLM introduces an efficient vision encoding method for Vision Language Models (VLMs) by optimizing the trade-off between resolution, latency, and accuracy. The proposed FastViTHD hybrid vision encoder reduces the number of visual tokens and encoding time for high-resolution images, significantly improving overall time-to-first-token (TTFT) while maintaining performance on VLM benchmarks. FastVLM outperforms existing models like ConvLLaVA and LLaVa-OneVision in text-rich evaluations and achieves comparable performance with faster processing and fewer resources.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Self-Supervised Learning",
            "FastViTHD",
            "Time-to-First-Token (TTFT)",
            "High-Resolution Image Processing"
        ]
    },
    {
        "Title": "Learning Person-Specific Animatable Face Models from In-the-Wild Images via a Shared Base Model",
        "Authors": "Yuxiang Mao \u00b7 Zhenfeng Fan \u00b7 Zhijie Zhang \u00b7 Zhiheng Zhang \u00b7 Shihong Xia",
        "Abstract": "Training a generic 3D face reconstruction model in a self-supervised manner using large-scale, in-the-wild 2D face image datasets enhances robustness to varying lighting conditions and occlusions while allowing the model to capture animatable wrinkle details across diverse facial expressions. However, a generic model often fails to adequately represent the unique characteristics of specific individuals. In this paper, we propose a method to train a generic base model and then transfer it to yield person-specific models by integrating lightweight adapters within the large-parameter ViT-MAE base model. These person-specific models excel at capturing individual facial shapes and detailed features while preserving the robustness and prior knowledge of detail variations from the base model. During training, we introduce a silhouette vertex re-projection loss to address boundary \"landmark marching\" issues on the 3D face caused by pose variations. Additionally, we employ an innovative teacher-student loss to leverage the inherent strengths of UNet in feature boundary localization for training our detail MAE. Quantitative and qualitative experiments demonstrate that our approach achieves state-of-the-art performance in face alignment, detail accuracy, and richness. The code will be released to the public upon the acceptance of this paper.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a method for training a generic 3D face reconstruction model using large-scale, in-the-wild 2D face images, which enhances robustness to varying lighting and occlusions while capturing animatable wrinkle details across diverse expressions. The approach involves transferring the generic base model to person-specific models by integrating lightweight adapters within a ViT-MAE base model, which excels at capturing individual facial shapes and detailed features. A silhouette vertex re-projection loss is introduced to address boundary issues caused by pose variations, and a teacher-student loss leverages UNet's strengths in feature boundary localization for training the detail MAE. The method achieves state-of-the-art performance in face alignment, detail accuracy, and richness.",
        "Tags": [
            "Avatars",
            "3D Reconstruction",
            "Self-Supervised Learning",
            "ViT-MAE",
            "Teacher-Student Loss",
            "Silhouette Vertex Re-projection Loss"
        ]
    },
    {
        "Title": "See Further When Clear: Curriculum Consistency Model",
        "Authors": "Yunpeng Liu \u00b7 Boxiao Liu \u00b7 Yi Zhang \u00b7 Xingzhong Hou \u00b7 Guanglu Song \u00b7 Yu Liu \u00b7 Haihang You",
        "Abstract": "Significant advances have been made in the sampling efficiency of diffusion and flow matching models, driven by Consistency Distillation (CD), which trains a student model to mimic the output of a teacher model at a later timestep. However, we found that the knowledge discrepancy between student and teacher varies significantly across different timesteps, leading to suboptimal performance in CD.To address this issue, we propose the Curriculum Consistency Model (CCM), which stabilizes and balances the knowledge discrepancy across timesteps. Specifically, we regard the distillation process at each timestep as a curriculum and introduce a metric based on the Peak Signal-to-Noise Ratio (PSNR) to quantify the knowledge discrepancy of this curriculum, then ensure that the curriculum maintains consistent knowledge discrepancy across different timesteps by having the teacher model iterate more steps when the noise intensity is low.Our method achieves competitive single-step sampling Fr\u00e9chet Inception Distance (FID) scores of 1.64 on CIFAR-10 and 2.18 on ImageNet 64x64.Moreover, we have extended our method to large-scale text-to-image models and confirmed that it generalizes well to both diffusion models (Stable Diffusion XL) and flow matching models (Stable Diffusion 3). The generated samples demonstrate improved image-text alignment and semantic structure since CCM enlarges the distillation step at large timesteps and reduces the accumulated error.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The Curriculum Consistency Model (CCM) is introduced to address the varying knowledge discrepancy between student and teacher models across different timesteps in Consistency Distillation (CD). By treating each timestep as a curriculum and using a Peak Signal-to-Noise Ratio (PSNR) metric to quantify knowledge discrepancy, CCM ensures consistent knowledge transfer. This approach achieves competitive Fr\u00e9chet Inception Distance (FID) scores on CIFAR-10 and ImageNet 64x64 and demonstrates improved image-text alignment and semantic structure in large-scale text-to-image models, including Stable Diffusion XL and Stable Diffusion 3.",
        "Tags": [
            "Diffusion Models",
            "Knowledge Distillation",
            "Curriculum Learning",
            "Peak Signal-to-Noise Ratio (PSNR)",
            "Image-Text Alignment"
        ]
    },
    {
        "Title": "ROD-MLLM: Towards More Reliable Object Detection in Multimodal Large Language Models",
        "Authors": "Heng Yin \u00b7 Yuqiang Ren \u00b7 Ke Yan \u00b7 Shouhong Ding \u00b7 Yongtao Hao",
        "Abstract": "Multimodal large language models (MLLMs) have demonstrated strong language understanding and generation capabilities, excelling in visual tasks like referring and grounding. However, due to task type limitations and dataset scarcity, existing MLLMs only ground objects present in images and cannot reject non-existent objects effectively, resulting in unreliable predictions. In this paper, we introduce ROD-MLLM, a novel MLLM for Reliable Object Detection using free-form language. We propose a query-based localization mechanism to extract low-level object features. By aligning global and object-level visual information with text space, we leverage the large language model (LLM) for high-level comprehension and final localization decisions, overcoming the language understanding limitations of normal detectors. To enhance language-based object detection, we design an automated data annotation pipeline and construct the dataset ROD. This pipeline uses the referring capabilities of existing MLLMs and chain-of-thought techniques to generate diverse expressions corresponding to zero or multiple objects, addressing the shortage of training data. Experiments across various tasks, including referring, grounding, and language-based object detection, show that ROD-MLLM achieves state-of-the-art performance among MLLMs. Notably, in language-based object detection, our model achieves a +13.7 mAP improvement over existing MLLMs and surpasses most specialized detection models, especially in scenarios requiring advanced complex language understanding.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces ROD-MLLM, a novel Multimodal Large Language Model (MLLM) designed for Reliable Object Detection using free-form language. The model addresses the limitations of existing MLLMs in grounding non-existent objects and unreliable predictions by proposing a query-based localization mechanism that aligns global and object-level visual information with text space. This approach leverages the large language model (LLM) for high-level comprehension and final localization decisions. Additionally, an automated data annotation pipeline is developed to construct the ROD dataset, enhancing language-based object detection by generating diverse expressions for zero or multiple objects. ROD-MLLM demonstrates state-of-the-art performance in tasks such as referring, grounding, and language-based object detection, with a significant improvement in mean average precision (mAP) over existing MLLMs and surpassing most specialized detection models, particularly in scenarios requiring complex language understanding.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Object Detection",
            "Query-based Localization",
            "Automated Data Annotation",
            "Chain-of-Thought Techniques"
        ]
    },
    {
        "Title": "PassionSR: Post-Training Quantization with Adaptive Scale in One-Step Diffusion based Image Super-Resolution",
        "Authors": "Zhu Li Bo \u00b7 Jianze Li \u00b7 Haotong Qin \u00b7 Wenbo Li \u00b7 Yulun Zhang \u00b7 Yong Guo \u00b7 Xiaokang Yang",
        "Abstract": "Diffusion-based image super-resolution (SR) models have shown superior performance at the cost of multiple denoising steps. However, even though the denoising step has been reduced to one, they require high computational costs and storage requirements, making it difficult for deployment on hardware devices. To address these issues, we propose a novel post-training quantization approach with adaptive scale in one-step diffusion (OSD) image SR, PassionSR. First, we simplify OSD model to two core components, UNet and Variational Autoencoder (VAE) by removing the CLIPEncoder. Secondly, we propose Learnable Boundary Quantizer (LBQ) and Learnable Equivalent Transformation (LET) to optimize the quantization process and manipulate activation distributions for better quantization. Finally, we design a Distributed Quantization Calibration (DQC) strategy that stabilizes the training of quantized parameters for rapid convergence. Comprehensive experiments demonstrate that PassionSR with 8-bit and 6-bit obtains comparable visual results with full-precision model. Moreover, our PassionSR achieves significant advantages over recent leading low-bit quantization methods for image SR. Our code will be released",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces PassionSR, a novel post-training quantization approach for one-step diffusion-based image super-resolution (SR) models. PassionSR simplifies the model by removing the CLIPEncoder, focusing on UNet and Variational Autoencoder (VAE) components. It introduces Learnable Boundary Quantizer (LBQ) and Learnable Equivalent Transformation (LET) to optimize quantization and activation distribution manipulation. Additionally, a Distributed Quantization Calibration (DQC) strategy is proposed to stabilize training and ensure rapid convergence. PassionSR achieves comparable visual results with full-precision models at 8-bit and 6-bit quantization, outperforming recent low-bit quantization methods in image SR.",
        "Tags": [
            "Super-Resolution",
            "Diffusion Models",
            "Post-Training Quantization",
            "Adaptive Scale Quantization",
            "One-Step Diffusion"
        ]
    },
    {
        "Title": "Decision SpikeFormer: Spike-Driven Transformer for Decision Making",
        "Authors": "Wei Huang \u00b7 Qinying Gu \u00b7 Nanyang Ye",
        "Abstract": "Offline reinforcement learning (RL) enables policy training solely on pre-collected data, avoiding direct environment interaction\u2014a crucial benefit for energy-constrained embodied AI applications. Although Artificial Neural Networks (ANN)-based methods perform well in offline RL, their high computational and energy demands motivate exploration of more efficient alternatives. Spiking Neural Networks (SNNs) show promise for such tasks, given their low power consumption. In this work, we introduce DSFormer, the first spike-driven transformer model designed to tackle offline RL via sequence modeling. Unlike existing SNN transformers focused on spatial dimensions for vision tasks, we develop Temporal Spiking Self-Attention (TSSA) and Positional Spiking Self-Attention (PSSA) in DSFormer to capture the temporal and positional dependencies essential for sequence modeling in RL. Additionally, we propose Progressive Threshold-dependent Batch Normalization (PTBN), which combines the benefits of LayerNorm and BatchNorm to preserve temporal dependencies while maintaining the spiking nature of SNNs. Comprehensive results in the D4RL benchmark show DSFormer\u2019s superiority over both SNN and ANN counterparts, achieving 78.4\\% energy savings, highlighting DSFormer's advantages not only in energy efficiency but also in competitive performance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces DSFormer, a spike-driven transformer model designed for offline reinforcement learning (RL) through sequence modeling. Unlike traditional Artificial Neural Networks (ANNs), DSFormer leverages Spiking Neural Networks (SNNs) for their low power consumption, making it suitable for energy-constrained embodied AI applications. The model incorporates Temporal Spiking Self-Attention (TSSA) and Positional Spiking Self-Attention (PSSA) to capture temporal and positional dependencies crucial for RL. Additionally, Progressive Threshold-dependent Batch Normalization (PTBN) is proposed to maintain temporal dependencies while preserving the spiking nature of SNNs. DSFormer demonstrates superior performance and energy efficiency on the D4RL benchmark, achieving 78.4% energy savings compared to both SNN and ANN counterparts.",
        "Tags": [
            "Embodied AI",
            "Spiking Neural Networks (SNNs)",
            "Temporal Spiking Self-Attention",
            "Progressive Threshold-dependent Batch Normalization",
            "Offline Reinforcement Learning"
        ]
    },
    {
        "Title": "Identity-Preserving Text-to-Video Generation by Frequency Decomposition",
        "Authors": "Shenghai Yuan \u00b7 Jinfa Huang \u00b7 Xianyi He \u00b7 Yunyang Ge \u00b7 Yujun Shi \u00b7 Liuhan Chen \u00b7 Jiebo Luo \u00b7 Li Yuan",
        "Abstract": "Identity-preserving text-to-video (IPT2V) generation aims to create high-fidelity videos with consistent human identity.  It is an important task in video generation but remains an open problem for generative models. This paper pushes the technical frontier of IPT2V in two directions that have not been resolved in the literature: (1) A tuning-free pipeline without tedious case-by-case finetuning, and (2) A frequency-aware heuristic identity-preserving Diffusion Transformer (DiT)-based control scheme. To achieve these goals, we propose ConsisID, a tuning-free DiT-based controllable IPT2V model to keep human-identity consistent in the generated video. Inspired by prior findings in frequency analysis of vision/diffusion transformers, it employs identity-control signals in the frequency domain, where facial features can be decomposed into low-frequency global features (e.g., profile, proportions) and high-frequency intrinsic features (e.g., identity markers that remain unaffected by pose changes). First, from a low-frequency perspective, we introduce a global facial extractor, which encodes the reference image and facial key points into a latent space, generating features enriched with low-frequency information. These features are then integrated into the shallow layers of the network to alleviate training challenges associated with DiT. Second, from a high-frequency perspective, we design a local facial extractor to capture high-frequency details and inject them into the transformer blocks, enhancing the model's ability to preserve fine-grained features. To leverage the frequency information for identity preservation, we propose a hierarchical training strategy, transforming a vanilla pre-trained video generation model into an IPT2V model. Extensive experiments demonstrate that our frequency-aware heuristic scheme provides an optimal control solution for DiT-based models. Thanks to this scheme, our ConsisID achieves excellent results in generating high-quality, identity-preserving videos, making strides towards more effective IPT2V.",
        "Link": "https://pku-yuangroup.github.io/ConsisID",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces ConsisID, a tuning-free Diffusion Transformer (DiT)-based model for identity-preserving text-to-video (IPT2V) generation. The model addresses two key challenges: eliminating the need for case-by-case finetuning and ensuring consistent human identity in generated videos. ConsisID employs a frequency-aware heuristic control scheme, decomposing facial features into low-frequency global features and high-frequency intrinsic features. A global facial extractor encodes reference images and facial key points into a latent space, while a local facial extractor captures high-frequency details. These features are integrated into the network to enhance identity preservation. The proposed hierarchical training strategy transforms a pre-trained video generation model into an IPT2V model, achieving high-quality, identity-consistent video generation.",
        "Tags": [
            "Video Generation",
            "Diffusion Models",
            "Frequency Decomposition",
            "Identity Preservation",
            "Hierarchical Training"
        ]
    },
    {
        "Title": "Data Distributional Properties As Inductive Bias for Systematic Generalization",
        "Authors": "Felipe del Rio \u00b7 Alain Raymond \u00b7 Daniel Florea \u00b7 Rodrigo Toro Icarte \u00b7 Julio Hurtado \u00b7 Cristian Buc Calderon \u00b7 Alvaro Soto",
        "Abstract": "Deep neural networks (DNNs) struggle at systematic generalization (SG). Several studies have evaluated the possibility to promote SG through the proposal of novel architectures, loss functions or training methodologies. Few studies, however, have focused on the role of training data properties in promoting SG. In this work, we investigate the impact of certain data distributional properties, as inductive biases for the SG ability of a multi-modal language model. To this end, we study three different properties. First, data diversity, instantiated as an increase in the possible values a latent property in the training distribution may take. Second, burstiness, where we probabilistically restrict the number of possible values of latent factors on particular inputs during training. Third, latent intervention, where a particular latent factor is altered randomly during training. We find that all three factors significantly enhance SG, with diversity contributing an 89\\% absolute increase in accuracy in the most affected property. Through a series of experiments, we test various hypotheses to understand why these properties promote SG. Finally, we find that Normalized Mutual Information (NMI) between latent attributes in the training distribution is strongly predictive of out-of-distribution generalization. We find that a mechanism by which lower NMI induces SG is in the geometry of representations. In particular, we find that NMI induces more parallelism in neural representations (i.e., input features coded in parallel neural vectors) of the model, a property related to the capacity of reasoning by analogy.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This study explores how specific data distributional properties can serve as inductive biases to enhance systematic generalization (SG) in multi-modal language models. The authors investigate three key properties: data diversity, burstiness, and latent intervention, all of which significantly improve SG. Data diversity, in particular, leads to an 89% increase in accuracy for the most affected property. The research also identifies that Normalized Mutual Information (NMI) between latent attributes in the training distribution is a strong predictor of out-of-distribution generalization. Lower NMI is found to induce more parallelism in neural representations, which is linked to the model's ability to reason by analogy.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Self-Supervised Learning",
            "Inductive Bias",
            "Normalized Mutual Information (NMI)",
            "Neural Representation Geometry"
        ]
    },
    {
        "Title": "Vision-Language Model IP Protection via Prompt-based Learning",
        "Authors": "Lianyu Wang \u00b7 Meng Wang \u00b7 Huazhu Fu \u00b7 Daoqiang Zhang",
        "Abstract": "Vision-language models (VLMs) like CLIP (Contrastive Language-Image Pre-Training) have seen remarkable success in visual recognition, highlighting the increasing need to safeguard the intellectual property (IP) of well-trained models. Effective IP protection extends beyond ensuring authorized usage; it also necessitates restricting model deployment to authorized data domains, particularly when the model is fine-tuned for specific target domains. However, current IP protection methods often rely solely on the visual backbone, which may lack sufficient semantic richness. To bridge this gap, we introduce IP-CLIP, a lightweight IP protection strategy tailored to CLIP, employing a prompt-based learning approach. By leveraging the frozen visual backbone of CLIP, we extract both image style and content information, incorporating them into the learning of IP prompt. This strategy acts as a robust barrier, effectively preventing the unauthorized transfer of features from authorized domains to unauthorized ones. Additionally, we propose a style-enhancement branch that constructs feature banks for both authorized and unauthorized domains. This branch integrates self-enhanced and cross-domain features, further strengthening IP-CLIP\u2019s capability to block features from unauthorized domains. Finally, we present new three metrics designed to better balance the performance degradation of authorized and unauthorized domains. Comprehensive experiments in various scenarios demonstrate its promising potential for application in IP protection tasks for VLMs.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the need for intellectual property (IP) protection in vision-language models (VLMs) like CLIP, which are widely used in visual recognition. Current IP protection methods often rely on the visual backbone, which may lack semantic richness. To overcome this, the authors introduce IP-CLIP, a lightweight IP protection strategy for CLIP using prompt-based learning. IP-CLIP leverages the frozen visual backbone of CLIP to extract image style and content information, integrating them into the learning of IP prompts to prevent unauthorized feature transfer. A style-enhancement branch is also proposed, which constructs feature banks for authorized and unauthorized domains, enhancing the model's ability to block unauthorized features. The paper introduces three new metrics to balance performance degradation between authorized and unauthorized domains, demonstrating the potential of IP-CLIP in IP protection tasks for VLMs.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Intellectual Property (IP) Protection",
            "Prompt-based Learning",
            "Feature Transfer Prevention",
            "Style-Enhancement Branch"
        ]
    },
    {
        "Title": "HiFi-Portrait: Zero-shot Identity-preserved Portrait Generation with High-fidelity Multi-face Fusion",
        "Authors": "Yifang Xu \u00b7 BenXiang Zhai \u00b7 Yunzhuo Sun \u00b7 Ming Li \u00b7 Yang Li \u00b7 Sidan Du",
        "Abstract": "Recent advancements in diffusion-based technologies have made significant strides, particularly in identity-preserved portrait generation (IPG). However, when using multiple reference images from the same ID, existing methods typically produce lower-fidelity portraits and struggle to customize face attributes precisely. To address these issues, this paper presents Method, a high-fidelity method for zero-shot portrait generation. Specifically, we first introduce the face refiner and landmark generator to obtain fine-grained multi-face features and 3D-aware face landmarks. The landmarks include the reference ID and the target attributes. Then, we design HiFi-Net to fuse multi-face features and align them with landmarks, which improves ID fidelity and face control. In addition, we devise an automated pipeline to construct an ID-based dataset for training Method. Extensive experimental results demonstrate that our method surpasses the state-of-the-art approaches in face similarity and controllability. Furthermore, our method is also compatible with previous SDXL-based works. Our code is available at -.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces HiFi-Portrait, a high-fidelity method for zero-shot identity-preserved portrait generation (IPG) that addresses the limitations of existing methods in producing lower-fidelity portraits and customizing face attributes. The proposed method utilizes a face refiner and landmark generator to extract fine-grained multi-face features and 3D-aware face landmarks, which include reference ID and target attributes. HiFi-Net is designed to fuse these multi-face features and align them with landmarks, enhancing ID fidelity and face control. An automated pipeline is also developed to construct an ID-based dataset for training. The method demonstrates superior performance in face similarity and controllability compared to state-of-the-art approaches and is compatible with previous SDXL-based works.",
        "Tags": [
            "Diffusion Models",
            "Image Generation",
            "3D-aware face landmarks",
            "Multi-face feature fusion",
            "Zero-shot learning"
        ]
    },
    {
        "Title": "Dynamic Camera Poses and Where to Find Them",
        "Authors": "Chris Rockwell \u00b7 Joseph Tung \u00b7 Tsung-Yi Lin \u00b7 Ming-Yu Liu \u00b7 David Fouhey \u00b7 Chen-Hsuan Lin",
        "Abstract": "Annotating camera poses on dynamic Internet videos at scale is critical for advancing fields like realistic video generation and simulation.However, collecting such a dataset is difficult, as most Internet videos are unsuitable for pose estimation.Furthermore, annotating dynamic Internet videos present significant challenges even for state-of-the-art methods.In this paper, we introduce DynPose-100K, a large-scale dataset of dynamic Internet videos annotated with camera poses.Our collection pipeline addresses filtering using a carefully combined set of task-specific and generalist models.For pose estimation, we combine the latest techniques of point tracking, dynamic masking, and structure-from-motion to achieve improvements over the state-of-the-art approaches.Our analysis and experiments demonstrate that DynPose-100K is both large-scale and diverse across several key attributes, opening up avenues for advancements in various downstream applications.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces DynPose-100K, a large-scale dataset of dynamic Internet videos annotated with camera poses, addressing the challenges of annotating such videos at scale. The dataset is created using a pipeline that combines task-specific and generalist models for filtering, and employs advanced techniques like point tracking, dynamic masking, and structure-from-motion for pose estimation. DynPose-100K is shown to be large-scale and diverse, enabling advancements in downstream applications.",
        "Tags": [
            "Video Understanding",
            "3D Reconstruction",
            "Datasets and Benchmarks",
            "Dynamic Masking",
            "Structure-from-Motion",
            "Point Tracking"
        ]
    },
    {
        "Title": "Estimating Body and Hand Motion in an Ego\u2011sensed World",
        "Authors": "Brent Yi \u00b7 Vickie Ye \u00b7 Maya Zheng \u00b7 Yunqi Li \u00b7 Vickie Ye \u00b7 Georgios Pavlakos \u00b7 Yi Ma \u00b7 Jitendra Malik \u00b7 Angjoo Kanazawa",
        "Abstract": "We present EgoAllo, a system for human motion estimation from a head-mounted device. Using only egocentric SLAM poses and images, EgoAllo guides sampling from a conditional diffusion model to estimate 3D body pose, height, and hand parameters that capture a device wearer's actions in the allocentric coordinate frame of the scene. To achieve this, our key insight is in representation: we propose spatial and temporal invariance criteria for improving model performance, from which we derive a head motion conditioning parameterization that improves estimation by up to 18%. We also show how the bodies estimated by our system can improve hand estimation: the resulting kinematic and temporal constraints can reduce world-frame errors in single-frame estimates by 40%.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces EgoAllo, a system for estimating human motion, including 3D body pose, height, and hand parameters, using only egocentric SLAM poses and images from a head-mounted device. The system leverages a conditional diffusion model guided by spatial and temporal invariance criteria, which improves estimation accuracy by up to 18%. Additionally, the system demonstrates that body estimation enhances hand estimation, reducing world-frame errors in single-frame estimates by 40%.",
        "Tags": [
            "3D Human Pose Estimation",
            "3D Human Mesh Estimation",
            "Ego-centric Sensing",
            "Conditional Diffusion Model",
            "Allocentric Coordinate Frame"
        ]
    },
    {
        "Title": "From Poses to Identity: Training-Free Person Re-Identification via Feature Centralization",
        "Authors": "Chao Yuan \u00b7 Guiwei Zhang \u00b7 Changxiao Ma \u00b7 Tianyi Zhang \u00b7 Guanglin Niu",
        "Abstract": "Person re-identification (ReID) aims to extract accurate identity representation features. However, during feature extraction, individual samples are inevitably affected by noise (background, occlusions, and model limitations). Considering that features from the same identity follow a normal distribution around identity centers after training, we propose a Training-Free Feature Centralization ReID framework by aggregating the same identity features to reduce individual sample noise and enhance the stability of identity representation, which preserves the feature's original distribution for following strategies such as re-ranking. Specifically, to obtain samples of the same identity, we introduce two components: Identity-Guided Pedestrian Generation: by leveraging identity features to guide the generation process, we obtain high-quality images with diverse poses, ensuring identity consistency even in complex scenarios such as infrared, and occlusion. Neighbor Feature Centralization: it explores each sample's potential positive samples from its neighborhood. Experiments demonstrate that our generative model exhibits strong generalization capabilities and maintains high identity consistency. With the Feature Centralization framework, we achieve impressive performance even with an ImageNet pre-trained model without ReID training, reaching mAP/Rank-1 of 52.81/78.92 on Market1501. Moreover, our method sets new state-of-the-art results across standard, cross-modality, and occluded ReID tasks, showcasing strong adaptability.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces a Training-Free Feature Centralization framework for Person Re-Identification (ReID), which enhances identity representation by aggregating features of the same identity to reduce noise from individual samples. This approach maintains the original feature distribution, beneficial for subsequent strategies like re-ranking. The framework includes Identity-Guided Pedestrian Generation for creating diverse, high-quality images with consistent identity, and Neighbor Feature Centralization for identifying potential positive samples within a neighborhood. The method demonstrates strong generalization and identity consistency, achieving notable performance on the Market1501 dataset without ReID-specific training, and sets new benchmarks in standard, cross-modality, and occluded ReID tasks.",
        "Tags": [
            "ReID (Person Re-identification)",
            "Data Augmentation",
            "Feature Centralization",
            "Identity-Guided Generation",
            "Training-Free Framework"
        ]
    },
    {
        "Title": "EvEnhancer: Empowering Effectiveness, Efficiency and Generalizability for Continuous Space-Time Video Super-Resolution with Events",
        "Authors": "Shuoyan Wei \u00b7 Feng Li \u00b7 Shengeng Tang \u00b7 Yao Zhao \u00b7 Huihui Bai",
        "Abstract": "Continuous space-time video super-resolution (C-STVSR) endeavors to upscale videos simultaneously at arbitrary spatial and temporal scales, which has recently garnered increasing interest. However, prevailing methods struggle to yield satisfactory videos at out-of-distribution spatial and temporal scales. On the other hand, event streams characterized by high temporal resolution and high dynamic range, exhibit compelling promise in vision tasks. This paper presents EvEnhancer, an innovative approach that marries the unique advantages of event streams to elevate effectiveness, efficiency, and generalizability for C-STVSR. Our approach hinges on two pivotal components: 1) Event-adapted synthesis capitalizes on the spatiotemporal correlations between frames and events to discern and learn long-term motion trajectories, enabling the adaptive interpolation and fusion of informative spatiotemporal features; 2) Local implicit video transformer integrates local implicit video neural function with cross-scale spatiotemporal attention to learn continuous video representations utilized to generate plausible videos at arbitrary resolutions and frame rates. Experiments show that EvEnhancer achieves superiority on synthetic and real-world datasets and preferable generalizability on out-of-distribution scales against state-of-the-art methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces EvEnhancer, a novel approach for continuous space-time video super-resolution (C-STVSR) that leverages event streams to enhance effectiveness, efficiency, and generalizability. The method utilizes two key components: 1) Event-adapted synthesis, which exploits spatiotemporal correlations between frames and events to learn long-term motion trajectories for adaptive interpolation and feature fusion, and 2) Local implicit video transformer, which combines local implicit video neural functions with cross-scale spatiotemporal attention to generate high-quality videos at arbitrary resolutions and frame rates. EvEnhancer demonstrates superior performance on both synthetic and real-world datasets and exhibits strong generalizability on out-of-distribution scales compared to state-of-the-art methods.",
        "Tags": [
            "Super-Resolution",
            "Video Understanding",
            "Event Streams",
            "Spatiotemporal Attention",
            "Continuous Video Representations"
        ]
    },
    {
        "Title": "EigenGS Representation: From Eigenspace to Gaussian Image Space",
        "Authors": "LO-WEI TAI \u00b7 Ching-En Ching En, Li \u00b7 Cheng-Lin Chen \u00b7 Chih-Jung Tsai \u00b7 Hwann-Tzong Chen \u00b7 Tyng-Luh Liu",
        "Abstract": "Principal Component Analysis (PCA), a classical dimensionality reduction technique, and Gaussian Splatting, a recent high-quality image synthesis method, represent fundamentally different approaches to image representation. Despite these significant differences, we present EigenGS, a novel method that bridges these two paradigms. By establishing an efficient transformation pipeline between eigenspace and image-space Gaussian representations, our approach enables instant initialization of Gaussian parameters for new images without requiring per-image training from scratch. Our method also introduces a frequency-aware learning mechanism that encourages Gaussians to adapt to different scales in order to better model spatial frequencies, effectively preventing artifacts in high-resolution reconstruction. Extensive experiments demonstrate that EigenGS not only achieves superior reconstruction quality but also dramatically accelerates convergence. The results highlight EigenGS's effectiveness and its ability to generalize across images with varying resolutions and diverse categories. This makes high-quality Gaussian Splatting practically viable for real-time applications.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces EigenGS, a novel method that bridges Principal Component Analysis (PCA) and Gaussian Splatting for image representation. EigenGS establishes an efficient transformation pipeline between eigenspace and image-space Gaussian representations, enabling instant initialization of Gaussian parameters for new images without per-image training. It incorporates a frequency-aware learning mechanism to adapt Gaussians to different scales, improving spatial frequency modeling and preventing artifacts in high-resolution reconstruction. The method achieves superior reconstruction quality, accelerates convergence, and generalizes across images of varying resolutions and categories, making Gaussian Splatting viable for real-time applications.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Image Generation",
            "Frequency-Aware Learning",
            "Real-Time Image Synthesis",
            "Dimensionality Reduction"
        ]
    },
    {
        "Title": "Track4Gen: Teaching Video Diffusion Models to Track Points Improves Video Generation",
        "Authors": "Hyeonho Jeong \u00b7 Chun-Hao P. Huang \u00b7 Jong Chul Ye \u00b7 Niloy J. Mitra \u00b7 Duygu Ceylan",
        "Abstract": "While recent foundational video generators produce visually rich output, they still struggle with appearance drift, where objects gradually degrade or change inconsistently across frames, breaking visual coherence. We hypothesize that this is because there is no explicit supervision in terms of spatial tracking at the feature level. We propose Track4Gen, a spatially aware video generator that combines video diffusion loss with point tracking across frames, providing enhanced spatial supervision on the diffusion features. Track4Gen merges the video generation and point tracking tasks into a single network by making minimal changes to existing video generation architectures. Using Stable Video Diffusion as a backbone, Track4Gen demonstrates that it is possible to unify video generation and point tracking, which are typically handled as separate tasks. Our extensive evaluations show that Track4Gen effectively reduces appearance drift, resulting in temporally stable and visually coherent video generation. Anonymous project page: https://anonymous1anonymous2.github.io/",
        "Link": "https://anonymous1anonymous2.github.io/",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Track4Gen addresses the issue of appearance drift in video generation, where objects degrade or change inconsistently across frames, by introducing explicit spatial supervision through point tracking. The method combines video diffusion loss with point tracking across frames, integrating both tasks into a single network with minimal architectural changes. Built on Stable Video Diffusion, Track4Gen demonstrates improved temporal stability and visual coherence in generated videos, effectively reducing appearance drift.",
        "Tags": [
            "Video Generation",
            "Diffusion Models",
            "Point Tracking",
            "Spatial Supervision",
            "Temporal Coherence"
        ]
    },
    {
        "Title": "LUMINET: Image-based Indoor Scene Relighting via Latent Intrinsics",
        "Authors": "Xiaoyan Xing \u00b7 Konrad Groh \u00b7 Sezer Karaoglu \u00b7 Theo Gevers \u00b7 Anand Bhattad",
        "Abstract": "We introduce LUMINET, a novel architecture that leverages generative models and latent intrinsic representations for effective lighting transfer. Given a source image and a target lighting image, LUMINET synthesizes a relit version of the source scene that captures the target's lighting.  Our approach makes two key contributions: a data curation strategy to enhance training efficiency for effective relighting, and a modified diffusion-based ControlNet that processes both latent intrinsic properties from the source image and latent extrinsic properties from the target image. We further improve lighting transfer through a learned adaptor (MLP) that injects the target's latent extrinsic properties via cross-attention and fine-tuning.Unlike traditional ControlNet, which generates images with conditional maps from a single scene, LUMINET processes latent representations from two different images - preserving geometry and albedo from the source while transferring lighting characteristics from the target. Experiments demonstrate that our method successfully transfers complex lighting phenomena including specular highlights and indirect illumination across scenes with varying spatial layouts and materials, outperforming existing approaches on challenging indoor scenes using only images as input.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "LUMINET is a novel architecture designed for image-based indoor scene relighting, utilizing generative models and latent intrinsic representations. It synthesizes a relit version of a source scene by transferring lighting from a target image. Key contributions include a data curation strategy for efficient training and a modified diffusion-based ControlNet that processes latent intrinsic properties from the source and latent extrinsic properties from the target. A learned adaptor (MLP) enhances lighting transfer by injecting target properties via cross-attention and fine-tuning. LUMINET outperforms existing methods in transferring complex lighting phenomena across diverse indoor scenes.",
        "Tags": [
            "Image Editing",
            "Diffusion Models",
            "Latent Intrinsic Representations",
            "Cross-Attention Mechanism",
            "Lighting Transfer"
        ]
    },
    {
        "Title": "NLPrompt: Noise-Label Prompt Learning for Vision-Language Models",
        "Authors": "Bikang Pan \u00b7 Qun Li \u00b7 Xiaoying Tang \u00b7 Wei Huang \u00b7 Zhen Fang \u00b7 Feng Liu \u00b7 Jingya Wang \u00b7 Jingyi Yu \u00b7 Ye Shi",
        "Abstract": "The emergence of vision-language foundation models, such as CLIP, has revolutionized image-text representation, enabling a broad range of applications via prompt learning. Despite its promise, real-world datasets often contain noisy labels that can degrade prompt learning performance. In this paper, we demonstrate that using mean absolute error (MAE) loss in prompt learning, named PromptMAE, significantly enhances robustness against noisy labels while maintaining high accuracy. Though MAE is straightforward and recognized for its robustness, it is rarely used in noisy-label learning due to its slow convergence and poor performance outside prompt learning scenarios. To elucidate the robustness of PromptMAE, we leverage feature learning theory to show that MAE can suppress the influence of noisy samples, thereby improving the signal-to-noise ratio and enhancing overall robustness. Additionally, we introduce PromptOT, a prompt-based optimal transport data purification method to enhance the robustness further. PromptOT employs text encoder representations in vision-language models as prototypes to construct an optimal transportation matrix. This matrix effectively partitions datasets into clean and noisy subsets, allowing for the application of cross-entropy loss to the clean subset and MAE loss to the noisy subset. Our Noise-Label Prompt Learning method, named NLPrompt, offers a simple and efficient approach that leverages the expressive representation and precise alignment capabilities of vision-language models for robust prompt learning.  We validate NLPrompt through extensive experiments across various noise settings, demonstrating significant performance improvements.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces NLPrompt, a novel approach for robust prompt learning in vision-language models, addressing the challenge of noisy labels in real-world datasets. The authors propose PromptMAE, which uses mean absolute error (MAE) loss to enhance robustness against noisy labels while maintaining high accuracy. They also introduce PromptOT, a prompt-based optimal transport data purification method that leverages text encoder representations to partition datasets into clean and noisy subsets, applying cross-entropy loss to clean data and MAE loss to noisy data. NLPrompt combines these methods to improve the signal-to-noise ratio and overall robustness in prompt learning. The approach is validated through extensive experiments, showing significant performance improvements across various noise settings.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Noisy-Label Learning",
            "Prompt Learning",
            "Optimal Transport",
            "Noisy-Label Robustness"
        ]
    },
    {
        "Title": "Scaling Inference Time Compute for Diffusion Models",
        "Authors": "Nanye Ma \u00b7 Shangyuan Tong \u00b7 Haolin Jia \u00b7 Hexiang Hu \u00b7 Yu-Chuan Su \u00b7 Mingda Zhang \u00b7 Xuan Yang \u00b7 Yandong Li \u00b7 Tommi Jaakkola \u00b7 Xuhui Jia \u00b7 Saining Xie",
        "Abstract": "Generative models have made significant impacts across various domains, largely due to their ability to scale during training by increasing data, computational resources, and model size, a phenomenon characterized by the scaling laws. Recent research has begun to explore inference-time scaling behavior in large language models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of function evaluations (NFE), although the performance gains typically flatten after a few dozen steps. In this work, we present a framework on the inference-time scaling for diffusion models, that enables diffusion models to further benefit from the increased computation beyond the NFE plateau. Specifically, we consider a search problem aimed at identifying better noises during the sampling process. We structure the design space along two axes: the verifiers used to provide feedback, and the algorithms used to find better candidates. Through extensive experiments on class-conditioned and text-conditioned image generation benchmarks, our findings reveal that increasing inference-time compute leads to substantial improvements in the quality of samples generated by diffusion models, and with the complicated nature of images, combinations of the components in the framework can be specifically chosen to conform with different application scenario.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper explores inference-time scaling for diffusion models, which inherently allow flexible computation adjustment via the number of function evaluations (NFE). While performance gains typically plateau after a few dozen steps, the authors propose a framework to further enhance performance by increasing inference-time computation. The framework focuses on identifying better noises during sampling, structured along two axes: verifiers for feedback and algorithms for candidate selection. Experiments on class-conditioned and text-conditioned image generation benchmarks demonstrate that increased inference-time compute significantly improves sample quality, with framework components adaptable to different application scenarios.",
        "Tags": [
            "Diffusion Models",
            "Inference-Time Scaling",
            "Noise Optimization",
            "Sampling Process",
            "Image Generation"
        ]
    },
    {
        "Title": "Uncertainty Weighted Gradients for Model Calibration",
        "Authors": "Jinxu Lin \u00b7 Linwei Tao \u00b7 Minjing Dong \u00b7 Chang Xu",
        "Abstract": "Model calibration is essential for ensuring that the predictions of deep neural networks accurately reflect true probabilities in real-world classification tasks.However, deep networks often produce over-confident or under-confident predictions, leading to miscalibration.Various methods have been proposed to address this issue by designing effective loss functions for calibration, such as focal loss. In this paper, we analyze its effectiveness and provide a unified loss framework of focal loss and its variants, where we mainly attribute their superiority in model calibration to the loss weighting factor that estimates sample-wise uncertainty.Based on our analysis, existing loss functions fail to achieve optimal calibration performance due to two main issues: including misalignment in optimization and insufficient precision in uncertainty estimation.Specifically, focal loss cannot align sample uncertainty with gradient scaling and the single logit cannot indicate the uncertainty.To address these issues, we reformulate the optimization from the perspective of gradients, which focuses on uncertain samples. Meanwhile, we propose to use the Brier Score as the loss weight factor, which provides a more accurate uncertainty estimation via all the logits. Extensive experiments on various models and datasets demonstrate that our method achieves state-of-the-art (SOTA) performance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the issue of model calibration in deep neural networks, which often produce over-confident or under-confident predictions. The authors analyze the effectiveness of focal loss and its variants, attributing their success in calibration to the loss weighting factor that estimates sample-wise uncertainty. They identify two main issues with existing loss functions: misalignment in optimization and insufficient precision in uncertainty estimation. To address these, the authors reformulate the optimization process to focus on uncertain samples and propose using the Brier Score as a more accurate loss weight factor. Their method achieves state-of-the-art performance across various models and datasets.",
        "Tags": [
            "Model Calibration",
            "Loss Functions",
            "Brier Score",
            "Gradient-based Optimization",
            "Uncertainty Estimation"
        ]
    },
    {
        "Title": "Lux Post Facto: Learning Portrait Performance Relighting with Conditional Video Diffusion and a Hybrid Dataset",
        "Authors": "Yiqun Mei \u00b7 Mingming He \u00b7 Li Ma \u00b7 Julien Philip \u00b7 Wenqi Xian \u00b7 David M George \u00b7 Xueming Yu \u00b7 Gabriel Dedic \u00b7 Ahmet Levent Ta\u015fel \u00b7 Ning Yu \u00b7 Vishal M. Patel \u00b7 Paul Debevec",
        "Abstract": "Video portrait relighting remains challenging because the results need to be both photorealistic and temporally stable.This typically requires a strong model design that can capture complex facial reflections as well as intensive training on a high-quality paired video dataset, such as dynamic one-light-at-a-time (OLAT). In this work, we introduce Lux Post Facto, a novel portrait video relighting method that produces both photorealistic and temporally consistent lighting effects. From the model side, we design a new conditional video diffusion model built upon state-of-the-art pre-trained video diffusion model, alongside a new lighting injection mechanism to enable precise control. This way we leverage strong spatial and temporal generative capability to generate plausible solutions to the ill-posed relighting problem. Our technique uses a hybrid dataset consisting of static expression OLAT data and in-the-wild portrait performance videos to jointly learn relighting and temporal modeling. This avoids the need to acquire paired video data in different lighting conditions. Our extensive experiments show that our model produces state-of-the-art results both in terms of photorealism and temporal consistency.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Lux Post Facto introduces a novel method for video portrait relighting, addressing the challenges of achieving photorealistic and temporally stable results. The approach leverages a conditional video diffusion model, enhanced with a new lighting injection mechanism for precise control, and utilizes a hybrid dataset combining static expression OLAT data and in-the-wild portrait performance videos. This method eliminates the need for paired video data in varied lighting conditions, demonstrating superior performance in photorealism and temporal consistency.",
        "Tags": [
            "Video Generation",
            "Diffusion Models",
            "Conditional Video Diffusion",
            "Lighting Injection Mechanism",
            "Hybrid Dataset Learning"
        ]
    },
    {
        "Title": "Synergizing Motion and Appearance: Multi-Scale Compensatory Codebooks for Talking Head Video Generation",
        "Authors": "Shuling Zhao \u00b7 Fa-Ting Hong \u00b7 Xiaoshui Huang \u00b7 Dan Xu",
        "Abstract": "Talking head video generation aims to generate a realistic talking head video that preserves the person\u2019s identity from a source image and the motion from a driving video. Despite the promising progress made in the field, it remains a challenging and critical problem to generate videos with accurate poses and fine-grained facial details simultaneously. Essentially, facial motion is often highly complex to model precisely, and the one-shot source face image cannot provide sufficient appearance guidance during generation due to dynamic pose changes. To tackle the problem, we propose to jointly learn motion and appearance codebooks and perform multi-scale codebook compensation to effectively refine both the facial motion conditions and appearance features for talking face image decoding. Specifically, the designed multi-scale motion and appearance codebooks are learned simultaneously in a unified framework to store representative global facial motion flow and appearance patterns.~Then, we present a novel multi-scale motion and appearance compensation module, which utilizes a transformer-based codebook retrieval strategy to query complementary information from the two codebooks for joint motion and appearance compensation. The entire process produces motion flows of greater flexibility and appearance features with fewer distortions across different scales, resulting in a high-quality talking head video generation framework.Extensive experiments on various benchmarks validate the effectiveness of our approach and demonstrate superior generation results from both qualitative and quantitative perspectives when compared to state-of-the-art competitors.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Talking head video generation aims to create realistic videos that preserve a person's identity from a source image and motion from a driving video. This paper addresses the challenge of generating videos with accurate poses and fine-grained facial details by proposing a method that jointly learns motion and appearance codebooks. A multi-scale compensation module, utilizing a transformer-based codebook retrieval strategy, refines facial motion and appearance features. This approach enhances motion flexibility and reduces appearance distortions, resulting in high-quality video generation. The method demonstrates superior performance on various benchmarks compared to state-of-the-art techniques.",
        "Tags": [
            "Video Generation",
            "Talking Head Video Generation",
            "Transformer",
            "Codebook Learning",
            "Facial Motion Modeling",
            "Multi-Scale Compensation"
        ]
    },
    {
        "Title": "DKC: Differentiated Knowledge Consolidation for Cloth-Hybrid Lifelong Person Re-identification",
        "Authors": "Zhenyu Cui \u00b7 Jiahuan Zhou \u00b7 Yuxin Peng",
        "Abstract": "Lifelong person re-identification (LReID) aims to match the same person using sequentially collected data. However, due to the long-term nature of lifelong learning, the inevitable changes in human clothes prevent the model from relying on unified discriminative information (e.g., clothing style) to match the same person in the streaming data, demanding differentiated cloth-irrelevant information. Unfortunately, existing LReID methods typically fail to leverage such knowledge resulting in the exacerbation of catastrophic forgetting issues. Therefore, in this paper, we focus on a challenging practical task called Cloth-Hybrid Lifelong Person Re-identification (CH-LReID), which requires matching the same person wearing different clothes using sequentially collected data. A Differentiated Knowledge Consolidation (DKC) framework is designed to unify and balance distinct knowledge across streaming data. The core idea is to adaptively balance differentiated knowledge and compatibly consolidate cloth-relevant and cloth-irrelevant information. To this end, a Differentiated Knowledge Transfer (DKT) module and a Latent Knowledge Consolidation (LKC) module are designed to adaptively discover differentiated new knowledge, while eliminating the derived domain shift of old knowledge via reconstructing the old latent feature space, respectively. Then, to further alleviate the catastrophic conflict between differentiated new and old knowledge, we further propose a Dual-level Distribution Alignment (DDA) module to align the distribution of discriminative knowledge at both the instance level and the fine-grained level. Extensive experiments on multiple benchmarks demonstrate the superiority of our method against existing methods in both CH-LReID and traditional LReID tasks. Our code will be released soon.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of Cloth-Hybrid Lifelong Person Re-identification (CH-LReID), where the goal is to match individuals across different clothing using sequentially collected data. The authors propose a Differentiated Knowledge Consolidation (DKC) framework to balance and unify cloth-relevant and cloth-irrelevant information across streaming data. The framework includes a Differentiated Knowledge Transfer (DKT) module and a Latent Knowledge Consolidation (LKC) module to adaptively discover new knowledge and reconstruct old latent feature spaces, respectively. Additionally, a Dual-level Distribution Alignment (DDA) module is introduced to align the distribution of discriminative knowledge at both instance and fine-grained levels, mitigating catastrophic forgetting. The proposed method demonstrates superior performance on multiple benchmarks for both CH-LReID and traditional LReID tasks.",
        "Tags": [
            "ReID (Person Re-identification)",
            "Long-Tail Learning",
            "Catastrophic Forgetting",
            "Domain Shift",
            "Feature Space Reconstruction"
        ]
    },
    {
        "Title": "Reasoning to Attend: Try to Understand How [SEG] Token Works",
        "Authors": "Rui Qian \u00b7 Xin Yin \u00b7 Dejing Dou",
        "Abstract": "Current Large Multimodal Models (LMMs) empowered tasks such as visual grounding and segmentation typically rely on $\\texttt{}$ token as a text prompting to jointly optimize the vision-language model (e.g., LLaVA) and the downstream task-specified model ($\\eg$, SAM). However, we observe that little research has looked into how it works when mapping language vocabulary embedding into corresponding vision codebook space. In this work, we first visualize the similarity maps, $\\aka$ pseudo images, which are obtained by computing the dot product similarity between the $\\texttt{}$ token and the image token embedings derived from the last hidden layer in both LLaVA and SAM models. Intriguingly, we have found that a striking consistency holds in terms of activation responses in the pseudo images, which reveals that what $\\texttt{}$ token  contributes to is the semantic correspondences from image-text pairs. Specifically, $\\texttt{}$ token, a placeholder expanded in text vocabulary, extensively queries within individual tokenized image patches to map the semantics of an object from text to the paired image while the Large Language Models (LLMs) is being fine tined. Upon above findings, we present READ, which facilitates LMMs' resilient $\\textbf{REA}$soning capability of where to atten\\textbf{D} under the guidance of highly activated points borrowed from pseudo images. Remarkably, READ features an intuitive design, Similarity as Points module (SasP), which can be seamlessly applied to existing $\\texttt{}$-like paradigms with negligible overheads in a plug-and-play fashion. Also, extensive experiments have been conducted on highly challenging reasoning segmentation dataset and widely used RefCOCO(+/g) referring segmentation dataset. To validate whether READ suffers from catastrophic forgetting of previous skills after fine-tuning, as observed in prior works ($\\eg$, LISA), we further assess its generation ability  on FP-RefCOCO(+/g) dataset. All code, models will be publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper investigates the functionality of the $\texttt{[SEG]}$ token in Large Multimodal Models (LMMs) for tasks like visual grounding and segmentation. The authors visualize similarity maps, or pseudo images, to understand how the $\texttt{[SEG]}$ token maps language vocabulary embeddings into vision codebook space. They find that the $\texttt{[SEG]}$ token contributes to semantic correspondences between image-text pairs by querying individual tokenized image patches. Based on these findings, they introduce READ, a method that enhances LMMs' reasoning capabilities by leveraging highly activated points from pseudo images. READ includes the Similarity as Points (SasP) module, which can be integrated into existing $\texttt{[SEG]}$-like paradigms with minimal overhead. The method is validated on challenging reasoning segmentation datasets and the RefCOCO(+/g) referring segmentation dataset, demonstrating its effectiveness without suffering from catastrophic forgetting.",
        "Tags": [
            "Large Language Models (LLMs)",
            "Vision-Language Models (VLMs)",
            "Semantic Segmentation",
            "Visual Grounding",
            "Text-to-Image Mapping",
            "Pseudo Images"
        ]
    },
    {
        "Title": "SF3D: Stable Fast 3D Mesh Reconstruction with UV-unwrapping and Illumination Disentanglement",
        "Authors": "Mark Boss \u00b7 Zixuan Huang \u00b7 Aaryaman Vasishta \u00b7 Varun Jampani",
        "Abstract": "We present SF3D, a novel method for rapid and high-quality textured object mesh reconstruction from a single image in just 0.5 seconds. Unlike most existing approaches, SF3D is explicitly trained for mesh generation, incorporating a fast UV unwrapping technique that enables swift texture generation rather than relying on vertex colors. The method also learns to predict material parameters and normal maps to enhance the visual quality of the reconstructed 3D meshes. Furthermore, SF3D integrates a delighting step to effectively remove low-frequency illumination effects, ensuring that the reconstructed meshes can be easily used in novel illumination conditions. Experiments demonstrate the superior performance of SF3D over the existing techniques.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "SF3D introduces a novel method for rapid and high-quality textured 3D mesh reconstruction from a single image, achieving results in just 0.5 seconds. The method is uniquely trained for mesh generation, utilizing a fast UV unwrapping technique for efficient texture generation instead of vertex colors. It also predicts material parameters and normal maps to improve visual quality and includes a delighting step to remove low-frequency illumination effects, making the meshes adaptable to new lighting conditions. SF3D outperforms existing techniques in performance.",
        "Tags": [
            "3D Reconstruction",
            "3D Generation",
            "UV Unwrapping",
            "Illumination Disentanglement",
            "Material Parameter Prediction"
        ]
    },
    {
        "Title": "HaWoR: World-Space Hand Motion Reconstruction from Egocentric Videos",
        "Authors": "Jinglei Zhang \u00b7 Jiankang Deng \u00b7 Chao Ma \u00b7 Rolandos Alexandros Potamias",
        "Abstract": "Despite the advent in 3D hand pose estimation, current methods predominantly focus on single-image 3D hand reconstruction in the camera frame, overlooking the world-space motion of the hands. Such limitation prohibits their direct use in egocentric video settings, where hands and camera are continuously in motion. In this work, we propose HaWoR, a high-fidelity method for hand motion reconstruction in world coordinates from egocentric videos. We propose to decouple the task by reconstructing the hand motion in the camera space and estimating the camera trajectory in the world coordinate system. To achieve precise camera trajectory estimation, we propose an adaptive egocentric SLAM framework that addresses the shortcomings of traditional SLAM methods, providing robust performance under challenging camera dynamics. To ensure robust hand motion trajectories, even when the hands move out of view frustum, we devise a novel motion infiller network that effectively completes the missing frames of the sequence. Through extensive quantitative and qualitative evaluations, we demonstrate that HaWoR achieves state-of-the-art performance on both hand motion reconstruction and world-frame camera trajectory estimation under different egocentric benchmark datasets. Code and models will be made publicly available for research purposes.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "HaWoR introduces a high-fidelity method for reconstructing hand motion in world coordinates from egocentric videos, addressing the limitations of existing methods that focus on single-image 3D hand reconstruction in the camera frame. The approach decouples the task into reconstructing hand motion in camera space and estimating the camera trajectory in the world coordinate system. An adaptive egocentric SLAM framework is proposed for precise camera trajectory estimation, overcoming challenges posed by dynamic camera movements. Additionally, a novel motion infiller network is developed to ensure robust hand motion trajectories, even when hands move out of the camera's view. HaWoR demonstrates state-of-the-art performance in both hand motion reconstruction and camera trajectory estimation across various egocentric benchmark datasets.",
        "Tags": [
            "3D Reconstruction",
            "Egocentric Vision",
            "Adaptive SLAM",
            "Motion Infiller Network",
            "World-Space Reconstruction"
        ]
    },
    {
        "Title": "Unlocking the potential of unlabeled data in semi-supervised domain generalization",
        "Authors": "Dongkwan Lee \u00b7 Kyomin Hwang \u00b7 Nojun Kwak",
        "Abstract": "We address the problem of semi-supervised domain generalization (SSDG), where the distributions of train and test data differ, and only a small amount of labeled data along with a larger amount of unlabeled data are available during training. Existing SSDG methods that leverage only the unlabeled samples for which the model's predictions are highly confident (confident-unlabeled samples), limit the full utilization of the available unlabeled data. To the best of our knowledge, we are the first to explore a method for incorporating the unconfident-unlabeled samples that were previously disregarded in SSDG setting. To this end, we propose UPCSC to utilize these unconfident-unlabeled samples in SSDG that consists of two modules: 1) Unlabeled Proxy-based Contrastive learning (UPC) module, treating unconfident-unlabeled samples as additional negative pairs and 2) Surrogate Class learning (SC) module, generating positive pairs for unconfident-unlabeled samples using their confusing class set. These modules are plug-and-play and do not require any domain labels, which can be easily integrated into existing approaches. Experiments on four widely used SSDG benchmarks demonstrate that our approach consistently improves performance when attached to baselines and outperforms competing plug-and-play methods. We also analyze the role of our method in SSDG, showing that it enhances class-level discriminability and mitigates domain gaps.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper tackles the challenge of semi-supervised domain generalization (SSDG), where training and test data distributions differ, and only limited labeled data alongside abundant unlabeled data are available. Existing SSDG methods primarily focus on confident-unlabeled samples, neglecting unconfident-unlabeled samples. The authors propose UPCSC, a novel method that incorporates unconfident-unlabeled samples through two modules: Unlabeled Proxy-based Contrastive Learning (UPC) and Surrogate Class Learning (SC). UPC treats unconfident-unlabeled samples as negative pairs, while SC generates positive pairs using a confusing class set. These modules are plug-and-play, require no domain labels, and can be integrated into existing methods. The approach demonstrates consistent performance improvements on four SSDG benchmarks, enhancing class-level discriminability and reducing domain gaps.",
        "Tags": [
            "Semi-Supervised Learning",
            "Domain Generalization",
            "Contrastive Learning",
            "Unconfident-Unlabeled Samples",
            "Plug-and-Play Modules"
        ]
    },
    {
        "Title": "Continuous Crowd Behavior Generation",
        "Authors": "Inhwan Bae \u00b7 Junoh Lee \u00b7 Hae-Gon Jeon",
        "Abstract": "Modeling and reproducing crowd behaviors are important in various domains including psychology, robotics, transport engineering and virtual environments. Conventional methods have focused on synthesizing momentary scenes, which have difficulty in replicating the continuous nature of real-world crowds. In this paper, we introduce a novel method for automatically generating continuous, realistic crowd trajectories with heterogeneous behaviors and interactions among individuals. We first design a crowd emitter model. To do this, we obtain spatial layouts from single input images, including a segmentation map, appearance map, population density map and population probability, prior to crowd generation. The emitter then continually places individuals on the timeline by assigning independent behavior characteristics such as agents' type, pace, and start/end positions using diffusion models. Next, our crowd simulator produces their long-term locomotions. To simulate diverse actions, it can augment their behaviors based on a Markov chain. As a result, our overall framework populates the scenes with heterogeneous crowd behaviors by alternating between the proposed emitter and simulator. Note that all the components in the proposed framework are user-controllable. Lastly, we propose a benchmark protocol to evaluate the realism and quality of the generated crowds in terms of the scene-level population dynamics and the individual-level trajectory accuracy. We demonstrate that our approach effectively models diverse crowd behavior patterns and generalizes well across different geographical environments. To validate its robustness, we submit our source code as supplementary material.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper presents a novel method for generating continuous, realistic crowd trajectories with heterogeneous behaviors and interactions. The approach involves a crowd emitter model that uses spatial layouts from single input images to place individuals with independent behavior characteristics, such as type, pace, and start/end positions, using diffusion models. A crowd simulator then produces long-term locomotions, augmenting behaviors based on a Markov chain. The framework alternates between the emitter and simulator to populate scenes with diverse crowd behaviors, all of which are user-controllable. A benchmark protocol is proposed to evaluate the realism and quality of the generated crowds, demonstrating the method's effectiveness in modeling diverse crowd behavior patterns and generalizing across different environments.",
        "Tags": [
            "Diffusion Models",
            "Crowd Simulation",
            "Behavior Modeling",
            "Markov Chain",
            "User-Controllable Simulation",
            "Heterogeneous Crowd Behaviors"
        ]
    },
    {
        "Title": "Flexible Selection for Efficient Video Reasoning",
        "Authors": "Shyamal Buch \u00b7 Arsha Nagrani \u00b7 Anurag Arnab \u00b7 Cordelia Schmid",
        "Abstract": "Video-language models have shown promise for addressing a range of multimodal tasks for video understanding, such as video question-answering. However, the inherent computational challenges of processing long video data and increasing model sizes have led to standard approaches that are limited by the number of frames they can process. In this work, we propose the Flexible Frame Selector (FFS), a learnable policy model with a new flexible selection operation, that helps alleviate input context restrictions by enabling video-language models to focus on the most informative frames for the downstream multimodal task, without adding undue processing cost. Our method differentiates from prior work due to its learnability, efficiency, and flexibility. We verify the efficacy of our method on standard video-question answering and reasoning benchmarks, and observe that our model can improve base video-language model accuracy while reducing the number of downstream processed frames.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces the Flexible Frame Selector (FFS), a learnable policy model designed to enhance video-language models by focusing on the most informative frames for downstream multimodal tasks, such as video question-answering. This approach addresses the computational challenges of processing long videos and large models by reducing the number of frames processed without compromising accuracy. The FFS is distinguished by its learnability, efficiency, and flexibility, and has been validated on standard benchmarks, showing improvements in model accuracy while decreasing the computational load.",
        "Tags": [
            "Video Understanding",
            "Multimodal Learning",
            "Frame Selection",
            "Computational Efficiency",
            "Learnable Policy Model"
        ]
    },
    {
        "Title": "Point Clouds Meets Physics: Dynamic Acoustic Field Fitting Network for Point Cloud Understanding",
        "Authors": "Changshuo Wang \u00b7 Shuting He \u00b7 Xiang Fang \u00b7 Jiawei Han \u00b7 Zhonghang Liu \u00b7 Xin Ning \u00b7 Weijun Li \u00b7 Prayag Tiwari",
        "Abstract": "While existing pre-training-based methods have enhanced point cloud model performance, they have not fundamentally resolved the challenge of local structure representation in point clouds. The limited representational capacity of pure point cloud models continues to constrain the potential of cross-modal fusion methods and performance across various tasks. To address this challenge, we propose a Dynamic Acoustic Field Fitting Network (DAF-Net), inspired by physical acoustic principles. Specifically, we represent local point clouds as acoustic fields and introduce a novel Acoustic Field Convolution (AF-Conv), which treats local aggregation as an acoustic energy field modeling problem and captures fine-grained local shape awareness by dividing the local area into near field and far field. Furthermore, drawing inspiration from multi-frequency wave phenomena and dynamic convolution, we develop the Dynamic Acoustic Field Convolution (DAF-Conv) based on AF-Conv. DAF-Conv dynamically generates multiple weights based on local geometric priors, effectively enhancing adaptability to diverse geometric features. Additionally, we design a Global Shape-Aware (GSA) layer incorporating EdgeConv and multi-head attention mechanisms, which combines with DAF-Conv to form the DAF Block. These blocks are then stacked to create a hierarchical DAFNet architecture. Extensive experiments on point cloud classification, part segmentation, and few-shot semantic segmentation demonstrate that DAFNet significantly outperforms existing methods across multiple tasks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces the Dynamic Acoustic Field Fitting Network (DAF-Net), a novel approach to enhance point cloud understanding by leveraging physical acoustic principles. DAF-Net represents local point clouds as acoustic fields and introduces Acoustic Field Convolution (AF-Conv) to model local aggregation as an acoustic energy field problem, capturing fine-grained local shape awareness. The Dynamic Acoustic Field Convolution (DAF-Conv) is developed to dynamically generate weights based on local geometric priors, improving adaptability to diverse geometric features. A Global Shape-Aware (GSA) layer, combining EdgeConv and multi-head attention mechanisms, is also introduced, forming the DAF Block. These blocks are stacked to create a hierarchical DAFNet architecture, which significantly outperforms existing methods in point cloud classification, part segmentation, and few-shot semantic segmentation tasks.",
        "Tags": [
            "3D Point Cloud",
            "3D Semantic Segmentation",
            "Acoustic Field Modeling",
            "Dynamic Convolution",
            "Hierarchical Architecture"
        ]
    },
    {
        "Title": "Neural Inverse Rendering from Propagating Light",
        "Authors": "Anagh Malik \u00b7 Benjamin Attal \u00b7 Andrew Xie \u00b7 Matthew O\u2019Toole \u00b7 David B. Lindell",
        "Abstract": "We present the first system for physically based, neural inverse rendering from multi-viewpoint videos of propagating light. Our approach relies on a time-resolved extension of neural radiance caching --- a technique that accelerates inverse rendering by storing infinite-bounce radiance arriving at any point from any direction. The resulting model accurately accounts for direct and indirect light transport effects and, when applied to captured measurements from a flash lidar system, enables state-of-the-art 3D reconstruction in the presence of strong indirect light. Further, we demonstrate view synthesis of propagating light, automatic decomposition of captured measurements into direct and indirect components, as well as novel capabilities such as multi-view transient relighting of captured scenes.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces the first system for physically based, neural inverse rendering from multi-viewpoint videos of propagating light. The approach extends neural radiance caching to a time-resolved framework, enabling accurate modeling of direct and indirect light transport effects. When applied to data from a flash lidar system, it achieves state-of-the-art 3D reconstruction, even in challenging lighting conditions. The system also supports view synthesis of propagating light, automatic decomposition of measurements into direct and indirect components, and novel capabilities like multi-view transient relighting.",
        "Tags": [
            "NeRF (Neural Radiance Fields)",
            "3D Reconstruction",
            "Inverse Rendering",
            "Time-Resolved Radiance",
            "Transient Relighting"
        ]
    },
    {
        "Title": "GlyphMastero: A Glyph Encoder for High-Fidelity Scene Text Editing",
        "Authors": "Tong Wang \u00b7 Ting Liu \u00b7 Xiaochao Qu \u00b7 WU CHENGJING \u00b7 Xiaochao Qu \u00b7 Xiaolin Hu",
        "Abstract": "Scene text editing, a subfield of image editing, requires modifying texts in images while preserving style consistency and visual coherence with the surrounding environment. While diffusion-based methods have shown promise in text generation, they still struggle to produce high-quality results. These methods often generate distorted or unrecognizable characters, particularly when dealing with complex characters like Chinese. In such systems, characters are composed of intricate stroke patterns and spatial relationships that must be precisely maintained. We present GlyphMastero, a specialized glyph encoder designed to guide the latent diffusion model for generating texts with stroke-level precision. Our key insight is that existing methods, despite using pretrained OCR models for feature extraction, fail to capture the hierarchical nature of text structures - from individual strokes to stroke-level interactions to overall character-level structure. To address this, our glyph encoder explicitly models and captures the cross-level interactions between local-level individual characters and global-level text lines through our novel glyph attention module. Meanwhile, our model implements a feature pyramid network to fuse the multi-scale OCR backbone features at the global-level. Through these cross-level and multi-scale fusions, we obtain more detailed glyph-aware guidance, enabling precise control over the scene text generation process. Our method achieves an 18.02\\% improvement in sentence accuracy over the state-of-the-art baseline, while simultaneously reducing the text-region Fr\u00e9chet inception distance by 53.28\\%.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Scene text editing involves modifying text in images while maintaining style consistency and visual coherence. Existing diffusion-based methods often produce distorted or unrecognizable characters, especially for complex scripts like Chinese. GlyphMastero introduces a specialized glyph encoder that guides a latent diffusion model to generate text with stroke-level precision. The encoder captures hierarchical text structures, from individual strokes to character-level interactions, using a novel glyph attention module and a feature pyramid network for multi-scale OCR feature fusion. This approach improves sentence accuracy by 18.02% and reduces text-region Fr\u00e9chet inception distance by 53.28% compared to state-of-the-art methods.",
        "Tags": [
            "Image Editing",
            "Text Detection",
            "Glyph Attention Module",
            "Feature Pyramid Network",
            "Stroke-Level Precision"
        ]
    },
    {
        "Title": "PolarNeXt: Rethink Instance Segmentation with Polar Representation",
        "Authors": "Jiacheng Sun \u00b7 Xinghong Zhou \u00b7 Yiqiang Wu \u00b7 Bin Zhu \u00b7 Jiaxuan Lu \u00b7 Yu Qin \u00b7 Xiaomao Li",
        "Abstract": "One of the roadblocks for instance segmentation today is heavy computational overhead and model parameters. Previous methods based on Polar Representation made the initial mark to address this challenge by formulating instance segmentation as polygon detection, but failed to align with mainstream methods in performance. In this paper, we highlight that Representation Errors, arising from the limited capacity of polygons to capture boundary details, have long been overlooked, which results in severe performance degradation. Observing that optimal starting point selection effectively alleviates this issue, we propose an Adaptive Polygonal Sample Decision strategy to dynamically capture the positional variation of representation errors across samples. Additionally, we design a Union-aligned Rasterization Module to incorporate these errors into polygonal assessment, further advancing the proposed strategy. With these components, our framework called PolarNeXt achieves a remarkable performance boost of over 4.8% AP compared to other polar-based methods. PolarNeXt is markedly more lightweight and efficient than state-of-the-art instance segmentation methods, while achieving comparable segmentation accuracy. We expect this work will open up a new direction for instance segmentation in high-resolution images and resource-limited scenarios.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "PolarNeXt addresses the computational overhead and model parameter challenges in instance segmentation by leveraging polar representation. The paper identifies Representation Errors, caused by polygons' limited ability to capture boundary details, as a critical overlooked issue. To mitigate this, the authors propose an Adaptive Polygonal Sample Decision strategy for dynamic error handling and a Union-aligned Rasterization Module to integrate errors into polygonal assessment. These innovations enable PolarNeXt to achieve a 4.8% AP improvement over other polar-based methods, offering a lightweight and efficient solution with competitive segmentation accuracy. The work paves the way for instance segmentation in high-resolution images and resource-limited scenarios.",
        "Tags": [
            "Instance Segmentation",
            "Polar Representation",
            "Adaptive Polygonal Sample Decision",
            "Union-aligned Rasterization Module",
            "Lightweight Instance Segmentation"
        ]
    },
    {
        "Title": "STiL: Semi-supervised Tabular-Image Learning for Comprehensive Task-Relevant Information Exploration in Multimodal Classification",
        "Authors": "Siyi Du \u00b7 Xinzhe Luo \u00b7 Declan ORegan \u00b7 Chen Qin",
        "Abstract": "Multimodal image-tabular learning is gaining attention, yet it faces challenges due to limited labeled data. While earlier work has applied self-supervised learning (SSL) to unlabeled data, its task-agnostic nature often results in learning suboptimal features for downstream tasks. Semi-supervised learning (SemiSL), which combines labeled and unlabeled data, offers a promising solution. However, existing multimodal SemiSL methods typically focus on unimodal or modality-shared features, ignoring valuable task-relevant modality-specific information, leading to a Modality Information Gap. In this paper, we propose STiL, a novel SemiSL tabular-image framework that addresses this gap by comprehensively exploring task-relevant information. STiL features a new disentangled contrastive consistency module to learn cross-modal invariant representations of shared information while retaining modality-specific information via disentanglement. We also propose a novel consensus-guided pseudo-labeling strategy to generate reliable pseudo-labels based on classifier consensus, along with a new prototype-guided label smoothing technique to refine pseudo-label quality with prototype embeddings, thereby enhancing task-relevant information learning in unlabeled data. Experiments on natural and medical image datasets show that STiL outperforms state-of-the-art supervised/SSL/SemiSL image/multimodal approaches. Our code will be available on GitHub.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Multimodal image-tabular learning faces challenges due to limited labeled data, with existing methods often ignoring task-relevant modality-specific information, leading to a Modality Information Gap. This paper introduces STiL, a novel semi-supervised tabular-image framework that addresses this gap by comprehensively exploring task-relevant information. STiL employs a disentangled contrastive consistency module to learn cross-modal invariant representations while retaining modality-specific information. Additionally, it introduces a consensus-guided pseudo-labeling strategy and a prototype-guided label smoothing technique to enhance the learning of task-relevant information in unlabeled data. STiL demonstrates superior performance over state-of-the-art methods on natural and medical image datasets.",
        "Tags": [
            "Multimodal Learning",
            "Semi-Supervised Learning",
            "Disentangled Contrastive Learning",
            "Pseudo-labeling Strategy",
            "Prototype-guided Label Smoothing"
        ]
    },
    {
        "Title": "Collaborative Tree Search for Enhancing Embodied Multi-Agent Collaboration",
        "Authors": "Lizheng Zu \u00b7 Lin Lin \u00b7 Song Fu \u00b7 Na Zhao \u00b7 Pan Zhou",
        "Abstract": "Embodied agents based on large language models (LLMs) face significant challenges in collaborative tasks, requiring effective communication and reasonable division of labor to ensure efficient and correct task completion. Previous approaches with simple communication patterns carry erroneous or incoherent agent actions, which can lead to additional risks. To address these problems, we propose Cooperative Tree Search (CoTS), a framework designed to significantly improve collaborative planning and task execution efficiency among embodied agents. CoTS guides multi-agents to discuss long-term strategic plans within a modified Monte Carlo tree, searching along LLM-driven reward functions to provide a more thoughtful and promising approach to cooperation. Another key feature of our method is the introduction of a plan evaluation module, which not only prevents agent action confusion caused by frequent plan updates but also ensures plan updates when the current plan becomes unsuitable. Experimental results show that the proposed method performs excellently in planning, communication, and collaboration on embodied environments (CWAH and TDW-MAT), efficiently completing long-term, complex tasks and significantly outperforming existing methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Cooperative Tree Search (CoTS), a framework aimed at enhancing collaborative planning and task execution efficiency among embodied agents based on large language models (LLMs). CoTS utilizes a modified Monte Carlo tree search guided by LLM-driven reward functions to facilitate long-term strategic planning and introduces a plan evaluation module to prevent action confusion and ensure timely plan updates. The method demonstrates superior performance in planning, communication, and collaboration in embodied environments, outperforming existing approaches.",
        "Tags": [
            "Embodied AI",
            "Large Language Models (LLMs)",
            "Monte Carlo Tree Search",
            "Plan Evaluation Module",
            "Multi-Agent Collaboration"
        ]
    },
    {
        "Title": "Guiding Human-Object Interactions with Rich Geometry and Relations",
        "Authors": "Mengqing Xue \u00b7 Yifei Liu \u00b7 Ling Guo \u00b7 Shaoli Huang \u00b7 Changxing Ding",
        "Abstract": "Human-object interaction (HOI) synthesis is crucial for creating immersive and realistic experiences for applications such as virtual reality. Existing methods often rely on simplified object representations, such as the object's centroid or the nearest point to a human, to achieve physically plausible motions. However, these approaches may overlook geometric complexity, resulting in suboptimal interaction fidelity. To address this limitation, we introduce ROG, a novel diffusion-based framework that models the spatiotemporal relationships inherent in HOIs with rich geometric detail. For efficient object representation, we select boundary-focused and fine-detail key points from the object mesh, ensuring a comprehensive depiction of the object's geometry. This representation is used to construct an interactive distance field (IDF), capturing the robust HOI dynamics. Furthermore, we develop a diffusion-based relation model that integrates spatial and temporal attention mechanisms, enabling a better understanding of intricate HOI relationships. This relation model refines the generated motion's IDF, guiding the motion generation process to produce relation-aware and semantically aligned movements. Experimental evaluations demonstrate that ROG significantly outperforms state-of-the-art methods in the realism and semantic accuracy of synthesized HOIs. This paper\u2019s code will be released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces ROG, a novel diffusion-based framework for human-object interaction (HOI) synthesis that addresses the limitations of existing methods by incorporating rich geometric detail. ROG selects boundary-focused and fine-detail key points from object meshes to create an interactive distance field (IDF), which captures robust HOI dynamics. A diffusion-based relation model with spatial and temporal attention mechanisms is developed to refine the IDF, enabling the generation of relation-aware and semantically aligned movements. The framework significantly improves the realism and semantic accuracy of synthesized HOIs compared to state-of-the-art methods.",
        "Tags": [
            "Human-Object Interaction (HOI)",
            "Diffusion Models",
            "Interactive Distance Field (IDF)",
            "Spatial-Temporal Attention",
            "Semantic Motion Generation"
        ]
    },
    {
        "Title": "CryptoFace: End-to-End Encrypted Face Recognition",
        "Authors": "Wei Ao \u00b7 Vishnu Naresh Boddeti",
        "Abstract": "Face recognition is central to many authentication, security, and personalized applications. Yet, it suffers from significant privacy risks, particularly concerning unauthorized access to sensitive biometric data. This paper presents CryptoFace, the first end-to-end encrypted face recognition system that ensures secure processing of facial data from acquisition through storage and matching without exposing raw facial images or features at any stage. CryptoFace leverages fully homomorphic encryption (FHE) for encrypted feature extraction, feature matching, and comparison while maintaining high face recognition performance. It employs a mixture of shallow patch convolutional networks (PCNNs), which can be evaluated in parallel with FHE and lead to much faster inference. It is scalable to high-resolution face data without sacrificing inference speed and optimizes homomorphic neural architecture by minimizing the multiplicative depth. We evaluate the performance and computational efficiency of CryptoFace on multiple encrypted benchmark face datasets. CryptoFace exhibits a significant acceleration of $7.2\\times$ ($9,845$s to $1,364$s per image on CPU) compared to state-of-the-art FHE-based neural networks adapted for face recognition. CryptoFace will facilitate the deployment of secure biometric authentication systems in applications requiring strict privacy and security guarantees.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "CryptoFace introduces the first end-to-end encrypted face recognition system, ensuring secure processing of facial data from acquisition to storage and matching without exposing raw images or features. Utilizing fully homomorphic encryption (FHE), it enables encrypted feature extraction, matching, and comparison while maintaining high recognition performance. The system employs shallow patch convolutional networks (PCNNs) for parallel evaluation with FHE, achieving faster inference and scalability to high-resolution data. CryptoFace demonstrates a significant acceleration in computational efficiency, making it suitable for applications requiring strict privacy and security.",
        "Tags": [
            "Face Recognition",
            "Fully Homomorphic Encryption (FHE)",
            "Encrypted Feature Extraction",
            "Parallel Evaluation with FHE",
            "Shallow Patch Convolutional Networks (PCNNs)"
        ]
    },
    {
        "Title": "SeeGround: See and Ground for Zero-shot Open-Vocabulary 3D Visual Grounding",
        "Authors": "Rong Li \u00b7 Shijie Li \u00b7 Lingdong Kong \u00b7 Xulei Yang \u00b7 Junwei Liang",
        "Abstract": "3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based on textual descriptions, essential for applications like augmented reality and robotics. Traditional 3DVG approaches rely on annotated 3D datasets and predefined object categories, limiting scalability and adaptability. To overcome these limitations, we introduce SeeGround, a zero-shot 3DVG framework leveraging 2D Vision-Language Models (VLMs) trained on large-scale 2D data. SeeGround represents 3D scenes as a hybrid of query-aligned rendered images and spatially enriched text descriptions, bridging the gap between 3D data and 2D-VLMs input formats. We propose two modules: the Perspective Adaptation Module, which dynamically selects viewpoints for query-relevant image rendering, and the Fusion Alignment Module, which integrates 2D images with 3D spatial descriptions to enhance object localization. Extensive experiments on ScanRefer and Nr3D demonstrate that our approach outperforms existing zero-shot methods by large margins. Notably, we exceed weakly supervised methods and rival some fully supervised ones, outperforming previous SOTA by 7.7\\% on ScanRefer and 7.1\\% on Nr3D, showcasing its effectiveness in complex 3DVG task.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "SeeGround introduces a zero-shot 3D Visual Grounding (3DVG) framework that leverages 2D Vision-Language Models (VLMs) trained on large-scale 2D data to locate objects in 3D scenes based on textual descriptions. The framework represents 3D scenes as a hybrid of query-aligned rendered images and spatially enriched text descriptions, bridging the gap between 3D data and 2D-VLMs input formats. It includes two key modules: the Perspective Adaptation Module for dynamic viewpoint selection and the Fusion Alignment Module for integrating 2D images with 3D spatial descriptions. SeeGround outperforms existing zero-shot methods and rivals some fully supervised approaches, demonstrating significant improvements on benchmark datasets.",
        "Tags": [
            "3D Visual Grounding",
            "Vision-Language Models (VLMs)",
            "Zero-Shot Learning",
            "Perspective Adaptation",
            "Fusion Alignment",
            "Hybrid Representation"
        ]
    },
    {
        "Title": "V2V3D: View-to-View Denoised 3D Reconstruction for Light Field Microscopy",
        "Authors": "Jiayin Zhao \u00b7 Zhenqi Fu \u00b7 Tao Yu \u00b7 Hui Qiao",
        "Abstract": "Light field microscopy (LFM) has gained significant attention due to its ability to capture snapshot-based, large-scale 3D fluorescence images. However, current LFM reconstruction algorithms are highly sensitive to sensor noise and lack robustness when applied to experimental data. To address these challenges, this paper presents an unsupervised view-to-view LFM 3D reconstruction framework, named V2V3D. Unlike existing methods that directly use all views for reconstruction, V2V3D divides the views into two subsets, with each subset generating corresponding volumes and working together to effectively remove sensor noise. To enhance the recovery of high-frequency details, we propose a novel wave-optics-based feature alignment technique, which transforms the point spread function, used for forward propagation in wave optics, into convolution kernels specifically designed for feature alignment.  Moreover, we introduce an LFM dataset generated using two-photon excitation, including both the light field images and the corresponding 3D intensity volumes. Extensive experiments demonstrate that our unsupervised approach achieves high computational efficiency and outperforms the other state-of-the-art methods. These advancements position V2V3D as a promising solution for 3D imaging under challenging conditions. Our code and dataset will be made publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces V2V3D, an unsupervised view-to-view 3D reconstruction framework designed for light field microscopy (LFM). LFM captures large-scale 3D fluorescence images but suffers from sensitivity to sensor noise. V2V3D addresses this by dividing views into two subsets, each generating volumes that collaboratively remove noise. A novel wave-optics-based feature alignment technique is proposed to enhance high-frequency detail recovery by transforming the point spread function into convolution kernels for feature alignment. Additionally, a new LFM dataset generated using two-photon excitation is introduced. The framework demonstrates superior computational efficiency and outperforms state-of-the-art methods, making it a robust solution for 3D imaging in challenging conditions.",
        "Tags": [
            "3D Reconstruction",
            "Denoising",
            "Light Field Microscopy",
            "Wave-Optics-Based Feature Alignment",
            "Two-Photon Excitation Dataset",
            "Unsupervised Learning"
        ]
    },
    {
        "Title": "Enhancing 3D Gaze Estimation in the Wild using Weak Supervision with Gaze Following Labels",
        "Authors": "Pierre Vuillecard \u00b7 Jean-marc Odobez",
        "Abstract": "Accurate 3D gaze estimation in unconstrained real-world environments remains a significant challenge due to variations in appearance, head pose, occlusion, and the limited availability of in-the-wild 3D gaze datasets. To address these challenges, we introduce a novel Self-Training Weakly-Supervised Gaze Estimation framework (ST-SWGE). This two-stage learning framework leverages diverse 2D gaze datasets, such as gaze-following data, which offer rich variations in appearances, natural scenes, and gaze distributions, and proposes an approach to generate 3D pseudo-labels and enhance model generalization. Furthermore, traditional modality-specific models, designed separately for images or videos, limit the effective use of available training data. To overcome this, we propose the Gaze Transformer (GaT), a modality-agnostic architecture capable of simultaneously learning static and dynamic gaze information from both image and video datasets. By combining 3D video datasets with 2D gaze target labels from gaze following tasks, our approach achieves the following key contributions: (i) Significant state-of-the-art improvements in within-domain and cross-domain generalization on unconstrained benchmarks like Gaze360 and GFIE, with notable cross-modal gains in video gaze estimation; (ii) Superior cross-domain performance on datasets such as MPIIFaceGaze and Gaze360 compared to frontal face methods. Code and pre-trained models will be released to the community.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the challenge of accurate 3D gaze estimation in unconstrained real-world environments by introducing a Self-Training Weakly-Supervised Gaze Estimation framework (ST-SWGE). This framework leverages diverse 2D gaze datasets to generate 3D pseudo-labels and enhance model generalization. Additionally, the authors propose the Gaze Transformer (GaT), a modality-agnostic architecture that learns static and dynamic gaze information from both image and video datasets. The approach achieves state-of-the-art improvements in within-domain and cross-domain generalization on benchmarks like Gaze360 and GFIE, and demonstrates superior cross-domain performance on datasets such as MPIIFaceGaze and Gaze360.",
        "Tags": [
            "3D Gaze Estimation",
            "Weak Supervision",
            "Gaze Transformer (GaT)",
            "Modality-Agnostic Learning",
            "Cross-Domain Generalization",
            "Pseudo-Label Generation"
        ]
    },
    {
        "Title": "Analyzing the Synthetic-to-Real Domain Gap in 3D Hand Pose Estimation",
        "Authors": "Zhuoran ZHAO \u00b7 Linlin Yang \u00b7 Pengzhan Sun \u00b7 Pan Hui \u00b7 Angela Yao",
        "Abstract": "Recent synthetic 3D human datasets for the face, body, and hands have pushed the limits on photorealism. Face recognition and body pose estimation have achieved state-of-the-art performance using synthetic training data alone, but for the hand, there is still a large synthetic-to-real gap. This paper presents the first systematic study of the synthetic-to-real gap of 3D hand pose estimation. We analyze the gap and identify key components such as the forearm, image frequency statistics, hand pose, and object occlusions. To facilitate our analysis, we propose a data synthesis pipeline to synthesize high-quality data. We demonstrate that synthetic hand data can achieve the same level of accuracy as real data when integrating our identified components, paving the path to use synthetic data alone for hand pose estimation. Source code and data will be released upon acceptance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper conducts the first systematic study of the synthetic-to-real domain gap in 3D hand pose estimation, identifying key factors such as forearm representation, image frequency statistics, hand pose, and object occlusions. A novel data synthesis pipeline is introduced to generate high-quality synthetic data. The study demonstrates that synthetic hand data, when incorporating these identified components, can achieve accuracy comparable to real data, suggesting the potential for using synthetic data exclusively in hand pose estimation.",
        "Tags": [
            "3D Human Pose Estimation",
            "Synthetic-to-Real Domain Gap",
            "Data Synthesis Pipeline",
            "Forearm Representation",
            "Object Occlusions"
        ]
    },
    {
        "Title": "Multi-Modal Aerial-Ground Cross-View Place Recognition with Neural ODEs",
        "Authors": "Sijie Wang \u00b7 Rui She \u00b7 Qiyu Kang \u00b7 Siqi Li \u00b7 Disheng Li \u00b7 Tianyu Geng \u00b7 Shangshu Yu \u00b7 Wee Peng Tay",
        "Abstract": "Place recognition (PR) aims at retrieving the query place from a database and plays a crucial role in various applications, including navigation, autonomous driving, and augmented reality. While previous multi-modal PR works have mainly focused on the same-view scenario in which ground-view descriptors are matched with a database of ground-view descriptors during inference, the multi-modal cross-view scenario, in which ground-view descriptors are matched with aerial-view descriptors in a database, remains under-explored. We propose AGPlace, a model that effectively integrates information from multi-modal ground sensors (cameras and LiDARs) to achieve accurate aerial-ground PR. AGPlace achieves effective aerial-ground cross-view PR by leveraging a manifold-based neural ordinary differential equation (ODE) framework with a multi-domain alignment loss. It outperforms existing state-of-the-art cross-view PR models on large-scale datasets. As most existing PR models are designed for ground-ground PR, we adapt these baselines into our cross-view pipeline. Experiments demonstrate that this direct adaptation performs worse than our overall model architecture AGPlace. AGPlace represents a significant advancement in multi-modal aerial-ground PR, with promising implications for real-world applications.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Place recognition (PR) is essential for applications like navigation, autonomous driving, and augmented reality. While most PR models focus on same-view scenarios, the cross-view scenario, where ground-view descriptors are matched with aerial-view descriptors, is less explored. This paper introduces AGPlace, a model that integrates multi-modal ground sensor data (cameras and LiDARs) for accurate aerial-ground PR. AGPlace employs a manifold-based neural ODE framework with a multi-domain alignment loss, outperforming existing cross-view PR models on large-scale datasets. Unlike traditional ground-ground PR models, AGPlace demonstrates superior performance in cross-view scenarios, marking a significant advancement in multi-modal aerial-ground PR with real-world applicability.",
        "Tags": [
            "Autonomous Driving",
            "3D Point Cloud",
            "Neural ODEs",
            "Cross-View Matching",
            "Multi-Modal Integration"
        ]
    },
    {
        "Title": "Temporal Action Detection Model Compression by Progressive Block Drop",
        "Authors": "Xiaoyong Chen \u00b7 Yong Guo \u00b7 Jiaming Liang \u00b7 Sitong Zhuang \u00b7 Runhao Zeng \u00b7 Xiping Hu",
        "Abstract": "Temporal action detection (TAD) aims to identify and localize action instances in untrimmed videos, which is essential for various video understanding tasks. However, recent improvements in model performance, driven by larger feature extractors and datasets, have led to increased computational demands. This presents a challenge for applications like autonomous driving and robotics, which rely on limited computational resources. While existing channel pruning methods can compress these models, reducing the number of channels often hinders the parallelization efficiency of GPU, due to the inefficient multiplication between small matrices. Instead of pruning channels, we propose a Progressive Block Drop method that reduces model depth while retaining layer width. In this way, we still use large matrices for computation but reduce the number of multiplications. Our approach iteratively removes redundant blocks in two steps: first, we drop blocks with minimal impact on model performance; and second, we employ a parameter-efficient cross-depth alignment technique, fine-tuning the pruned model to restore model accuracy. Our method achieves a 25\\% reduction in computational overhead on two TAD benchmarks (THUMOS14 and ActivityNet-1.3) to achieve lossless compression. More critically, we empirically show that our method is orthogonal to channel pruning methods and can be combined with it to yield further efficiency gains.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Temporal action detection (TAD) is crucial for identifying and localizing actions in untrimmed videos, but recent advancements have increased computational demands. To address this, the authors propose a Progressive Block Drop method that reduces model depth while maintaining layer width, thereby preserving GPU parallelization efficiency. The method involves iteratively removing redundant blocks and fine-tuning the pruned model using a parameter-efficient cross-depth alignment technique. This approach achieves a 25% reduction in computational overhead on TAD benchmarks THUMOS14 and ActivityNet-1.3 without loss of accuracy. Additionally, the method is shown to be compatible with channel pruning techniques, offering further efficiency improvements.",
        "Tags": [
            "Temporal Action Detection",
            "Model Compression",
            "Autonomous Driving",
            "Robotics",
            "Progressive Block Drop",
            "Cross-Depth Alignment",
            "Lossless Compression"
        ]
    },
    {
        "Title": "Probability Density Geodesics in Image Diffusion Latent Space",
        "Authors": "Qingtao Yu \u00b7 Jaskirat Singh \u00b7 Zhaoyuan Yang \u00b7 Peter Henry Tu \u00b7 Jing Zhang \u00b7 Richard Hartley \u00b7 Hongdong Li \u00b7 Dylan Campbell",
        "Abstract": "Diffusion models indirectly estimate the probability density over a data space, which can be used to study its structure. In this work, we show that geodesics can be computed in diffusion latent space, where the norm induced by the spatially-varying inner product is inversely proportional to the probability density. In this formulation, a path that traverses a high density (that is, probable) region of image latent space is shorter than the equivalent path through a low density region. We present algorithms for solving the associated initial and boundary value problems and show how to compute the probability density along the path and the geodesic distance between two points. Using these techniques, we analyze how closely video clips approximate geodesics in a pre-trained image diffusion space. Finally, we demonstrate how these techniques can be applied to training-free image sequence interpolation and extrapolation, given a pre-trained image diffusion model.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper explores the computation of geodesics in the latent space of diffusion models, where the norm is inversely proportional to the probability density. The authors present algorithms for solving initial and boundary value problems, enabling the calculation of probability density along paths and geodesic distances between points. These techniques are applied to analyze video clips in a pre-trained image diffusion space and demonstrate training-free image sequence interpolation and extrapolation.",
        "Tags": [
            "Diffusion Models",
            "Image Generation",
            "Latent Space Analysis",
            "Geodesic Computation",
            "Probability Density Analysis",
            "Training-Free Interpolation"
        ]
    },
    {
        "Title": "3D-MVP: 3D Multiview Pretraining for Robotic Manipulation",
        "Authors": "Shengyi Qian \u00b7 Kaichun Mo \u00b7 Valts Blukis \u00b7 David Fouhey \u00b7 Dieter Fox \u00b7 Ankit Goyal",
        "Abstract": "Recent works have shown that visual pretraining on egocentric datasets using masked autoencoders (MAE) can improve generalization for downstream robotics tasks. However, these approaches pretrain only on 2D images, while many robotics applications require 3D scene understanding. In this work, we propose 3D-MVP, a novel approach for 3D multi-view pretraining using masked autoencoders. We leverage Robotic View Transformer (RVT), which uses a multi-view transformer to understand the 3D scene and predict gripper pose actions. We split RVT's multi-view transformer into visual encoder and action decoder, and pretrain its visual encoder using masked autoencoding on large-scale 3D datasets such as Objaverse. We evaluate 3D-MVP on a suite of virtual robot manipulation tasks and demonstrate improved performance over baselines. Our results suggest that 3D-aware pretraining is a promising approach to improve sample efficiency and generalization of vision-based robotic manipulation policies. We will release code and pretrained models for 3D-MVP to facilitate future research.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces 3D-MVP, a novel approach for 3D multi-view pretraining using masked autoencoders (MAE) to enhance 3D scene understanding for robotic manipulation tasks. Unlike previous methods that pretrain on 2D images, 3D-MVP leverages a Robotic View Transformer (RVT) with a multi-view transformer architecture, split into a visual encoder and an action decoder. The visual encoder is pretrained using masked autoencoding on large-scale 3D datasets like Objaverse. Evaluations on virtual robot manipulation tasks show that 3D-MVP outperforms baseline methods, indicating that 3D-aware pretraining improves sample efficiency and generalization in vision-based robotic manipulation policies.",
        "Tags": [
            "3D Reconstruction",
            "Embodied AI",
            "Masked Autoencoders",
            "Robotic View Transformer",
            "3D Scene Understanding"
        ]
    },
    {
        "Title": "Sonata: Self-Supervised Learning of Reliable Point Representations",
        "Authors": "Xiaoyang Wu \u00b7 Daniel DeTone \u00b7 Duncan Frost \u00b7 TIANWEI SHEN \u00b7 Chris Xie \u00b7 Nan Yang \u00b7 Jakob Engel \u00b7 Richard Newcombe \u00b7 Hengshuang Zhao \u00b7 Julian Straub",
        "Abstract": "In this paper, we question whether we have a reliable self-supervised point cloud model that can be used for diverse 3D tasks via simple linear probing, even with limited data and minimal computation. We find that existing 3D self-supervised learning approaches fall short when evaluated on representation quality through linear probing. We hypothesize that this is due to what we term the geometric shortcut, which causes representations to collapse to low-level spatial features. This challenge is unique to 3D and arises from the sparse nature of point cloud data. We address it through two key strategies: obscuring spatial information and enhancing the reliance on input features, ultimately composing a Sonata of 140k point clouds through self-distillation. Sonata is simple and intuitive, yet its learned representations are strong and reliable: zero-shot visualizations demonstrate semantic grouping, alongside strong spatial reasoning through nearest-neighbor relationships. Sonata demonstrates exceptional parameter and data efficiency, tripling linear probing accuracy (from 21.8% to 72.5%) on ScanNet and nearly doubling performance with only 1% of the data compared to previous approaches. Full fine-tuning further advances SOTA across both 3D indoor and outdoor perception tasks. All code and weights will be made available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Sonata, a self-supervised learning approach designed to produce reliable point cloud representations for diverse 3D tasks, even with limited data and minimal computation. Existing 3D self-supervised learning methods often fail to deliver high-quality representations when evaluated through linear probing, primarily due to the 'geometric shortcut' that causes representations to collapse to low-level spatial features. Sonata addresses this issue by obscuring spatial information and enhancing reliance on input features, using self-distillation to create a robust model. The approach demonstrates significant improvements in parameter and data efficiency, achieving a tripling of linear probing accuracy on ScanNet and nearly doubling performance with only 1% of the data compared to previous methods. Sonata also advances state-of-the-art performance in both 3D indoor and outdoor perception tasks through full fine-tuning.",
        "Tags": [
            "Self-Supervised Learning",
            "3D Point Cloud",
            "Self-Distillation",
            "Geometric Shortcut",
            "Linear Probing"
        ]
    },
    {
        "Title": "DFormerv2: Geometry Self-Attention for RGBD Semantic Segmentation",
        "Authors": "Bo-Wen Yin \u00b7 Jiao-Long Cao \u00b7 Ming-Ming Cheng \u00b7 Qibin Hou",
        "Abstract": "Recent advances in scene understanding benefit a lot from depth maps because of the 3D geometry information, especially in complex conditions (e.g., low light and overexposed). Existing approaches encode depth maps along with RGB images and perform feature fusion between them to enable more robust predictions. Taking into account that depth can be regarded as a geometry supplement for RGB images, a straightforward question arises: Do we really need to explicitly encode depth information with neural networks as done for RGB images? Based on this insight, in this paper, we investigate a new way to learn RGBD feature representations and present DFormerv2, a strong RGBD encoder that explicitly uses depth maps as geometry priors rather than encoding depth information with neural networks. Our goal is to leverage a memory token as the query to extract the geometry clues from the depth and spatial distances among all the image patch tokens, which will then be used as geometry priors to allocate attention weights in self-attention. Extensive experiments demonstrate that \\nameofmethod{} exhibits exceptional performance in various RGBD semantic segmentation benchmarks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "DFormerv2 introduces a novel approach to RGBD semantic segmentation by leveraging depth maps as geometry priors rather than encoding them with neural networks. The method uses a memory token to extract geometry clues from depth and spatial distances among image patch tokens, which are then used to allocate attention weights in self-attention. This approach demonstrates superior performance across various RGBD semantic segmentation benchmarks.",
        "Tags": [
            "Semantic Segmentation",
            "3D Point Cloud",
            "Geometry Self-Attention",
            "Memory Token",
            "RGBD Feature Representation"
        ]
    },
    {
        "Title": "FFR:Frequecny Feature Rectification for Weakly Supervised Semantic Segmentation",
        "Authors": "Ziqian Yang \u00b7 Xinqiao Zhao \u00b7 Xiaolei Wang \u00b7 Quan Zhang \u00b7 Jimin Xiao",
        "Abstract": "Image-level Weakly Supervised Semantic Segmentation (WSSS) has garnered significant attention due to its low annotation costs. Current single-stage state-of-the-art WSSS methods mainly reply on ViT to extract features from input images, generating more complete segmentation results based on comprehensive semantic information. However, these ViT-based methods often suffer from over-smoothing issues in segmentation results. In this paper, we identify that attenuated high-frequency features mislead the decoder of ViT-based WSSS models, resulting in over-smoothed false segmentation. To address this, we propose a Frequency Feature Rectification (FFR) framework. Quantitative and qualitative experimental results demonstrate that our FFR framework can effectively address the attenuated high-frequency caused over-smoothed segmentation issue and achieve new state-of-the-art WSSS performances. Codes will be released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Image-level Weakly Supervised Semantic Segmentation (WSSS) has gained attention due to its low annotation costs. Current state-of-the-art WSSS methods rely on Vision Transformers (ViTs) to extract features and generate segmentation results. However, these methods often suffer from over-smoothing issues caused by attenuated high-frequency features, leading to inaccurate segmentation. To address this, the authors propose a Frequency Feature Rectification (FFR) framework, which effectively mitigates the over-smoothing problem and achieves state-of-the-art performance in WSSS.",
        "Tags": [
            "Weakly Supervised Semantic Segmentation",
            "Vision Transformer (ViT)",
            "Frequency Feature Rectification",
            "Over-smoothing Issue",
            "High-Frequency Features"
        ]
    },
    {
        "Title": "EchoMatch: Partial-to-Partial Shape Matching via Correspondence Reflection",
        "Authors": "Yizheng Xie \u00b7 Viktoria Ehm \u00b7 Paul Roetzer \u00b7 Nafie El Amrani \u00b7 Maolin Gao \u00b7 Florian Bernard \u00b7 Daniel Cremers",
        "Abstract": "Finding correspondences between 3D shapes is a crucial problem in computer vision and graphics. While most research has focused on finding correspondences in settings where at least one of the shapes is complete, the realm of partial-to-partial shape matching remains under-explored. Yet it is of importance since, in many applications, shapes are only observed partially due to occlusion or scanning.Finding correspondences between partial shapes comes with an additional challenge: We not only want to identify correspondences between points on either shape but also have to determine which points of each shape actually have a partner.To tackle this challenging problem, we present EchoMatch, a novel framework for partial-to-partial shape matching that incorporates the concept of correspondence reflection to enable an overlap prediction within a functional map framework.With this approach, we show that we can outperform current SOTA methods in challenging partial-to-partial shape matching problems.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "EchoMatch introduces a novel framework for partial-to-partial shape matching, addressing the challenge of finding correspondences between incomplete 3D shapes due to occlusion or scanning. The framework utilizes correspondence reflection within a functional map framework to predict overlaps, demonstrating superior performance over current state-of-the-art methods in partial-to-partial shape matching scenarios.",
        "Tags": [
            "3D Point Cloud",
            "3D Registration",
            "Correspondence Reflection",
            "Functional Map Framework",
            "Overlap Prediction"
        ]
    },
    {
        "Title": "Dual-Agent Optimization framework for Cross-Domain Few-Shot Segmentation",
        "Authors": "Zhaoyang Li \u00b7 Yuan Wang \u00b7 Wangkai Li \u00b7 Tianzhu Zhang \u00b7 Xiang Liu",
        "Abstract": "Cross-Domain Few-Shot Segmentation (CD-FSS) extends the generalization ability of Few-Shot Segmentation (FSS) beyond a single domain, enabling more practical applications. However, directly employing conventional FSS methods suffers from severe performance degradation in cross-domain settings, primarily due to feature sensitivity and support-to-query matching process sensitivity across domains. Existing methods for CD-FSS either focus on domain adaptation of features or delve into designing matching strategies for enhanced cross-domain robustness. Nonetheless, they overlook the fact that these two issues are interdependent and should be addressed jointly. In this work, we tackle these two issues within a unified framework by optimizing features in the frequency domain and enhancing the matching process in the spatial domain, working jointly to handle the deviations introduced by the domain gap. To this end, we proposed a coherent Dual-Agent Optimization (DATO) framework, including a consistent mutual aggregation (CMA) and a correlation rectification strategy (CRS). In the consistent mutual aggregation module, we employ a set of agents to learn domain-invariant features across domains, and then use these features to enhance the original representations for feature adaptation. In the correlation rectification strategy, the agent-aggregated domain-invariant features serve as a bridge, transforming the support-to-query matching process into a referable feature space and reducing its domain sensitivity. Extensive experiments demonstrate the efficacy of our approach.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Cross-Domain Few-Shot Segmentation (CD-FSS) aims to generalize Few-Shot Segmentation (FSS) across domains, addressing challenges like feature sensitivity and support-to-query matching sensitivity. Existing methods focus on either feature adaptation or matching strategies but fail to address their interdependence. This paper introduces a Dual-Agent Optimization (DATO) framework, which jointly optimizes features in the frequency domain and enhances the matching process in the spatial domain. The framework includes a Consistent Mutual Aggregation (CMA) module for learning domain-invariant features and a Correlation Rectification Strategy (CRS) to reduce domain sensitivity in the matching process. The proposed approach demonstrates significant improvements in cross-domain performance.",
        "Tags": [
            "Cross-Domain Few-Shot Segmentation",
            "Feature Adaptation",
            "Matching Strategies",
            "Dual-Agent Optimization",
            "Frequency Domain Optimization",
            "Spatial Domain Matching"
        ]
    },
    {
        "Title": "SVDC: Consistent Direct Time-of-Flight Video Depth Completion with Frequency Selective Fusion",
        "Authors": "Xuan Zhu \u00b7 Jijun Xiang \u00b7 Xianqi Wang \u00b7 Longliang Liu \u00b7 Yu Wang \u00b7 Hong Zhang \u00b7 Fei Guo \u00b7 Xin Yang",
        "Abstract": "Lightweight direct Time-of-Flight (dToF) sensors are ideal for 3D sensing on mobile devices. However, due to the manufacturing constraints of compact devices and the inherent physical principles of imaging, dToF depth maps are sparse and noisy. In this paper, we propose a novel video depth completion method, called SVDC, by fusing the sparse dToF data with the corresponding RGB guidance. Our method employs a multi-frame fusion scheme to mitigate the spatial ambiguity resulting from the sparse dToF imaging. Misalignment between consecutive frames during multi-frame fusion could cause blending between object edges and the background, which results in a loss of detail. To address this, we introduce an adaptive frequency selective fusion (AFSF) module, which automatically selects convolution kernel sizes to fuse multi-frame features. Our AFSF utilizes a channel-spatial enhancement attention (CSEA) module to enhance features and generates an attention map as fusion weights. The AFSF ensures edge detail recovery while suppressing high-frequency noise in smooth regions. To further enhance temporal consistency, We propose a cross-window consistency loss to ensure consistent predictions across different windows, effectively reducing flickering. Our proposed SVDC achieves optimal accuracy and consistency on the TartanAir and Dynamic Replica datasets.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces SVDC, a novel video depth completion method designed to enhance the quality of sparse and noisy depth maps from direct Time-of-Flight (dToF) sensors by fusing them with RGB guidance. The method employs a multi-frame fusion scheme to address spatial ambiguity and introduces an adaptive frequency selective fusion (AFSF) module to prevent detail loss at object edges. The AFSF module uses a channel-spatial enhancement attention (CSEA) mechanism to refine feature fusion and employs a cross-window consistency loss to improve temporal consistency and reduce flickering. SVDC demonstrates superior accuracy and consistency on benchmark datasets.",
        "Tags": [
            "3D Reconstruction",
            "Depth Estimation",
            "Frequency Selective Fusion",
            "Temporal Consistency",
            "Attention Mechanisms"
        ]
    },
    {
        "Title": "Noise-Resistant Video Anomaly Detection via RGB Error-Guided Multiscale Predictive Coding and Dynamic Memory",
        "Authors": "Han Hu \u00b7 Wenli Du \u00b7 Peng Liao \u00b7 Bing Wang \u00b7 Siyuan Fan",
        "Abstract": "Due to the interference of background noise, existing video anomaly detection methods are prone to detect some normal events in complex scenes as anomalies. Meanwhile, we note that the diversity of normal patterns has not been adequately considered, i.e., the normal events that are worthy of reference in the test data have not been properly utilized, which raises the risk of missed and false detections. In this work, we combine the tasks of next-frame prediction and predicted-frame reconstruction to propose a noise-resistant video anomaly detection method. For the prediction task, we develop an RGB Error-Guided Multiscale Predictive Coding (EG-MPC) framework to overcome the interference of background noise on the learning of appearance and motion features of objects at various scales, thus achieving high-quality frame prediction. For the reconstruction task, we introduce the Dynamic Memory Modules (DMMs) into the reconstruction network, and design sparse aggregation and selective update strategies for the memory items in the DMMs to effectively represent diverse normal patterns while increasing the difficulty of reconstructing anomalies, thus making it easier to distinguish between normal and abnormal frames. Extensive experiments on four benchmark datasets demonstrate that our proposed method outperforms state-of-the-art approaches, especially in complex scenarios.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of video anomaly detection in complex scenes, where background noise often leads to the misclassification of normal events as anomalies. The authors propose a noise-resistant method that integrates next-frame prediction and predicted-frame reconstruction. They introduce an RGB Error-Guided Multiscale Predictive Coding (EG-MPC) framework to enhance the learning of appearance and motion features across different scales, thereby improving frame prediction quality. Additionally, Dynamic Memory Modules (DMMs) are incorporated into the reconstruction network, employing sparse aggregation and selective update strategies to better represent diverse normal patterns and increase the difficulty of reconstructing anomalies. This dual approach facilitates the distinction between normal and abnormal frames, demonstrating superior performance over existing methods in complex scenarios.",
        "Tags": [
            "Anomaly Detection",
            "Video Understanding",
            "Dynamic Memory Modules",
            "Multiscale Predictive Coding",
            "Noise Resistance"
        ]
    },
    {
        "Title": "NoiseCtrl: A Sampling-Algorithm-Agnostic Conditional Generation Method for Diffusion Models",
        "Authors": "Longquan Dai \u00b7 He Wang \u00b7 Jinhui Tang",
        "Abstract": "In training-free conditional generative tasks, diffusion models utilize differentiable loss functions to steer the generative reverse process, necessitating modifications to sampling algorithms like DDPM and DDIM. However, such adjustments likely reduce flexibility and reliability. In this paper, we propose NoiseCtrl, a sampling-algorithm-agnostic technique for controlled image generation. Essentially, diffusion models generate denoised results  $z_{t-1} $ by adding a predicted mean $\\mu_{{t}}$ with random noise $\\epsilon_{{t}}$. NoiseCtrl specifically adjusts the random noise while leaving the underlying sampling algorithms unchanged. At each  step ${t}$, NoiseCtrl converts the unconditional Gaussian noise into conditional noise $\\epsilon'_{{t}}$ by substituting the isotropic Gaussian distribution with the von Mises\u2013Fisher distribution. This substitution introduces a directional focus while preserving the randomness required for conditional image generation. Thanks to this non-intrusive design, NoiseCtrl is straightforward to integrate and has been extensively validated through experiments, demonstrating its adaptability for different  diffusion algorithms and superior performance across various conditional generation tasks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces NoiseCtrl, a novel technique for controlled image generation in diffusion models that does not require modifications to existing sampling algorithms like DDPM or DDIM. Unlike traditional methods that adjust the sampling process, NoiseCtrl modifies the random noise component by replacing the isotropic Gaussian distribution with the von Mises\u2013Fisher distribution, thereby introducing a directional focus while maintaining randomness. This approach allows for conditional image generation without altering the underlying sampling algorithms, enhancing flexibility and reliability. NoiseCtrl has been validated across various tasks and diffusion algorithms, demonstrating its adaptability and superior performance.",
        "Tags": [
            "Diffusion Models",
            "Image Generation",
            "Von Mises\u2013Fisher Distribution",
            "Sampling-Algorithm-Agnostic",
            "Conditional Noise Adjustment"
        ]
    },
    {
        "Title": "Modeling Thousands of Human Annotators for Generalizable Text-to-Image Person Re-identification",
        "Authors": "Jiayu Jiang \u00b7 Changxing Ding \u00b7 Wentao Tan \u00b7 Junhong Wang \u00b7 JIN Tao \u00b7 Xiangmin Xu",
        "Abstract": "Text-to-image person re-identification (ReID) aims to retrieve the images of an interested person based on textual descriptions. One main challenge for this task is the high cost in manually annotating large-scale databases, which affects the generalization ability of ReID models. Recent works handle this problem by leveraging Multi-modal Large Language Models (MLLMs) to describe pedestrian images automatically. However, the captions produced by MLLMs lack diversity in description styles. To address this issue, we propose a Human Annotator Modeling (HAM) approach to enable MLLMs to mimic the description styles of thousands of human annotators. Specifically, we first extract style features from human textual descriptions and perform clustering on them. This allows us to group textual descriptions with similar styles into the same cluster. Then, we employ a prompt to represent each of these clusters and apply prompt learning to mimic the description styles of different human annotators. Furthermore, we define a style feature space and perform uniform sampling in this space to obtain more diverse clustering prototypes, which further enriches the diversity of the MLLM-generated captions. Finally, we adopt HAM to automatically annotate a massive-scale database for text-to-image ReID. Extensive experiments on this database demonstrate that it significantly improves the generalization ability of ReID models. Code of this paper will be released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Text-to-image person re-identification (ReID) faces challenges due to the high cost of manual annotation, which limits model generalization. To address this, the paper introduces a Human Annotator Modeling (HAM) approach that enables Multi-modal Large Language Models (MLLMs) to mimic diverse human description styles. HAM extracts and clusters style features from human descriptions, uses prompt learning to represent clusters, and samples uniformly in a style feature space to enhance caption diversity. This method is applied to annotate a large-scale database, significantly improving ReID model generalization.",
        "Tags": [
            "ReID (Person Re-identification)",
            "Multimodal Large Language Models (MLLMs)",
            "Prompt Learning",
            "Style Feature Clustering",
            "Diversity Enhancement"
        ]
    },
    {
        "Title": "Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator",
        "Authors": "Chaehun Shin \u00b7 Jooyoung Choi \u00b7 Heeseung Kim \u00b7 Sungroh Yoon",
        "Abstract": "Subject-driven text-to-image generation aims to produce images of a new subject within a desired context by accurately capturing both the visual characteristics of the subject and the semantic content of a text prompt. Traditional methods rely on time- and resource-intensive fine-tuning for subject alignment, while recent zero-shot approaches leverage on-the-fly image prompting, often sacrificing subject alignment. In this paper, we introduce Diptych Prompting, a novel zero-shot approach that reinterprets as an inpainting task with precise subject alignment by leveraging the emergent property of diptych generation in large-scale text-to-image models. Diptych Prompting arranges an incomplete diptych with the reference image in the left panel, and performs text-conditioned inpainting on the right panel. We further prevent unwanted content leakage by removing the background in the reference image and improve fine-grained details in the generated subject by enhancing attention weights between the panels during inpainting. Experimental results confirm that our approach significantly outperforms zero-shot image prompting methods, resulting in images that are visually preferred by users. Additionally, our method supports not only subject-driven generation but also stylized image generation and subject-driven image editing, demonstrating versatility across diverse image generation applications.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Diptych Prompting, a novel zero-shot approach for subject-driven text-to-image generation that reinterprets the task as an inpainting problem. This method leverages the emergent property of diptych generation in large-scale text-to-image models, arranging an incomplete diptych with a reference image in the left panel and performing text-conditioned inpainting on the right panel. The approach improves subject alignment by removing the background in the reference image and enhancing attention weights during inpainting. Diptych Prompting outperforms existing zero-shot methods, producing images preferred by users and supporting diverse applications such as stylized image generation and subject-driven image editing.",
        "Tags": [
            "Text-to-Image Generation",
            "Zero-Shot Learning",
            "Inpainting",
            "Attention Mechanisms",
            "Diptych Generation"
        ]
    },
    {
        "Title": "Diff2Flow: Bridging the Gap between Diffusion and Flow Matching with Minimal Cost",
        "Authors": "Johannes Schusterbauer \u00b7 Ming Gui \u00b7 Frank Fundel \u00b7 Bj\u00f6rn Ommer",
        "Abstract": "Recent advancements in diffusion models have established new benchmarks in both generative tasks and downstream applications. In contrast, flow matching models have shown promising improvements in performance but have not been as extensively explored, particularly due to the difficulty of inheriting knowledge from a pretrained diffusion prior foundation model.In this work, we propose a novel method to bridge the gap between pretrained diffusion models and flow matching models by aligning their trajectories and matching their objectives. Our approach mathematically formalizes this alignment and enables the efficient transfer of knowledge from diffusion priors to flow matching models. We demonstrate that our method outperforms traditional diffusion and flow matching finetuning, achieving competitive results across a variety of tasks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Diff2Flow, a novel method that bridges the gap between pretrained diffusion models and flow matching models by aligning their trajectories and matching their objectives. The approach mathematically formalizes this alignment, enabling efficient knowledge transfer from diffusion priors to flow matching models. The proposed method outperforms traditional diffusion and flow matching fine-tuning, achieving competitive results across various tasks.",
        "Tags": [
            "Diffusion Models",
            "Flow Matching",
            "Knowledge Transfer",
            "Trajectory Alignment",
            "Objective Matching"
        ]
    },
    {
        "Title": "Cross-modal Causal Relation Alignment for Video Question Grounding",
        "Authors": "weixing chen \u00b7 Yang Liu \u00b7 Binglin Chen \u00b7 Jiandong Su \u00b7 Yongsen Zheng \u00b7 Liang Lin",
        "Abstract": "Video question grounding (VideoQG) requires models to answer the questions and simultaneously infer the relevant video segments to support the answers. However, existing VideoQG methods usually suffer from spurious cross-modal correlations, leading to a failure to identify the dominant visual scenes that align with the intended question. Moreover, although large models possess extensive prior knowledge and can demonstrate strong performance in a zero-shot setting, issues such as spurious correlations persist, making their application to specific downstream tasks challenging. In this work, we propose a novel causality-ware VideoQG framework named Cross-modal Causality Relation Alignment (CRA), to eliminate spurious correlations and improve the causal consistency between question-answering and video temporal grounding. Our CRA involves three essential components: i) Gaussian Smoothing Attention Grounding (GSAG) module for estimating the time interval via cross-modal attention, which is de-noised by an adaptive Gaussian filter. ii) Cross-modal Alignment (CA) enhances the performance of weakly supervised VideoQG by leveraging bidirectional contrastive learning between estimated video segments and QA features. iii) Explicit Causal Intervention (ECI) module for multimodal deconfounding, which involves front-door intervention for vision and back-door intervention for language. Extensive experiments on two VideoQG datasets demonstrate the superiority of our CRA in discovering visually grounded content and achieving robust question reasoning. Codes will be available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces a novel causality-aware framework, Cross-modal Causality Relation Alignment (CRA), for Video Question Grounding (VideoQG). The CRA framework addresses the issue of spurious cross-modal correlations that hinder the identification of relevant video segments for question answering. It comprises three key components: the Gaussian Smoothing Attention Grounding (GSAG) module for time interval estimation, the Cross-modal Alignment (CA) module for enhancing weakly supervised VideoQG through bidirectional contrastive learning, and the Explicit Causal Intervention (ECI) module for multimodal deconfounding. The framework demonstrates superior performance in discovering visually grounded content and robust question reasoning on two VideoQG datasets.",
        "Tags": [
            "Video Understanding",
            "Visual Question Answering",
            "Causal Inference",
            "Multimodal Learning",
            "Weakly Supervised Learning"
        ]
    },
    {
        "Title": "Pose-Guided Temporal Enhancement for Robust Low-Resolution Hand Reconstruction",
        "Authors": "Kaixin Fan \u00b7 Pengfei Ren \u00b7 Jingyu Wang \u00b7 Haifeng Sun \u00b7 Qi Qi \u00b7 Zirui Zhuang \u00b7 Jianxin Liao",
        "Abstract": "3D hand reconstruction is essential in non-contact human-computer interaction applications, but existing methods struggle with low-resolution images, which occur in slightly distant interactive scenes. Leveraging temporal information can mitigate the limitations of individual low-resolution images that lack detailed appearance information, thereby enhancing the robustness and accuracy of hand reconstruction. Existing temporal methods typically use joint features to represent temporal information, avoiding interference from redundant background information. However, joint features excessively disregard the spatial context of visual features, limiting hand reconstruction accuracy. We propose to integrate temporal joint features with visual features to construct a robust low-resolution visual representation. We introduce Triplane Features, a dense representation with 3D spatial awareness, to bridge the gap between the joint features and visual features that are misaligned in terms of representation form and semantics. Triplane Features are obtained by orthogonally projecting the joint features, embedding hand structure information into the 3D spatial context. Furthermore, we compress the spatial information of the three planes into a 2D dense feature thourgh Spatial-Aware Fusion to enhance the visual features. By using enhanced visual features enriched with temporal information for hand reconstruction, our method achieves competitive performance at much lower resolutions compared to state-of-the-art methods operating at high resolution on DexYCB, HanCo and H2O.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "3D hand reconstruction is crucial for non-contact human-computer interaction, but existing methods often fail with low-resolution images, which are common in slightly distant interactive scenarios. Temporal information can help overcome the limitations of individual low-resolution images by enhancing the robustness and accuracy of hand reconstruction. Current temporal methods rely on joint features to represent temporal information, avoiding redundant background information but neglecting spatial context, which limits accuracy. This paper proposes integrating temporal joint features with visual features to create a robust low-resolution visual representation. The authors introduce Triplane Features, a dense 3D spatial representation, to align joint and visual features. These features are obtained by orthogonally projecting joint features, embedding hand structure into the 3D spatial context. Spatial-Aware Fusion is used to compress the spatial information of the three planes into a 2D dense feature, enhancing visual features. The method achieves competitive performance at lower resolutions compared to state-of-the-art high-resolution methods on datasets like DexYCB, HanCo, and H2O.",
        "Tags": [
            "3D Human Pose Estimation",
            "3D Reconstruction",
            "Temporal Information Integration",
            "Triplane Features",
            "Spatial-Aware Fusion"
        ]
    },
    {
        "Title": "A Unified Latent Schr\u00f6dinger Bridge Diffusion Model for Unsupervised Anomaly Detection and Localization",
        "Authors": "Shilhora Akshay \u00b7 Niveditha Lakshmi Narasimhan \u00b7 Jacob George \u00b7 Vineeth Balasubramanian",
        "Abstract": "Anomaly detection and localization remain pivotal challenges in computer vision, with applications ranging from industrial inspection to medical diagnostics. While current supervised methods offer high precision, they are often impractical due to the scarcity of annotated data and the infrequent occurrence of anomalies. Recent advancements in unsupervised approaches, particularly reconstruction-based methods, have addressed these issues by training models exclusively on normal data, enabling them to identify anomalies during inference. However, these methods frequently rely on auxiliary networks or specialized adaptations, which can limit their robustness and practicality. This work introduces the Latent Anomaly Schrodinger Bridge (LASB), a unified unsupervised anomaly detection model that operates entirely in the latent space without requiring additional networks or custom modifications. LASB transforms anomaly images into normal images by preserving structural integrity across varying anomaly classes, lighting, and pose conditions, making it highly robust and versatile. Unlike previous methods, LASB does not focus solely on reconstructing anomaly features but emphasizes anomaly transformation, achieving smooth anomaly-to-normal image conversions. Our method achieves state-of-the-art performance on both the MVTec-AD and VisA datasets, excelling in detection and localization tasks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces the Latent Anomaly Schr\u00f6dinger Bridge (LASB), a novel unsupervised model for anomaly detection and localization. LASB operates entirely in the latent space, eliminating the need for auxiliary networks or custom modifications. It transforms anomaly images into normal images while preserving structural integrity across various conditions, such as anomaly classes, lighting, and pose. This approach focuses on anomaly transformation rather than reconstruction, enabling smooth conversions from anomaly to normal images. LASB achieves state-of-the-art performance on the MVTec-AD and VisA datasets, demonstrating superior robustness and versatility in detection and localization tasks.",
        "Tags": [
            "Anomaly Detection",
            "Unsupervised Learning",
            "Latent Space Transformation",
            "Anomaly-to-Normal Conversion",
            "Structural Integrity Preservation"
        ]
    },
    {
        "Title": "StageDesigner: Artistic Stage Generation for Scenography via Theater Scripts",
        "Authors": "Zhaoxing Gan \u00b7 Mengtian Li \u00b7 Ruhua Chen \u00b7 Zhongxia JI \u00b7 Sichen Guo \u00b7 Huanling Hu \u00b7 Guangnan Ye \u00b7 Zuo Hu",
        "Abstract": "In this work, we introduce $\\textbf{StageDesigner}$, the first comprehensive framework for artistic stage generation using large language models (LLMs) combined with layout-controlled diffusion models. Given the professional requirements of stage scenography, StageDesigner simulates the workflows of seasoned artists to generate immersive 3D stage scenes. Specifically, our approach is divided into three primary modules: $\\textit{Script Analysis}$, which extracts thematic and spatial cues from input scripts; $\\textit{Foreground Generation}$, which constructs and arranges essential 3D objects; and $\\textit{Background Generation}$, which produces a harmonious background aligned with the narrative atmosphere and maintains spatial coherence by managing occlusions between foreground and background elements. Furthermore, we introduce the $\\textbf{StagePro-V1}$ dataset, a dedicated dataset with 276 unique stage scenes spanning different historical styles and annotated with scripts, images, and detailed 3D layouts, specifically tailored for this task. Finally, evaluations using both standard and newly proposed metrics, along with extensive user studies, demonstrate the effectiveness of StageDesigner, showcasing its ability to produce visually and thematically cohesive stages that meet both artistic and spatial coherence standards.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces StageDesigner, a framework for artistic stage generation that integrates large language models (LLMs) with layout-controlled diffusion models to create immersive 3D stage scenes. The framework consists of three modules: Script Analysis, which extracts thematic and spatial cues from scripts; Foreground Generation, which arranges essential 3D objects; and Background Generation, which produces a harmonious background aligned with the narrative atmosphere. The authors also introduce the StagePro-V1 dataset, containing 276 unique stage scenes annotated with scripts, images, and 3D layouts. Evaluations and user studies demonstrate the framework's ability to generate visually and thematically cohesive stages.",
        "Tags": [
            "Large Language Models (LLMs)",
            "Diffusion Models",
            "3D Generation",
            "Theater Script Analysis",
            "3D Scene Coherence",
            "Artistic Stage Design"
        ]
    },
    {
        "Title": "Enhancing Testing-Time Robustness for Trusted Multi-View Classification in the Wild",
        "Authors": "Wei Liu \u00b7 Yufei Chen \u00b7 Xiaodong Yue",
        "Abstract": "Trusted multi-view classification (TMVC) addresses variations in data quality by evaluating the reliability of each view based on prediction uncertainty at the evidence level, reducing the impact of low-quality views commonly encountered in real-world scenarios. However, existing TMVC methods often struggle to maintain robustness during testing, particularly when integrating noisy or corrupted views.  This limitation arises because the evidence collected by TMVC may be unreliable, frequently providing incorrect information due to complex view distributions and optimization challenges, ultimately leading to classification performance degradation. To enhance the robustness of TMVC methods in real-world conditions, we propose a generalized evidence filtering mechanism that is compatible with various fusion strategies commonly used in TMVC, including Belief Constraint Fusion, Aleatory Cumulative Belief Fusion, and Averaging Belief Fusion. Specifically, we frame the identification of unreliable evidence as a multiple testing problem and introduce p-values to control the risk of false identification. By selectively down-weighting unreliable evidence during testing, our mechanism ensures robust fusion and mitigates performance degradation. Both theoretical guarantees and empirical results demonstrate significant improvements in the classification performance of TMVC methods, supporting their reliable application in challenging, real-world environments.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Trusted multi-view classification (TMVC) evaluates the reliability of each view based on prediction uncertainty to handle data quality variations, but struggles with robustness during testing, especially with noisy or corrupted views. To address this, the authors propose a generalized evidence filtering mechanism compatible with various fusion strategies, such as Belief Constraint Fusion and Averaging Belief Fusion. This mechanism identifies unreliable evidence using p-values and selectively down-weights it during testing, ensuring robust fusion and mitigating performance degradation. Theoretical and empirical results show significant improvements in TMVC classification performance, enhancing its applicability in real-world scenarios.",
        "Tags": [
            "Multimodal Learning",
            "Data Augmentation",
            "Evidence Filtering",
            "Fusion Strategies",
            "Uncertainty Estimation"
        ]
    },
    {
        "Title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents",
        "Authors": "Yunseok Jang \u00b7 Yeda Song \u00b7 Sungryull Sohn \u00b7 Lajanugen Logeswaran \u00b7 Tiange Luo \u00b7 Dong-Ki Kim \u00b7 GyungHoon Bae \u00b7 Honglak Lee",
        "Abstract": "Recent advancements in Large Language Models (LLMs) and Vision-Language Models (VLMs) have sparked significant interest in developing agents capable of mobile operating system (mobile OS) navigation. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from YouTube), a large-scale dataset of 313K annotated frames from 20K instructional videos capturing diverse real-world mobile OS navigation across multiple platforms. Models trained on MONDAY demonstrate robust cross-platform generalization capabilities, consistently outperforming models trained on existing single OS datasets while achieving 21.41\\%p better performance on previously unseen mobile OS configurations. To enable continuous dataset expansion as mobile platforms evolve, we present an automated framework that leverages publicly available video content to create comprehensive task datasets without manual annotation. Our framework combines robust OCR-based scene detection (95.04% F1-score), near-perfect UI component detection (99.87% hit ratio), and novel multi-step action identification to extract reliable action sequences across diverse interface configurations. We contribute both the MONDAY dataset and our automated collection framework to facilitate future research in mobile OS navigation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces MONDAY, a large-scale dataset of 313K annotated frames from 20K instructional videos for mobile OS navigation across multiple platforms. Models trained on MONDAY show strong cross-platform generalization, outperforming those trained on single OS datasets by 21.41% on unseen configurations. An automated framework is presented for dataset expansion, utilizing OCR-based scene detection, UI component detection, and multi-step action identification to extract action sequences without manual annotation. The MONDAY dataset and the automated framework are made available to support further research in mobile OS navigation.",
        "Tags": [
            "Large Language Models (LLMs)",
            "Vision-Language Models (VLMs)",
            "Mobile OS Navigation",
            "Automated Dataset Generation",
            "Cross-Platform Generalization"
        ]
    },
    {
        "Title": "Navigating Image Restoration with VAR\u2019s Distribution Alignment Prior",
        "Authors": "Siyang Wang \u00b7 Naishan Zheng \u00b7 Jie Huang \u00b7 Feng Zhao",
        "Abstract": "Generative models trained on extensive high-quality datasets effectively capture the structural and statistical properties of clean images, rendering them powerful priors for transforming degraded features into clean ones in image restoration. VAR, a novel image generative paradigm, surpasses diffusion models in generation quality by applying a next-scale prediction approach. It progressively captures both global structures and fine-grained details through the autoregressive process, consistent with the multi-scale restoration principle widely acknowledged in the restoration community. Furthermore, we observe that during the image reconstruction process utilizing VAR, scale predictions automatically modulate the input, facilitating the alignment of representations at subsequent scales with the distribution of clean images. To harness VAR's adaptive distribution alignment capability in image restoration tasks, we formulate the multi-scale latent representations within VAR as the restoration prior, thus advancing our delicately designed VarFormer framework. The strategic application of these priors enables our VarFormer to achieve remarkable generalization on unseen tasks while also reducing training computational costs. Extensive experiments underscores that our VarFormer outperforms existing multi-task image restoration methods across various restoration tasks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces VAR, a novel image generative paradigm that outperforms diffusion models in generation quality by employing a next-scale prediction approach. This method progressively captures global structures and fine details through an autoregressive process, aligning with the multi-scale restoration principle. The authors leverage VAR's adaptive distribution alignment capability to develop the VarFormer framework, which uses multi-scale latent representations as a restoration prior. This approach enhances generalization on unseen tasks and reduces computational costs. The VarFormer framework demonstrates superior performance across various image restoration tasks compared to existing methods.",
        "Tags": [
            "Image Restoration",
            "Generative Models",
            "Distribution Alignment",
            "Multi-Scale Restoration",
            "VarFormer",
            "Next-Scale Prediction",
            "Autoregressive Process",
            "Adaptive Distribution Alignment"
        ]
    },
    {
        "Title": "Towards Million-Scale Adversarial Robustness Evaluation With Stronger Individual Attacks",
        "Authors": "Yong Xie \u00b7 Weijie Zheng \u00b7 Hanxun Huang \u00b7 Guangnan Ye \u00b7 Xingjun Ma",
        "Abstract": "As deep learning models are increasingly deployed in safety-critical applications, evaluating their vulnerabilities to adversarial perturbations is essential for ensuring their reliability and trustworthiness. Over the past decade, a large number of white-box adversarial robustness methods (i.e., attacks) have been proposed, ranging from single-step to multi-step methods and from individual to ensemble methods. Despite these advances, challenges remain in conducting meaningful and comprehensive robustness evaluations, particularly when it comes to large-scale testing and ensuring evaluations reflect real-world adversarial risks.In this work, we focus on image classification models and propose a novel individual attack method, Probability Margin Attack (PMA), which defines the adversarial margin in the probability space rather than the logits space. We analyze the relationship between PMA and existing cross-entropy or logits-margin-based attacks, showing that PMA outperforms the current state-of-the-art individual methods.Building on PMA, we propose two types of ensemble attacks that balance effectiveness and efficiency. Furthermore, we create a million-scale dataset, CC1M, derived from the existing CC3M dataset, and use it to conduct the first million-scale white-box adversarial robustness evaluation of adversarially-trained ImageNet models. Our findings provide valuable insights into the robustness gaps between individual versus ensemble attacks and small-scale versus million-scale evaluations.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the critical need for evaluating the adversarial robustness of deep learning models, particularly in safety-critical applications. The authors introduce a novel individual attack method, the Probability Margin Attack (PMA), which operates in the probability space rather than the logits space, demonstrating superior performance over existing methods. Additionally, they propose two ensemble attacks that balance effectiveness and efficiency. To facilitate large-scale testing, a million-scale dataset, CC1M, is created from the CC3M dataset, enabling the first million-scale white-box adversarial robustness evaluation of adversarially-trained ImageNet models. The study highlights significant robustness gaps between individual and ensemble attacks and between small-scale and million-scale evaluations.",
        "Tags": [
            "Adversarial Robustness",
            "Image Classification",
            "Probability Margin Attack",
            "Ensemble Attacks",
            "Million-Scale Evaluation"
        ]
    },
    {
        "Title": "Vision-Language Gradient Descent-driven All-in-One Deep Unfolding Networks",
        "Authors": "Haijin Zeng \u00b7 \u6e58\u94ed \u738b \u00b7 Yongyong Chen \u00b7 Jingyong Su \u00b7 Jie Liu",
        "Abstract": "Dynamic image degradations, including noise, blur and lighting inconsistencies, pose significant challenges in image restoration, often due to sensor limitations or adverse environmental conditions. Existing Deep Unfolding Networks (DUNs) offer stable restoration performance but require manual selection of degradation matrices for each degradation type, limiting their adaptability across diverse scenarios.To address this issue, we propose the Vision-Language-guided Unfolding Network (VLU-Net), a unified DUN framework for handling multiple degradation types simultaneously.VLU-Net leverages a Vision-Language Model (VLM) refined on degraded image-text pairs to align image features with degradation descriptions, selecting the appropriate transform for target degradation.By integrating an automatic VLM-based gradient estimation strategy into the Proximal Gradient Descent (PGD) algorithm, VLU-Net effectively tackles complex multi-degradation restoration tasks while maintaining interpretability. Furthermore, we design a hierarchical feature unfolding structure to enhance VLU-Net framework, efficiently synthesizing degradation patterns across various levels.VLU-Net is the first all-in-one DUN framework and outperforms current leading one-by-one and all-in-one end-to-end methods by 3.74 dB on the SOTS dehazing dataset and 1.70 dB on the Rain100L deraining dataset.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces the Vision-Language-guided Unfolding Network (VLU-Net), a unified Deep Unfolding Network (DUN) framework designed to handle multiple image degradation types simultaneously. VLU-Net utilizes a Vision-Language Model (VLM) trained on degraded image-text pairs to align image features with degradation descriptions, enabling the automatic selection of appropriate transforms for target degradations. By integrating a VLM-based gradient estimation strategy into the Proximal Gradient Descent (PGD) algorithm, VLU-Net effectively addresses complex multi-degradation restoration tasks while maintaining interpretability. The framework also features a hierarchical feature unfolding structure to synthesize degradation patterns across various levels. VLU-Net demonstrates superior performance, outperforming existing methods on standard datasets.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Image Restoration",
            "Deep Unfolding Networks (DUNs)",
            "Proximal Gradient Descent (PGD)",
            "Hierarchical Feature Unfolding",
            "Multi-Degradation Restoration"
        ]
    },
    {
        "Title": "Depth Any Camera: Zero-Shot Metric Depth Estimation from Any Camera",
        "Authors": "Yuliang Guo \u00b7 Sparsh Garg \u00b7 S. Mahdi H. Miangoleh \u00b7 Xinyu Huang \u00b7 Liu Ren",
        "Abstract": "Accurate metric depth estimation from monocular cameras is essential for applications such as autonomous driving, AR/VR, and robotics. While recent depth estimation methods demonstrate strong zero-shot generalization, achieving accurate metric depth across diverse camera types\u2014particularly those with large fields of view (FoV) like fisheye and $360^\\circ$ cameras\u2014remains challenging. This paper introduces Depth Any Camera (DAC), a novel zero-shot metric depth estimation framework that extends a perspective-trained model to handle varying FoVs effectively. Notably, DAC is trained exclusively on perspective images, yet it generalizes seamlessly to fisheye and $360^\\circ$ cameras without requiring specialized training. DAC leverages Equi-Rectangular Projection (ERP) as a unified image representation, enabling consistent processing of images with diverse FoVs. Key components include an efficient Image-to-ERP patch conversion for online ERP-space augmentation, a FoV alignment operation to support effective training across a broad range of FoVs, and multi-resolution data augmentation to address resolution discrepancies between training and testing. DAC achieves state-of-the-art zero-shot metric depth estimation, improving $\\delta_1$ accuracy by up to 50\\% on multiple indoor fisheye and $360^\\circ$ datasets, demonstrating robust generalization across camera types while relying only on perspective training data.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Depth Any Camera (DAC) is a novel framework for zero-shot metric depth estimation that generalizes across diverse camera types, including fisheye and 360\u00b0 cameras, without requiring specialized training. DAC extends a perspective-trained model to handle varying fields of view (FoV) by leveraging Equi-Rectangular Projection (ERP) as a unified image representation. Key innovations include an efficient Image-to-ERP patch conversion, FoV alignment, and multi-resolution data augmentation. DAC achieves state-of-the-art performance, improving depth estimation accuracy by up to 50% on indoor fisheye and 360\u00b0 datasets, demonstrating robust generalization using only perspective training data.",
        "Tags": [
            "Depth Estimation",
            "Zero-Shot Learning",
            "Equi-Rectangular Projection",
            "FoV Alignment",
            "Multi-Resolution Augmentation"
        ]
    },
    {
        "Title": "Data Synthesis with Diverse Styles for Face Recognition via 3DMM-Guided Diffusion",
        "Authors": "Yuxi Mi \u00b7 Zhizhou Zhong \u00b7 Yuge Huang \u00b7 Qiuyang Yuan \u00b7 Xuan Zhao \u00b7 Jianqing Xu \u00b7 Shouhong Ding \u00b7 ShaoMing Wang \u00b7 Rizen Guo \u00b7 Shuigeng Zhou",
        "Abstract": "Identity-preserving face synthesis aims to generate synthetic face images of virtual subjects that can substitute real-world data for training face recognition models. While prior arts strive to create images with consistent identities and diverse styles, they face a trade-off between them. Identifying their limitation of treating style variation as subject-agnostic and observing that real-world persons actually have distinct, subject-specific styles, this paper introduces MorphFace, a diffusion-based face generator. The generator learns fine-grained facial styles, e.g., shape, pose and expression, from the renderings of a 3D morphable model (3DMM). It also learns identities from an off-the-shelf recognition model. To create virtual faces, the generator is conditioned on novel identities of unlabeled synthetic faces, and novel styles that are statistically sampled from a real-world prior distribution. The sampling especially accounts for both intra-subject variation and subject distinctiveness. A context blending strategy is employed to enhance the generator's responsiveness to identity and style conditions. Extensive experiments show that MorphFace outperforms the best prior arts in face recognition efficacy.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces MorphFace, a diffusion-based face generator designed to synthesize identity-preserving face images with diverse styles for training face recognition models. Unlike previous methods that treat style variation as subject-agnostic, MorphFace learns fine-grained facial styles (e.g., shape, pose, expression) from 3D morphable model (3DMM) renderings and identities from an off-the-shelf recognition model. It generates virtual faces by conditioning on novel identities and styles sampled from a real-world prior distribution, accounting for intra-subject variation and subject distinctiveness. A context blending strategy enhances the generator's responsiveness to identity and style conditions. MorphFace demonstrates superior performance in face recognition compared to prior methods.",
        "Tags": [
            "Face Recognition",
            "Diffusion Models",
            "3D Morphable Model (3DMM)",
            "Identity-Preserving Synthesis",
            "Context Blending"
        ]
    },
    {
        "Title": "ProxyTransformation: Preshaping Point Cloud Manifold With Proxy Attention For 3D Visual Grounding",
        "Authors": "Qihang Peng \u00b7 Henry Zheng \u00b7 Gao Huang",
        "Abstract": "Embodied intelligence requires agents to interact with 3D environments in real time based on language instructions. A foundational task in this domain is ego-centric 3D visual grounding. However, the point clouds rendered from RGB-D images retain a large amount of redundant background data and inherent noise, both of which can interfere with the manifold structure of the target regions. Existing point cloud enhancement methods often require a tedious process to improve the manifold, which is not suitable for real-time tasks. We propose Proxy Transformation suitable for multimodal task to efficiently improve the point cloud manifold. Our method first leverages Deformable Point Clustering to identify the point cloud sub-manifolds in target regions. Then, we propose a Proxy Attention module that utilizes multimodal proxies to guide point cloud transformation. Built upon Proxy Attention, we design a submanifold transformation generation module where textual information globally guides translation vectors for different submanifolds, optimizing relative spatial relationships of target regions. Simultaneously, image information guides linear transformations within each submanifold, refining the local point cloud manifold of target regions. Extensive experiments demonstrate that Proxy Transformation significantly outperforms all existing methods, achieving an impressive improvement of 7.49% on easy targets and 4.60% on hard targets, while reducing the computational overhead of attention blocks by 40.6%. These results establish a new SOTA in ego-centric 3D visual grounding, showcasing the effectiveness and robustness of our approach.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Proxy Transformation, a novel method for improving the manifold structure of point clouds in ego-centric 3D visual grounding tasks. The approach addresses the challenges posed by redundant background data and noise in point clouds rendered from RGB-D images. It employs Deformable Point Clustering to identify sub-manifolds in target regions and a Proxy Attention module that uses multimodal proxies to guide point cloud transformation. The method also includes a submanifold transformation generation module where textual information globally guides translation vectors, and image information refines local point cloud manifolds. The proposed method significantly outperforms existing approaches, achieving notable improvements in accuracy and computational efficiency, thereby setting a new state-of-the-art in the field.",
        "Tags": [
            "3D Point Cloud",
            "Multimodal Learning",
            "Embodied AI",
            "Proxy Attention",
            "Deformable Point Clustering",
            "Ego-centric 3D Visual Grounding"
        ]
    },
    {
        "Title": "Symbolic Representation for Any-to-Any Generative Tasks",
        "Authors": "Jiaqi Chen \u00b7 Xiaoye Zhu \u00b7 Yue Wang \u00b7 Tianyang Liu \u00b7 Xinhui Chen \u00b7 Ying Chen \u00b7 Chak Tou Leong \u00b7 Yifei Ke \u00b7 Joseph Liu \u00b7 Yiwen Yuan \u00b7 Julian McAuley \u00b7 Li-jia Li",
        "Abstract": "We propose a symbolic generative task description language and inference engine, capable of representing arbitrary multimodal tasks as symbolic flows.The inference engine maps natural language instructions to symbolic flow, eliminating the need for task-specific training.Conventional generative models rely heavily on large-scale training and implicit neural representation to learn cross-modal mappings, which demands extensive computational resources and restricts expandability. In this paper, we propose an explicit symbolic task descriptive language, comprising three types of primitives: functions, parameters, and topological logic. Using a pre-trained language model to infer symbolic workflows in a training-free manner, our framework successfully performs over 12 multimodal generative tasks based on user instructions, demonstrating enhanced efficiency and flexibility. Extensive experiments demonstrate that our approach can generate multimodal content competitive with, and often surpassing, that of previous state-of-the-art unified models, while offering robust interruptibility and editability. We believe that symbolic task representations are capable of cost-effectively expanding the boundaries of generative AI capabilities. All code and results are available in the Supplementary Materials.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces a symbolic generative task description language and inference engine designed to represent arbitrary multimodal tasks as symbolic flows. This approach eliminates the need for task-specific training by mapping natural language instructions to symbolic workflows using a pre-trained language model. The framework, which includes primitives like functions, parameters, and topological logic, successfully performs over 12 multimodal generative tasks based on user instructions. It demonstrates enhanced efficiency, flexibility, and competitive performance compared to state-of-the-art unified models, while also offering robust interruptibility and editability. The authors argue that symbolic task representations can cost-effectively expand the capabilities of generative AI.",
        "Tags": [
            "Multimodal Learning",
            "Generative Models",
            "Symbolic Representation",
            "Training-Free Inference",
            "Multimodal Generative Tasks"
        ]
    },
    {
        "Title": "Hash3D: Training-free Acceleration for 3D Generation",
        "Authors": "Xingyi Yang \u00b7 Songhua Liu \u00b7 Xinchao Wang",
        "Abstract": "The quality of 3D generative modeling has been notably improved by the adoption of 2D diffusion models. Despite this progress, the cumbersome optimization process \\emph{per se}presents a critical problem to efficiency.  In this paper, we introduce Hash3D, a universal acceleration for 3D score distillation sampling~(SDS) without model training.Central to Hash3D is the observation that images rendered from similar camera positions and diffusion time-steps often have redundant feature maps. By hashing and reusing these feature maps across nearby timesteps and camera angles, Hash3D eliminates unnecessary calculations. We implement this through an adaptive grid-based hashing. As a result, it largely speeds up the process of 3D generation. Surprisingly, this feature-sharing mechanism not only makes generation faster but also improves the smoothness and view consistency of the synthesized 3D objects. Our experiments covering 5 text-to-3D and 3 image-to-3D models, demonstrate Hash3D\u2019s versatility to speed up optimization, enhancing efficiency by $1.5\\sim 4\\times$. Additionally, Hash3D's integration with 3D Gaussian splatting largely speeds up 3D model creation, reducing text-to-3D conversion to about 10 minutes and image-to-3D conversion to 30 seconds.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Hash3D introduces a training-free acceleration method for 3D generation using 2D diffusion models. By leveraging the redundancy in feature maps from similar camera positions and diffusion time-steps, Hash3D employs an adaptive grid-based hashing technique to reuse these features, significantly speeding up the 3D generation process. This approach not only enhances efficiency by 1.5 to 4 times but also improves the smoothness and view consistency of the generated 3D objects. Hash3D is versatile, applicable across multiple text-to-3D and image-to-3D models, and integrates effectively with 3D Gaussian splatting, drastically reducing generation times.",
        "Tags": [
            "3D Generation",
            "Diffusion Models",
            "Feature Reuse",
            "Adaptive Grid-based Hashing",
            "3D Gaussian Splatting"
        ]
    },
    {
        "Title": "A Simple Data Augmentation for Feature Distribution Skewed Federated Learning",
        "Authors": "Yunlu Yan \u00b7 Huazhu Fu \u00b7 Yuexiang Li \u00b7 Jinheng Xie \u00b7 Jun Ma \u00b7 Guang Yang \u00b7 Lei Zhu",
        "Abstract": "Federated Learning (FL) facilitates collaborative learning among multiple clients in a distributed manner and ensures the security of privacy. However, its performance inevitably degrades with non-Independent and Identically Distributed (non-IID) data. In this paper, we focus on the feature distribution skewed FL scenario, a common non-IID situation in real-world applications where data from different clients exhibit varying underlying distributions.  This variation leads to feature shift, which is a key issue of this scenario.  While previous works have made notable progress, few  pay attention to the data itself, i.e., the root of this issue. The primary goal of this paper is to mitigate feature shift from the  perspective of data. To this end, we propose a simple yet remarkably effective input-level data augmentation method, namely FedRDN, which randomly injects the statistical information of the local distribution from the entire federation into the client's data. This is beneficial to improve the generalization of local feature representations, thereby mitigating feature shift. Moreover, our FedRDN is a plug-and-play component, which can be seamlessly integrated into the data augmentation flow with only a few lines of code. Extensive experiments on several datasets show that the performance of various representative FL methods can be further improved by integrating our FedRDN, demonstrating its effectiveness, strong compatibility and generalizability. Code will be released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Federated Learning (FL) enables collaborative learning across distributed clients while preserving privacy, but its performance suffers under non-Independent and Identically Distributed (non-IID) data, particularly in feature distribution skewed scenarios. This paper addresses the challenge of feature shift in such scenarios by proposing FedRDN, a simple yet effective input-level data augmentation method. FedRDN injects statistical information from the global federation into local client data, improving the generalization of local feature representations and mitigating feature shift. The method is plug-and-play, requiring minimal code integration, and demonstrates strong compatibility and generalizability across various FL methods and datasets.",
        "Tags": [
            "Federated Learning",
            "Data Augmentation",
            "Feature Shift Mitigation",
            "Plug-and-Play Augmentation",
            "Non-IID Data Handling"
        ]
    },
    {
        "Title": "Photorealistic Simulation-Ready Garments from a Single Pose",
        "Authors": "Michelle Guo \u00b7 Matt Jen-Yuan Chiang \u00b7 Igor Santesteban \u00b7 Nikolaos Sarafianos \u00b7 Hsiaoyu Chen \u00b7 Oshri Halimi \u00b7 Alja\u017e Bo\u017ei\u010d \u00b7 Shunsuke Saito \u00b7 Jiajun Wu \u00b7 Karen Liu \u00b7 Tuur Stuyck \u00b7 Egor Larionov",
        "Abstract": "We introduce a novel approach to reconstruct simulation-ready garments with intricate appearance. Despite recent advancements, existing methods often struggle to balance the need for accurate garment reconstruction with the ability to generalize to new poses and body shapes or require large amounts of data to achieve this. In contrast, our method only requires a multi-view capture of a single static frame. We represent garments as hybrid mesh-embedded 3D Gaussian splats (or simply Gaussians), where the Gaussians capture near-field shading and high-frequency details, while the mesh encodes far-field albedo and optimized reflectance parameters. We achieve novel pose generalization by exploiting the mesh from our hybrid approach, enabling physics-based simulation and surface rendering techniques, while also capturing fine details with Gaussians that accurately reconstruct garment details. Our optimized garments can be used for simulating garments on novel poses, and garment relighting.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper presents a novel method for reconstructing simulation-ready garments with intricate appearance using only a multi-view capture of a single static frame. The approach represents garments as hybrid mesh-embedded 3D Gaussian splats, where Gaussians capture near-field shading and high-frequency details, and the mesh encodes far-field albedo and optimized reflectance parameters. This hybrid method enables novel pose generalization, physics-based simulation, and accurate garment detail reconstruction, making it suitable for simulating garments on new poses and relighting.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Avatars",
            "3D Reconstruction",
            "Hybrid Mesh-Embedded Representation",
            "Physics-Based Simulation",
            "Garment Relighting"
        ]
    },
    {
        "Title": "Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction",
        "Authors": "Seungtae Nam \u00b7 Xiangyu Sun \u00b7 Gyeongjin Kang \u00b7 Younggeun Lee \u00b7 Seungjun Oh \u00b7 Eunbyung Park",
        "Abstract": "Generalized feed-forward Gaussian models have shown remarkable progress in sparse-view 3D reconstruction, leveraging prior knowledge learned from large multi-view datasets. However, these models often struggle to represent high-frequency details due to the limited number of generated Gaussians. While the densification strategy used in per-scene 3D Gaussian splatting (3D-GS) optimization can be extended and applied to the feed-forward models, it may not be ideally suited for generalized settings. In this paper, we present Generative Densification, an efficient and generalizable densification strategy that can selectively generate fine Gaussians for high-fidelity 3D reconstruction. Unlike the 3D-GS densification strategy, we densify the feature representations from the feed-forward models rather than the raw Gaussians, making use of the prior knowledge embedded in the features for enhanced generalization. Experimental results demonstrate the effectiveness of our approach, achieving the state-of-the-art rendering quality in both object-level and scene-level reconstruction, with noticeable improvements in representing fine details.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Generative Densification, a novel strategy for enhancing high-fidelity 3D reconstruction in generalized settings. Unlike traditional methods that densify raw Gaussians, this approach densifies feature representations from feed-forward models, leveraging embedded prior knowledge for better generalization. The method significantly improves rendering quality in both object-level and scene-level reconstructions, particularly in capturing fine details.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "3D Reconstruction",
            "Feature Densification",
            "High-Fidelity Rendering",
            "Generalized 3D Reconstruction"
        ]
    },
    {
        "Title": "PoseTraj: Pose-Aware Trajectory Control in Video Diffusion",
        "Authors": "longbin ji \u00b7 Lei Zhong \u00b7 Pengfei Wei \u00b7 Changjian Li",
        "Abstract": "Recent advancements in trajectory-guided video generation have achieved notable progress. However, existing models still face challenges in generating object motions with potentially changing 6D poses under large-angle rotations, due to limited 3D understanding. To address this problem, we introduce PoseTraj, an open-domain, Pose-Aware video dragging model for reliable 3D-aligned animations from 2D trajectories. Our method incorporates a novel Two-Stage Pose-Aware Pretraining framework, improving 3D comprehension across diverse trajectories. Specifically, we 1) construct a large-scale synthetic dataset containing 10k videos of objects following rotational trajectories and 2) enhance the model perception of object pose changes by generating 3D bounding boxes as intermediate supervision signals. Following this, we fine-tune the trajectory-controlling module on open-domain videos, applying additional camera-disentanglement module to further refine motion accuracy. Experiments on various benchmark scenarios demonstrate that PoseTraj not only excels in 3D Pose-Aligned dragging for rotational scenarios but also outperforms existing baselines in trajectory accuracy and video quality.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "PoseTraj introduces a novel approach to trajectory-guided video generation, specifically addressing the challenge of generating object motions with changing 6D poses under large-angle rotations. The method employs a Two-Stage Pose-Aware Pretraining framework, utilizing a large-scale synthetic dataset and 3D bounding boxes for enhanced 3D comprehension. PoseTraj demonstrates superior performance in 3D pose-aligned dragging and trajectory accuracy, outperforming existing baselines in video quality.",
        "Tags": [
            "Video Generation",
            "3D Generation",
            "6D Pose Estimation",
            "Camera-Disentanglement",
            "Synthetic Dataset"
        ]
    },
    {
        "Title": "GOAL: Global-local Object Alignment Learning",
        "Authors": "Hyungyu Choi \u00b7 Young Kyun Jang \u00b7 Chanho Eom",
        "Abstract": "Vision-language models like CLIP have shown impressive capabilities in aligning images and text, but they often struggle with lengthy, detailed text descriptions due to their training focus on concise captions. We present GOAL (Global-local Object Alignment Learning), a novel fine-tuning method that enhances CLIP's ability to handle lengthy text by leveraging both global and local semantic alignments. Our approach consists of two key components: Local Image-Sentence Matching (LISM), which identifies corresponding pairs between image segments and descriptive sentences, and Token Similarity-based Learning (TSL), which efficiently propagates local element attention through these matched pairs. Evaluating GOAL on three new benchmarks for image-lengthy text retrieval, we demonstrate significant improvements over baseline CLIP fine-tuning, establishing a simple yet effective approach for adapting CLIP to detailed textual descriptions. Through extensive experiments, we show that our method's focus on local semantic alignment alongside global context leads to more nuanced and representative embeddings, particularly beneficial for tasks requiring fine-grained understanding of lengthy text descriptions.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces GOAL (Global-local Object Alignment Learning), a fine-tuning method designed to enhance CLIP's performance with lengthy text descriptions by leveraging both global and local semantic alignments. The method includes two main components: Local Image-Sentence Matching (LISM) for identifying corresponding pairs between image segments and descriptive sentences, and Token Similarity-based Learning (TSL) for propagating local element attention through these pairs. GOAL significantly improves over baseline CLIP fine-tuning on new benchmarks for image-lengthy text retrieval, demonstrating its effectiveness in tasks requiring fine-grained understanding of detailed textual descriptions.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "CLIP",
            "Data Augmentation",
            "Local Image-Sentence Matching",
            "Token Similarity-based Learning",
            "Fine-grained Text Understanding"
        ]
    },
    {
        "Title": "FedSPA: Generalizable Federated Graph Learning under Homophily Heterogeneity",
        "Authors": "Zihan Tan \u00b7 Guancheng Wan \u00b7 Wenke Huang \u00b7 Guibin Zhang \u00b7 He Li \u00b7 Carl Yang \u00b7 Mang Ye",
        "Abstract": "Federated Graph Learning (FGL) has emerged as a solution to address real-world privacy concerns and data silos in graph learning, which relies on Graph Neural Networks (GNNs).Nevertheless, the homophily level discrepancies within the local graph data of clients, termed homophily heterogeneity, significantly degrade the generalizability of a global GNN. Existing research ignores this issue and suffers from unpromising collaboration. In this paper, we propose $\\textbf{FedSPA}$, an effective hyperparameter-free framework that addresses homophily heterogeneity from the perspectives of homophily conflict and homophily bias, concepts that have yet to be defined or explored.In the first place, the homophily conflict arises when training on inconsistent homophily levels across clients. Correspondingly, we propose $\\textbf{S}$ubgraph Feature $\\textbf{P}$ropagation Decoupling (SFPD), thereby achieving collaboration on unified homophily levels across clients. To further address homophily bias, we design Homophily Bias-Driven $\\textbf{A}$ggregation (HBDA) which emphasizes clients with lower biases. It enables the adaptive adjustment of each client contribution to the global GNN based on its homophily bias. The superiority of $\\textbf{FedSPA}$ is validated through extensive experiments.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Federated Graph Learning (FGL) addresses privacy concerns and data silos in graph learning using Graph Neural Networks (GNNs). However, homophily heterogeneity\u2014discrepancies in homophily levels across clients\u2014degrades the generalizability of global GNNs. Existing research overlooks this issue, leading to ineffective collaboration. This paper introduces FedSPA, a hyperparameter-free framework tackling homophily heterogeneity through two novel concepts: homophily conflict and homophily bias. FedSPA employs Subgraph Feature Propagation Decoupling (SFPD) to unify homophily levels across clients and Homophily Bias-Driven Aggregation (HBDA) to adaptively adjust client contributions based on their homophily bias. The framework's effectiveness is demonstrated through extensive experiments.",
        "Tags": [
            "Graph Neural Networks (GNNs)",
            "Federated Learning",
            "Homophily Heterogeneity",
            "Subgraph Feature Propagation",
            "Bias-Driven Aggregation"
        ]
    },
    {
        "Title": "Towards Scalable Human-aligned Benchmark for Text-guided Image Editing",
        "Authors": "Suho Ryu \u00b7 Kihyun Kim \u00b7 Eugene Baek \u00b7 Dongsoo Shin \u00b7 Joonseok Lee",
        "Abstract": "A variety of text-guided image editing models have been proposed recently. However, there is no widely-accepted standard evaluation method mainly due to the subjective nature of the task, letting researchers rely on manual user study. To address this, we introduce a novel Human-Aligned benchmark for Text-guided Image Editing (HATIE). Providing a large-scale benchmark set covering a wide range of editing tasks, it allows reliable evaluation, not limited to specific easy-to-evaluate cases. Also, HATIE provides a fully-automated and omnidirectional evaluation pipeline. Particularly, we combine multiple scores measuring various aspects of editing so as to align with human perception. We empirically verify that the evaluation of HATIE is indeed human-aligned in various aspects, and provide benchmark results on several state-of-the-art models to provide deeper insights on their performance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces HATIE, a Human-Aligned benchmark for Text-guided Image Editing, addressing the lack of a standard evaluation method for text-guided image editing models. HATIE offers a large-scale benchmark set covering diverse editing tasks and an automated, omnidirectional evaluation pipeline that combines multiple scores to align with human perception. The benchmark's human-aligned evaluation is empirically verified, and it provides insights into the performance of state-of-the-art models.",
        "Tags": [
            "Image Editing",
            "Text-to-Image Generation",
            "Benchmarking",
            "Human Perception Alignment",
            "Automated Evaluation"
        ]
    },
    {
        "Title": "MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D Content Creation",
        "Authors": "Sankalp Sinha \u00b7 Mohammad Sadil Khan \u00b7 Muhammad Usama \u00b7 Shino Sam \u00b7 Didier Stricker \u00b7 Sk Aziz Ali \u00b7 Muhammad Zeshan Afzal",
        "Abstract": "Generating high-fidelity 3D content from text prompts remains a significant challenge in computer vision due to the limited size, diversity, and annotation depth of the existing datasets. To address this, we introduce MARVEL-$40$M+, an extensive dataset with $40$ million text annotations for over $8.9$ million 3D assets aggregated from seven major 3D datasets. Our contribution is a novel multi-stage annotation pipeline that integrates open-source pretrained multi-view VLMs and LLMs to automatically produce multi-level descriptions, ranging from detailed ($150$-$200$ words) to concise semantic tags ($10$-$20$ words). This structure supports both fine-grained 3D reconstruction and rapid prototyping. Furthermore, we incorporate human metadata from source datasets into our annotation pipeline to add domain-specific information in our annotation and reduce VLM hallucinations. Additionally, we develop MARVEL-FX3D, a two-stage text-to-3D pipeline. We fine-tune Stable Diffusion with our annotations and use a pretrained image-to-3D network to generate 3D textured meshes within 15s.  Extensive evaluations show that MARVEL-40M+ significantly outperforms existing datasets in annotation quality and linguistic diversity, achieving win rates of $72.41\\%$ by GPT-4 and $73.40\\%$ by human evaluators.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces MARVEL-40M+, a large-scale dataset designed to enhance high-fidelity 3D content generation from text prompts. It features 40 million text annotations for 8.9 million 3D assets, sourced from seven major datasets. A novel multi-stage annotation pipeline is developed, utilizing pretrained multi-view Vision-Language Models (VLMs) and Large Language Models (LLMs) to produce detailed and concise descriptions. This approach supports both detailed 3D reconstruction and rapid prototyping. The paper also presents MARVEL-FX3D, a two-stage text-to-3D pipeline that fine-tunes Stable Diffusion and uses a pretrained image-to-3D network to generate textured meshes quickly. Evaluations indicate that MARVEL-40M+ surpasses existing datasets in annotation quality and linguistic diversity, with high win rates in assessments by GPT-4 and human evaluators.",
        "Tags": [
            "3D Generation",
            "Text-to-Image Generation",
            "Multimodal Large Language Models (MLLMs)",
            "Large Language Models (LLMs)",
            "Multi-view VLMs",
            "Text-to-3D Pipeline",
            "Stable Diffusion Fine-tuning"
        ]
    },
    {
        "Title": "Revealing Key Details to See Differences: A Novel Prototypical Perspective for Skeleton-based Action Recognition",
        "Authors": "Hongda Liu \u00b7 Yunfan Liu \u00b7 Min Ren \u00b7 Hao Wang \u00b7 Yunlong Wang \u00b7 Zhenan Sun",
        "Abstract": "In skeleton-based action recognition, a key challenge is distinguishing between actions with similar trajectories of joints due to the lack of image-level details in skeletal representations. Recognizing that the differentiation of similar actions relies on subtle motion details in specific body parts, we direct our approach to focus on the fine-grained motion of local skeleton components. To this end, we introduce ProtoGCN, a Graph Convolutional Network (GCN)-based model that breaks down the dynamics of entire skeleton sequences into a combination of learnable prototypes representing core motion patterns of action units. By contrasting the reconstruction of prototypes, ProtoGCN can effectively identify and enhance the discriminative representation of similar actions. Without bells and whistles, ProtoGCN achieves state-of-the-art performance on multiple benchmark datasets, including NTU RGB+D, NTU RGB+D 120, Kinetics-Skeleton, and FineGYM, which demonstrates the effectiveness of the proposed method.  The source code is enclosed in the supplementary material and will be released upon acceptance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the challenge of distinguishing between similar actions in skeleton-based action recognition by focusing on fine-grained motion details of local skeleton components. The authors propose ProtoGCN, a Graph Convolutional Network-based model that decomposes skeleton sequences into learnable prototypes representing core motion patterns. By contrasting these prototypes, ProtoGCN enhances the discriminative representation of similar actions, achieving state-of-the-art performance on multiple benchmark datasets.",
        "Tags": [
            "Graph Neural Networks (GNNs)",
            "Action Detection",
            "Prototype Learning",
            "Fine-Grained Motion Analysis",
            "Skeleton-Based Recognition"
        ]
    },
    {
        "Title": "Point-Level Visual Affordance Guided Retrieval and Adaptation for Cluttered Garments Manipulation",
        "Authors": "Ruihai Wu \u00b7 Ziyu Zhu \u00b7 Yuran Wang \u00b7 Yue Chen \u00b7 Jiarui Wang \u00b7 Hao Dong",
        "Abstract": "Cluttered garments manipulation poses significant challenges in robotics due to the complex, deformable nature of garments and intricate garment relations. Unlike single-garment manipulation, cluttered scenarios require managing complex garment entanglements and interactions, while maintaining garment cleanliness and manipulation stability. To address these demands, we propose to learn point-level affordance, the dense representation modeling the complex space and multi-modal manipulation candidates, with novel designs for the awareness of garment geometry, structure, and inter-object relations. Additionally, we introduce an adaptation module, informed by learned affordance, to reorganize cluttered garments into configurations conducive to manipulation. Our framework demonstrates effectiveness over environments featuring diverse garment types and pile scenarios in both simulation and the real world.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenges of cluttered garments manipulation in robotics by proposing a framework that learns point-level affordance, which models the complex space and multi-modal manipulation candidates. The approach incorporates novel designs for garment geometry, structure, and inter-object relations awareness. An adaptation module, informed by the learned affordance, is introduced to reorganize cluttered garments into configurations suitable for manipulation. The framework is shown to be effective in both simulated and real-world environments with diverse garment types and pile scenarios.",
        "Tags": [
            "Embodied AI",
            "3D Point Cloud",
            "Point-Level Affordance",
            "Garment Manipulation",
            "Adaptation Module"
        ]
    },
    {
        "Title": "PTDiffusion: Free Lunch for Generating Optical Illusion Hidden Pictures using Phase-Transferred Diffusion Model",
        "Authors": "Xiang Gao \u00b7 Shuai Yang \u00b7 Jiaying Liu",
        "Abstract": "Optical illusion hidden picture is an interesting visual perceptual phenomenon where an image is cleverly integrated into another picture in a way that is not immediately obvious to the viewer. Established on off-the-shelf text-to-image diffusion model, we propose a novel \\textbf{P}hase-\\textbf{T}ransferred \\textbf{Diffusion} Model (PTDiffusion) for hidden art syntheses. PTDiffusion embeds an input reference image into arbitrary scenes that are faithful to text prompts, while exhibiting hidden visual cues of the reference image. At the heart of our method is a plug-and-play phase transfer mechanism that dynamically and progressively transplants diffusion features' phase spectrum from the denoising process to reconstruct the reference image into the one to sample the illusion picture, realizing harmonious fusion of the reference structural information and the target semantic information. Furthermore, we propose an asynchronous phase transfer mechanism to flexibly control the degree of hidden image discernability. Our method is training-free, all while substantially outperforming related methods in image quality, text fidelity, visual discernibility, and contextual naturalness for illusion picture synthesis, as fully demonstrated by extensive qualitative and quantitative experiments.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces PTDiffusion, a novel Phase-Transferred Diffusion Model designed for synthesizing optical illusion hidden pictures. This model integrates a reference image into arbitrary scenes based on text prompts, embedding hidden visual cues of the reference image. The core innovation is a plug-and-play phase transfer mechanism that dynamically transplants diffusion features' phase spectrum during the denoising process, ensuring a harmonious fusion of the reference's structural information with the target's semantic content. Additionally, an asynchronous phase transfer mechanism is proposed to adjust the discernibility of the hidden image. PTDiffusion operates without the need for training and demonstrates superior performance in image quality, text fidelity, visual discernibility, and contextual naturalness compared to existing methods.",
        "Tags": [
            "Diffusion Models",
            "Image Generation",
            "Phase Transfer Mechanism",
            "Training-Free Model",
            "Optical Illusion Synthesis"
        ]
    },
    {
        "Title": "VLMs-Guided Representation Distillation for Efficient Vision-Based Reinforcement Learning",
        "Authors": "Haoran Xu \u00b7 Peixi Peng \u00b7 Guang Tan \u00b7 Yiqian Chang \u00b7 Luntong Li \u00b7 Yonghong Tian",
        "Abstract": "Vision-based Reinforcement Learning (VRL) attempts to establish associations between visual inputs and optimal actions through interactions with the environment. Given the high-dimensional and complex nature of visual data, it becomes essential to learn policy upon high-quality state representation. To this end, existing VRL methods primarily rely on interaction-collected data, combined with self-supervised auxiliary tasks. However, two key challenges remain: limited data samples and a lack of task-relevant semantic constraints. To tackle this, we propose \\textbf{DGC}, a method that \\textbf{d}istills \\textbf{g}uidance from Visual Language Models (VLMs) alongside self-supervised learning into a \\textbf{c}ompact VRL agent. Notably, we leverage the state representation capabilities of VLMs, rather than their decision-making abilities. Within DGC, a novel prompting-reasoning pipeline is designed to convert historical observations and actions into usable supervision signals, enabling semantic understanding within the compact visual encoder. By leveraging these distilled semantic representations, the VRL agent achieves significant improvements in the sample efficiency. Extensive experiments on the Carla benchmark demonstrate our state-of-the-art performance. The source code is available in the supplementary material.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Vision-based Reinforcement Learning (VRL) faces challenges due to the high-dimensional and complex nature of visual data, requiring high-quality state representations. Existing methods rely on interaction-collected data and self-supervised tasks but struggle with limited data and lack of task-relevant semantic constraints. To address this, the authors propose DGC, a method that distills guidance from Visual Language Models (VLMs) into a compact VRL agent. DGC uses a novel prompting-reasoning pipeline to convert historical observations and actions into supervision signals, enabling semantic understanding in the visual encoder. This approach significantly improves sample efficiency, as demonstrated by state-of-the-art performance on the Carla benchmark.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Reinforcement Learning",
            "Self-Supervised Learning",
            "Semantic Representation Distillation",
            "Prompting-Reasoning Pipeline",
            "Sample Efficiency"
        ]
    },
    {
        "Title": "DH-Set: Improving Vision-Language Alignment with Diverse and Hybrid Set-Embeddings Learning",
        "Authors": "Kun Zhang \u00b7 Jingyu Li \u00b7 Zhe Li \u00b7 S Kevin Zhou",
        "Abstract": "Vision-Language (VL) alignment across image and text modalities is a challenging task due to the inherent semantic ambiguity of data with multiple possible meanings. Existing methods typically solve it by learning multiple sub-representation spaces to encode each input data as a set of embeddings, and constraining diversity between whole subspaces to capture diverse semantics for accurate VL alignment. Despite their promising outcomes, existing methods suffer two imperfections: 1) actually, specific semantics is mainly expressed by some local dimensions within the subspace. Ignoring this intrinsic property, existing diversity constraints imposed on the whole subspace may impair diverse embedding learning; 2) multiple embeddings are inevitably introduced, sacrificing computational and storage efficiency. In this paper, we propose a simple yet effective Diverse and Hybrid Set-embeddings learning framework (DH-Set), which is distinct from prior work in three aspects. DH-Set 1) devises a novel semantic importance dissecting method to focus on key local dimensions within each subspace; and thereby 2) not only imposes finer-grained diversity constraint to improve the accuracy of diverse embedding learning, 3) but also mixes key dimensions of all subspaces into the single hybrid embedding to boost inference efficiency. Extensive experiments on various benchmarks and model backbones show the superiority of DH-Set over state-of-the-art methods, achieving substantial 2.3%-14.7% rSum improvements while lowering computational and storage complexity. Codes will be released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces DH-Set, a novel framework for improving Vision-Language (VL) alignment by addressing the limitations of existing methods that use multiple sub-representation spaces. DH-Set focuses on key local dimensions within each subspace through a semantic importance dissecting method, applies finer-grained diversity constraints to enhance embedding accuracy, and combines key dimensions into a single hybrid embedding to improve computational and storage efficiency. This approach significantly outperforms state-of-the-art methods in terms of accuracy and efficiency, as demonstrated by extensive experiments across various benchmarks and model backbones.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Self-Supervised Learning",
            "Semantic Importance Dissecting",
            "Hybrid Embedding",
            "Finer-Grained Diversity Constraints"
        ]
    },
    {
        "Title": "Bringing CLIP to the Clinic: Dynamic Soft Labels and Negation-Aware Learning for Medical Analysis",
        "Authors": "Hanbin Ko \u00b7 Chang Min Park",
        "Abstract": "The development of large-scale image-text pair datasets has significantly advanced self-supervised learning in Vision-Language Processing (VLP). However, directly applying general-domain architectures such as CLIP to medical data presents challenges, particularly in handling negations and addressing the inherent data imbalance of medical datasets. To address these issues, we propose a novel approach that integrates clinically-enhanced dynamic soft labels and medical graphical alignment, thereby improving clinical comprehension and improving the applicability of contrastive loss in medical contexts. Furthermore, we introduce negation-based hard negatives to deepen the model\u2019s understanding of the complexities of clinical language. Our approach integrates seamlessly into any medical CLIP training pipeline and achieves state-of-the-art performance across multiple tasks, including zero-shot, fine-tuned classification and report retrieval. To further assess our model\u2019s capacity for clinical language comprehension, we introduce CXR-Align, a benchmark uniquely designed to evaluate the understanding of negation and clinical information within chest X-ray (CXR) datasets. Experimental results demonstrate that our proposed methods are straightforward to implement and generalize effectively across contrastive learning frameworks, enhancing medical VLP capabilities and advancing clinical language understanding in medical imaging.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenges of applying general-domain architectures like CLIP to medical data, particularly in handling negations and data imbalance. The authors propose a novel approach that integrates clinically-enhanced dynamic soft labels and medical graphical alignment to improve clinical comprehension and contrastive loss applicability. They also introduce negation-based hard negatives to enhance the model's understanding of clinical language. The method integrates seamlessly into medical CLIP training pipelines and achieves state-of-the-art performance in tasks such as zero-shot classification and report retrieval. A new benchmark, CXR-Align, is introduced to evaluate negation and clinical information understanding in chest X-ray datasets. The proposed methods are shown to be effective and generalizable across contrastive learning frameworks, enhancing medical vision-language processing capabilities.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Medical Image Analysis",
            "Dynamic Soft Labels",
            "Negation-Aware Learning",
            "CXR-Align Benchmark"
        ]
    },
    {
        "Title": "Incremental Object Keypoint Learning",
        "Authors": "Mingfu Liang \u00b7 Jiahuan Zhou \u00b7 Xu Zou \u00b7 Ying Wu",
        "Abstract": "Existing progress in object keypoint estimation primarily benefits from the conventional supervised learning paradigm based on numerous data labeled with pre-defined keypoints. However, these well-trained models can hardly detect the undefined new keypoints in test time, which largely hinders their feasibility for diverse downstream tasks. To handle this, various solutions are explored but still suffer from either limited generalizability or transferability. Therefore, in this paper, we explore a novel keypoint learning paradigm in that we only annotate new keypoints in the new data and incrementally train the model, without retaining any old data, called \\textbf{I}ncremental object \\textbf{K}eypoint \\textbf{L}earning~(IKL). A two-stage learning scheme as a novel baseline tailored to IKL is developed. In the first \\textit{Knowledge Association} stage, given the data labeled with only new keypoints, an auxiliary KA-Net is trained to automatically associate the old keypoints to these new ones based on their spatial and intrinsic anatomical relations. In the second \\textit{Mutual Promotion} stage, based on a keypoint-oriented spatial distillation loss, we jointly leverage the auxiliary KA-Net and the old model for knowledge consolidation to mutually promote the estimation of all old and new keypoints. Owing to the investigation of the correlations between new and old keypoints, our proposed method can not just effectively mitigate the catastrophic forgetting of old keypoints, but may even further improve the estimation of the old ones and achieve a positive transfer beyond anti-forgetting. Such an observation has been solidly verified by extensive experiments on different keypoint datasets, where our method exhibits superiority in alleviating the forgetting issue and boosting performance while enjoying labeling efficiency even under the low-shot data regime.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Incremental Object Keypoint Learning (IKL), a novel paradigm for object keypoint estimation that addresses the limitation of existing models in detecting undefined new keypoints during test time. IKL incrementally trains models by annotating only new keypoints in new data, without retaining old data. The approach features a two-stage learning scheme: the Knowledge Association stage, where an auxiliary KA-Net associates old keypoints with new ones based on spatial and anatomical relations, and the Mutual Promotion stage, which uses a keypoint-oriented spatial distillation loss to consolidate knowledge from the KA-Net and the old model. This method not only mitigates catastrophic forgetting of old keypoints but also improves their estimation, demonstrating superior performance and labeling efficiency across various keypoint datasets.",
        "Tags": [
            "Object Detection",
            "Self-Supervised Learning",
            "Incremental Learning",
            "Keypoint Estimation",
            "Knowledge Distillation"
        ]
    },
    {
        "Title": "Investigating the Role of Weight Decay in Enhancing Nonconvex SGD",
        "Authors": "Tao Sun \u00b7 Yuhao Huang \u00b7 Li Shen \u00b7 Kele Xu \u00b7 Bao Wang",
        "Abstract": "Weight decay is a widely used technique in training machine learning models, known to empirically enhance the generalization of Stochastic Gradient Descent (SGD). While intuitively weight decay allows SGD to train a regularized model rather than the original one, there is limited theoretical understanding of why SGD with weight decay (SGDW) yields results consistent with the unregularized model, or how weight decay improves generalization. This paper establishes a convergence theory for SGDW in the context of the unregularized model, under weaker assumptions than previous analyses of weight decay. Our theory demonstrates that weight decay does not accelerate the convergence of SGD. For generalization, we provide the first theoretical proof of weight decay's benefit in nonconvex optimization. Additionally, we extend our results to sign-based stochastic gradient algorithms, such as SignSGD. Numerical experiments on classical benchmarks validate our theoretical findings.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper investigates the role of weight decay in enhancing the performance of Stochastic Gradient Descent (SGD) for nonconvex optimization. While weight decay is empirically known to improve generalization, its theoretical underpinnings remain poorly understood. The authors establish a convergence theory for SGD with weight decay (SGDW) under weaker assumptions than previous analyses, showing that weight decay does not accelerate convergence but improves generalization. They also extend their findings to sign-based stochastic gradient algorithms like SignSGD. Theoretical proofs and numerical experiments on classical benchmarks support these conclusions.",
        "Tags": [
            "Stochastic Gradient Descent (SGD)",
            "Nonconvex Optimization",
            "Weight Decay",
            "Generalization Theory",
            "SignSGD",
            "Nonconvex Convergence"
        ]
    },
    {
        "Title": "MAR-3D: Progressive Masked Auto-regressor for High-Resolution 3D Genaration",
        "Authors": "Jinnan Chen \u00b7 Tao Hu \u00b7 Hao Zhang \u00b7 Lingting Zhu \u00b7 Zeyu HU \u00b7 Shengju Qian \u00b7 Yugang Chen \u00b7 Xin Wang \u00b7 Gim Hee Lee",
        "Abstract": "Recent advances in auto-regressive transformers have revolutionized generative modeling across domains, from language processing to visual generation, demonstrating remarkable capabilities. However, applying these advances to 3D generation presents three key challenges: the unordered nature of 3D data conflicts with sequential prediction paradigms, conventional vector quantization approaches incur substantial compression loss when applied to 3D meshes, and the lack of efficient scaling strategies for higher resolution. To address these limitations, we introduce MAR-3D, which integrates a pyramid variational autoencoder with a cascaded masked auto-regressive transformer (Cascaded MAR) for progressive latent token denoising. Our architecture employs random masking during training and auto-regressive denoising in random order during inference, naturally accommodating the unordered property of 3D latent tokens. Additionally, we propose a cascaded training strategy with condition augmentation that enables efficient up-scaling the latent token resolution. Extensive experiments demonstrate that MAR-3D not only achieves superior performance and generalization capabilities compared to existing methods but also exhibits enhanced scaling properties over joint distribution modeling approaches like diffusion transformers in 3D generation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces MAR-3D, a novel framework for high-resolution 3D generation that addresses key challenges in the field, such as the unordered nature of 3D data, compression loss from vector quantization, and scaling inefficiencies. MAR-3D combines a pyramid variational autoencoder with a cascaded masked auto-regressive transformer (Cascaded MAR) to progressively denoise latent tokens. The architecture uses random masking during training and auto-regressive denoising in random order during inference, effectively handling the unordered property of 3D data. A cascaded training strategy with condition augmentation is also proposed to enable efficient up-scaling of latent token resolution. MAR-3D demonstrates superior performance, generalization, and scaling properties compared to existing methods, including diffusion transformers.",
        "Tags": [
            "3D Generation",
            "Diffusion Models",
            "Cascaded Masked Auto-regressive Transformer",
            "Pyramid Variational Autoencoder",
            "Condition Augmentation"
        ]
    },
    {
        "Title": "Recurrence-Enhanced Vision-and-Language Transformers for Robust Multimodal Document Retrieval",
        "Authors": "Davide Caffagni \u00b7 Sara Sarto \u00b7 Marcella Cornia \u00b7 Lorenzo Baraldi \u00b7 Rita Cucchiara",
        "Abstract": "Cross-modal retrieval is gaining increasing efficacy and interest from the research community, thanks to large-scale training, novel architectural and learning designs, and its application in LLMs and multimodal LLMs. In this paper, we move a step forward and design an approach that allows for multimodal queries -- composed of both an image and a text -- and can search within collections of multimodal documents, where images and text are interleaved. Our model, ReT, employs multi-level representations extracted from different layers of both visual and textual backbones, both at the query and document side. To allow for multi-level and cross-modal understanding and feature extraction, ReT employs a novel Transformer-based recurrent cell that integrates both textual and visual features at different layers, and leverages sigmoidal gates inspired by the classical design of LSTMs. Extensive experiments on M2KR and M-BEIR benchmarks show that Ret achieves state-of-the-art performance across diverse settings.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces ReT, a novel approach for robust multimodal document retrieval that supports queries composed of both images and text. The model leverages multi-level representations from visual and textual backbones and employs a Transformer-based recurrent cell with sigmoidal gates inspired by LSTMs to integrate cross-modal features. ReT demonstrates state-of-the-art performance on benchmarks like M2KR and M-BEIR.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Multimodal Learning",
            "Recurrent Transformers",
            "Cross-Modal Feature Integration",
            "Sigmoidal Gates"
        ]
    },
    {
        "Title": "Z-Magic: Zero-shot Multiple Attributes Guided Image Creator",
        "Authors": "Yingying Deng \u00b7 Xiangyu He \u00b7 Fan Tang \u00b7 Weiming Dong",
        "Abstract": "The customization of multiple attributes has gained increasing popularity with the rising demand for personalized content creation. Despite promising empirical results, the contextual coherence between different attributes has been largely overlooked. In this paper, we argue that subsequent attributes should follow the multivariable conditional distribution introduced by former attributes creation. In light of this, we reformulate multi-attribute creation from a conditional probability theory perspective and tackle the challenging zero-shot setting. By explicitly modeling the dependencies between attributes, we further enhance the coherence of generated images across diverse attribute combinations. Furthermore, we identify connections between multi-attribute customization and multi-task learning, effectively addressing the high computing cost encountered in multi-attribute synthesis. Extensive experiments demonstrate that Z-Magic outperforms existing models in zero-shot image generation, with broad implications for AI-driven design and creative applications.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Z-Magic, a novel approach for zero-shot multiple attributes guided image creation. It addresses the challenge of maintaining contextual coherence between different attributes by reformulating multi-attribute creation from a conditional probability theory perspective. The method explicitly models dependencies between attributes to enhance coherence in generated images and draws connections to multi-task learning to reduce computational costs. Z-Magic demonstrates superior performance in zero-shot image generation, offering significant potential for AI-driven design and creative applications.",
        "Tags": [
            "Zero-Shot Learning",
            "Image Generation",
            "Conditional Probability Modeling",
            "Multi-Attribute Customization",
            "Computational Efficiency"
        ]
    },
    {
        "Title": "Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding",
        "Authors": "Wenxuan Guo \u00b7 Xiuwei Xu \u00b7 Ziwei Wang \u00b7 Jianjiang Feng \u00b7 Jie Zhou \u00b7 Jiwen Lu",
        "Abstract": "In this paper, we propose an efficient multi-level convolution architecture for 3D visual grounding. Conventional methods are difficult to meet the requirements of real-time inference due to the two-stage or point-based architecture. Inspired by the success of multi-level fully sparse convolutional architecture in 3D object detection, we aim to build a new 3D visual grounding framework following this technical route. However, as in 3D visual grounding task the 3D scene representation should be deeply interacted with text features, sparse convolution-based architecture is inefficient for this interaction due to the large amount of voxel features. To this end, we propose text-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D scene representation and text features in an efficient way by gradual region pruning and target completion. Specifically, TGP iteratively sparsifies the 3D scene representation and thus efficiently interacts the voxel features with text features by cross-attention. To mitigate the affect of pruning on delicate geometric information, CBA adaptively fixes the over-pruned region by voxel completion with negligible computational overhead. Compared with previous single-stage methods, our method achieves top inference speed and surpasses previous fastest method by 100\\% FPS. Our method also achieves state-of-the-art accuracy even compared with two-stage methods, with $+1.13$ lead of Acc@0.5 on ScanRefer, and $+2.6$ and $+3.2$ leads on NR3D and SR3D respectively. The Code will be released soon.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces an efficient multi-level convolution architecture for 3D visual grounding, addressing the limitations of conventional methods in real-time inference. The proposed framework utilizes text-guided pruning (TGP) and completion-based addition (CBA) to effectively fuse 3D scene representations with text features, enhancing interaction efficiency through gradual region pruning and target completion. TGP sparsifies the 3D scene representation iteratively, facilitating efficient voxel-text feature interaction via cross-attention, while CBA corrects over-pruned regions with minimal computational cost. The method significantly outperforms existing single-stage methods in inference speed and achieves state-of-the-art accuracy, demonstrating substantial improvements on benchmark datasets.",
        "Tags": [
            "3D Visual Grounding",
            "Sparse Convolution",
            "Text-Guided Pruning",
            "Completion-Based Addition",
            "Cross-Attention Mechanism"
        ]
    },
    {
        "Title": "Active Event-based Stereo Vision",
        "Authors": "Jianing Li \u00b7 Yunjian Zhang \u00b7 Haiqian Han \u00b7 Xiangyang Ji",
        "Abstract": "Conventional frame-based imaging for active stereo systems has encountered major challenges in fast-motion scenarios. However, how to design a novel paradigm for high-speed depth sensing still remains an open issue. In this paper, we propose a novel problem setting, namely active stereo event-based vision, which first integrates binocular event cameras and an infrared projector for high-speed depth sensing. Technically, we first build a stereo camera prototype system and present a real-world dataset with over 21.5k spatiotemporal synchronized labels at 15 Hz, while also creating a realistic synthetic dataset with stereo event streams and 23.8k synchronized labels at 20 Hz. Then, we propose ActiveEventNet, a lightweight yet effective active event-based stereo matching neural network that learns to generate high-quality dense disparity maps from stereo event streams with low latency. Experiments demonstrate that our ActiveEventNet outperforms state-of-the-art methods meanwhile significantly reducing computational complexity. Our solution offers superior depth sensing compared to conventional stereo cameras in high-speed scenarios, while also achieving the inference speed of up to 150 FPS with our prototype. We believe that this novel paradigm will provide new insights into future depth sensing systems. Our dataset descriptions and open-source code will be available in the supplemental material.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a novel paradigm for high-speed depth sensing called active stereo event-based vision, which combines binocular event cameras with an infrared projector. The authors develop a stereo camera prototype and create both real-world and synthetic datasets with synchronized labels. They propose ActiveEventNet, a lightweight neural network for stereo matching that generates high-quality dense disparity maps from stereo event streams with low latency. The network outperforms state-of-the-art methods in high-speed scenarios, achieving an inference speed of up to 150 FPS, and significantly reduces computational complexity. This approach offers superior depth sensing compared to conventional stereo cameras and provides new insights for future depth sensing systems.",
        "Tags": [
            "Stereo Matching",
            "Depth Estimation",
            "Event-based Vision",
            "High-Speed Depth Sensing",
            "Low Latency",
            "Infrared Projector Integration"
        ]
    },
    {
        "Title": "UniSTD: Towards Unified Spatio-Temporal Prediction across Diverse Disciplines",
        "Authors": "Chen Tang \u00b7 Xinzhu Ma \u00b7 Encheng Su \u00b7 Xiufeng Song \u00b7 Xiaohong Liu \u00b7 Wei-Hong Li \u00b7 Lei Bai \u00b7 Wanli Ouyang \u00b7 Xiangyu Yue",
        "Abstract": "Traditional spatiotemporal models generally rely on task-specific architectures, which limit their generalizability and scalability across diverse tasks due to domain-specific design requirements. In this paper, we introduce UniSTD, a unified Transformer-based framework for spatiotemporal modeling, which is inspired by advances in recent foundation models with the two-stage pretraining-then-adaption paradigm. Specifically, our work demonstrates that task-agnostic pretraining on 2D vision and vision-text datasets can build a generalizable model foundation for spatiotemporal learning, followed by specialized joint training on spatiotemporal datasets to enhance task-specific adaptability. To improve the learning capabilities across domains, our framework employs a rank-adaptive mixture-of-expert adaptation by using fractional interpolation to relax the discrete variables so that can be optimized in the continuous space.  Additionally, we introduce a temporal module to incorporate temporal dynamics explicitly. We evaluate our approach on a large-scale dataset covering 10 tasks across 4 disciplines, demonstrating that a unified spatiotemporal model can achieve scalable, cross-task learning and support up to 10 tasks simultaneously within one model while reducing training costs in multi-domain applications. Our code and dataset will be released soon.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces UniSTD, a unified Transformer-based framework for spatiotemporal modeling, designed to overcome the limitations of task-specific architectures. UniSTD employs a two-stage pretraining-then-adaption paradigm, starting with task-agnostic pretraining on 2D vision and vision-text datasets to establish a generalizable model foundation. This is followed by specialized joint training on spatiotemporal datasets to enhance task-specific adaptability. The framework incorporates a rank-adaptive mixture-of-expert adaptation using fractional interpolation and a temporal module to explicitly incorporate temporal dynamics. Evaluated on a large-scale dataset covering 10 tasks across 4 disciplines, UniSTD demonstrates scalable, cross-task learning capabilities, supporting up to 10 tasks simultaneously within one model while reducing training costs in multi-domain applications.",
        "Tags": [
            "Vision Transformer (ViT)",
            "Multimodal Learning",
            "Spatiotemporal Modeling",
            "Transformer-based Framework",
            "Cross-task Learning"
        ]
    },
    {
        "Title": "Lifelong Knowledge Editing for Vision Language Models with Low-Rank Mixture-of-Experts",
        "Authors": "Qizhou Chen \u00b7 Chengyu Wang \u00b7 Dakan Wang \u00b7 Taolin Zhang \u00b7 Wangyue Li \u00b7 Xiaofeng He",
        "Abstract": "Model editing aims to correct inaccurate knowledge, update outdated information, and incorporate new data into Large Language Models (LLMs) without the need for retraining. This task poses challenges in lifelong scenarios where edits must be continuously applied for real-world applications. While some editors demonstrate strong robustness for lifelong editing in pure LLMs, Vision LLMs (VLLMs), which incorporate an additional vision modality, are not directly adaptable to existing LLM editors. In this paper, we propose LiveEdit, a lifelong vision language model edit to bridge the gap between lifelong LLM editing and VLLMs. We begin by training an editing expert generator to independently produce low-rank experts for each editing instance, with the goal of correcting the relevant responses of the VLLM. A hard filtering mechanism is developed to utilize visual semantic knowledge, thereby coarsely eliminating visually irrelevant experts for input queries during the inference stage of the post-edited model. Finally, to integrate visually relevant experts, we introduce a soft routing mechanism based on textual semantic relevance to achieve multi-expert fusion. For evaluation, we establish a benchmark for lifelong VLLM editing. Extensive experiments demonstrate that LiveEdit offers significant advantages in lifelong VLLM editing scenarios. Further experiments validate the rationality and effectiveness of each module design in LiveEdit.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces LiveEdit, a method for lifelong knowledge editing in Vision Language Models (VLLMs), addressing the challenge of updating and correcting knowledge without retraining. LiveEdit employs a low-rank mixture-of-experts approach, where an editing expert generator produces specialized experts for each editing instance. It incorporates a hard filtering mechanism to exclude visually irrelevant experts and a soft routing mechanism for integrating relevant experts based on textual semantic relevance. The method is evaluated through a newly established benchmark for lifelong VLLM editing, demonstrating its effectiveness and the rationality of its design components.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Large Language Models (LLMs)",
            "Low-Rank Mixture-of-Experts",
            "Lifelong Learning",
            "Model Editing"
        ]
    },
    {
        "Title": "EmotiveTalk: Expressive Talking Head Generation through Audio Information Decoupling and Emotional Video Diffusion",
        "Authors": "Haotian Wang \u00b7 Yuzhe Weng \u00b7 Yueyan Li \u00b7 Zilu Guo \u00b7 Jun Du \u00b7 Shutong Niu \u00b7 Jiefeng Ma \u00b7 Shan He \u00b7 Wu Xiaoyan \u00b7 Qiming Hu \u00b7 Bing Yin \u00b7 Cong Liu \u00b7 Qingfeng Liu",
        "Abstract": "Diffusion models have revolutionized the field of talking head generation, yet still face challenges in expressiveness, controllability, and stability in long-time generation. In this research, we propose an EmotiveTalk framework to address these issues. Firstly, to realize better control over the generation of lip movement and facial expression, a Vision-guided Audio Information Decoupling (V-AID) approach is designed to generate audio-based decoupled representations aligned with lip movements and expression. Specifically, to achieve alignment between audio and facial expression representation spaces, we present a Diffusion-based Co-speech Temporal Expansion (Di-CTE) module within V-AID to generate expression-related representations under multi-source emotion condition constraints. Then we propose a well-designed Emotional Talking Head Diffusion (ETHD) backbone to efficiently generate highly expressive talking head videos, which contains an Expression Decoupling Injection (EDI) module to automatically decouple the expressions from reference portraits while integrating the target expression information, achieving more expressive generation performance. Experimental results show that EmotiveTalk can generate expressive talking head videos, ensuring the promised controllability of emotions and stability during long-time generation, yielding state-of-the-art performance compared to existing methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The EmotiveTalk framework addresses challenges in expressiveness, controllability, and stability in talking head generation using diffusion models. It introduces a Vision-guided Audio Information Decoupling (V-AID) approach to align audio with lip movements and facial expressions, and a Diffusion-based Co-speech Temporal Expansion (Di-CTE) module for expression-related representations under multi-source emotion constraints. The Emotional Talking Head Diffusion (ETHD) backbone, with an Expression Decoupling Injection (EDI) module, enhances expressive generation by decoupling expressions from reference portraits and integrating target expression information. EmotiveTalk demonstrates superior performance in generating expressive and stable talking head videos.",
        "Tags": [
            "Diffusion Models",
            "Talking Head Generation",
            "Audio-Visual Alignment",
            "Emotion-Driven Generation",
            "Long-Time Video Stability"
        ]
    },
    {
        "Title": "DexDiffuser: Interaction-aware Diffusion Planning for Adaptive Dexterous Manipulation",
        "Authors": "Zhixuan Liang \u00b7 Yao Mu \u00b7 Yixiao Wang \u00b7 Fei Ni \u00b7 Tianxing Chen \u00b7 Wenqi Shao \u00b7 Wei Zhan \u00b7 Masayoshi Tomizuka \u00b7 Ping Luo \u00b7 Mingyu Ding",
        "Abstract": "Dexterous manipulation with contact-rich interactions is crucial for advanced robotics. While recent diffusion-based planning approaches show promise for simpler manipulation tasks, they often produce unrealistic ghost states (e.g., the object automatically moves without hand contact) or lack adaptability when handling complex sequential interactions. In this work, we introduce DexDiffuser, an interaction-aware diffusion planning framework for adaptive dexterous manipulation. DexDiffuser models joint state-action dynamics through a dual-phase diffusion process which consists of pre-interaction contact alignment and post-contact goal-directed control, enabling goal-adaptive generalizable dexterous manipulation. Additionally, we incorporate dynamics model-based dual guidance and leverage large language models for automated guidance function generation, enhancing generalizability for physical interactions and facilitating diverse goal adaptation through language cues. Experiments on physical interaction tasks such as door opening, pen and block re-orientation, and hammer striking demonstrate DexDiffuser's effectiveness on goals outside training distributions, achieving over twice the average success rate (59.2% vs. 29.5%) compared to existing methods. Our framework achieves 70.0% success on 30-degree door opening, 40.0% and 36.7% on pen and block half-side re-orientation respectively, and 46.7% on hammer nail half drive, highlighting its robustness and flexibility in contact-rich manipulation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "DexDiffuser introduces an interaction-aware diffusion planning framework for adaptive dexterous manipulation, addressing the limitations of existing methods that produce unrealistic states or lack adaptability in complex interactions. The framework employs a dual-phase diffusion process for joint state-action dynamics, combining pre-interaction contact alignment and post-contact goal-directed control. It integrates dynamics model-based dual guidance and leverages large language models for automated guidance generation, enhancing adaptability and generalizability. DexDiffuser demonstrates superior performance in tasks like door opening, pen and block re-orientation, and hammer striking, achieving significantly higher success rates compared to existing methods.",
        "Tags": [
            "Dexterous Manipulation",
            "Diffusion Models",
            "Dual-Phase Diffusion Process",
            "Large Language Model Integration",
            "Contact-Rich Manipulation"
        ]
    },
    {
        "Title": "Dual-view X-ray Detection: Can AI Detect Prohibited Items from Dual-view X-ray Images like Humans?",
        "Authors": "Renshuai Tao \u00b7 Haoyu Wang \u00b7 Yuzhe Guo \u00b7 Hairong Chen \u00b7 Li Zhang \u00b7 Xianglong Liu \u00b7 Yunchao Wei \u00b7 Yao Zhao",
        "Abstract": "To detect prohibited items in challenging categories, human inspectors typically rely on images from two distinct views (vertical and side). Can AI detect prohibited items from dual-view X-ray images in the same way humans do? Existing X-ray datasets often suffer from limitations, such as single-view imaging or insufficient sample diversity. To address these gaps, we introduce the Large-scale Dual-view X-ray (LDXray), which consists of 353,646 instances across 12 categories, providing a diverse and comprehensive resource for training and evaluating models. To emulate human intelligence in dual-view detection, we propose the Auxiliary-view Enhanced Network (AENet), a novel detection framework that leverages both the main and auxiliary views of the same object. The main-view pipeline focuses on detecting common categories, while the auxiliary-view pipeline handles more challenging categories using ``expert models\" learned from the main view. Extensive experiments on the LDXray dataset demonstrate that the dual-view mechanism significantly enhances detection performance, e.g., achieving improvements of up to +24.7\\%for the challenging category of umbrellas. Furthermore, our results show that AENet exhibits strong generalization across seven different detection models.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper explores whether AI can detect prohibited items from dual-view X-ray images as effectively as human inspectors. To address limitations in existing datasets, the authors introduce the Large-scale Dual-view X-ray (LDXray) dataset, containing 353,646 instances across 12 categories. They propose the Auxiliary-view Enhanced Network (AENet), a detection framework that leverages both main and auxiliary views to improve detection performance. AENet uses a main-view pipeline for common categories and an auxiliary-view pipeline for challenging categories, achieving significant improvements, such as a +24.7% increase in detecting umbrellas. The framework also demonstrates strong generalization across multiple detection models.",
        "Tags": [
            "Object Detection",
            "Dual-view X-ray Detection",
            "Auxiliary-view Enhanced Network (AENet)",
            "Dual-view Mechanism",
            "Prohibited Item Detection"
        ]
    },
    {
        "Title": "Speedy-Splat: Fast 3D Gaussian Splatting with Sparse Pixels and Sparse Primitives",
        "Authors": "Alex Hanson \u00b7 Allen Tu \u00b7 Geng Lin \u00b7 Vasu Singla \u00b7 Matthias Zwicker \u00b7 Tom Goldstein",
        "Abstract": "3D Gaussian Splatting (3D-GS) is a recent 3D scene reconstruction technique that enables real-time rendering of novel views by modeling scenes as parametric point clouds of differentiable 3D Gaussians.However, its rendering speed and model size still present bottlenecks, especially in resource-constrained settings.In this paper, we identify and address two key inefficiencies in 3D-GS, achieving substantial improvements in rendering speed, model size, and training time.First, we optimize the rendering pipeline to precisely localize Gaussians in the scene, boosting rendering speed without altering visual fidelity.Second, we introduce a novel pruning technique and integrate it into the training pipeline, significantly reducing model size and training time while further raising rendering speed.Our Speedy-Splat approach combines these techniques to accelerate average rendering speed by a drastic $\\mathit{6.71\\times}$ across scenes from the Mip-NeRF 360, Tanks \\& Temples, and Deep Blending datasets  with $\\mathit{10.6\\times}$ fewer primitives than 3D-GS.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Speedy-Splat, an optimized approach to 3D Gaussian Splatting (3D-GS) that addresses inefficiencies in rendering speed, model size, and training time. By precisely localizing Gaussians in the scene and introducing a novel pruning technique, Speedy-Splat achieves a 6.71\u00d7 improvement in rendering speed and reduces the number of primitives by 10.6\u00d7 compared to traditional 3D-GS, without compromising visual fidelity.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "3D Reconstruction",
            "Rendering Optimization",
            "Model Pruning",
            "Training Efficiency"
        ]
    },
    {
        "Title": "SAR3D: Autoregressive 3D Object Generation and Understanding via Multi-scale 3D VQVAE",
        "Authors": "YONGWEI CHEN \u00b7 Yushi Lan \u00b7 Shangchen Zhou \u00b7 Tengfei Wang \u00b7 Xingang Pan",
        "Abstract": "Autoregressive models have demonstrated remarkable success across various fields, from large language models (LLMs) to large multimodal models (LMMs) and 2D content generation, moving closer to artificial general intelligence (AGI). Despite these advances, applying autoregressive approaches to 3D object generation and understanding remains largely unexplored. This paper introduces Scale AutoRegressive 3D (SAR3D), a novel framework that leverages a multi-scale 3D vector-quantized variational autoencoder (VQVAE) to tokenize 3D objects for efficient autoregressive generation and detailed understanding. By predicting the next scale in a multi-scale latent representation instead of the next single token, SAR3D reduces generation time significantly, achieving fast 3D object generation in just $0.82$ seconds on an A6000 GPU. Additionally, given the tokens enriched with hierarchical 3D-aware information, we finetune a pretrained LLM on them, enabling multimodal comprehension of 3D content.Our experiments show that SAR3D surpasses current 3D generation methods in both speed and quality and allows LLMs to interpret and caption 3D models comprehensively.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces SAR3D, a novel framework for 3D object generation and understanding using a multi-scale 3D vector-quantized variational autoencoder (VQVAE). SAR3D tokenizes 3D objects for efficient autoregressive generation by predicting the next scale in a multi-scale latent representation, significantly reducing generation time to 0.82 seconds on an A6000 GPU. The framework also enables multimodal comprehension of 3D content by finetuning a pretrained large language model (LLM) on tokens enriched with hierarchical 3D-aware information. SAR3D outperforms existing 3D generation methods in speed and quality and allows LLMs to interpret and caption 3D models effectively.",
        "Tags": [
            "3D Generation",
            "Autoregressive Models",
            "Multimodal Learning",
            "3D VQVAE",
            "Hierarchical 3D Representation",
            "Fast 3D Generation"
        ]
    },
    {
        "Title": "LP-Diff: Towards Improved Restoration of Real-World Degraded License Plate",
        "Authors": "Haoyan Gong \u00b7 Zhenrong Zhang \u00b7 Yuzheng Feng \u00b7 Anh Nguyen \u00b7 Hongbin Liu",
        "Abstract": "License plate (LP) recognition is crucial in intelligent traffic management systems. However, factors such as long distances and poor camera quality often lead to severe degradation of captured LP images, posing challenges to accurate recognition. The design of License Plate Image Restoration (LPIR) methods frequently relies on synthetic degraded data, which limits their effectiveness on real-world severely degraded LP images. To address this issue, we introduce the first paired LPIR dataset collected in real-world scenarios, named MDLP, including 10,245 pairs of multi-frame severely degraded LP images and their corresponding clear images. To better restore severely degraded LP, we propose a novel Diffusion-based network, called LP-Diff, to tackle real-world LPIR tasks. Our approach incorporates (1) an Inter-frame Cross Attention Module to fuse temporal information across multiple frames, (2) a Texture Enhancement Module to restore texture information in degraded images, and (3) a Dual-Pathway Fusion Module to select effective features from both channel and spatial dimensions. Extensive experiments demonstrate the reliability of our dataset for model training and evaluation. Our proposed LP-Diff consistently outperforms other state-of-the-art image restoration methods on real-world LPIR tasks. Our dataset and code will be released after the paper is accepted to facilitate reproducibility and future research.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "License plate recognition is vital for intelligent traffic systems, but real-world conditions often degrade image quality, hindering accuracy. Current restoration methods rely on synthetic data, limiting their effectiveness. This paper introduces MDLP, the first real-world paired dataset for License Plate Image Restoration (LPIR), containing 10,245 pairs of degraded and clear images. A novel Diffusion-based network, LP-Diff, is proposed to address real-world LPIR challenges. LP-Diff features an Inter-frame Cross Attention Module for temporal fusion, a Texture Enhancement Module for texture restoration, and a Dual-Pathway Fusion Module for effective feature selection. The method outperforms existing restoration techniques on real-world LPIR tasks, demonstrating the dataset's reliability for training and evaluation.",
        "Tags": [
            "Image Restoration",
            "License Plate Recognition",
            "Diffusion Models",
            "Inter-frame Cross Attention",
            "Texture Enhancement",
            "Dual-Pathway Fusion"
        ]
    },
    {
        "Title": "Self-Supervised Large Scale Point Cloud Completion for Archaeological Site Restoration",
        "Authors": "Aocheng Li \u00b7 James R. Zimmer-Dauphinee \u00b7 Rajesh Kalyanam \u00b7 Ian Lindsay \u00b7 Parker VanValkenburgh \u00b7 Steven Wernke \u00b7 Daniel Aliaga",
        "Abstract": "Point cloud completion helps restore partial incomplete point clouds suffering occlusions. Current self-supervised methods fail to give high fidelity completion for large objects with missing surfaces and unbalanced distribution of available points. In this paper, we present a novel method for restoring large-scale point clouds with limited and imbalanced ground-truth. Using rough boundary annotations for a region of interest, we project the original point clouds into a multiple-center-of-projection (MCOP) image, where fragments are projected to images of 5 channels (RGB, depth, and rotation). Completion of the original point cloud is reduced to inpainting the missing pixels in the MCOP images. Due to lack of complete structures and an unbalanced distribution of existing parts, we develop a self-supervised scheme which learns to infill the MCOP image with points resembling existing \"complete\" patches. Special losses are applied to further enhance the regularity and consistency of completed MCOP images, which is mapped back to 3D to form final restoration. Extensive experiments demonstrate the superiority of our method in completing 600+ incomplete and unbalanced archaeological structures in Peru.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a novel self-supervised method for large-scale point cloud completion, particularly for restoring archaeological sites with incomplete and unbalanced point distributions. The approach projects point clouds into a multiple-center-of-projection (MCOP) image, using rough boundary annotations to guide the process. Completion is achieved by inpainting missing pixels in the MCOP images, leveraging a self-supervised scheme that learns to infill points resembling existing complete patches. Special losses are employed to ensure regularity and consistency in the completed images, which are then mapped back to 3D for final restoration. The method demonstrates superior performance in completing over 600 incomplete and unbalanced archaeological structures in Peru.",
        "Tags": [
            "3D Point Cloud",
            "Self-Supervised Learning",
            "Point Cloud Completion",
            "MCOP Image",
            "Archaeological Restoration"
        ]
    },
    {
        "Title": "DTOS: Dynamic Time Object Sensing with Multimodal Large Language Model",
        "Authors": "Jirui Tian \u00b7 Jinrong Zhang \u00b7 Shenglan Liu \u00b7 Luhao Xu \u00b7 Zhixiong Huang \u00b7 Gao Huang",
        "Abstract": "Existing multimodal large language models (MLLM) face significant challenges in Referring Video Object Segmentation(RVOS). We identify three critical challenges: (C1) insufficient quantitative representation of textual numerical data, (C2) repetitive and degraded response templates for spatiotemporal referencing, and (C3) loss of visual information in video sampling queries lacking textual guidance. To address these, we propose a novel framework, \\textbf{Dynamic Time Object Sensing (DTOS)}, specifically designed for RVOS. To tackle (C1) and (C2), we introduce specialized tokens to construct multi-answer response templates, enabling regression of event boundaries and target localization. This approach improves the accuracy of numerical regression while mitigating the issue of repetitive degradation. To address (C3), we propose a Text-guided Clip Sampler (TCS) that selects video clips aligned with user instructions, preventing visual information loss and ensuring consistent temporal resolution. TCS is also applicable to Moment Retrieval tasks, with enhanced multimodal input sequences preserving spatial details and maximizing temporal resolution. DTOS demonstrates exceptional capability in flexibly localizing multiple spatiotemporal targets based on user-provided textual instructions. Extensive experiments validate the effectiveness of our approach, with DTOS achieving state-of-the-art performance in J&F scores: an improvement of +4.36 on MeViS, +4.48 on Ref-DAVIS17, and +3.02 on Ref-YT-VOS. Additionally, our TCS demonstrates exceptional performance in Moment Retrieval. All code, models, and datasets will be made publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses challenges in Referring Video Object Segmentation (RVOS) faced by existing multimodal large language models (MLLMs), specifically insufficient numerical representation, repetitive response templates, and visual information loss. The authors propose a novel framework, Dynamic Time Object Sensing (DTOS), which introduces specialized tokens for multi-answer response templates and a Text-guided Clip Sampler (TCS) to align video clips with user instructions. DTOS demonstrates superior performance in localizing spatiotemporal targets and achieves state-of-the-art results on benchmark datasets. The TCS also excels in Moment Retrieval tasks.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Referring Image Segmentation",
            "Text-guided Clip Sampler (TCS)",
            "Spatiotemporal Localization",
            "Moment Retrieval"
        ]
    },
    {
        "Title": "A Selective Re-learning Mechanism for Hyperspectral Fusion Imaging",
        "Authors": "Yuanye Liu \u00b7 jinyang liu \u00b7 Renwei Dian \u00b7 Shutao Li",
        "Abstract": "Hyperspectral fusion imaging is challenged by high computational cost due to the abundant spectral information. We find that pixels in regions with smooth spatial-spectral structure can be reconstructed well using a shallow network, while only those in regions with complex spatial-spectral structure require a deeper network. However, existing methods process all pixels uniformly, which ignores  this property. To leverage this property, we propose a Selective Re-learning Fusion Network (SRLF) that initially extracts features from all pixels uniformly and then selectively refines distorted feature points. Specifically, SRLF first employs a Preliminary Fusion Module with robust global modeling capability to generate a preliminary fusion feature. Afterward, it applies a Selective Re-learning Module to focus on improving distorted feature points in the preliminary fusion feature. To achieve targeted learning, we present a novel Spatial-Spectral Structure-Guided Selective Re-learning Mechanism (SSG-SRL) that integrates the observation model to identify the feature points with spatial or spectral distortions. Only these distorted points are sent to the corresponding re-learning blocks, reducing both computational cost and the risk of overfitting. Finally, we develop an SRLF-Net, composed of multiple cascaded SRLFs, which surpasses multiple state-of-the-art methods on several datasets with minimal computational cost.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Hyperspectral fusion imaging faces high computational costs due to its abundant spectral information. This paper introduces a Selective Re-learning Fusion Network (SRLF) that leverages the observation that pixels in smooth spatial-spectral regions can be reconstructed with shallow networks, while complex regions require deeper networks. The SRLF initially extracts features uniformly and then selectively refines distorted feature points using a Spatial-Spectral Structure-Guided Selective Re-learning Mechanism (SSG-SRL). This mechanism identifies and improves only the distorted points, reducing computational costs and overfitting risks. The proposed SRLF-Net, composed of multiple cascaded SRLFs, outperforms state-of-the-art methods on several datasets with minimal computational expense.",
        "Tags": [
            "Hyperspectral Imaging",
            "Selective Re-learning",
            "Feature Fusion",
            "Spatial-Spectral Structure",
            "Computational Efficiency",
            "Feature Distortion Correction"
        ]
    },
    {
        "Title": "FedCALM: Conflict-aware Layer-wise Mitigation for Selective Aggregation in Deeper Personalized Federated Learning",
        "Authors": "Hao Zheng \u00b7 Zhigang Hu \u00b7 Boyu Wang \u00b7 Liu Yang \u00b7 Meiguang Zheng \u00b7 Aikun Xu",
        "Abstract": "Server aggregation conflict is a key challenge in personalized federated learning (PFL). While existing PFL methods have achieved significant progress with shallow base models (e.g., four-layer CNNs), they often overlook the negative impacts of deeper base models on personalization mechanisms. In this paper, we identify the phenomenon of deep model degradation in PFL, where as base model depth increases, the model becomes more sensitive to local client data distributions, thereby exacerbating server aggregation conflicts and ultimately reducing overall model performance. Moreover, we show that these conflicts manifest in insufficient global average updates and mutual constraints between clients. Motivated by our analysis, we proposed a two-stage conflict-aware layer-wise mitigation algorithm, which first constructs a conflict-free global update to alleviate negative conflicts, and then alleviates the conflicts between clients through a conflict-aware strategy.Notably, our method naturally leads to a selective mechanism that balances the tradeoff between clients involved in aggregation and the tolerance for conflicts. Consequently, it can boost the positive contribution to the clients even with the greatest conflicts with the global update.Extensive experiments across multiple datasets and deeper base models demonstrate that FedCALM outperforms four state-of-the-art (SOTA) methods by up to 9.88\\% and seamlessly integrates into existing PFL methods with performance improvements of up to 9.01\\%. Moreover, FedCALM achieves comparable or even better communication and computational efficiency than other SOTA methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the challenge of server aggregation conflict in personalized federated learning (PFL), particularly with deeper base models. It identifies deep model degradation in PFL, where increased model depth exacerbates sensitivity to local client data distributions, leading to server aggregation conflicts and reduced model performance. The authors propose FedCALM, a two-stage conflict-aware layer-wise mitigation algorithm that constructs a conflict-free global update and employs a conflict-aware strategy to alleviate client conflicts. This approach introduces a selective mechanism to balance aggregation participation and conflict tolerance, enhancing client contributions even in high-conflict scenarios. FedCALM demonstrates superior performance over state-of-the-art methods across various datasets and deeper models, offering improvements up to 9.88% and integrating seamlessly into existing PFL methods with up to 9.01% performance gains. Additionally, it maintains or improves communication and computational efficiency compared to other methods.",
        "Tags": [
            "Federated Learning",
            "Personalized Federated Learning",
            "Deep Learning",
            "Conflict Mitigation",
            "Layer-wise Aggregation",
            "Selective Aggregation"
        ]
    },
    {
        "Title": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research",
        "Authors": "James Burgess \u00b7 Jeffrey J Nirschl \u00b7 Laura Bravo-S\u00e1nchez \u00b7 Alejandro Lozano \u00b7 Sanket Rajan Gupte \u00b7 Jesus G. Galaz-Montoya \u00b7 Yuhui Zhang \u00b7 Yuchang Su \u00b7 Disha Bhowmik \u00b7 Zachary Coman \u00b7 Sarina M. Hasan \u00b7 Alexandra Johannesson \u00b7 William D. Leineweber \u00b7 Malvika G Nair \u00b7 Ridhi Yarlagadda \u00b7 Connor Zuraski \u00b7 Wah Chiu \u00b7 Sarah Cohen \u00b7 Jan N. Hansen \u00b7 Manuel D Leonetti \u00b7 Chad Liu \u00b7 Emma Lundberg \u00b7 Serena Yeung",
        "Abstract": "Scientific research demands sophisticated reasoning over multimodal data, a challenge especially prevalent in biology. Despite recent advances in multimodal large language models (MLLMs) for AI-assisted research, existing multimodal reasoning benchmarks target up to college level difficulty, while research-level benchmarks emphasize lower-level perception, falling short of the complex multimodal reasoning needed for scientific discovery. To bridge this gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark designed to assess three reasoning capabilities vital in research workflows: expert image understanding, hypothesis generation, and experiment proposal. MicroVQA consists of 1,061 multiple-choice questions (MCQs) curated by biological experts across diverse microscopy modalities, ensuring VQA samples represent real scientific practice. We find that standard MCQ creation methods do not properly test our targeted reasoning capabilities, motivating a new two stage pipeline: an optimized LLM prompt structures question-answer pairs into MCQs; then, an agent-based `RefineBot' generates more challenging distractors. Benchmarking on state-of-the-art MLLMs reveal a peak performance of 43%; models with smaller LLMs only slightly underperform top models, suggesting that language-based reasoning is less challenging than multimodal reasoning; and tuning with scientific articles enhances performance. Expert analysis of chain-of-thought reasoning failures indicates that multimodal reasoning errors are frequent, followed by knowledge errors and overgeneralization. These insights highlight the challenges in multimodal scientific reasoning, showing MicroVQA is a valuable resource advancing AI-driven biomedical research.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "MicroVQA is introduced as a visual-question answering benchmark to address the gap in multimodal reasoning benchmarks for scientific research, particularly in biology. It assesses three key reasoning capabilities: expert image understanding, hypothesis generation, and experiment proposal. The benchmark comprises 1,061 multiple-choice questions curated by biological experts across various microscopy modalities, reflecting real scientific practice. A novel two-stage pipeline was developed to create more challenging questions, involving an optimized LLM prompt and an agent-based 'RefineBot' for generating distractors. Benchmarking on state-of-the-art MLLMs revealed a peak performance of 43%, with smaller LLMs performing slightly worse, indicating that language-based reasoning is less challenging than multimodal reasoning. Tuning with scientific articles improved performance. Expert analysis identified frequent multimodal reasoning errors, knowledge errors, and overgeneralization, underscoring the challenges in multimodal scientific reasoning and the value of MicroVQA in advancing AI-driven biomedical research.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Visual Question Answering (VQA)",
            "Scientific Reasoning",
            "Microscopy Modalities",
            "Chain-of-Thought Reasoning"
        ]
    },
    {
        "Title": "Effective SAM Combination for Open-Vocabulary Semantic Segmentation",
        "Authors": "Minhyeok Lee \u00b7 Suhwan Cho \u00b7 Jungho Lee \u00b7 Sunghun Yang \u00b7 Heeseung Choi \u00b7 Ig-Jae Kim \u00b7 Sangyoun Lee",
        "Abstract": "Open-vocabulary semantic segmentation aims to assign pixel-level labels to images across an unlimited range of classes. Traditional methods address this by sequentially connecting a powerful mask proposal generator, such as the Segment Anything Model (SAM), with a pre-trained vision-language model like CLIP. But these two-stage approaches often suffer from high computational costs, memory inefficiencies. In this paper, we propose ESC-Net, a novel one-stage open-vocabulary segmentation model that leverages the SAM decoder blocks for class-agnostic segmentation within an efficient inference framework. By embedding pseudo prompts generated from image-text correlations into SAM\u2019s promptable segmentation framework, ESC-Net achieves refined spatial aggregation for accurate mask predictions. Additionally, a Vision-Language Fusion (VLF) module enhances the final mask prediction through image and text guidance. ESC-Net achieves superior performance on standard benchmarks, including ADE20K, PASCAL-VOC, and PASCAL-Context, outperforming prior methods in both efficiency and accuracy. Comprehensive ablation studies further demonstrate its robustness across challenging conditions.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces ESC-Net, a one-stage open-vocabulary semantic segmentation model designed to address the limitations of traditional two-stage approaches, such as high computational costs and memory inefficiencies. ESC-Net integrates the Segment Anything Model (SAM) decoder blocks within an efficient inference framework, utilizing pseudo prompts derived from image-text correlations for refined spatial aggregation. A Vision-Language Fusion (VLF) module further enhances mask predictions by incorporating image and text guidance. The model demonstrates superior performance on benchmarks like ADE20K, PASCAL-VOC, and PASCAL-Context, achieving improvements in both efficiency and accuracy. Ablation studies confirm its robustness under challenging conditions.",
        "Tags": [
            "Semantic Segmentation",
            "Vision-Language Models (VLMs)",
            "Open-Vocabulary Learning",
            "Pseudo Prompts",
            "Vision-Language Fusion",
            "Efficient Inference"
        ]
    },
    {
        "Title": "Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions",
        "Authors": "Stefan Andreas Baumann \u00b7 Felix Krause \u00b7 Michael Neumayr \u00b7 Nick Stracke \u00b7 Melvin Sevi \u00b7 Vincent Tao Hu \u00b7 Bj\u00f6rn Ommer",
        "Abstract": "Recent advances in text-to-image (T2I) diffusion models have significantly improved the quality of generated images. However, providing efficient control over individual subjects, particularly the attributes characterizing them, remains a key challenge. While existing methods have introduced mechanisms to modulate attribute expression, they typically provide either detailed, object-specific localization of such a modification or fine-grained, nuanced control of attributes. No current approach offers both simultaneously, resulting in a gap when trying to achieve precise continuous and subject-specific attribute modulation in image generation. In this work, we demonstrate that token-level directions exist within commonly used CLIP text embeddings that enable fine-grained, subject-specific control of high-level attributes in T2I models. We introduce two methods to identify these directions: a simple, optimization-free technique and a learning-based approach that utilizes the T2I model to characterize semantic concepts more specifically. Our methods allow the augmentation of the prompt text input, enabling fine-grained control over multiple attributes of individual subjects simultaneously, without requiring any modifications to the diffusion model itself. This approach offers a unified solution that fills the gap between global and localized control, providing competitive flexibility and precision in text-guided image generation.",
        "Link": "https://compvis.github.io/attribute-control/",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of achieving precise, continuous, and subject-specific attribute control in text-to-image (T2I) diffusion models. The authors identify token-level directions within CLIP text embeddings that enable fine-grained control over high-level attributes. They propose two methods to identify these directions: an optimization-free technique and a learning-based approach. These methods allow for the augmentation of prompt text inputs, enabling simultaneous control over multiple attributes of individual subjects without modifying the diffusion model. The approach bridges the gap between global and localized control, offering enhanced flexibility and precision in text-guided image generation.",
        "Tags": [
            "Text-to-Image Generation",
            "Diffusion Models",
            "CLIP Embeddings",
            "Token-Level Directions",
            "Subject-Specific Control"
        ]
    },
    {
        "Title": "Implicit Correspondence Learning for  Image-to-Point Cloud Registration",
        "Authors": "Xinjun Li \u00b7 Wenfei Yang \u00b7 Jiacheng Deng \u00b7 Zhixin Cheng \u00b7 Xu Zhou \u00b7 Tianzhu Zhang",
        "Abstract": "Image-to-point cloud registration aims to estimate the camera pose of a given image within a 3D scene point cloud. In this area, matching-based methods have achieved leading performance by first detecting the overlapping region, then matching point and pixel features learned by neural networks and finally using the PnP-RANSAC algorithm to estimate camera pose. However, achieving accurate image-to-point cloud registration remains challenging because the overlapping region detection is unreliable merely relying on point-wise classification, direct alignment of cross-modal data is difficult and indirect optimization objective leads to unstable registration results. To address these challenges, we propose a novel implicit correspondence learning method, including a Geometric Prior-guided overlapping region Detection Module (GPDM), an Implicit Correspondence Learning Module (ICLM), and a Pose Regression Module (PRM). The proposed method enjoys several merits. First, the proposed GPDM can precisely detect the overlapping region. Second, the ICLM can generate robust cross-modality correspondences. Third, the PRM can enable end-to-end optimization. Extensive experimental results on KITTI and nuScenes datasets demonstrate that the proposed model sets a new state-of-the-art performance in registration accuracy.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the challenge of image-to-point cloud registration, which involves estimating the camera pose of an image within a 3D scene point cloud. Existing methods rely on matching-based techniques, but these face issues such as unreliable overlapping region detection, difficulty in aligning cross-modal data, and unstable registration results. To overcome these challenges, the authors propose a novel implicit correspondence learning method comprising three modules: a Geometric Prior-guided overlapping region Detection Module (GPDM), an Implicit Correspondence Learning Module (ICLM), and a Pose Regression Module (PRM). The GPDM improves overlapping region detection, the ICLM generates robust cross-modality correspondences, and the PRM enables end-to-end optimization. The method achieves state-of-the-art performance on the KITTI and nuScenes datasets.",
        "Tags": [
            "3D Registration",
            "3D Point Cloud",
            "Cross-Modal Learning",
            "End-to-End Optimization",
            "Camera Pose Estimation"
        ]
    },
    {
        "Title": "Taste More, Taste Better: Diverse Data and Strong Model Boost Semi-Supervised Crowd Counting",
        "Authors": "Maochen Yang \u00b7 Zekun Li \u00b7 Jian Zhang \u00b7 Lei Qi \u00b7 Yinghuan Shi",
        "Abstract": "Semi-supervised crowd counting is crucial for addressing the high annotation costs of densely populated scenes. Although several methods based on pseudo-labeling have been proposed, it remains challenging to effectively and accurately utilize unlabeled data. In this paper, we propose a novel framework called $\\textit{\\textbf{Taste More Taste Better} (TMTB)}$, which emphasizes both data and model aspects. Firstly, we explore a data augmentation technique well-suited for the crowd counting task. By inpainting the background regions, this technique can effectively enhance data diversity while preserving the fidelity of the entire scenes. Secondly, we introduce the Visual State Space Model (VSSM) as backbone to capture the global context information from crowd scenes, which is crucial for extremely crowded, low-light, and adverse weather scenarios. In addition to the traditional regression head for exact prediction, we employ an Anti-Noise classification head to provide less exact but more accurate supervision, since the regression head is sensitive to noise in manual annotations. We conduct extensive experiments on four benchmark datasets and show that our method outperforms state-of-the-art methods by a large margin. The source code is provided in the supplementary material.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Semi-supervised crowd counting is essential for reducing annotation costs in densely populated scenes. This paper introduces the $\textit{\textbf{Taste More Taste Better} (TMTB)}$ framework, which enhances both data diversity and model robustness. A novel data augmentation technique is proposed, focusing on inpainting background regions to increase scene diversity while maintaining fidelity. The Visual State Space Model (VSSM) is introduced as the backbone to capture global context, particularly useful in crowded, low-light, and adverse weather conditions. The framework also includes an Anti-Noise classification head alongside the traditional regression head to mitigate the impact of noise in manual annotations. The method demonstrates superior performance over state-of-the-art techniques on multiple benchmark datasets.",
        "Tags": [
            "Crowd Counting",
            "Semi-Supervised Learning",
            "Data Augmentation",
            "Visual State Space Model (VSSM)",
            "Anti-Noise Classification Head",
            "Global Context Modeling",
            "Crowd Scene Analysis"
        ]
    },
    {
        "Title": "Monocular Depth Priors for Robust Structure-from-Motion",
        "Authors": "Zador Pataki \u00b7 Paul-Edouard Sarlin \u00b7 Johannes Sch\u00f6nberger \u00b7 Marc Pollefeys",
        "Abstract": "While Structure-from-Motion (SfM) has seen much progress over the years, state-of-the-art systems are prone to failure when facing extreme viewpoint changes in low-overlap or low-parallax conditions.Because capturing images that avoid both pitfalls is challenging, this severely limits the wider use of SfM, especially by non-expert users.In this paper, we overcome both limitations by augmenting the classical SfM paradigm with monocular depth and normal priors, which can be inferred by deep neural networks with increasing accuracy.Our approach is significantly more robust than existing ones in extreme low- or high-overlap scenarios but retains state-of-the-art performance in easier, nominal conditions thanks to a tight integration of monocular and multi-view constraints.We also show that monocular priors can help reject faulty associations due to symmetries, which is a long-standing problem for SfM.Thanks to principled uncertainty propagation, our approach is robust to errors in the priors, can handle priors inferred by different models with little tuning, and will thus easily benefit from future progress in monocular depth and normal estimation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the limitations of Structure-from-Motion (SfM) in extreme viewpoint changes and low-overlap or low-parallax conditions by integrating monocular depth and normal priors inferred by deep neural networks. The proposed approach enhances robustness in challenging scenarios while maintaining state-of-the-art performance in nominal conditions. It also improves the rejection of faulty associations caused by symmetries, a persistent issue in SfM. The method is robust to errors in priors, adaptable to different models, and poised to benefit from advancements in monocular depth and normal estimation.",
        "Tags": [
            "3D Reconstruction",
            "Depth Estimation",
            "Monocular Depth Priors",
            "Uncertainty Propagation",
            "Symmetry Handling"
        ]
    },
    {
        "Title": "Hierarchical Knowledge Prompt Tuning for Multi-task Test-Time Adaptation",
        "Authors": "Qiang Zhang \u00b7 Mengsheng Zhao \u00b7 Jiawei Liu \u00b7 Fanrui Zhang \u00b7 Yongchao Xu \u00b7 Zheng-Jun Zha",
        "Abstract": "Test-time adaptation using vision-language model (such as CLIP) to quickly adjust to distributional shifts of downstream tasks has shown great potential. Despite significant progress, existing methods are still limited to single-task test-time adaptation scenarios and have not effectively explored the issue of multi-task adaptation. To address this practical problem, we propose a novel Hierarchical Knowledge Prompt Tuning (HKPT) method, which achieves joint adaptation to multiple target domains by mining more comprehensive source domain discriminative knowledge and hierarchically modeling task-specific and task-shared knowledge. Specifically, HKPT constructs a CLIP prompt distillation framework that utilizes the broader source domain knowledge of large teacher CLIP to guide prompt tuning for lightweight student CLIP from multiple views during testing. Meanwhile, HKPT establishes task-specific dual dynamic knowledge graph to capture fine-grained contextual knowledge from continuous test data. And to fully exploit the complementarity among multiple target tasks, HKPT employs an adaptive task grouping strategy for achieving inter-task knowledge sharing. Furthermore, HKPT can seamlessly transfer to basic single-task test-time adaptation scenarios while maintaining robust performance. Extensive experimental results in both multi-task and single-task test-time adaptation settings demonstrate that our HKPT significantly outperforms state-of-the-art methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Hierarchical Knowledge Prompt Tuning (HKPT), a novel method for multi-task test-time adaptation using vision-language models like CLIP. HKPT addresses the limitations of existing single-task adaptation methods by leveraging comprehensive source domain knowledge and hierarchically modeling task-specific and task-shared knowledge. It employs a CLIP prompt distillation framework guided by a large teacher CLIP to tune a lightweight student CLIP, constructs task-specific dual dynamic knowledge graphs for fine-grained contextual knowledge, and uses an adaptive task grouping strategy for inter-task knowledge sharing. HKPT also maintains robust performance in single-task adaptation scenarios. Experimental results show that HKPT outperforms state-of-the-art methods in both multi-task and single-task settings.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Test-Time Adaptation",
            "Prompt Tuning",
            "Knowledge Graphs",
            "Multi-task Learning"
        ]
    },
    {
        "Title": "A Closer Look at Time Steps is Worthy of Triple Speed-Up for Diffusion Model Training",
        "Authors": "Kai Wang \u00b7 Mingjia Shi \u00b7 YuKun Zhou \u00b7 Zekai Li \u00b7 Xiaojiang Peng \u00b7 Zhihang Yuan \u00b7 Yuzhang Shang \u00b7 Hanwang Zhang \u00b7 Yang You",
        "Abstract": "Training diffusion models is always a computation-intensive task. In this paper, we introduce a novel speed-up method for diffusion model training, called, which is based on a closer look at time steps. Our key findings are: i) Time steps can be empirically divided into acceleration, deceleration, and convergence areas based on the process increment. ii) These time steps are imbalanced, with many concentrated in the convergence area. iii) The concentrated steps provide limited benefits for diffusion training. To address this, we design an asymmetric sampling strategy that reduces the frequency of steps from the convergence area while increasing the sampling probability for steps from other areas. Additionally, we propose a weighting strategy to emphasize the importance of time steps with rapid-change process increments. As a plug-and-play and architecture-agnostic approach, SpeeD consistently achieves 3-times acceleration across various diffusion architectures, datasets, and tasks. Notably, due to its simple design, our approach significantly reduces the cost of diffusion model training with minimal overhead. Our research enables more researchers to train diffusion models at a lower cost.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper presents a novel method to accelerate the training of diffusion models by analyzing time steps. The authors identify that time steps can be categorized into acceleration, deceleration, and convergence areas based on process increments, with many steps concentrated in the convergence area, which offers limited training benefits. To address this, they propose an asymmetric sampling strategy that reduces the frequency of convergence-area steps and increases sampling for other areas, alongside a weighting strategy to prioritize rapid-change steps. This approach, which is plug-and-play and architecture-agnostic, achieves a consistent 3-times acceleration across various diffusion architectures, datasets, and tasks, significantly reducing training costs with minimal overhead.",
        "Tags": [
            "Diffusion Models",
            "Data Augmentation",
            "Asymmetric Sampling",
            "Time Step Analysis",
            "Training Efficiency"
        ]
    },
    {
        "Title": "DeformCL: Learning Deformable Centerline Representation for Vessel Extraction in 3D Medical Image",
        "Authors": "Ziwei Zhao \u00b7 Zhixing Zhang \u00b7 Yuhang Liu \u00b7 Zhao Zhang \u00b7 Haojun Yu \u00b7 Dong Wang \u00b7 Liwei Wang",
        "Abstract": "In the field of 3D medical imaging, accurately extracting and representing the blood vessels with curvilinear structures holds paramount importance for clinical diagnosis. Previous methods have commonly relied on discrete representation like mask, often resulting in local fractures or scattered fragments due to the inherent limitations of the per-pixel classification paradigm. In this work, we introduce DeformCL, a new continuous representation based on Deformable Centerlines, where centerline points act as nodes connected by edges that capture spatial relationships. Compared with previous representations, DeformCL offers three key advantages: natural connectivity, noise robustness, and interaction facility. We present a comprehensive training pipeline structured in a cascaded manner to fully exploit these favorable properties of DeformCL. Extensive experiments on four 3D vessel segmentation datasets demonstrate the effectiveness and superiority of our method. Furthermore, the visualization of curved planar reformation images validates the clinical significance of the proposed framework.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces DeformCL, a novel continuous representation for extracting blood vessels in 3D medical images using Deformable Centerlines. This method addresses the limitations of traditional discrete representations by ensuring natural connectivity, noise robustness, and interaction facility. A cascaded training pipeline is developed to leverage these advantages, and extensive experiments on four 3D vessel segmentation datasets demonstrate the method's effectiveness and superiority. Visualization of curved planar reformation images further validates the clinical relevance of the framework.",
        "Tags": [
            "3D Semantic Segmentation",
            "Medical Image Segmentation",
            "Deformable Centerlines",
            "Continuous Representation",
            "Curvilinear Structures"
        ]
    },
    {
        "Title": "TimeTracker: Event-based Continuous Point Tracking for Video Frame Interpolation with Non-linear Motion",
        "Authors": "Haoyue Liu \u00b7 Jinghan Xu \u00b7 Yi Chang \u00b7 Hanyu Zhou \u00b7 Haozhi Zhao \u00b7 Lin Wang \u00b7 Luxin Yan",
        "Abstract": "Video frame interpolation (VFI) that leverages the bio-inspired event cameras as guidance has recently shown better performance and memory efficiency than the frame-based methods, thanks to the event cameras' advantages, such as high temporal resolution. A hurdle for event-based VFI is how to effectively deal with non-linear motion, caused by the dynamic changes in motion direction and speed within the scene. Existing methods either use events to estimate sparse motion fields or fuse events with image features to estimate dense motion fields. Unfortunately, motion errors often degrade the VFI quality as the continuous motion cues from events does not align with the dense spatial information of images in the temporal dimension. In this paper, we find that object motion is continuous in space, tracking local regions over continuous time enables more accurate identification of spatiotemporal feature correlations. In light of this, we propose a novel continuous point tracking-based VFI framework, named TimeTracker. Specifically, we first design a Scene-Aware Region Segmentation (SARS) module to divide the scene into similar patches. Then, a Continuous Trajectory guided Motion Estimation (CTME) module is proposed to track the continuous motion trajectory of each patch through events. Finally, intermediate frames at any given time are generated through global motion field optimization and frame refinement. Moreover, we collect a real-world dataset that features fast non-linear motion. Extensive experiments show that our method outperforms prior arts in both motion estimation and frame interpolation quality.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Video frame interpolation (VFI) using event cameras has demonstrated superior performance and memory efficiency compared to traditional frame-based methods, due to the high temporal resolution of event cameras. However, handling non-linear motion, characterized by dynamic changes in direction and speed, remains a challenge. Existing approaches either estimate sparse motion fields or fuse events with image features for dense motion fields, but motion errors often degrade VFI quality. This paper introduces TimeTracker, a novel VFI framework based on continuous point tracking. The framework includes a Scene-Aware Region Segmentation (SARS) module to divide the scene into similar patches and a Continuous Trajectory guided Motion Estimation (CTME) module to track the motion trajectory of each patch using events. Intermediate frames are generated through global motion field optimization and frame refinement. The method is validated on a real-world dataset featuring fast non-linear motion, showing superior performance in motion estimation and frame interpolation quality.",
        "Tags": [
            "Video Frame Interpolation",
            "Event-based Vision",
            "Non-linear Motion Handling",
            "Continuous Point Tracking",
            "Scene-Aware Segmentation"
        ]
    },
    {
        "Title": "OPTICAL: Leveraging Optimal Transport for Contribution Allocation in Dataset Distillation",
        "Authors": "Xiao Cui \u00b7 Yulei Qin \u00b7 Wengang Zhou \u00b7 Hongsheng Li \u00b7 Houqiang Li",
        "Abstract": "The demands for increasingly large-scale datasets pose substantial storage and computation challenges to building deep learning models.Dataset distillation methods,especially those via sample generation techniques,rise in response to condensing large original datasets into small synthetic ones while preserving critical information.Existing subset synthesis methods simply minimize the homogeneous distance where uniform contributions from all real instances are allocated to shaping each synthetic sample.We demonstrate that such equal allocation fails to consider the instance-level relationship between each real-synthetic pair and gives rise to insufficient modeling of geometric structural nuances between the distilled and original sets.In this paper,we propose a novel framework named OPTICAL to reformulate the homogeneous distance minimization into a bi-level optimization problem via matching-and-approximating.In the matching step,we leverage optimal transport matrix to dynamically allocate contributions from real instances.Subsequently,we polish the generated samples in accordance with the established allocation scheme for approximating the real ones.Such a strategy better measures intricate geometric characteristics and handles intra-class variations for high fidelity of data distillation.Extensive experiments across seven datasets and three model architectures demonstrate our method's versatility and effectiveness.Its plug-and-play characteristic makes it compatible with a wide range of distillation frameworks.Codes are available at https://anonymous.4open.science/r/CVPR2025_696.",
        "Link": "https://anonymous.4open.science/r/CVPR2025_696",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces OPTICAL, a novel framework for dataset distillation that addresses the limitations of existing methods which uniformly allocate contributions from real instances to synthetic samples. OPTICAL reformulates the problem into a bi-level optimization involving matching and approximating steps. It uses an optimal transport matrix to dynamically allocate contributions from real instances, enhancing the modeling of geometric structures and intra-class variations. This approach improves the fidelity of data distillation and is compatible with various distillation frameworks.",
        "Tags": [
            "Dataset Distillation",
            "Optimal Transport",
            "Bi-level Optimization",
            "Geometric Structure Modeling",
            "Intra-class Variation Handling"
        ]
    },
    {
        "Title": "Bridging the Vision-Brain Gap with an Uncertainty-Aware Blur Prior",
        "Authors": "Haitao Wu \u00b7 Qing Li \u00b7 Changqing Zhang \u00b7 Zhen He \u00b7 Xiaomin Ying",
        "Abstract": "Can our brain signals faithfully reflect the original visual stimuli, even including high-frequency details? Although human perceptual and cognitive capacities enable us to process and remember visual information, these abilities are constrained by several factors, such as limited attentional resources and the finite capacity of visual working memory. When visual stimuli are processed by the human visual system into brain signals, some information is inevitably lost, leading to a discrepancy known as the \\textbf{System GAP}.Additionally, perceptual and cognitive dynamics, along with technical noise in signal acquisition, reduce the fidelity of brain signals relative to the original visual stimuli, known as the \\textbf{Random GAP}.When encoded brain signal representations are directly aligned with the corresponding pretrained image features, the System GAP and Random GAP between paired data challenge the model, requiring it to bridge these gaps.However, in the context of limited paired data, these gaps become difficult for the model to learn, leading to overfitting and poor generalization to new data. To address these GAPs, we propose a simple yet effective approach called the \\textbf{Uncertainty-aware Blur Prior (UBP)}.It estimates the uncertainty within the paired data, reflecting the mismatch between brain signals and visual stimuli. Based on this uncertainty, UBP dynamically blurs the high-frequency details of the original images, reducing the impact of the mismatch and improving alignment.Our method achieves a top-1 accuracy of \\textbf{50.9\\%} and a top-5 accuracy of \\textbf{79.7\\%} on the zero-shot brain-to-image retrieval task, surpassing previous state-of-the-art methods by margins of \\textbf{13.7\\%} and \\textbf{9.8\\%}, respectively. Code is released in the supplemental material.",
        "Link": "https://github.com/HaitaoWuTJU/Uncertainty-aware-Blur-Prior",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the discrepancy between visual stimuli and brain signals, termed the System GAP and Random GAP, which arise due to limitations in human perceptual and cognitive capacities and technical noise in signal acquisition. To bridge these gaps, the authors propose an Uncertainty-aware Blur Prior (UBP) that estimates uncertainty within paired data and dynamically blurs high-frequency details in images to improve alignment. This approach significantly enhances performance in zero-shot brain-to-image retrieval tasks, achieving top-1 and top-5 accuracies of 50.9% and 79.7%, respectively, outperforming previous methods.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Self-Supervised Learning",
            "Uncertainty Estimation",
            "Brain Signal Processing",
            "Zero-Shot Retrieval"
        ]
    },
    {
        "Title": "ParaHome: Parameterizing Everyday Home Activities Towards 3D Generative Modeling of Human-Object Interactions",
        "Authors": "Jeonghwan Kim \u00b7 Jisoo Kim \u00b7 Jeonghyeon Na \u00b7 Hanbyul Joo",
        "Abstract": "To enable machines to understand the way humans interact with the physical world in daily life, 3D interaction signals should be captured in natural settings, allowing people to engage with multiple objects in a range of sequential and casual manipulations. To achieve this goal, we introduce our ParaHox0008me system designed to capture dynamic 3D movements of humans and objects within a common home environment. Our system features a multi-view setup with 70 synchronized RGB cameras, along with wearable motion capture devices including an IMU-based body suit and hand motion capture gloves. By leveraging the ParaHome system, we collect a new human-object interaction dataset, including 486 minutes of sequences across 208 captures with 38 participants, offering advancements with three key aspects: (1) capturing body motion and dexterous hand manipulation motion alongside multiple objects within a contextual home environment; (2) encompassing sequential and concurrent manipulations paired with text descriptions; and (3) including articulated objects with multiple parts represented by 3D parameterized models. We present detailed design justifications for our system, and perform key generative modeling experiments to demonstrate the potential of our dataset.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The ParaHome system is introduced to capture dynamic 3D movements of humans and objects in a home environment, utilizing a multi-view setup with 70 synchronized RGB cameras and wearable motion capture devices. This system facilitates the collection of a comprehensive human-object interaction dataset, featuring 486 minutes of sequences across 208 captures with 38 participants. The dataset advances the field by capturing body and hand movements with multiple objects, including sequential and concurrent manipulations with text descriptions, and articulated objects represented by 3D parameterized models. The paper also discusses the system's design and demonstrates its potential through generative modeling experiments.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Avatars",
            "3D Reconstruction",
            "Human-Object Interaction",
            "Motion Capture",
            "3D Parameterized Models"
        ]
    },
    {
        "Title": "Segment Any Motion in Videos",
        "Authors": "Nan Huang \u00b7 Wenzhao Zheng \u00b7 Chenfeng Xu \u00b7 Kurt Keutzer \u00b7 Shanghang Zhang \u00b7 Angjoo Kanazawa \u00b7 Qianqian Wang",
        "Abstract": "Moving object segmentation is a crucial task for achieving a high-level understanding of visual scenes and has numerous downstream applications. Humans can effortlessly segment moving objects in videos. Previous work has largely relied on optical flow to provide motion cues; however, this approach often results in imperfect predictions due to challenges such as partial motion, complex deformations, motion blur and background distractions. We propose a novel approach for moving object segmentation that combines long-range trajectory motion cues with DINO-based semantic features and leverages SAM2 for pixel-level mask densification through an iterative prompting strategy. Our model employs Spatio-Temporal Trajectory Attention and Motion-Semantic Decoupled Embedding to prioritize motion while integrating semantic support. Extensive testing on diverse datasets demonstrates state-of-the-art performance, excelling in challenging scenarios and fine-grained segmentation of multiple objects.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Moving object segmentation is essential for high-level visual scene understanding and has broad applications. While humans can easily segment moving objects in videos, existing methods relying on optical flow often produce imperfect results due to issues like partial motion, complex deformations, and motion blur. This paper introduces a novel approach that combines long-range trajectory motion cues with DINO-based semantic features and uses SAM2 for pixel-level mask refinement through iterative prompting. The proposed model employs Spatio-Temporal Trajectory Attention and Motion-Semantic Decoupled Embedding to emphasize motion while incorporating semantic context. The method achieves state-of-the-art performance across diverse datasets, particularly excelling in challenging scenarios and fine-grained segmentation of multiple objects.",
        "Tags": [
            "Video Object Segmentation",
            "Self-Supervised Learning",
            "Trajectory Attention",
            "Motion-Semantic Decoupling",
            "Iterative Prompting"
        ]
    },
    {
        "Title": "AerialMegaDepth: Learning Aerial-Ground Reconstruction and View Synthesis",
        "Authors": "Khiem Vuong \u00b7 Anurag Ghosh \u00b7 Deva Ramanan \u00b7 Srinivasa G. Narasimhan \u00b7 Shubham Tulsiani",
        "Abstract": "We explore the task of geometric reconstruction of images captured from a mixture of ground and aerial views. Current state-of-the-art learning-based approaches fail to handle the extreme viewpoint variation between aerial-ground image pairs. Our hypothesis is that the lack of high-quality, co-registered aerial-ground datasets for training is a key reason for this failure. Such data is difficult to assemble precisely because it is difficult to reconstruct in a scalable way. To overcome this challenge, we propose a scalable framework combining pseudo-synthetic renderings from 3D city-wide meshes (e.g., Google Earth) with real, ground-level crowd-sourced images (e.g., MegaDepth). The pseudo-synthetic data simulates a wide range of aerial viewpoints, while the real, crowd-sourced images help improve visual fidelity for ground-level images where mesh-based renderings lack sufficient detail, effectively bridging the domain gap between real images and pseudo-synthetic renderings. Using this hybrid dataset, we fine-tune several state-of-the-art algorithms and achieve significant improvements on real-world, zero-shot aerial-ground tasks. For example, we observe that baseline DUSt3R localizes fewer than 3% of aerial-ground pairs within 5 degrees of camera rotation error, while fine-tuning with our data raises accuracy to nearly 50%, addressing a major failure point in handling large viewpoint changes. Beyond camera estimation and scene reconstruction, our dataset also improves performance on downstream tasks like novel-view synthesis in challenging aerial-ground scenarios, demonstrating the practical value of our approach in real-world applications.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the challenge of geometric reconstruction and view synthesis between aerial and ground images, which current learning-based methods struggle with due to extreme viewpoint variations. The authors hypothesize that the lack of high-quality, co-registered aerial-ground datasets is a major limitation. To overcome this, they propose a scalable framework combining pseudo-synthetic renderings from 3D city-wide meshes with real, ground-level crowd-sourced images. This hybrid dataset bridges the domain gap between real images and pseudo-synthetic renderings, enabling significant improvements in real-world, zero-shot aerial-ground tasks. Fine-tuning state-of-the-art algorithms with this dataset enhances camera estimation, scene reconstruction, and novel-view synthesis, demonstrating practical value in real-world applications.",
        "Tags": [
            "3D Reconstruction",
            "Novel View Synthesis",
            "Aerial-Ground Reconstruction",
            "Hybrid Dataset",
            "Zero-Shot Learning"
        ]
    },
    {
        "Title": "CaricatureBooth: Data-Free Interactive Caricature Generation in a Photo Booth",
        "Authors": "Zhiyu Qu \u00b7 Yunqi Miao \u00b7 Zhensong Zhang \u00b7 Jifei Song \u00b7 Jiankang Deng \u00b7 Yi-Zhe Song",
        "Abstract": "We present CaricatureBooth, a system that transforms caricature creation into a simple interactive experience -- as easy as using a photo booth! A key challenge in caricature generation is two-fold: the scarcity of high-quality caricature data and the difficulty in enabling precise creative control over the exaggeration process while maintaining identity. Prior approaches either require large-scale caricature and photo data or lack intuitive mechanisms for users to guide the deformation without losing identity. We address the data scarcity by synthesising training data through Thin Plate Spline (TPS) deformation of standard face images. For creative control, we design a B\u00e9zier curve interface where users can easily manipulate facial features, with these edits then driving TPS transformations at inference time. When combined with a pre-trained ID-preserving diffusion model, our system maintains both identity preservation and creative flexibility. Through extensive experiments, we demonstrate that CaricatureBooth achieves state-of-the-art quality while making the joy of caricature creation as accessible as taking a photo -- just walk in and walk out with your personalised caricature! Code will be made available at the first instance to facilitate follow-up efforts.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "CaricatureBooth introduces an interactive system for caricature generation, simplifying the process to resemble using a photo booth. It tackles the dual challenges of high-quality caricature data scarcity and the need for precise creative control over facial feature exaggeration while preserving identity. The system synthesizes training data by applying Thin Plate Spline (TPS) deformation to standard face images and employs a B\u00e9zier curve interface for user-guided feature manipulation, which informs TPS transformations during inference. Integrated with a pre-trained ID-preserving diffusion model, CaricatureBooth ensures both identity preservation and creative flexibility, achieving state-of-the-art quality in caricature generation.",
        "Tags": [
            "Image Generation",
            "Data Augmentation",
            "Thin Plate Spline (TPS) Deformation",
            "B\u00e9zier Curve Interface",
            "ID-Preserving Diffusion Model"
        ]
    },
    {
        "Title": "DART: Disease-aware Image-Text Alignment and Self-correcting Re-alignment for Trustworthy Radiology Report Generation",
        "Authors": "Sang-Jun Park \u00b7 Keun-Soo Heo \u00b7 Dong-Hee Shin \u00b7 Young-Han Son \u00b7 Ji-Hye Oh \u00b7 Tae-Eui Kam",
        "Abstract": "The automatic generation of radiology reports has emerged as a promising solution to reduce a time-consuming task and accurately capture critical disease-relevant findings in X-ray images. Previous approaches for radiology report generation have shown impressive performance. However, there remains significant potential to improve accuracy by ensuring that retrieved reports contain disease-relevant findings similar to those in the X-ray images and by refining generated reports. In this study, we propose a Disease-aware image-text Alignment and self-correcting Re-alignment for Trustworthy radiology report generation (DART) framework. In the first stage, we generate initial reports based on image-to-text retrieval with disease-matching, embedding both images and texts in a shared embedding space through contrastive learning. This approach ensures the retrieval of reports with similar disease-relevant findings that closely align with the input X-ray images. In the second stage, we further enhance the initial reports by introducing a self-correction module that re-aligns them with the X-ray images. Our proposed framework achieves state-of-the-art results on the MIMIC-CXR and IU X-ray benchmarks, surpassing previous approaches in both report generation and disease classification, thereby enhancing the trustworthiness of radiology reports.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The study introduces DART, a framework for trustworthy radiology report generation that improves accuracy by ensuring disease-relevant findings in X-ray images align with retrieved reports. The framework operates in two stages: first, it generates initial reports using image-to-text retrieval with disease-matching, embedding both images and texts in a shared space via contrastive learning. Second, it refines these reports with a self-correction module that re-aligns them with the images. DART achieves state-of-the-art results on the MIMIC-CXR and IU X-ray benchmarks, outperforming previous methods in report generation and disease classification.",
        "Tags": [
            "Medical Image Analysis",
            "Vision-Language Models (VLMs)",
            "Contrastive Learning",
            "Self-correction Module",
            "Disease-matching"
        ]
    },
    {
        "Title": "STEP: Enhancing Video-LLMs\u2019 Compositional Reasoning by Spatio-Temporal Graph-guided Self-Training",
        "Authors": "Haiyi Qiu \u00b7 Minghe Gao \u00b7 Long Qian \u00b7 Kaihang Pan \u00b7 Qifan Yu \u00b7 Juncheng Li \u00b7 Wenjie Wang \u00b7 Siliang Tang \u00b7 Yueting Zhuang \u00b7 Tat-seng Chua",
        "Abstract": "Video Large Language Models (Video-LLMs) have recently shown strong performance in basic video understanding tasks, such as captioning and coarse-grained question answering, but struggle with compositional reasoning that requires multi-step spatio-temporal inference across object relations, interactions, and events. The hurdles to enhancing this capability include extensive manual labor, the lack of spatio-temporal compositionality in existing data and the absence of explicit reasoning supervision. In this paper, we propose STEP, a novel graph-guided self-training method that enables Video-LLMs to generate reasoning-rich fine-tuning data from any raw videos to improve itself. Specifically, we first induce Spatial-Temporal Scene Graph (STSG) representation of diverse videos to capture fine-grained, multi-granular video semantics. Then, the STSG guided the derivation of multi-step reasoning Question-Answer (QA) data with Chain-of-Thought (CoT) rationales. Both answers and rationales are integrated as training objective, aiming to enhance model's reasoning abilities by supervision over explicit reasoning steps. Experimental results demonstrate the effectiveness of STEP across models of varying scales, with a significant 21.3\\% improvement in tasks requiring three or more reasoning steps. Furthermore, it achieves superior performance with a minimal amount of self-generated rationale-enriched training samples in both compositional reasoning and comprehensive understanding benchmarks, highlighting the broad applicability and vast potential. The code is available in https://anonymous.4open.science/r/STEP-FE2C/",
        "Link": "https://anonymous.4open.science/r/STEP-FE2C/",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Video Large Language Models (Video-LLMs) excel in basic video understanding tasks but face challenges with compositional reasoning requiring multi-step spatio-temporal inference. The paper introduces STEP, a graph-guided self-training method that enhances Video-LLMs' reasoning capabilities by generating reasoning-rich fine-tuning data from raw videos. STEP employs Spatial-Temporal Scene Graphs (STSG) to capture detailed video semantics and derives multi-step reasoning Question-Answer (QA) data with Chain-of-Thought (CoT) rationales. This approach significantly improves model performance, particularly in tasks requiring complex reasoning, demonstrating the method's effectiveness and potential for broad applicability.",
        "Tags": [
            "Video Understanding",
            "Multimodal Large Language Models (MLLMs)",
            "Self-Supervised Learning",
            "Spatio-Temporal Scene Graphs",
            "Chain-of-Thought Rationales",
            "Compositional Reasoning"
        ]
    },
    {
        "Title": "Empowering LLMs to Understand and Generate Complex Vector Graphics",
        "Authors": "XiMing Xing \u00b7 Juncheng Hu \u00b7 Guotao Liang \u00b7 Jing Zhang \u00b7 Dong Xu \u00b7 Qian Yu",
        "Abstract": "The unprecedented advancements in Large Language Models (LLMs) have shown a profound impact on natural language processing but are yet to fully embrace the realm of scalable vector graphics (SVG) generation. While LLMs encode partial knowledge of SVG data from web pages during training, recent findings suggest that semantically ambiguous and tokenized representations within LLMs may result in hallucinations in vector primitive predictions. Furthermore, LLM training lacks modeling and understanding of the rendering sequence of vector paths, resulting in occlusion between output vector primitives. In this paper, we present LLM4SVG, an initial yet substantial step toward bridging this gap by enabling LLMs to better understand and generate vector graphics.   LLM4SVG facilitates a deeper understanding of SVG components through learnable semantic tokens, precisely encoding these tokens and their corresponding properties to generate semantically aligned SVG output. Using a series of learnable semantic tokens, a structured dataset for instruction following is developed to support comprehension and generation across two primary tasks. Our method introduces a modular architecture to existing large language models (LLMs), integrating semantic tags, vector instruction encoders, fine-tuned commands, and powerful LLMs to tightly combine geometric, appearance, and language information. To overcome the scarcity of SVG-text instruction data, we developed an automated data generation pipeline that collected a massive data set of more than 250k SVG data and 580k SVG-text instructions, which facilitated the adoption of the two-stage training strategy popular in LLM development. By exploring various training strategies, we developed LLM4SVG, significantly moving beyond an optimized rendering-based approach and a language-model-based baseline to achieve remarkable results in human evaluation tasks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces LLM4SVG, a novel approach to enhance Large Language Models (LLMs) in understanding and generating scalable vector graphics (SVG). LLM4SVG addresses the limitations of LLMs in handling SVG data, such as semantic ambiguity and rendering sequence issues, by employing learnable semantic tokens and a modular architecture. This architecture integrates semantic tags, vector instruction encoders, and fine-tuned commands to combine geometric, appearance, and language information effectively. A significant contribution is the development of an automated data generation pipeline that amassed over 250k SVG data and 580k SVG-text instructions, enabling a two-stage training strategy. The method surpasses traditional rendering-based approaches and language-model baselines, achieving superior results in human evaluation tasks.",
        "Tags": [
            "Large Language Models (LLMs)",
            "Image Generation",
            "Multimodal Learning",
            "Scalable Vector Graphics (SVG)",
            "Semantic Tokens",
            "Automated Data Generation"
        ]
    },
    {
        "Title": "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis",
        "Authors": "Chaoyou Fu \u00b7 Yuhan Dai \u00b7 Yongdong Luo \u00b7 Lei Li \u00b7 Shuhuai Ren \u00b7 Renrui Zhang \u00b7 Zihan Wang \u00b7 Chenyu Zhou \u00b7 Yunhang Shen \u00b7 Mengdan Zhang \u00b7 Peixian Chen \u00b7 Yanwei Li \u00b7 Shaohui Lin \u00b7 Sirui Zhao \u00b7 Ke Li \u00b7 Tong Xu \u00b7 Xiawu Zheng \u00b7 Enhong Chen \u00b7 Caifeng Shan \u00b7 Ran He \u00b7 Xing Sun",
        "Abstract": "In the quest for artificial general intelligence, Multi-modal Large Language Models (MLLMs) have emerged as a focal point in recent advancements. However, the predominant focus remains on developing their capabilities in static image understanding. The potential of MLLMs to process sequential visual data is still insufficiently explored, highlighting the lack of a comprehensive, high-quality assessment of their performance. In this paper, we introduce Video-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of MLLMs in Video analysis. Our work distinguishes from existing benchmarks through four key features: 1) Diversity in video types, spanning 6 primary visual domains with 30 subfields to ensure broad scenario generalizability; 2) Duration in temporal dimension, encompassing both short-, medium-, and long-term videos, ranging from 11 seconds to 1 hour, for robust contextual dynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides video frames, including subtitles and audios, to unveil the all-round capabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual labeling by expert annotators to facilitate precise and reliable model assessment. With Video-MME, we extensively evaluate various state-of-the-art MLLMs, and reveal that Gemini 1.5 Pro is the best-performing commercial model, significantly outperforming the open-source models with an average accuracy of 75\\%, compared to 71.9% for GPT-4o. The results also demonstrate that Video-MME is a universal benchmark that applies to both image and video MLLMs. Further analysis indicates that subtitle and audio information could significantly enhance video understanding. Besides, a decline in MLLM performance is observed as video duration increases for all models. Our dataset along with these findings underscores the need for further improvements in handling longer sequences and multi-modal data, shedding light on future MLLM development.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Video-MME, the first comprehensive evaluation benchmark for Multi-modal Large Language Models (MLLMs) in video analysis. Video-MME addresses the gap in assessing MLLMs' capabilities in processing sequential visual data by offering a diverse, high-quality benchmark. It features diversity in video types, duration, data modalities, and quality annotations. The benchmark evaluates state-of-the-art MLLMs, revealing that Gemini 1.5 Pro outperforms others, including GPT-4o, with an average accuracy of 75%. The study highlights the importance of subtitles and audio in enhancing video understanding and notes a decline in performance with longer video durations. These findings emphasize the need for further advancements in MLLMs to handle longer sequences and multi-modal data effectively.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Video Understanding",
            "Video Analysis Benchmark",
            "Multi-modal Data Integration",
            "Temporal Context Dynamics"
        ]
    },
    {
        "Title": "Audio-Visual Semantic Graph Network for Audio-Visual Event Localization",
        "Authors": "Liang Liu \u00b7 Shuaiyong Li \u00b7 Yongqiang Zhu",
        "Abstract": "Audio-visual event localization (AVEL) involves identifying the category and the corresponding temporal boundary of an event that is both audible and visible in unconstrained videos. However, the semantic gap between heterogeneous modalities often leads to audio-visual semantic inconsistency. In this paper, we propose a novel Audio-Visual Semantic Graph Network (AVSGN) to facilitate cross-modal alignment and cross-temporal interaction. Unlike previous methods (e.g., audio-guided, visual-guided, or both), we introduce shared semantic textual labels to bridge the semantic gap between audio and visual modalities. Specifically, we present a cross-modal semantic alignment (CMSA) module to explore the cross-modal complementary relationships across  heterogeneous modalities (i.e., visual, audio and text), promoting the convergence of multimodal distributions into a common semantic space. Additionally, in order to capture cross-temporal associations sufficiently, we devise a cross-modal graph interaction (CMGI) module, which disentangles complicated interactions across modalities into three complementary subgraphs. Extensive experiments on the AVE dataset comprehensively demonstrate the superiority and effectiveness of the proposed model in both fully- and weakly-supervised AVE settings.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces the Audio-Visual Semantic Graph Network (AVSGN) to address the challenge of audio-visual event localization (AVEL) by bridging the semantic gap between audio and visual modalities. The proposed model utilizes shared semantic textual labels to align heterogeneous modalities and employs a cross-modal semantic alignment (CMSA) module to integrate visual, audio, and text into a common semantic space. Furthermore, a cross-modal graph interaction (CMGI) module is developed to effectively capture cross-temporal associations through three complementary subgraphs. The effectiveness of AVSGN is validated through extensive experiments, demonstrating its superiority in both fully- and weakly-supervised settings.",
        "Tags": [
            "Multimodal Learning",
            "Audio-Visual Event Localization",
            "Cross-Modal Alignment",
            "Semantic Graph Networks",
            "Temporal Boundary Detection"
        ]
    },
    {
        "Title": "Continuous Space-Time Video Resampling with  Invertible Motion Steganography",
        "Authors": "Yuantong zhang \u00b7 Zhenzhong Chen",
        "Abstract": "Space-time video resampling aims to conduct both spatial-temporal downsampling and upsampling processes to achieve high-quality video reconstruction.Although there has been much progress, some major challenges still exist, such as how to preserve motion information during temporal resampling while avoiding blurring artifacts, and how to achieve flexible temporal and spatial resampling factors. In this paper, we introduce an Invertible Motion Steganography Module (IMSM), designed to preserve motion information in high-frame-rate videos. This module embeds motion information from high-frame-rate videos into downsampled frames with lower frame rates in a visually imperceptible manner. Its reversible nature allows the motion information to be recovered, facilitating the reconstruction of high-frame-rate videos. Furthermore, we propose a 3D implicit feature modulation technique that enables continuous spatiotemporal resampling. With tailored training strategies, our method supports flexible frame rate conversions, including non-integer changes like 30 FPS to 24 FPS and vice versa.  Extensive experiments show that our method significantly outperforms existing solutions across multiple datasets in various video resampling tasks with high flexibility. Codes will be made available upon publication.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenges of space-time video resampling, focusing on preserving motion information and achieving flexible temporal and spatial resampling factors. The authors introduce an Invertible Motion Steganography Module (IMSM) that embeds motion information from high-frame-rate videos into downsampled frames in a visually imperceptible manner, allowing for the recovery of motion information and reconstruction of high-frame-rate videos. Additionally, a 3D implicit feature modulation technique is proposed to enable continuous spatiotemporal resampling. The method supports flexible frame rate conversions, including non-integer changes, and demonstrates superior performance in various video resampling tasks.",
        "Tags": [
            "Video Generation",
            "Super-Resolution",
            "Invertible Motion Steganography",
            "3D Implicit Feature Modulation",
            "Flexible Frame Rate Conversion"
        ]
    },
    {
        "Title": "VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis",
        "Authors": "Enric Corona \u00b7 Andrei Zanfir \u00b7 Eduard Gabriel Bazavan \u00b7 NIKOS KOLOTOUROS \u00b7 Thiemo Alldieck \u00b7 Cristian Sminchisescu",
        "Abstract": "We propose VLOGGER, a method for audio-driven human video generation from a single input image of a person, which builds on the success of recent generative diffusion models. Our method consists of 1) a stochastic human-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture that augments text-to-image models with both spatial and temporal controls. This supports the generation of high quality video of variable length, easily controllable through high-level representations of human faces and bodies. In contrast to previous work, our method does not require training for each person, does not rely on face detection and cropping, generates the complete image (not just the face or the lips), and considers a broad spectrum of scenarios (e.g. visible torso or diverse subject identities) that are critical to correctly synthesize humans who communicate.We also curate MENTOR, a new and diverse dataset with 3d pose and expression annotations, one order of magnitude larger than previous ones (800,000 identities) and with dynamic gestures, on which we train and ablate our main technical contributions.VLOGGER outperforms state-of-the-art methods in three public benchmarks, considering lip-syncing, image quality, identity preservation and temporal consistency while also generating upper-body gestures.We analyze the performance of VLOGGER with respect to multiple diversity metrics, showing that our architectural choices and the use of MENTOR benefit training a fair and unbiased model at scale. Finally we show applications in video editing and personalization.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "VLOGGER introduces a novel method for audio-driven human video generation from a single input image, leveraging generative diffusion models. The approach includes a stochastic human-to-3D-motion diffusion model and a diffusion-based architecture that enhances text-to-image models with spatial and temporal controls. This enables the generation of high-quality, variable-length videos controllable through high-level representations of human faces and bodies. Unlike previous methods, VLOGGER does not require per-person training, avoids face detection and cropping, and generates complete images, accommodating diverse scenarios and identities. The method is trained on MENTOR, a new, large-scale dataset with 3D pose and expression annotations, significantly larger than previous datasets. VLOGGER demonstrates superior performance in lip-syncing, image quality, identity preservation, and temporal consistency, and is effective in video editing and personalization applications.",
        "Tags": [
            "Avatars",
            "Diffusion Models",
            "3D Generation",
            "Multimodal Learning",
            "Audio-Driven Video Generation",
            "Stochastic Human-to-3D-Motion",
            "High-Level Human Representation Control"
        ]
    },
    {
        "Title": "Parallel Sequence Modeling via Generalization Spatial Propagation Network",
        "Authors": "Hongjun Wang \u00b7 Wonmin Byeon \u00b7 Jiarui Xu \u00b7 Jinwei Gu \u00b7 Ka Chun Cheung \u00b7 Jan Kautz \u00b7 Xiaolong Wang \u00b7 Kai Han \u00b7 Sifei Liu",
        "Abstract": "We present the Generalized Spatial Propagation Network (GSPN), a new attention mechanism optimized for vision tasks that inherently captures 2D spatial structures. Existing attention models, including transformers, linear attention, and state-space models like Mamba, process multi-dimensional data as 1D sequences, compromising spatial coherence and efficiency. GSPN overcomes these limitations by directly operating on spatially coherent image data and forming dense pairwise connections through a unique line-scan approach. Central to GSPN is the Stability-Context Condition, which ensures stable, context-aware propagation across 2D sequences and reduces the effective sequence length to $\\sqrt{N}$, significantly enhancing computational efficiency. With learnable, input-dependent weights and no reliance on positional embeddings, GSPN achieves superior spatial fidelity and state-of-the-art performance in vision tasks, including ImageNet classification, class-guided image generation, and text-to-image generation. Notably, GSPN accelerates SD-XL with softmax-attention by over $84\\times$ when generating 16K images. Codes will be released upon publication.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces the Generalized Spatial Propagation Network (GSPN), a novel attention mechanism designed for vision tasks that effectively captures 2D spatial structures. Unlike existing models that process multi-dimensional data as 1D sequences, GSPN operates directly on spatially coherent image data, forming dense pairwise connections through a unique line-scan approach. The Stability-Context Condition ensures stable, context-aware propagation across 2D sequences, reducing the effective sequence length to \u221aN and enhancing computational efficiency. GSPN achieves superior spatial fidelity and state-of-the-art performance in various vision tasks, including ImageNet classification and text-to-image generation, while significantly accelerating image generation processes.",
        "Tags": [
            "Attention Mechanism",
            "Vision Transformer (ViT)",
            "Spatial Propagation",
            "Line-Scan Approach",
            "Stability-Context Condition"
        ]
    },
    {
        "Title": "Stop learning it all to mitigate visual hallucination, Focus on the hallucination target.",
        "Authors": "Dokyoon Yoon \u00b7 Youngsook Song \u00b7 Woomyoung Park",
        "Abstract": "Multimodal Large Language Models (MLLMs) frequently suffer from hallucination issues, generating information about objects that are not present in input images during vision-language tasks. These hallucinations particularly undermine model reliability in practical applications requiring accurate object identification. To address this challenge, we propose TL-DPO, a preference learning approach that mitigates hallucinations by focusing on targeted areas where they occur. To implement this, we build a dataset containing hallucinated responses, correct responses, and target information (i.e., objects present in the images and the corresponding chunk positions in responses affected by hallucinations). By applying a preference learning method restricted to these specific targets, the model can filter out irrelevant signals and focus on correcting hallucinations. This allows the model to produce more factual responses by concentrating solely on relevant information. Experimental results demonstrate that TL-DPO effectively reduces hallucinations across multiple vision hallucination tasks, improving the reliability and performance of MLLMs without diminishing overall performance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Multimodal Large Language Models (MLLMs) often generate hallucinated information about non-existent objects in vision-language tasks, compromising their reliability. To address this, the authors propose TL-DPO, a preference learning approach that targets specific areas where hallucinations occur. By constructing a dataset with hallucinated and correct responses, along with target information, the model focuses on correcting hallucinations while filtering out irrelevant signals. This targeted approach enables the model to produce more factual responses, improving reliability and performance without degrading overall capabilities.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Vision-Language Models (VLMs)",
            "Hallucination Mitigation",
            "Preference Learning",
            "Targeted Learning"
        ]
    },
    {
        "Title": "Hierarchical Features Matter: A Deep Exploration of Progressive Parameterization Method for Dataset Distillation",
        "Authors": "Xinhao Zhong \u00b7 Hao Fang \u00b7 Bin Chen \u00b7 Xulin Gu \u00b7 Meikang Qiu \u00b7 Shuhan Qi \u00b7 Shu-Tao Xia",
        "Abstract": "Dataset distillation is an emerging dataset reduction method, which condenses large-scale datasets while maintaining task accuracy. Current parameterization methods achieve enhanced performance under extremely high compression ratio by optimizing determined synthetic dataset in informative feature domain. However, they limit themselves to a fixed optimization space for distillation, neglecting the diverse guidance across different informative latent spaces. To overcome this limitation, we propose a novel parameterization method dubbed Hierarchical Parameterization Distillation (H-PD), to systematically explore hierarchical feature within provided feature space (e.g., layers within pre-trained generative adversarial networks). We verify the correctness of our insights by applying the hierarchical optimization strategy on GAN-based parameterization method. In addition, we introduce a novel class-relevant feature distance metric to alleviate the computational burden associated with synthetic dataset evaluation, bridging the gap between synthetic and original datasets. Experimental results demonstrate that the proposed H-PD achieves a significant performance improvement under various settings with equivalent time consumption, and even surpasses current generative distillation using diffusion models under extreme compression ratios IPC=1 and IPC=10.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Dataset distillation is a method for reducing large-scale datasets while preserving task accuracy. Current methods optimize synthetic datasets in a fixed feature space, limiting their effectiveness. This paper introduces Hierarchical Parameterization Distillation (H-PD), which explores hierarchical features within pre-trained GANs to improve distillation. A novel class-relevant feature distance metric is also proposed to reduce computational costs during synthetic dataset evaluation. H-PD demonstrates superior performance under various settings, outperforming existing methods, including those using diffusion models, at extreme compression ratios.",
        "Tags": [
            "Dataset Distillation",
            "Generative Adversarial Networks (GANs)",
            "Hierarchical Feature Exploration",
            "Class-Relevant Feature Distance",
            "Extreme Compression Ratios"
        ]
    },
    {
        "Title": "Towards Optimizing Large-Scale Multi-Graph Matching in Bioimaging",
        "Authors": "Max Kahl \u00b7 Sebastian Stricker \u00b7 Lisa Hutschenreiter \u00b7 Florian Bernard \u00b7 Carsten Rother \u00b7 Bogdan Savchynskyy",
        "Abstract": "Multi-graph matching is an important problem in computer vision. Our task comes from bioimaging, where a set of 29 3D-microscopic images of worms have to be brought into correspondence. Surprisingly, virtually all existing methods are not applicable to this large-scale, real-world problem since they either assume a complete or dense problem setting, and they have so far only been applied to small-scale, toy or synthetic problems. Despite claims in literature that methods addressing complete multi-graph matching are applicable in an incomplete setting, our first contribution is to prove that their runtime would be excessive and impractical.  Our second contribution is a new method for incomplete multi-graph matching that applies to real-world, larger-scale problems.We experimentally show that for our bioimaging application we are able to attain results in less than two minutes, whereas the only competing approach requires at least half an hour while producing far worse results. Furthermore, even for small-scale, dense or complete problem instances we achieve results that are at least on par with the leading methods, but an order of magnitude faster.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the challenge of large-scale multi-graph matching in bioimaging, specifically for aligning 3D-microscopic images of worms. Existing methods are unsuitable due to their assumptions of complete or dense problem settings and their limited application to small-scale or synthetic problems. The authors demonstrate that these methods are impractical for large-scale, real-world scenarios due to excessive runtime. They introduce a novel method for incomplete multi-graph matching, which is effective for larger-scale problems. Their approach significantly outperforms competing methods in terms of runtime and accuracy, achieving results in under two minutes compared to half an hour for the next best method, while also matching or exceeding the performance of leading methods on small-scale problems.",
        "Tags": [
            "Graph Neural Networks (GNNs)",
            "3D Reconstruction",
            "Bioimaging",
            "Incomplete Graph Matching",
            "Runtime Optimization"
        ]
    },
    {
        "Title": "UCM-VeID V2: A Richer Dataset and A Pre-training Method for UAV Cross-Modality Vehicle Re-Identification",
        "Authors": "Xingyue Liu \u00b7 Jiahao Qi \u00b7 Chen Chen \u00b7 Kangcheng Bin \u00b7 Ping Zhong",
        "Abstract": "Cross-Modality Re-Identification (VI-ReID) aims to achieve around-the-clock target matching, benefiting from the strengths of both  RGB and infrared (IR) modalities. However, the field is hindered by limited datasets, particularly for vehicle VI-ReID, and by challenges such as modality bias training (MBT), stemming from biased pre-training on ImageNet. To tackle the above issues, this paper introduces an UCM-VeID V2 dataset benchmark for vehicle VI-ReID, and proposes a new self-supervised pre-training method, Cross-Modality Patch-Mixed Self-supervised Learning (PMSL). UCM-VeID V2 dataset features a significant increase in data volume, along with enhancements in multiple aspects. PMSL addresses MBT by learning modality-invariant features through Patch-Mixed Image Reconstruction (PMIR) and Modality Discrimination Adversarial Learning (MDAL), and enhances discriminability with Modality-Augmented Contrasting Cluster (MACC).  Comprehensive experiments are carried out to validate the effectiveness of the proposed method.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenges in Cross-Modality Re-Identification (VI-ReID) for vehicles, particularly the limitations posed by small datasets and modality bias training (MBT) from ImageNet pre-training. The authors introduce the UCM-VeID V2 dataset, which significantly expands data volume and improves various aspects of vehicle VI-ReID. Additionally, they propose a novel self-supervised pre-training method called Cross-Modality Patch-Mixed Self-supervised Learning (PMSL). PMSL mitigates MBT by learning modality-invariant features through Patch-Mixed Image Reconstruction (PMIR) and Modality Discrimination Adversarial Learning (MDAL), while enhancing discriminability with Modality-Augmented Contrasting Cluster (MACC). The effectiveness of the proposed method is validated through comprehensive experiments.",
        "Tags": [
            "ReID (Person Re-identification)",
            "Self-Supervised Learning",
            "Cross-Modality Learning",
            "Adversarial Learning",
            "Patch-Mixed Reconstruction"
        ]
    },
    {
        "Title": "A Flag Decomposition for Hierarchical Datasets",
        "Authors": "Nathan Mankovich \u00b7 Ignacio Santamaria \u00b7 Gustau Camps-Valls \u00b7 Tolga Birdal",
        "Abstract": "Flag manifolds encode hierarchical nested sequences of subspaces and serve as powerful structures for various computer vision and machine learning applications. Despite their utility in tasks such as dimensionality reduction, motion averaging, and subspace clustering, current applications are often restricted to extracting flags using common matrix decomposition methods like the singular value decomposition. Here, we address the need for a general algorithm to factorize and work with hierarchical datasets. In particular, we propose a novel, flag-based method that decomposes arbitrary hierarchical real-valued data into a hierarchy-preserving flag representation in Stiefel coordinates. Our work harnesses the potential of flag manifolds in applications including denoising, clustering, and few-shot learning.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Flag manifolds are powerful structures for hierarchical nested sequences of subspaces, widely used in computer vision and machine learning tasks like dimensionality reduction, motion averaging, and subspace clustering. Current methods often rely on matrix decompositions such as singular value decomposition, limiting their applicability. This paper introduces a novel flag-based method to decompose hierarchical real-valued datasets into a hierarchy-preserving flag representation using Stiefel coordinates. The proposed approach enhances the utility of flag manifolds in applications such as denoising, clustering, and few-shot learning.",
        "Tags": [
            "Dimensionality Reduction",
            "Subspace Clustering",
            "Flag Manifolds",
            "Stiefel Coordinates",
            "Hierarchical Data Representation"
        ]
    },
    {
        "Title": "Model Poisoning Attacks to Federated Learning via Multi-Round Consistency",
        "Authors": "Yueqi Xie \u00b7 Minghong Fang \u00b7 Neil Zhenqiang Gong",
        "Abstract": "Model poisoning attacks are critical security threats to Federated Learning (FL). Existing model poisoning attacks suffer from two key limitations: 1) they achieve suboptimal effectiveness when defenses are deployed, and/or 2) they require knowledge of the model updates or local training data on genuine clients. In this work, we make a key observation that their suboptimal effectiveness arises from only leveraging model-update consistency among malicious clients within individual training rounds, making the attack effect self-cancel across training rounds. In light of this observation, we propose PoisonedFL, which enforces multi-round consistency among the malicious clients' model updates while not requiring any knowledge about the genuine clients.Our empirical evaluation on five benchmark datasets shows that \\ourmodel{} breaks eight state-of-the-art defenses and outperforms seven existing model poisoning attacks. Our study shows that FL systems are considerably less robust than previously thought, underlining the urgency for the development of new defense mechanisms.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the limitations of existing model poisoning attacks in Federated Learning (FL), which are either less effective against defenses or require knowledge of genuine clients' data. The authors identify that the suboptimal effectiveness of these attacks is due to leveraging only single-round model-update consistency among malicious clients, leading to self-cancellation across rounds. To overcome this, they propose PoisonedFL, a novel attack that enforces multi-round consistency among malicious clients' updates without needing knowledge of genuine clients. The proposed method is shown to break eight state-of-the-art defenses and outperform seven existing attacks, demonstrating that FL systems are more vulnerable than previously believed and highlighting the need for new defense mechanisms.",
        "Tags": [
            "Federated Learning",
            "Model Poisoning Attacks",
            "Multi-Round Consistency",
            "Security Threats",
            "Defense Mechanisms"
        ]
    },
    {
        "Title": "Mosaic of Modalities: A Comprehensive Benchmark for Multimodal Graph Learning",
        "Authors": "Jing Zhu \u00b7 Yuhang Zhou \u00b7 Shengyi Qian \u00b7 Zhongmou He \u00b7 Tong Zhao \u00b7 Neil Shah \u00b7 Danai Koutra",
        "Abstract": "Graph machine learning has made significant strides in recent years, yet the integration of visual information with graph structures remains an underexplored area. To address this critical gap, we introduce the Multimodal Graph Benchmark (MM-GRAPH), a pioneering benchmark that incorporates both visual and textual information into graph learning tasks. MM-GRAPH extends beyond existing text-attributed graph benchmarks, offering a more comprehensive evaluation framework for multimodal graph neural networks (GNNs). Our benchmark comprises seven diverse datasets of varying scales, designed to assess graph learning algorithms across different tasks in real-world scenarios. These datasets feature rich multimodal node attributes, including visual data, which enables a more holistic evaluation of GNN performance in complex, multimodal environments. To support advancements in this emerging field, we provide an extensive empirical study on the performance of various graph learning frameworks when presented with features from multiple modalities, particularly emphasizing the impact of visual information. This study offers valuable insights into the challenges and opportunities of integrating visual data into graph learning algorithms.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces the Multimodal Graph Benchmark (MM-GRAPH), a novel benchmark designed to evaluate the integration of visual and textual information in graph learning tasks. MM-GRAPH extends existing benchmarks by incorporating multimodal node attributes, including visual data, across seven diverse datasets. The benchmark aims to provide a comprehensive framework for assessing graph learning algorithms in real-world scenarios, particularly focusing on the impact of visual information. An empirical study is conducted to evaluate the performance of various graph learning frameworks when handling multimodal features, offering insights into the challenges and opportunities of integrating visual data into graph learning.",
        "Tags": [
            "Graph Neural Networks (GNNs)",
            "Multimodal Learning",
            "Multimodal Graph Benchmark",
            "Visual Data Integration",
            "Graph Learning Evaluation"
        ]
    },
    {
        "Title": "Unbiased Video Scene Graph Generation via Visual and Semantic Dual Debiasing",
        "Authors": "Yanjun Li \u00b7 Zhaoyang Li \u00b7 Honghui Chen \u00b7 li&#x27;Zhi Xu",
        "Abstract": "Video Scene Graph Generation (VidSGG) aims to capture dynamic relationships among entities by sequentially analyzing video frames and integrating visual and semantic information. However, VidSGG is challenged by significant biases that skew predictions. To mitigate these biases, we propose a \\textbf{VI}sual and \\textbf{S}emantic \\textbf{A}wareness (VISA) framework for unbiased VidSGG. VISA addresses visual bias through an innovative memory update mechanism that enhances object representations and concurrently reduces semantic bias by iteratively integrating object features with comprehensive semantic information derived from triplet relationships. This visual-semantics dual debiasing approach results in more unbiased representations of complex scene dynamics. Extensive experiments demonstrate the effectiveness of our method, where VISA outperforms existing unbiased VidSGG approaches by a substantial margin (e.g., +13.1\\% improvement in mR@20 and mR@50 for the SGCLS task under Semi Constraint).",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Video Scene Graph Generation (VidSGG) aims to model dynamic relationships among entities in video frames by integrating visual and semantic information. However, VidSGG is hindered by significant biases that affect predictions. To address this, the authors propose the Visual and Semantic Awareness (VISA) framework, which mitigates visual bias through a memory update mechanism and reduces semantic bias by iteratively combining object features with semantic information from triplet relationships. This dual debiasing approach results in more accurate and unbiased representations of scene dynamics. The proposed method demonstrates superior performance compared to existing unbiased VidSGG approaches.",
        "Tags": [
            "Scene Graph Generation",
            "Video Understanding",
            "Bias Mitigation",
            "Memory Update Mechanism",
            "Triplet Relationship Integration"
        ]
    },
    {
        "Title": "MAP: Unleashing Hybrid Mamba-Transformer Vision Backbone's Potential with Masked Autoregressive Pretraining",
        "Authors": "Yunze Liu \u00b7 Li Yi",
        "Abstract": "Hybrid Mamba-Transformer networks have recently garnered broad attention. These networks can leverage the scalability of Transformers while capitalizing on Mamba's strengths in long-context modeling and computational efficiency. However, the challenge of effectively pretraining such hybrid networks remains an open question. Existing methods, such as Masked Autoencoders (MAE) or autoregressive (AR) pretraining, primarily focus on single-type network architectures. In contrast, pretraining strategies for hybrid architectures must be effective for both Mamba and Transformer components. Based on this, we propose Masked Autoregressive Pretraining (MAP) to pretrain a hybrid Mamba-Transformer vision backbone network. This strategy combines the strengths of both MAE and Autoregressive pretraining, improving the performance of Mamba and Transformer modules within a unified paradigm. Experimental results show that the hybrid Mamba-Transformer vision backbone network pretrained with MAP significantly outperforms other pretraining strategies, achieving state-of-the-art performance. We validate the method's effectiveness on both 2D and 3D datasets and provide detailed ablation studies to support the design choices for each component. We will release the code and checkpoints.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Hybrid Mamba-Transformer networks combine the scalability of Transformers with Mamba's long-context modeling and computational efficiency. However, pretraining such hybrid architectures effectively remains a challenge. This paper introduces Masked Autoregressive Pretraining (MAP), a strategy that integrates the strengths of Masked Autoencoders (MAE) and autoregressive pretraining to pretrain hybrid Mamba-Transformer vision backbone networks. MAP improves the performance of both Mamba and Transformer modules within a unified framework, achieving state-of-the-art results on 2D and 3D datasets. The method's effectiveness is validated through detailed ablation studies.",
        "Tags": [
            "Mamba",
            "Backbone",
            "Self-Supervised Learning",
            "Hybrid Architectures",
            "Pretraining Strategies",
            "Long-Context Modeling"
        ]
    },
    {
        "Title": "Evaluating generated 3D assets using multiview Large Language Models",
        "Authors": "Shalini Maiti \u00b7 Lourdes Agapito \u00b7 Filippos Kokkinos",
        "Abstract": "The rapid advancements in text-to-3D generation necessitate robust and scalable evaluation metrics that align closely with human judgment\u2014a need unmet by current metrics such as PSNR and CLIP, which require ground-truth data or focus only on prompt fidelity. To address this, we introduce Gen3DEval, a novel evaluation framework that leverages vision large language models (vLLMs) specifically fine-tuned for 3D object quality assessment. Gen3DEval evaluates text fidelity, appearance, and surface quality\u2014by analyzing 3D surface normals\u2014without requiring ground-truth comparisons, bridging the gap between automated metrics and user preferences.Compared to state-of-the-art task-agnostic models, Gen3DEval demonstrates superior performance in user-aligned evaluations, establishing itself as a comprehensive and accessible benchmark for future research in text-to-3D generation. To support and encourage further research in this field, we will release both our code and benchmark, establishing Gen3DEval as a comprehensive and accessible tool for text-to-3D evaluation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Gen3DEval, a novel evaluation framework for text-to-3D generation that leverages fine-tuned vision large language models (vLLMs) to assess text fidelity, appearance, and surface quality without requiring ground-truth data. Gen3DEval outperforms state-of-the-art task-agnostic models in aligning with user preferences, offering a robust and scalable benchmark for future research in text-to-3D generation.",
        "Tags": [
            "3D Generation",
            "Vision-Language Models (VLMs)",
            "3D Object Quality Assessment",
            "Text-to-3D Evaluation",
            "Surface Normal Analysis"
        ]
    },
    {
        "Title": "Common3D: Self-Supervised Learning of 3D Morphable Models for Common Objects in Neural Feature Space",
        "Authors": "Leonhard Sommer \u00b7 Olaf D\u00fcnkel \u00b7 Christian Theobalt \u00b7 Adam Kortylewski",
        "Abstract": "3D morphable models (3DMMs) are a powerful tool to represent the possible shapes and appearances of an object category. Given a single test image, 3DMMs can be used to solve various tasks, such as predicting the 3D shape, pose, semantic correspondence, and instance segmentation of an object. Unfortunately, 3DMMs are only available for very few object categories that are of particular interest, like faces or human bodies, as they require a demanding 3D data acquisition and category-specific training process. In contrast, we introduce a new method, Common3D, that learns 3DMMs of common objects in a fully self-supervised manner from a collection of object-centric videos. For this purpose, our model represents objects as a learned 3D template mesh and a deformation field that is parameterized as an image-conditioned neural network. Different from prior works, Common3D represents the object appearance with neural features instead of RGB colors, which enables the learning of more generalizable representations through an abstraction from pixel intensities. Importantly, we train the appearance features using a contrastive objective by exploiting the correspondences defined through the deformable template mesh. This leads to higher quality correspondence features compared to related works and a significantly improved model performance at estimating 3D object pose and semantic correspondence. Common3D is the first completely self-supervised method that can solve various vision tasks in a zero-shot manner.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Common3D, a novel method for learning 3D morphable models (3DMMs) of common objects in a fully self-supervised manner from object-centric videos. Unlike traditional 3DMMs that require extensive 3D data and category-specific training, Common3D represents objects using a learned 3D template mesh and a deformation field parameterized by an image-conditioned neural network. It employs neural features instead of RGB colors for object appearance, enhancing generalizability. The method uses a contrastive objective to train appearance features, leveraging correspondences from the deformable template mesh, which improves the quality of correspondence features and model performance in tasks like 3D object pose estimation and semantic correspondence. Common3D is the first completely self-supervised method capable of solving various vision tasks in a zero-shot manner.",
        "Tags": [
            "3D Reconstruction",
            "Self-Supervised Learning",
            "Neural Feature Space",
            "Contrastive Learning",
            "Zero-Shot Learning"
        ]
    },
    {
        "Title": "GenPC: Zero-shot Point Cloud Completion via 3D Generative Priors",
        "Authors": "li an \u00b7 Zhe Zhu \u00b7 Mingqiang Wei",
        "Abstract": "Existing point cloud completion methods, which typically depend on predefined synthetic training datasets, encounter significant challenges when applied to out-of-distribution, real-world scans. To overcome this limitation, we introduce a zero-shot completion framework, termed GenPC, designed to reconstruct high-quality real-world scans by leveraging explicit 3D generative priors.Our key insight is that recent feed-forward 3D generative models, trained on extensive internet-scale data, have demonstrated the ability to perform 3D generation from single-view images in a zero-shot setting. To harness this for completion, we first develop a Depth Prompting module that links partial point clouds with image-to-3D generative models by leveraging depth images as a stepping stone. To retain the original partial structure in the final results, we designed the Geometric Preserving Fusion module that aligns the generated shape with input by adaptively adjusting its pose and scale.Extensive experiments on widely used benchmarks validate the superiority and generalizability of our approach, bringing us a step closer to robust real-world scan completion.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces GenPC, a zero-shot point cloud completion framework that leverages 3D generative priors to reconstruct high-quality real-world scans without relying on predefined synthetic training datasets. The approach utilizes a Depth Prompting module to connect partial point clouds with image-to-3D generative models through depth images and a Geometric Preserving Fusion module to align generated shapes with input structures. This method demonstrates superior performance and generalizability on widely used benchmarks, advancing the field of robust real-world scan completion.",
        "Tags": [
            "3D Generation",
            "Zero-Shot Learning",
            "Depth Prompting",
            "Geometric Preserving Fusion",
            "Real-world Scan Completion"
        ]
    },
    {
        "Title": "Designing Scale-Wise Transformers for Text-to-Image Synthesis",
        "Authors": "Anton Voronov \u00b7 Denis Kuznedelev \u00b7 Mikhail Khoroshikh \u00b7 Valentin Khrulkov \u00b7 Dmitry Baranchuk",
        "Abstract": "This work presents Switti, a scale-wise transformer for text-to-image generation. Starting from existing next-scale prediction AR models, we first explore them for T2I generation and propose architectural modifications to improve their convergence and overall performance. We then observe that self-attention maps of the pretrained scale-wise AR model exhibit weak dependence on preceding scales. Based on this insight, we propose a non-AR counterpart facilitating ${\\sim}11$% faster sampling and lower memory usage while achieving the same generation performance.Furthermore, we reveal that classifier-free guidance at high-resolution scales may be not only unnecessary but potentially detrimental. By disabling guidance at these scales, we achieve a total sampling acceleration of ${\\sim}30$% and enhance the generation of fine-grained details. Extensive human preference studies and automated evaluations confirm that Switti significantly outperforms existing T2I AR models and competes with state-of-the-art T2I diffusion models while being up to $7{\\times}$ faster.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Switti, a scale-wise transformer designed for text-to-image (T2I) generation. The authors build upon existing autoregressive (AR) models for next-scale prediction, proposing architectural modifications to enhance convergence and performance. They identify weak dependencies in self-attention maps of pretrained AR models and introduce a non-AR alternative that reduces sampling time by ~11% and memory usage while maintaining generation quality. Additionally, they demonstrate that classifier-free guidance at high-resolution scales can be counterproductive, and disabling it results in ~30% faster sampling and improved fine-grained detail generation. Switti outperforms existing T2I AR models and rivals state-of-the-art diffusion models, achieving up to 7x faster sampling speeds.",
        "Tags": [
            "Text-to-Image Generation",
            "Vision Transformer (ViT)",
            "Scale-Wise Transformers",
            "Classifier-Free Guidance",
            "Sampling Acceleration"
        ]
    },
    {
        "Title": "EgoLife: Towards Egocentric Life Assistant",
        "Authors": "Jingkang Yang \u00b7 Shuai Liu \u00b7 Hongming Guo \u00b7 Yuhao Dong \u00b7 Xiamengwei Zhang \u00b7 Sicheng Zhang \u00b7 Pengyun Wang \u00b7 Zitang Zhou \u00b7 Binzhu Xie \u00b7 Ziyue Wang \u00b7 Bei Ouyang \u00b7 Zhengyu Lin \u00b7 Marco Cominelli \u00b7 Zhongang Cai \u00b7 Bo Li \u00b7 Yuanhan Zhang \u00b7 Peiyuan Zhang \u00b7 Fangzhou Hong \u00b7 Joerg Widmer \u00b7 Francesco Gringoli \u00b7 Lei Yang \u00b7 Ziwei Liu",
        "Abstract": "We introduce EgoLife, a project to develop an egocentric life assistant that accompanies and enhances personal efficiency through AI-powered wearable glasses. To lay the foundation for this assistant, we conducted a comprehensive data collection study where six participants lived together for one week, continuously recording their daily activities\u2014including discussions, shopping, cooking, socializing, and entertainment\u2014using AI glasses for multimodal egocentric video capture, along with synchronized third-person-view video references. This effort resulted in the EgoLife Dataset, a comprehensive 300-hour egocentric, interpersonal, multiview, and multimodal daily life dataset with intensive annotation. Leveraging this dataset, we introduce EgoLifeQA, a suite of long-context, life-oriented question-answering tasks designed to provide meaningful assistance in daily life by addressing practical questions such as recalling past relevant events, monitoring health habits, and offering personalized recommendations.To address the key technical challenges of 1) developing robust visual-audio models for egocentric data, 2) enabling accurate identity recognition, and 3) facilitating long-context question answering over extensive temporal information, we introduce EgoButler, an integrated system comprising EgoGPT and EgoRAG. EgoGPT is a vision-language model trained on egocentric datasets, achieving state-of-the-art performance on egocentric video understanding. EgoRAG is a retrieval-based component that supports answering ultra-long-context questions. Our experimental studies verify their working mechanisms and reveal critical factors and bottlenecks, guiding future improvements. By releasing our datasets, models, and benchmarks, we aim to stimulate further research in egocentric AI assistants.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces EgoLife, a project aimed at developing an egocentric life assistant using AI-powered wearable glasses. The foundation for this assistant is the EgoLife Dataset, a comprehensive 300-hour dataset of daily activities recorded from six participants over one week, featuring multimodal egocentric video and synchronized third-person views. The dataset supports EgoLifeQA, a suite of tasks designed for practical daily life assistance, such as event recall and health monitoring. The authors also present EgoButler, an integrated system combining EgoGPT, a vision-language model for egocentric video understanding, and EgoRAG, a retrieval-based component for long-context question answering. The study highlights the technical challenges and solutions in visual-audio modeling, identity recognition, and long-context question answering, with the aim of advancing research in egocentric AI assistants.",
        "Tags": [
            "Embodied AI",
            "Multimodal Learning",
            "Egocentric Video Understanding",
            "Long-Context Question Answering",
            "Wearable AI"
        ]
    },
    {
        "Title": "JTD-UAV: MLLM-Enhanced Joint Tracking and Description Framework for Anti-UAV Systems",
        "Authors": "Yifan Wang \u00b7 Jian Zhao \u00b7 Zhaoxin Fan \u00b7 Xin Zhang \u00b7 Xuecheng Wu \u00b7 Yudian Zhang \u00b7 Lei Jin \u00b7 Xinyue Li \u00b7 Gang Wang \u00b7 Mengxi Jia \u00b7 Ping Hu \u00b7 Zheng Zhu \u00b7 Xuelong Li",
        "Abstract": "Unmanned Aerial Vehicles (UAVs) are widely adopted across various fields, yet they raise significant privacy and safety concerns, demanding robust monitoring solutions. Existing anti-UAV methods primarily focus on position tracking but fail to capture UAV behavior and intent. To address this, we introduce a novel task\u2014UAV Tracking and Intent Understanding (UTIU)\u2014which aims to track UAVs while inferring and describing their motion states and intent for a more comprehensive monitoring approach. To tackle the task, we propose JTD-UAV, the first joint tracking, and intent description framework based on large language models. Our dual-branch architecture integrates UAV tracking with Visual Question Answering (VQA), allowing simultaneous localization and behavior description. To benchmark this task, we introduce the TDUAV dataset, the largest dataset for joint UAV tracking and intent understanding, featuring 1,328 challenging video sequences, over 163K annotated thermal frames, and 3K VQA pairs. Our benchmark demonstrates the effectiveness of JTD-UAV, and both the dataset and code will be publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces a novel task called UAV Tracking and Intent Understanding (UTIU), which aims to track UAVs while inferring and describing their motion states and intent. To address this, the authors propose JTD-UAV, a joint tracking and intent description framework leveraging large language models. The framework integrates UAV tracking with Visual Question Answering (VQA) for simultaneous localization and behavior description. A new dataset, TDUAV, is introduced to benchmark the task, featuring 1,328 video sequences, over 163K annotated thermal frames, and 3K VQA pairs. The framework demonstrates effectiveness in comprehensive UAV monitoring.",
        "Tags": [
            "Visual Tracking",
            "Vision-Language Models (VLMs)",
            "UAV Monitoring",
            "Visual Question Answering (VQA)",
            "Thermal Imaging"
        ]
    },
    {
        "Title": "DPC: Dual-Prompt Collaboration for Tuning Vision-Language Models",
        "Authors": "Haoyang Li \u00b7 Liang Wang \u00b7 Chao Wang \u00b7 Jing Jiang \u00b7 Yan Peng \u00b7 Guodong Long",
        "Abstract": "The Base-New Trade-off (BNT) problem universally exists during the optimization of CLIP-based prompt tuning, where continuous fine-tuning on base (target) classes leads to a simultaneous decrease of generalization ability on new (unseen) classes. Existing approaches attempt to regulate the prompt tuning process to balance BNT by appending constraints. However, imposed on the same target prompt, these constraints fail to fully avert the mutual exclusivity between the optimization directions for base and new. As a novel solution to this challenge, we propose the plug-and-play Dual-Prompt Collaboration (DPC) framework, the first that decoupling the optimization processes of base and new tasks at the prompt level. Specifically, we clone a learnable parallel prompt based on the backbone prompt, and introduce a variable Weighting-Decoupling framework to independently control the optimization directions of dual prompts specific to base or new tasks, thus avoiding the conflict in generalization. Meanwhile, we propose a Dynamic Hard Negative Optimizer, utilizing dual prompts to construct a more challenging optimization task on base classes for enhancement. For interpretability, we prove the feature channel invariance of the prompt vector during the optimization process, providing theoretical support for the Weighting-Decoupling of DPC. Extensive experiments on multiple backbones demonstrate that DPC can significantly improve base performance without introducing any external knowledge beyond the base classes, while maintaining generalization to new classes.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the Base-New Trade-off (BNT) problem in CLIP-based prompt tuning, where fine-tuning on base classes reduces generalization to new classes. Existing methods impose constraints on the same prompt, failing to resolve the conflict between base and new class optimization. The authors propose the Dual-Prompt Collaboration (DPC) framework, which decouples the optimization processes for base and new tasks at the prompt level. DPC introduces a parallel learnable prompt and a Weighting-Decoupling framework to independently control optimization directions, avoiding generalization conflicts. Additionally, a Dynamic Hard Negative Optimizer enhances base class optimization. Theoretical analysis supports the feature channel invariance of the prompt vector, validating the framework. Experiments show DPC improves base performance without external knowledge while maintaining generalization to new classes.",
        "Tags": [
            "Vision-Language Models",
            "Prompt Tuning",
            "Weighting-Decoupling Framework",
            "Dynamic Hard Negative Optimizer",
            "Feature Channel Invariance"
        ]
    },
    {
        "Title": "Towards Smart Point-and-Shoot Photography",
        "Authors": "Jiawan Li \u00b7 Fei Zhou \u00b7 Zhipeng Zhong \u00b7 Jiongzhi Lin \u00b7 Guoping Qiu",
        "Abstract": "Hundreds of millions of people routinely take photos using their smartphones as point and shoot (PAS) cameras, yet very few would have the photography skills to compose a good shot of a scene. While traditional PAS cameras have built-infunctions to ensure a photo is well focused and has the right brightness, they cannot tell the users how to compose the best shot of a scene. In this paper, we present a first of its kind smart point and shoot (SPAS) system to help users to take good photos. Our SPAS proposes to help users to compose a good shot of a scene by automatically guiding the users to adjust the camera pose live on the scene. We first constructed a large dataset containing $320K$ images with camera pose information from 4000 scenes. We then developed an innovative CLIP-based Composition Quality Assessment (CCQA) model to assign pseudo labels to these images. The CCQA introduces a unique learnable text embedding technique to learn continuous word embeddings capable of discerning subtle visual quality differences in the range covered by five levels of quality description words {$bad, poor, fair, good, perfect$}. And finally we have developed a camera pose adjustment model (CPAM) which first determines if the current view can be further improved and if so it outputs the adjust suggestion in the form of two camera pose adjustment angles. The two tasks of CPAM make decisions in a sequential manner and each involves different sets of training samples, we have developed a mixture-of-experts model with a gated loss function to train the CPAM in an end-to-end manner. We will present extensive results to demonstrate the performances of our SPAS system using publicly available image composition datasets.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a smart point-and-shoot (SPAS) system designed to assist users in composing better photographs by automatically guiding them to adjust the camera pose. The system leverages a large dataset of 320K images with camera pose information from 4000 scenes. A novel CLIP-based Composition Quality Assessment (CCQA) model is developed to assign pseudo labels to images, utilizing a unique learnable text embedding technique to discern subtle visual quality differences. Additionally, a camera pose adjustment model (CPAM) is created to determine if the current view can be improved and to suggest adjustments through two camera pose angles. The CPAM is trained using a mixture-of-experts model with a gated loss function, enabling end-to-end training.",
        "Tags": [
            "CLIP",
            "Image Quality Assessment",
            "Camera Pose Adjustment",
            "Learnable Text Embedding",
            "Mixture-of-Experts Model"
        ]
    },
    {
        "Title": "Spectral Informed Mamba for Robust Point Cloud Processing",
        "Authors": "Ali Bahri \u00b7 Moslem Yazdanpanah \u00b7 Mehrdad Noori \u00b7 Sahar Dastani \u00b7 Milad Cheraghalikhani \u00b7 David OSOWIECHI \u00b7 Gustavo Vargas Hakim \u00b7 Farzad Beizaee \u00b7 Ismail Ben Ayed \u00b7 Christian Desrosiers",
        "Abstract": "State Space Models (SSMs) have shown significant promise in Natural Language Processing (NLP) and, more recently, computer vision. This paper introduces a new methodology leveraging Mamba and Masked Autoencoder (MAE) networks for point cloud data in both supervised and self-supervised learning. We propose three key contributions to enhance Mamba's capability in processing complex point cloud structures. First, we exploit the spectrum of a graph Laplacian to capture patch connectivity, defining an isometry-invariant traversal order that is robust to viewpoints and better captures shape manifolds than traditional 3D grid-based traversals. Second, we adapt segmentation via a recursive patch partitioning strategy informed by Laplacian spectral components, allowing finer integration and segment analysis. Third, we address token placement in MAE for Mamba by restoring tokens to their original positions, which preserves essential order and improves learning. Extensive experiments demonstrate our approach\u2019s improvements in classification, segmentation, and few-shot tasks over state-of-the-art (SOTA) baselines.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a novel approach for robust point cloud processing by leveraging Mamba and Masked Autoencoder (MAE) networks. The methodology enhances Mamba's capability through three key contributions: (1) utilizing the spectrum of a graph Laplacian to define an isometry-invariant traversal order, improving robustness to viewpoints and shape manifold capture; (2) adapting segmentation via a recursive patch partitioning strategy informed by Laplacian spectral components for finer analysis; and (3) restoring tokens to their original positions in MAE for Mamba to preserve essential order and enhance learning. The approach demonstrates superior performance in classification, segmentation, and few-shot tasks compared to state-of-the-art baselines.",
        "Tags": [
            "Mamba",
            "3D Point Cloud",
            "Self-Supervised Learning",
            "Graph Laplacian Spectrum",
            "Isometry-Invariant Traversal",
            "Token Restoration"
        ]
    },
    {
        "Title": "Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level",
        "Authors": "Andong Deng \u00b7 Tongjia Chen \u00b7 Shoubin Yu \u00b7 Taojiannan Yang \u00b7 Lincoln Spencer \u00b7 Yapeng Tian \u00b7 Ajmal Mian \u00b7 Mohit Bansal \u00b7 Chen Chen",
        "Abstract": "In this paper, we introduce Motion-Grounded Video Reasoning, a new motionunderstanding task that requires generating visual answers (video segmentationmasks) according to the input question, and hence needs implicit spatiotemporalreasoning and grounding. This task extends existing spatiotemporal groundingwork focusing on explicit action/motion grounding, to a more general format byenabling implicit reasoning via questions. To facilitate the development of the newtask, we collect a large-scale dataset called GROUNDMORE, which comprises1,715 video clips, 249K object masks that are deliberately designed with 4 questiontypes (Causal, Sequential, Counterfactual, and Descriptive) for benchmarkingdeep and comprehensive motion reasoning abilities. GROUNDMORE uniquelyrequires models to generate visual answers, providing a more concrete and visuallyinterpretable response than plain texts. It evaluates models on both spatiotemporalgrounding and reasoning, fostering to address complex challenges in motion-relatedvideo reasoning, temporal perception, and pixel-level understanding. Furthermore,we introduce a novel baseline model named Motion-Grounded Video ReasoningAssistant (MORA). MORA incorporates the multimodal reasoning ability from theMultimodal LLM, the pixel-level perception capability from the grounding model(SAM), and the temporal perception ability from a lightweight localization head.MORA achieves respectable performance on GROUNDMORE outperforming thebest existing visual grounding baseline model by an average of 21.5% relatively.We hope this novel and challenging task will pave the way for future advancementsin robust and general motion understanding via video reasoning segmentation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Motion-Grounded Video Reasoning, a novel task that generates visual answers (video segmentation masks) based on input questions, requiring implicit spatiotemporal reasoning and grounding. The task extends existing spatiotemporal grounding by incorporating implicit reasoning through questions. A large-scale dataset, GROUNDMORE, is introduced, comprising 1,715 video clips and 249K object masks with four question types (Causal, Sequential, Counterfactual, and Descriptive) to benchmark deep motion reasoning abilities. The dataset uniquely requires models to produce visual answers, offering more interpretable responses than text. A baseline model, Motion-Grounded Video Reasoning Assistant (MORA), is proposed, combining multimodal reasoning from a Multimodal LLM, pixel-level perception from a grounding model (SAM), and temporal perception from a lightweight localization head. MORA outperforms existing visual grounding baselines by 21.5% on GROUNDMORE, demonstrating significant advancements in motion understanding and video reasoning segmentation.",
        "Tags": [
            "Video Understanding",
            "Semantic Segmentation",
            "Multimodal Learning",
            "Spatiotemporal Reasoning",
            "Pixel-Level Perception",
            "Video Segmentation"
        ]
    },
    {
        "Title": "Hierarchy-Aware Evaluation of Free-Form Predictions From Vision-And-Language Models",
        "Authors": "V\u00e9steinn Sn\u00e6bjarnarson \u00b7 Kevin Du \u00b7 Niklas Stoehr \u00b7 Serge Belongie \u00b7 Ryan Cotterell \u00b7 Nico Lang \u00b7 Stella Frank",
        "Abstract": "When a vision-and-language model (VLM) is prompted to identify an entity in an image, it may err on the side of caution and answer with \"tree\", instead of a more specific description such as \"Pine tree''. Traditional binary accuracy metrics cannot differentiate between wrong predictions and insufficiently specific ones. They also do not give partial credit for close answers: \"pine tree'' for a Norway Spruce should be better than \"cypress'', taxonomically speaking, but string matching-based similarity measures will reject both equally.To address this shortcoming, we propose a framework for evaluating open-ended text predictions against a taxonomic hierarchy,using measures of hierarchical precision and recall to measure the level of correctness and specificity of predictions.We first show that existing text similarity measures and accuracy-based evaluation metrics do not capture taxonomic similarity well. We then develop and compare different methods to map textual VLM predictions onto a taxonomy. This allows us to compute hierarchical similarity measures between the free-form outputs and the ground truth labels.Finally, we analyze modern VLMs on fine-grained visual classification tasks based on our taxonomic evaluation. We find that models respond differently to instructions prompting for more specific answers, with GPT4V responding most specifically and others showing a trade-off between hierarchical precision and recall.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the limitations of traditional binary accuracy metrics in evaluating vision-and-language models (VLMs) when predicting entities in images. These metrics fail to distinguish between incorrect predictions and those that are insufficiently specific, and do not provide partial credit for taxonomically close answers. To overcome this, the authors propose a framework that evaluates open-ended text predictions using a taxonomic hierarchy, employing hierarchical precision and recall to measure correctness and specificity. They demonstrate that existing text similarity measures and accuracy-based metrics poorly capture taxonomic similarity. The paper develops methods to map VLM predictions onto a taxonomy, enabling the computation of hierarchical similarity measures between predictions and ground truth labels. The framework is applied to analyze modern VLMs on fine-grained visual classification tasks, revealing differences in how models respond to prompts for more specific answers, with GPT4V showing the highest specificity.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Zero-Shot Learning",
            "Taxonomic Hierarchy",
            "Hierarchical Precision and Recall",
            "Fine-Grained Visual Classification"
        ]
    },
    {
        "Title": "JamMa: Ultra-lightweight Local Feature Matching with Joint Mamba",
        "Authors": "Xiaoyong Lu \u00b7 Songlin Du",
        "Abstract": "Existing state-of-the-art feature matchers capture long-range dependencies with Transformers but are hindered by high spatial complexity,leading to demanding training and high-latency inference.Striking a better balance between performance and efficiency remains a critical challenge in feature matching.Inspired by the linear complexity $\\mathcal{O}(N)$ of Mamba, we propose an ultra-lightweight Mamba-based matcher, named JamMa, which converges on a single GPU and achieves an impressive performance-efficiency balance in inference.To unlock the potential of Mamba for feature matching,we propose Joint Mamba with a scan-merge strategy named $\\textbf{JEGO}$, which enables:(1) $\\textbf{J}$oint scan of two images to achieve high-frequency mutual interaction, (2) $\\textbf{E}$fficient scan with skip steps to reduce sequence length, (3) $\\textbf{G}$lobal receptive field, and (4) $\\textbf{O}$mnidirectional feature representation.With the above properties, the JEGO strategy significantly outperforms the scan-merge strategies proposed in VMamba and EVMamba in the feature matching task.Compared to attention-based sparse and semi-dense matchers, JamMa demonstrates a notably superior balance between performance and efficiency,delivering better performance with less than $50$% of the parameters and FLOPs.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces JamMa, an ultra-lightweight Mamba-based feature matcher designed to address the high spatial complexity and inefficiency of existing Transformer-based feature matchers. JamMa leverages the linear complexity of Mamba to achieve a superior balance between performance and efficiency during inference. The authors propose a novel Joint Mamba approach with a scan-merge strategy called JEGO, which enables joint scanning of two images for high-frequency interaction, efficient scanning with skip steps, a global receptive field, and omnidirectional feature representation. This strategy outperforms existing scan-merge methods in feature matching tasks. JamMa demonstrates better performance with fewer parameters and computational requirements compared to attention-based matchers.",
        "Tags": [
            "Feature Matching",
            "Mamba",
            "Low-Level Vision",
            "Linear Complexity",
            "Scan-Merge Strategy",
            "Efficient Inference"
        ]
    },
    {
        "Title": "Exploring Sparse MoE in GANs for Text-conditioned Image Synthesis",
        "Authors": "Jiapeng Zhu \u00b7 Ceyuan Yang \u00b7 Kecheng Zheng \u00b7 Yinghao Xu \u00b7 Zifan Shi \u00b7 Yifei Zhang \u00b7 Qifeng Chen \u00b7 Yujun Shen",
        "Abstract": "Due to the difficulty in scaling up, generative adversarial networks (GANs) seem to be falling out of grace with the task of text-conditioned image synthesis. Sparsely activated mixture-of-experts (MoE) has recently been demonstrated as a valid solution to training large-scale models with limited resources. Inspired by this, we present Aurora, a GAN-based text-to-image generator that employs a collection of experts to learn feature processing, together with a sparse router to adaptively select the most suitable expert for each feature point. We adopt a two-stage training strategy, which first learns a base model at $64\\times64$ resolution followed by an upsampler to produce $512\\times512$ images. Trained with only public data, our approach encouragingly closes the performance gap between GANs and industry-level diffusion models, maintaining a fast inference speed. We will release the code and checkpoints to facilitate the community for more comprehensive studies of GANs.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Aurora, a GAN-based text-to-image generator that leverages sparsely activated mixture-of-experts (MoE) to enhance feature processing and selection. Aurora employs a two-stage training strategy, starting with a base model at $64\\times64$ resolution and then upsampling to $512\\times512$ images. The approach demonstrates competitive performance with industry-level diffusion models while maintaining fast inference speeds, using only publicly available data.",
        "Tags": [
            "Generative Adversarial Networks (GANs)",
            "Text-to-Image Generation",
            "Sparse Mixture-of-Experts (MoE)",
            "Two-Stage Training",
            "Fast Inference"
        ]
    },
    {
        "Title": "Dyn-HaMR: Recovering 4D Interacting Hand Motion from a Dynamic Camera",
        "Authors": "Zhengdi Yu \u00b7 Stefanos Zafeiriou \u00b7 Tolga Birdal",
        "Abstract": "We propose Dyn-HaMR, to the best of our knowledge, the first approach to reconstruct 4D global hand motion from monocular videos recorded by dynamic cameras in the wild. Reconstructing accurate 3D hand meshes from monocular videos is a crucial task for understanding human behaviour, with significant applications in augmented and virtual reality (AR/VR). However, existing methods for monocular hand reconstruction typically rely on a weak perspective camera model, which simulates hand motion within a limited camera frustum. As a result, these approaches struggle to recover the full 3D global trajectory and often produce noisy or incorrect depth estimations, particularly when the video is captured by dynamic or moving cameras, which is common in egocentric scenarios. Our \\name~consists of a multi-stage, multi-objective optimization pipeline, that factors in (i) simultaneous localization and mapping (SLAM) to robustly estimate relative camera motion, (ii) an interacting-hand prior for generative infilling and to refine the interaction dynamics, ensuring plausible recovery under (self-)occlusions, and (iii) hierarchical initialization through a combination of state-of-the-art hand tracking methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Dyn-HaMR is introduced as the first method to reconstruct 4D global hand motion from monocular videos captured by dynamic cameras in real-world settings. This approach addresses the limitations of existing methods that rely on a weak perspective camera model, which often results in inaccurate depth estimations and noisy trajectories, especially with moving cameras. Dyn-HaMR employs a multi-stage, multi-objective optimization pipeline that integrates simultaneous localization and mapping (SLAM) for camera motion estimation, an interacting-hand prior for refining interaction dynamics and handling occlusions, and hierarchical initialization using advanced hand tracking techniques.",
        "Tags": [
            "3D Human Pose Estimation",
            "3D Reconstruction",
            "Dynamic Camera",
            "Hand Motion Reconstruction",
            "SLAM Integration"
        ]
    },
    {
        "Title": "LLaVA-Critic: Learning to Evaluate Multimodal Models",
        "Authors": "Tianyi Xiong \u00b7 Xiyao Wang \u00b7 Dong Guo \u00b7 Qinghao Ye \u00b7 Haoqi Fan \u00b7 Quanquan Gu \u00b7 Heng Huang \u00b7 Chunyuan Li",
        "Abstract": "We introduce LLaVA-Critic, the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess performance across a wide range of multimodal tasks. LLaVA-Critic is trained using a high-quality critic instruction-following dataset that incorporates diverse evaluation criteria and scenarios. Our experiments demonstrate the model's effectiveness in two key areas: $(i)$ LMM-as-a-Judge, where LLaVA-Critic provides reliable evaluation scores, performing on par with or surpassing GPT models on multiple evaluation benchmarks; and $(ii)$ Preference Learning, where it generates reward signals for preference learning, enhancing model alignment capabilities. This work underscores the potential of open-source LMMs in self-critique and evaluation, setting the stage for future research into scalable, superhuman alignment feedback mechanisms for LMMs.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "LLaVA-Critic is introduced as the first open-source large multimodal model (LMM) designed to evaluate performance across diverse multimodal tasks. Trained on a high-quality critic instruction-following dataset, LLaVA-Critic excels in two areas: (i) LMM-as-a-Judge, where it provides reliable evaluation scores comparable to or better than GPT models on multiple benchmarks, and (ii) Preference Learning, where it generates reward signals to enhance model alignment capabilities. This work highlights the potential of open-source LMMs in self-critique and evaluation, paving the way for scalable, superhuman alignment feedback mechanisms.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Preference Learning",
            "Model Alignment",
            "Self-Critique",
            "Reward Signal Generation"
        ]
    },
    {
        "Title": "DiffLocks: Reconstructing 3D Hair from a Single Image using Diffusion Models",
        "Authors": "Radu Alexandru Rosu \u00b7 Keyu Wu \u00b7 Yao Feng \u00b7 Youyi Zheng \u00b7 Michael J. Black",
        "Abstract": "We address the task of reconstructing 3D hair geometry from a single image, which is challenging due to the diversity of hairstyles and the lack of paired image-to-3D hair data.Previous methods are primarily trained on synthetic data and cope with the limited amount of such data by using low-dimensional intermediate representations, such as guide strands and scalp-level embeddings, that require post-processing to decode, upsample, and add realism. These approaches fail to reconstruct detailed hair, struggle with curly hair, or are limited to handling only a few hairstyles. To overcome these limitations, we propose DiffLocks, a novel framework that enables detailed reconstruction of a wide variety of hairstyles directly from a single image.First, we address the lack of 3D hair data by automating the creation of the largest synthetic hair dataset to date, containing 40K hairstyles. Second, we leverage the synthetic hair dataset to learn an image-conditioned diffusion-transfomer model that reconstructs accurate 3D strands from a single frontal image. By using a pretrained image backbone, our method generalizes to in-the-wild images despite being trained only on synthetic data.Our diffusion model predicts a scalp texture map in which any point in the map contains the latent code for an individual hair strand. These codes are directly decoded to 3D strands without post-processing techniques.Representing individual strands, instead of guide strands, enables the transformer to model the detailed spatial structure of complex hairstyles.With this, DiffLocks can reconstruct highly curled hair, like afro hairstyles, from a single image for the first time.Qualitative and quantitative results demonstrate that DiffLocks outperforms exising state-of-the-art approaches. Data and code will be available for research.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces DiffLocks, a novel framework for reconstructing detailed 3D hair geometry from a single image, addressing challenges posed by the diversity of hairstyles and the scarcity of paired image-to-3D hair data. DiffLocks leverages a large synthetic hair dataset of 40K hairstyles and employs an image-conditioned diffusion-transformer model to predict 3D strands directly from a single frontal image. This approach eliminates the need for post-processing by representing individual strands through a scalp texture map, enabling the reconstruction of complex hairstyles, including highly curled hair like afro hairstyles, with unprecedented detail and accuracy. The framework demonstrates superior performance over existing methods, as evidenced by qualitative and quantitative results.",
        "Tags": [
            "3D Generation",
            "Diffusion Models",
            "Image-conditioned Diffusion",
            "Scalp Texture Map",
            "3D Strand Reconstruction"
        ]
    },
    {
        "Title": "Curriculum Direct Preference Optimization for Diffusion and Consistency Models",
        "Authors": "Florinel Croitoru \u00b7 Vlad Hondru \u00b7 Radu Tudor Ionescu \u00b7 Nicu Sebe \u00b7 Mubarak Shah",
        "Abstract": "Direct Preference Optimization (DPO) has been proposed as an effective and efficient alternative to reinforcement learning from human feedback (RLHF). In this paper, we propose a novel and enhanced version of DPO based on curriculum learning for text-to-image generation. Our method is divided into two training stages. First, a ranking of the  examples generated for each prompt is obtained by employing a reward model. Then, increasingly difficult pairs of examples are sampled and provided to a text-to-image generative (diffusion or consistency) model. Generated samples that are far apart in the ranking are considered to form easy pairs, while those that are close in the ranking form hard pairs. In other words, we use the rank difference between samples as a measure of difficulty. The sampled pairs are split into batches according to their difficulty levels, which are gradually used to train the generative model. Our approach, Curriculum DPO, is compared against state-of-the-art fine-tuning approaches on six benchmarks, outperforming the competing methods in terms of text alignment, aesthetics and human preference. Our code is available at https://anonymous.4open.science/r/Curriculum-DPO-757F.",
        "Link": "https://anonymous.4open.science/r/Curriculum-DPO-757F",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Curriculum Direct Preference Optimization (Curriculum DPO), an enhanced version of Direct Preference Optimization (DPO) for text-to-image generation using diffusion or consistency models. The method employs a two-stage training process: first, a reward model ranks generated examples for each prompt, and then, pairs of examples are sampled based on their ranking difference, with larger differences indicating easier pairs and smaller differences indicating harder pairs. These pairs are batched by difficulty and used to train the generative model progressively. Curriculum DPO outperforms state-of-the-art fine-tuning methods on six benchmarks, achieving superior results in text alignment, aesthetics, and human preference.",
        "Tags": [
            "Diffusion Models",
            "Text-to-Image Generation",
            "Curriculum Learning",
            "Human Preference Optimization",
            "Text Alignment"
        ]
    },
    {
        "Title": "Seeing Speech and Sound: Distinguishing and Locating Audio Sources in Visual Scenes",
        "Authors": "Hyeonggon Ryu \u00b7 Seongyu Kim \u00b7 Joon Chung \u00b7 Arda Senocak",
        "Abstract": "We present a unified model capable of simultaneously grounding both spoken language and non-speech sounds within a visual scene, addressing key limitations in current audio-visual grounding models. Existing approaches are typically limited to handling either speech or non-speech sounds independently, or at best, together but sequentially without mixing. This limitation prevents them from capturing the complexity of real-world audio sources that are often mixed. Our approach introduces a \"mix-and-separate\" framework with audio-visual alignment objectives that jointly learn correspondence and disentanglement using mixed audio. Through these objectives, our model learns to produce distinct embeddings for each audio type, enabling effective disentanglement and grounding across mixed audio sources.Additionally, we created a new dataset to evaluate simultaneous grounding of mixed audio sources, demonstrating that our model outperforms prior methods. Our approach also achieves state-of-the-art performance in standard segmentation and cross-modal retrieval tasks, highlighting the benefits of our mix-and-separate approach.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces a unified model that simultaneously grounds spoken language and non-speech sounds within visual scenes, overcoming limitations in existing audio-visual grounding models. These models typically handle speech and non-speech sounds separately or sequentially, failing to address mixed real-world audio sources. The proposed 'mix-and-separate' framework employs audio-visual alignment objectives to jointly learn correspondence and disentanglement from mixed audio, producing distinct embeddings for each audio type. This enables effective disentanglement and grounding across mixed audio sources. The model is evaluated on a new dataset for mixed audio grounding, outperforming prior methods and achieving state-of-the-art results in segmentation and cross-modal retrieval tasks.",
        "Tags": [
            "Multimodal Learning",
            "Audio-Visual Grounding",
            "Mix-and-Separate Framework",
            "Audio-Visual Alignment",
            "Disentangled Embeddings"
        ]
    },
    {
        "Title": "FIRE: Robust Detection of Diffusion-Generated Images via Frequency-Guided Reconstruction Error",
        "Authors": "Beilin Chu \u00b7 Xuan Xu \u00b7 Xin Wang \u00b7 Yufei Zhang \u00b7 Weike You \u00b7 Linna Zhou",
        "Abstract": "The rapid advancement of diffusion models has significantly improved high-quality image generation, making generated content increasingly challenging to distinguish from real images and raising concerns about potential misuse. In this paper, we observe that diffusion models struggle to accurately reconstruct mid-band frequency information in real images, suggesting the limitation could serve as a cue for detecting diffusion model generated images. Motivated by this observation, we propose a novel method called $\\textbf{F}$requency-gu$\\textbf{I}$ded $\\textbf{R}$econstruction $\\textbf{E}$rror (FIRE), which, to the best of our knowledge, is the first to investigate the influence of frequency decomposition on reconstruction error. FIRE assesses the variation in reconstruction error before and after the frequency decomposition, offering a robust method for identifying diffusion model generated images. Extensive experiments show that FIRE generalizes effectively to unseen diffusion models and maintains robustness against diverse perturbations.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the challenge of distinguishing diffusion-generated images from real ones due to the high quality of generated content. It introduces a novel method, FIRE (Frequency-guided Reconstruction Error), which leverages the observation that diffusion models inaccurately reconstruct mid-band frequency information in real images. FIRE evaluates the reconstruction error variation before and after frequency decomposition, providing a robust detection mechanism for diffusion-generated images. The method demonstrates effectiveness across unseen diffusion models and maintains robustness against various perturbations.",
        "Tags": [
            "Diffusion Models",
            "Image Quality Assessment",
            "Frequency Decomposition",
            "Reconstruction Error",
            "Image Forensics"
        ]
    },
    {
        "Title": "Reasoning in visual navigation of end-to-end trained agents: a dynamical systems approach",
        "Authors": "Steeven JANNY \u00b7 Herv\u00e9 Poirier \u00b7 Leonid Antsfeld \u00b7 Guillaume Bono \u00b7 Gianluca Monaci \u00b7 Boris Chidlovskii \u00b7 Francesco Giuliari \u00b7 Alessio Del Bue \u00b7 Christian Wolf",
        "Abstract": "Progress in Embodied AI has made it possible for end-to-end-trained agents to navigate in photo-realistic environments with high-level reasoning and zero-shot or language-conditioned behavior, but evaluations and benchmarks are still dominated by simulation. In this work, we focus on the fine-grained behavior of fast-moving real robots and present a large-scale experimental study involving \\numepisodes{} navigation episodes in a real environment with a physical robot, where we analyze the type of reasoning emerging from end-to-end training. In particular, we study the presence of realistic dynamics which the agent learned for open-loop forecasting, and their interplay with sensing. We analyze the way the agent uses latent memory to hold elements of the scene structure and information gathered during exploration. We probe the planning capabilities of the agent, and find in its memory evidence for somewhat precise plans over a limited horizon. Furthermore, we show in a post-hoc analysis that the value function learned by the agent relates to long-term planning. Put together, our experiments paint a new picture on how using tools from computer vision and sequential decision making have led to new capabilities in robotics and control. An interactive tool is available at https://visual-navigation-reasoning.github.io",
        "Link": "https://visual-navigation-reasoning.github.io",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This study investigates the fine-grained behavior of end-to-end-trained agents in real-world navigation tasks using a physical robot. The research focuses on the reasoning capabilities emerging from such training, particularly the agent's ability to learn realistic dynamics for open-loop forecasting and its interplay with sensing. The analysis reveals how the agent utilizes latent memory to retain scene structure and exploration information, demonstrating limited but precise planning capabilities. Additionally, the value function learned by the agent is shown to relate to long-term planning. The findings highlight advancements in robotics and control through the integration of computer vision and sequential decision-making tools.",
        "Tags": [
            "Embodied AI",
            "Visual Navigation",
            "Latent Memory",
            "Value Function",
            "Real-World Robotics"
        ]
    },
    {
        "Title": "SP3D: Boosting Sparsely-Supervised 3D Object Detection via Accurate Cross-Modal Semantic Prompts",
        "Authors": "Shijia Zhao \u00b7 Qiming Xia \u00b7 Xusheng Guo \u00b7 Pufan Zou \u00b7 Maoji Zheng \u00b7 Hai Wu \u00b7 Chenglu Wen \u00b7 Cheng Wang",
        "Abstract": "Recently, sparsely-supervised 3D object detection has gained great attention, achieving performance close to fully-supervised 3D objectors while requiring only a few annotated instances. Nevertheless, these methods suffer challenges when accurate labels are extremely absent. In this paper, we propose a boosting strategy, termed SP3D, explicitly utilizing the cross-modal semantic prompts generated from Large Multimodal Models (LMMs) to boost the 3D detector with robust feature discrimination capability under sparse annotation settings. Specifically, we first develop a Confident Points Semantic Transfer (CPST) module that generates accurate cross-modal semantic prompts through boundary-constrained center cluster selection. Based on these accurate semantic prompts, which we treat as seed points, we introduce a Dynamic Cluster Pseudo-label Generation (DCPG) module to yield pseudo-supervision signals from the geometry shape of multi-scale neighbor points. Additionally, we design a Distribution Shape score (DS score) that chooses high-quality supervision signals for the initial training of the 3D detector. Experiments on the KITTI dataset and Waymo Open Dataset (WOD) have validated that SP3D can enhance the performance of sparsely supervised detectors by a large margin under meager labeling conditions.Moreover, we verified SP3D in the zero-shot setting, where its performance exceeded that of the state-of-the-art methods. The code will be made publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces SP3D, a strategy to enhance sparsely-supervised 3D object detection by leveraging cross-modal semantic prompts from Large Multimodal Models (LMMs). SP3D includes a Confident Points Semantic Transfer (CPST) module for generating accurate semantic prompts and a Dynamic Cluster Pseudo-label Generation (DCPG) module for creating pseudo-supervision signals. A Distribution Shape score (DS score) is also designed to select high-quality supervision signals. SP3D significantly improves detector performance under sparse annotation and zero-shot settings, as validated on the KITTI and Waymo Open datasets.",
        "Tags": [
            "3D Object Detection",
            "Sparsely-Supervised Learning",
            "Cross-Modal Learning",
            "Pseudo-label Generation",
            "Zero-Shot Learning"
        ]
    },
    {
        "Title": "DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos",
        "Authors": "Wenbo Hu \u00b7 Xiangjun Gao \u00b7 Xiaoyu Li \u00b7 Sijie Zhao \u00b7 Xiaodong Cun \u00b7 Yong Zhang \u00b7 Long Quan \u00b7 Ying Shan",
        "Abstract": "Estimating video depth in open-world scenarios is challenging due to the diversity of videos in appearance, content motion, camera movement, and length. We present DepthCrafter, an innovative method for generating temporally consistent long depth sequences with intricate details for open-world videos, without requiring any supplementary information such as camera poses or optical flow. The generalization ability to open-world videos is achieved by training the video-to-depth model from a pre-trained image-to-video diffusion model, through our meticulously designed three-stage training strategy. Our training approach enables the model to generate depth sequences with variable lengths at one time, up to 110 frames, and harvest both precise depth details and rich content diversity from realistic and synthetic datasets. We also propose an inference strategy that can process extremely long videos through segment-wise estimation and seamless stitching. Comprehensive evaluations on multiple datasets reveal that DepthCrafter achieves state-of-the-art performance in open-world video depth estimation under zero-shot settings. Furthermore, DepthCrafter facilitates various downstream applications, including depth-based visual effects and conditional video generation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "DepthCrafter introduces a novel method for generating temporally consistent long depth sequences for open-world videos without requiring additional information like camera poses or optical flow. The approach leverages a pre-trained image-to-video diffusion model, refined through a three-stage training strategy, to produce depth sequences of up to 110 frames with intricate details and content diversity. An inference strategy enables processing of extremely long videos via segment-wise estimation and seamless stitching. Evaluations show state-of-the-art performance in zero-shot video depth estimation, with applications in depth-based visual effects and conditional video generation.",
        "Tags": [
            "Depth Estimation",
            "Video Understanding",
            "Diffusion Models",
            "Zero-Shot Learning",
            "Temporal Consistency"
        ]
    },
    {
        "Title": "MV-SSM: Multi-View State Space Modeling for 3D Human Pose Estimation",
        "Authors": "Aviral Chharia \u00b7 Wenbo Gou \u00b7 Haoye Dong",
        "Abstract": "Though single-view 3D human pose estimation has gained much attention, 3D multi-view multi-person pose estimation faces several challenges including the presence of occlusions and generalizability to new camera arrangements or scenarios. Existing transformer-based approaches often struggle to accurately model joint spatial sequences, especially in occluded scenarios. To address this, we present a novel Multi-View State Space Modeling framework, named MV-SSM for robustly reconstructing 3D human poses, by explicitly modeling the joint spatial sequence at two distinct levels: the feature level from multi-view images and the joint level of the person. Specifically, we propose a Projective State Space (PSS) block to learn the joint spatial sequences using state space modeling. Furthermore, we modify Mamba's unidirectional scanning into an effective Grid token-guided Bidirectional scan (GTBS) which is integral to the PSS block. Experiments on multiple challenging benchmarks demonstrate that MV-SSM archives highly accurate 3D pose estimation and is generalizable across the number of cameras (+10.8 on AP25 on the challenging 3 camera setting in CMU Panoptic), varying camera arrangements (+7.0 on AP25), and cross-datasets (+15.3 PCP on Campus A1), significantly outperforming SOTAs. The code has been submitted and will be open-sourced with model weights upon acceptance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces MV-SSM, a Multi-View State Space Modeling framework designed to address challenges in 3D multi-view multi-person pose estimation, such as occlusions and adaptability to new camera setups. The framework models joint spatial sequences at both feature and joint levels using a novel Projective State Space (PSS) block and a Grid token-guided Bidirectional scan (GTBS) technique. MV-SSM demonstrates superior performance in accuracy and generalizability across various benchmarks and camera configurations, significantly outperforming existing state-of-the-art methods.",
        "Tags": [
            "3D Human Pose Estimation",
            "Multimodal Learning",
            "State Space Modeling",
            "Bidirectional Scanning",
            "Occlusion Handling"
        ]
    },
    {
        "Title": "Difference Inversion : Interpolate and Isolate the Difference with Token Consistency for Image Analogy Generation",
        "Authors": "Hyunsoo Kim \u00b7 Donghyun Kim \u00b7 Suhyun Kim",
        "Abstract": "How can we generate an image $B^\\prime$ that satisfies $A:A^\\prime::B:B^\\prime$, given the input images $A$,$A^\\prime$ and $B$?Recent works have tackled this challenge through approaches like visual in-context learning or visual instruction. However, these methods are typically limited to specific models (\\eg InstructPix2Pix. Inpainting models) rather than general diffusion models (\\eg Stable Diffusion, SDXL). This dependency may lead to inherited biases or lower editing capabilities. In this paper, we propose Difference Inversion, a method that isolates only the difference from $A$ and $A^\\prime$ and applies it to $B$ to generate a plausible $B^\\prime$. To address model dependency, it is crucial to structure prompts in the form of a \"Full Prompt\" suitable for input to stable diffusion models, rather than using an \"Instruction Prompt\". To this end, we accurately extract the Difference between $A$ and $A^\\prime$ and combine it with the prompt of $B$, enabling a plug-and-play application of the difference. To extract a precise difference, we first identify it through 1) Delta Interpolation. Additionally, to ensure accurate training, we propose the 2) Token Consistency Loss and 3) Zero Initialization of Token Embeddings. Our extensive experiments demonstrate that Difference Inversion outperforms existing baselines both quantitatively and qualitatively, indicating its ability to generate more feasible $B^\\prime$ in a model-agnostic manner.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Difference Inversion, a novel method for generating an image $B^\\prime$ that satisfies the analogy $A:A^\\prime::B:B^\\prime$ using input images $A$, $A^\\prime$, and $B$. Unlike previous approaches that rely on specific models, Difference Inversion is designed to work with general diffusion models like Stable Diffusion and SDXL. The method isolates the difference between $A$ and $A^\\prime$ and applies it to $B$ to create $B^\\prime$. Key innovations include Delta Interpolation for identifying differences, Token Consistency Loss for accurate training, and Zero Initialization of Token Embeddings. The approach demonstrates superior performance over existing methods, offering a model-agnostic solution for image analogy generation.",
        "Tags": [
            "Diffusion Models",
            "Image Editing",
            "Delta Interpolation",
            "Token Consistency Loss",
            "Zero Initialization"
        ]
    },
    {
        "Title": "Attention IoU: Examining Biases in CelebA using Attention Maps",
        "Authors": "Aaron Serianni \u00b7 Tyler Zhu \u00b7 Olga Russakovsky \u00b7 Vikram V. Ramaswamy",
        "Abstract": "Computer vision models have been shown to exhibit and amplify biases across a wide array of datasets and tasks. Existing methods for quantifying bias in classification models primarily focus on dataset distribution and model performance on subgroups, overlooking the internal workings of a model. We introduce the Attention-IoU (Attention Intersection over Union) metric and related scores, which use attention maps to reveal biases within a model's internal representations and identify image features potentially causing the biases. First, we validate Attention-loU on the synthetic Waterbirds dataset, showing that the metric accurately measures model bias. We then analyze the CelebA dataset, finding that Attention-loU uncovers correlations beyond accuracy disparities. Through an investigation of individual attributes through the protected attribute of Male, we examine the distinct ways biases are represented in CelebA. Lastly, by subsampling the training set to change attribute correlations, we demonstrate that Attention-loU reveals potential confounding variables not present in dataset labels.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the issue of biases in computer vision models, particularly focusing on the internal representations of models rather than just dataset distribution or performance disparities. The authors introduce the Attention-IoU metric, which utilizes attention maps to uncover biases within a model's internal representations and identify image features that may contribute to these biases. The metric is validated on the synthetic Waterbirds dataset and then applied to the CelebA dataset, where it reveals correlations beyond accuracy disparities. The study also investigates biases related to the 'Male' attribute in CelebA and demonstrates how subsampling the training set can reveal confounding variables not captured by dataset labels.",
        "Tags": [
            "Bias Detection",
            "Attention Maps",
            "Attention-IoU Metric",
            "Model Bias Analysis",
            "Protected Attribute Analysis"
        ]
    },
    {
        "Title": "AnySat: An Earth Observation Model for Any Modalities, Resolutions, and Scales",
        "Authors": "Guillaume Astruc \u00b7 Nicolas Gonthier \u00b7 Clement Mallet \u00b7 Loic Landrieu",
        "Abstract": "Geospatial models must adapt to the diversity of Earth observation data in terms of resolutions, scales, and modalities. However, existing approaches expect fixed input configurations, which limits their practical applicability. We propose AnySat, a multimodal model based on joint embedding predictive architecture (JEPA) and resolution-adaptive spatial encoders, allowing us to train a single model on highly heterogeneous data in a self-supervised manner. To demonstrate the advantages of this unified approach, we compile GeoPlex, a collection of $5$ multimodal datasets with varying characteristics and $11$ distinct sensors. We then train a single powerful model on these diverse datasets simultaneously. Once fine-tuned, we achieve better or near state-of-the-art results on the datasets of GeoPlex and $3$ additional ones for $4$ environment monitoring tasks: land cover mapping, crop type classification, change detection, and forest analysis.  We will release all codes, models, and data.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces AnySat, a geospatial model designed to handle diverse Earth observation data across resolutions, scales, and modalities. Unlike existing approaches that require fixed input configurations, AnySat leverages a joint embedding predictive architecture (JEPA) and resolution-adaptive spatial encoders, enabling self-supervised training on heterogeneous data. The model is trained on GeoPlex, a collection of 5 multimodal datasets with 11 distinct sensors, and achieves state-of-the-art or near state-of-the-art results on 4 environment monitoring tasks: land cover mapping, crop type classification, change detection, and forest analysis.",
        "Tags": [
            "Multimodal Learning",
            "Remote Sensing Image Analysis",
            "Self-Supervised Learning",
            "Joint Embedding Predictive Architecture",
            "Resolution-Adaptive Encoders"
        ]
    },
    {
        "Title": "From Words to Structured Visuals: A Benchmark and Framework for Text-to-Diagram Generation and Editing",
        "Authors": "Jingxuan Wei \u00b7 Cheng Tan \u00b7 Qi Chen \u00b7 Gaowei Wu \u00b7 Siyuan Li \u00b7 Zhangyang Gao \u00b7 Linzhuang Sun \u00b7 Bihui Yu \u00b7 Ruifeng Guo",
        "Abstract": "We introduce the task of text-to-diagram generation, which focuses on creating structured visual representations directly from textual descriptions. Existing approaches in text-to-image and text-to-code generation lack the logical organization and flexibility needed to produce accurate, editable diagrams, often resulting in outputs that are either unstructured or difficult to modify. To address this gap, we introduce DiagramGenBenchmark, a comprehensive evaluation framework encompassing eight distinct diagram categories, including flowcharts, model architecture diagrams, and mind maps. Additionally, we present DiagramAgent, an innovative framework with four core modules\u2014Plan Agent, Code Agent, Check Agent, and Diagram-to-Code Agent\u2014designed to facilitate both the generation and refinement of complex diagrams. Our extensive experiments, which combine objective metrics with human evaluations, demonstrate that DiagramAgent significantly outperforms existing baseline models in terms of accuracy, structural coherence, and modifiability. This work not only establishes a foundational benchmark for the text-to-diagram generation task but also introduces a powerful toolset to advance research and applications in this emerging area.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces the task of text-to-diagram generation, which aims to create structured visual representations from textual descriptions. Existing methods in text-to-image and text-to-code generation often fail to produce logically organized and editable diagrams. To address this, the authors propose DiagramGenBenchmark, a comprehensive evaluation framework covering eight diagram categories, and DiagramAgent, a framework with four core modules designed for generating and refining complex diagrams. The framework demonstrates superior performance in accuracy, structural coherence, and modifiability compared to baseline models, establishing a foundational benchmark and advancing research in this area.",
        "Tags": [
            "Text-to-Image Generation",
            "Multimodal Learning",
            "Diagram Generation",
            "Structured Visuals",
            "Diagram Editing"
        ]
    },
    {
        "Title": "VODiff: Controlling Object Visibility Order in Text-to-Image Generation",
        "Authors": "Dong Liang \u00b7 Jinyuan Jia \u00b7 Yuhao Liu \u00b7 Zhanghan Ke \u00b7 Hongbo Fu \u00b7 Rynson W.H. Lau",
        "Abstract": "Recent advancements in diffusion models have significantly enhanced the performance of text-to-image models in image synthesis. To enable control over the the spatial locations of the generated objects,diffusion-based methods typically utilizeobject layout as an auxiliary input. However, we observe that this approach treats all objects as being on the same layer and neglect their visibility order, leading to the synthesis of overlapping objects with incorrect occlusions.To address this limitation, we introduce in this paper a new training-free framework that considers object visibility order explicitly and allows users to place overlapping objects in a stack of layers. Our framework consists of two visibility-based designs. First, we propose a novel Sequential Denoising Process (SDP) to divide the whole image generation into multiple stages for different objects, each stage primarily focuses on an object. Second, we propose a novel Visibility-Order-Aware (VOA) Loss to transform the layout and occlusion constraints into an attention map optimization process to improve the accuracy of synthesizing object occlusions in complex scenes. By merging these two novel components, our framework, dubbed VODiff, enables the generation of photorealistic images that satisfy user-specified spatial constraints and object occlusion relationships. In addition, we introduce VOBench, a diverse benchmark dataset containing 200 curated samples, each with a reference image, text prompts, object visibility orders and layout maps. We conduct extensive evaluations on this dataset to demonstrate the superiority of our approach.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Recent advancements in diffusion models have significantly improved text-to-image generation, but existing methods often fail to account for object visibility order, leading to incorrect occlusions. To address this, we introduce VODiff, a training-free framework that explicitly considers object visibility order. VODiff employs a Sequential Denoising Process (SDP) to generate objects in stages and a Visibility-Order-Aware (VOA) Loss to optimize attention maps for accurate occlusion synthesis. The framework generates photorealistic images adhering to user-specified spatial and occlusion constraints. Additionally, we present VOBench, a benchmark dataset with 200 samples featuring text prompts, object visibility orders, and layout maps, to validate the effectiveness of our approach.",
        "Tags": [
            "Diffusion Models",
            "Text-to-Image Generation",
            "Object Visibility Order",
            "Attention Map Optimization",
            "Training-Free Framework"
        ]
    },
    {
        "Title": "AnyCam: Learning to Recover Camera Poses and Intrinsics from Casual Videos",
        "Authors": "Felix Wimbauer \u00b7 Weirong Chen \u00b7 Dominik Muhle \u00b7 Christian Rupprecht \u00b7 Daniel Cremers",
        "Abstract": "Estimating camera motion and intrinsics from casual videos is a core challenge in computer vision. Traditional bundle-adjustment based methods, such as SfM and SLAM, struggle to perform reliably on arbitrary data. Although specialized SfM approaches have been developed for handling dynamic scenes, they either require intrinsics or computationally expensive test-time optimization and often fall short in performance. Recently, methods like Dust3r have reformulated the SfM problem in a more data-driven way. While such techniques show promising results, they are still 1) not robust towards dynamic objects and 2) require labeled data for supervised training.As an alternative, we propose AnyCam, a fast transformer model that directly estimates camera poses and intrinsics from a dynamic video sequence in feed-forward fashion. Our intuition is that such a network can learn strong priors over realistic camera motions. To scale up our training, we rely on an uncertainty-based loss formulation and pre-trained depth and flow networks instead of motion or trajectory supervision. This allows us to use diverse, unlabelled video datasets obtained mostly from YouTube. Additionally, we ensure that the predicted trajectory does not accumulate drift over time through a lightweight trajectory refinement step. We test AnyCam on established datasets, where it delivers accurate camera poses and intrinsics both qualitatively and quantitatively. Furthermore, even with trajectory refinement, AnyCam is significantly faster than existing works for SfM in dynamic settings. Finally, by combining camera information, uncertainty, and depth, our model can produce high-quality 4D pointclouds in a feed-forward fashion.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces AnyCam, a transformer-based model designed to estimate camera poses and intrinsics directly from dynamic video sequences without requiring labeled data or computationally expensive optimization. Unlike traditional methods such as SfM and SLAM, AnyCam leverages pre-trained depth and flow networks and an uncertainty-based loss formulation to train on diverse, unlabelled video datasets. The model incorporates a lightweight trajectory refinement step to prevent drift accumulation, ensuring accurate and fast performance. Evaluations on established datasets demonstrate that AnyCam produces high-quality 4D pointclouds and outperforms existing methods in dynamic settings.",
        "Tags": [
            "3D Reconstruction",
            "Camera Pose Estimation",
            "Transformer Model",
            "Uncertainty-Based Loss",
            "Trajectory Refinement"
        ]
    },
    {
        "Title": "vesselFM: A Foundation Model for Universal 3D Blood Vessel Segmentation",
        "Authors": "Bastian Wittmann \u00b7 Yannick Wattenberg \u00b7 Tamaz Amiranashvili \u00b7 Suprosanna Shit \u00b7 Bjoern Menze",
        "Abstract": "Segmenting 3D blood vessels is a critical yet challenging task in medical image analysis. This is due to significant imaging modality-specific variations in artifacts, vascular patterns and scales, signal-to-noise ratios, and background tissues. These variations, along with domain gaps arising from varying imaging protocols, limit the generalization of existing supervised learning-based methods, requiring tedious voxel-level annotations for each dataset separately. While foundation models promise to alleviate this limitation, they typically fail to generalize to the task of blood vessel segmentation, posing a unique, complex problem. In this work, we present vesselFM, a foundation model designed specifically for the broad task of 3D blood vessel segmentation. Unlike previous models, vesselFM can effortlessly generalize to unseen domains. To achieve zero-shot generalization, we train vesselFM on three heterogeneous data sources: a large, curated annotated dataset, data generated by a domain randomization scheme, and data sampled from a flow matching-based generative model. Extensive evaluations show that vesselFM outperforms state-of-the-art medical image segmentation foundation models across four (pre-)clinically relevant imaging modalities in zero-, one-, and few-shot scenarios, therefore providing a universal solution for 3D blood vessel segmentation.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces vesselFM, a foundation model tailored for universal 3D blood vessel segmentation, addressing the challenges posed by imaging modality-specific variations and domain gaps. Unlike existing methods, vesselFM achieves zero-shot generalization by training on a combination of a large annotated dataset, domain-randomized data, and data from a flow matching-based generative model. It demonstrates superior performance over state-of-the-art models across multiple imaging modalities in zero-, one-, and few-shot scenarios, offering a universal solution for 3D blood vessel segmentation.",
        "Tags": [
            "Medical Image Segmentation",
            "3D Semantic Segmentation",
            "Zero-Shot Learning",
            "Domain Randomization",
            "Flow Matching"
        ]
    },
    {
        "Title": "DistinctAD: Distinctive Audio Description Generation in Contexts",
        "Authors": "Bo Fang \u00b7 Wenhao Wu \u00b7 Qiangqiang Wu \u00b7 YuXin Song \u00b7 Antoni B. Chan",
        "Abstract": "Audio Descriptions (ADs) aim to provide a narration of a movie in text form, describing non-dialogue-related narratives, such as characters, actions, or scene establishment. Automatic generation of ADs remains challenging due to: i) the domain gap between movie-AD data and existing data used to train vision-language models, and ii) the issue of contextual redundancy arising from highly similar neighboring visual clips in a long movie. In this work, we propose DistinctAD, a novel two-stage framework for generating ADs that emphasize distinctiveness to produce better narratives. To address the domain gap, we introduce a CLIP-AD adaptation strategy that does not require additional AD corpora, enabling more effective alignment between movie and AD modalities at both global and fine-grained levels. In Stage-II, DistinctAD incorporates two key innovations: (i) a Contextual Expectation-Maximization Attention (EMA) module that reduces redundancy by extracting common bases from consecutive video clips, and (ii) an explicit distinctive word prediction loss that filters out repeated words in the context, ensuring the prediction of unique terms specific to the current AD. Comprehensive evaluations on MAD-Eval, CMD-AD, and TV-AD benchmarks demonstrate the superiority of DistinctAD, with the model consistently outperforming baselines, particularly in Recall@k/N, highlighting its effectiveness in producing high-quality, distinctive ADs.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces DistinctAD, a two-stage framework designed to generate distinctive Audio Descriptions (ADs) for movies, addressing challenges such as the domain gap between movie-AD data and existing vision-language model training data, and contextual redundancy in long movies. The first stage employs a CLIP-AD adaptation strategy for effective modality alignment without additional AD corpora. The second stage features a Contextual Expectation-Maximization Attention (EMA) module to reduce redundancy and a distinctive word prediction loss to ensure unique term prediction. Evaluations on benchmarks like MAD-Eval, CMD-AD, and TV-AD show DistinctAD's superior performance in generating high-quality, distinctive ADs.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Multimodal Learning",
            "Contextual Redundancy Reduction",
            "Distinctive Word Prediction",
            "CLIP-AD Adaptation"
        ]
    },
    {
        "Title": "HOT: Hadamard-based Optimized Training",
        "Authors": "Seonggon Kim \u00b7 Juncheol Shin \u00b7 Seung-taek Woo \u00b7 Eunhyeok Park",
        "Abstract": "It has become increasingly important to optimize backpropagation to reduce memory usage and computational overhead. Achieving this goal is highly challenging, as multiple objectives must be considered jointly while maintaining training quality. In this paper, we focus on matrix multiplication, which accounts for the largest portion of training costs, and analyze its backpropagation in detail to identify lightweight techniques that offer the best benefits. Based on this analysis, we introduce a novel method, Hadamard-based Optimized Training (HOT). In this approach, we apply Hadamard-based optimizations, such as Hadamard quantization and Hadamard low-rank approximation, selectively and with awareness of the suitability of each optimization for different backward paths. Additionally, we introduce two enhancements: activation buffer compression and layer-wise quantizer selection. Our extensive analysis shows that HOT achieves up to 75\\% memory savings and a 2.6$\\times$ acceleration on real GPUs, with negligible accuracy loss compared to FP32 precision.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Hadamard-based Optimized Training (HOT), a novel method aimed at optimizing backpropagation to reduce memory usage and computational overhead. By focusing on matrix multiplication, which constitutes the largest portion of training costs, the authors analyze backpropagation in detail to identify lightweight techniques. HOT employs Hadamard-based optimizations, including Hadamard quantization and low-rank approximation, tailored to different backward paths. The method also incorporates activation buffer compression and layer-wise quantizer selection. The results demonstrate significant improvements, with up to 75% memory savings and a 2.6\u00d7 acceleration on real GPUs, while maintaining negligible accuracy loss compared to FP32 precision.",
        "Tags": [
            "Backbone",
            "Data Augmentation",
            "Hadamard Quantization",
            "Low-Rank Approximation",
            "Activation Buffer Compression"
        ]
    },
    {
        "Title": "Omnia de EgoTempo: Benchmarking Temporal Understanding of Multi-Modal LLMs in Egocentric Videos",
        "Authors": "Chiara Plizzari \u00b7 Alessio Tonioni \u00b7 Yongqin Xian \u00b7 Achin Kulshrestha \u00b7 Federico Tombari",
        "Abstract": "Understanding fine-grained temporal dynamics is crucial in egocentric videos, where continuous streams capture frequent, close-up interactions with objects. In this work, we bring to light that current egocentric video question-answering datasets often include questions that can be answered using only few frames or commonsense reasoning, without being necessarily grounded in the actual video. Our analysis shows that state-of-the-art Multi-Modal Large Language Models (MLLMs) on these benchmarks achieve remarkably high performance using just text or a single frame as input.To address these limitations, we introduce EgoTempo, a dataset specifically designed to evaluate temporal understanding in the egocentric domain. EgoTempo emphasizes tasks that require integrating information across the entire video, ensuring that models would need to rely on temporal patterns rather than static cues or pre-existing knowledge. Extensive experiments on EgoTempo show that current MLLMs still fall short in temporal reasoning on egocentric videos, and thus we hope EgoTempo will catalyze new research in the field and inspire models that better capture the complexity of temporal dynamics in egocentric settings.The dataset will be made publicly available upon acceptance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper highlights the limitations of current egocentric video question-answering datasets, which often allow models to answer questions using minimal frames or commonsense reasoning without grounding in the actual video. To address this, the authors introduce EgoTempo, a dataset designed to evaluate temporal understanding in egocentric videos by requiring integration of information across the entire video. Experiments reveal that state-of-the-art Multi-Modal Large Language Models (MLLMs) still struggle with temporal reasoning in this domain, underscoring the need for improved models that better capture temporal dynamics in egocentric settings.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Video Understanding",
            "Temporal Reasoning",
            "Egocentric Video Analysis",
            "Dataset Design"
        ]
    },
    {
        "Title": "FrugalNeRF: Fast Convergence for Extreme Few-shot Novel View Synthesis without Learned Priors",
        "Authors": "Chin-Yang Lin \u00b7 Chung-Ho Wu \u00b7 Changhan Yeh \u00b7 Shih Han Yen \u00b7 Cheng Sun \u00b7 Yu-Lun Liu",
        "Abstract": "Neural Radiance Fields (NeRF) face significant challenges in extreme few-shot scenarios, primarily due to overfitting and long training times. Existing methods, such as FreeNeRF and SparseNeRF, use frequency regularization or pre-trained priors but struggle with complex scheduling and bias. We introduce FrugalNeRF, a novel few-shot NeRF framework that leverages weight-sharing voxels across multiple scales to efficiently represent scene details. Our key contribution is a cross-scale geometric adaptation scheme that selects pseudo ground truth depth based on reprojection errors across scales. This guides training without relying on externally learned priors, enabling full utilization of the training data. It can also integrate pre-trained priors, enhancing quality without slowing convergence.  Experiments on LLFF, DTU, and RealEstate-10K show that FrugalNeRF outperforms other few-shot NeRF methods while significantly reducing training time, making it a practical solution for efficient and accurate 3D scene reconstruction.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "FrugalNeRF addresses the challenges of Neural Radiance Fields (NeRF) in extreme few-shot scenarios, such as overfitting and long training times, by introducing a novel framework that uses weight-sharing voxels across multiple scales. The key innovation is a cross-scale geometric adaptation scheme that selects pseudo ground truth depth based on reprojection errors, enabling efficient training without relying on externally learned priors. This approach not only improves the quality of 3D scene reconstruction but also significantly reduces training time, making it a practical solution for few-shot NeRF applications.",
        "Tags": [
            "NeRF (Neural Radiance Fields)",
            "Novel View Synthesis",
            "Weight-Sharing Voxels",
            "Cross-Scale Geometric Adaptation",
            "Few-Shot Learning"
        ]
    },
    {
        "Title": "Training Data Provenance Verification: Did Your Model Use Synthetic Data from My Generative Model for Training?",
        "Authors": "Yuechen Xie \u00b7 Jie Song \u00b7 Huiqiong Wang \u00b7 Mingli Song",
        "Abstract": "High-quality open-source text-to-image models have lowered the threshold for obtaining photorealistic images significantly, but also face potential risks of misuse. Specifically, suspects may use synthetic data generated by these generative models to train models for specific tasks without permission, when lacking real data resources especially. Protecting these generative models is crucial for the well-being of their owners. In this work, we propose the first method to this important yet unresolved issue, called Training data Provenance Verification (TrainProVe). The rationale behind TrainProVe is grounded in the principle of generalization error bound, which suggests that, for two models with the same task, if the distance between their training data distributions is smaller, their generalization ability will be closer. We validate the efficacy of TrainProVe across four text-to-image models (Stable Diffusion v1.4, latent consistency model, PixArt-$\\alpha$, and Stable Cascade). The results show that TrainProVe achieves a verification accuracy of over 99\\% in determining the provenance of suspicious model training data, surpassing all previous methods. Code will be publicly available soon.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The proliferation of high-quality open-source text-to-image models has made it easier to generate photorealistic images, but it also raises concerns about misuse, such as unauthorized use of synthetic data for training specific task models. To address this, the authors introduce Training data Provenance Verification (TrainProVe), a novel method based on the generalization error bound principle. TrainProVe determines the provenance of training data by measuring the distance between training data distributions of models performing the same task. The method was tested on four text-to-image models, achieving over 99% accuracy in verifying the origin of suspicious training data, outperforming existing methods.",
        "Tags": [
            "Text-to-Image Generation",
            "Data Provenance",
            "Generalization Error Bound",
            "Synthetic Data Misuse",
            "Model Training Verification"
        ]
    },
    {
        "Title": "Context-Enhanced Memory-Refined Transformer for Online Action Detection",
        "Authors": "Zhanzhong Pang \u00b7 Fadime Sener \u00b7 Angela Yao",
        "Abstract": "Online Action Detection (OAD) detects actions in streaming videos using past observations. State-of-the-art OAD approaches model past observations and their interactions with an anticipated future. The past is encoded using short- and long-term memories to capture immediate and long-range dependencies, while anticipation compensates for missing future context.We uncover a training-inference discrepancy in existing OAD methods that hinders learning effectiveness. The training uses varying lengths of short-term memory, while inference relies on a full-length short-term memory. As a remedy, we propose a Context-enhanced Memory-Refined Transformer (CMeRT).  CMeRT introduces a context-enhanced encoder to improve frame representations using additional near-past context.  It also features a memory-refined decoder to leverage near-future generation to enhance performance.CMeRT improves learning for OAD and achieves state-of-the-art in online detection and anticipation on THUMOS'14, CrossTask, and EPIC-Kitchens-100.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of Online Action Detection (OAD) in streaming videos, where actions are detected using past observations and their interactions with an anticipated future. The authors identify a training-inference discrepancy in existing OAD methods, which limits learning effectiveness. To overcome this, they propose the Context-enhanced Memory-Refined Transformer (CMeRT). CMeRT enhances frame representations by incorporating additional near-past context through a context-enhanced encoder and improves performance by leveraging near-future generation with a memory-refined decoder. The proposed method achieves state-of-the-art results in online detection and anticipation on benchmark datasets THUMOS'14, CrossTask, and EPIC-Kitchens-100.",
        "Tags": [
            "Video Understanding",
            "Action Detection",
            "Transformer Models",
            "Temporal Context Modeling",
            "Streaming Video Analysis"
        ]
    },
    {
        "Title": "MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts",
        "Authors": "Peijie Wang \u00b7 Zhong-Zhi Li \u00b7 Fei Yin \u00b7 Dekang Ran \u00b7 Cheng-Lin Liu",
        "Abstract": "Multimodal Large Language Models (MLLMs) have shown promising capabilities in mathematical reasoning within visual contexts across various datasets. However, most existing multimodal math benchmarks are limited to single-visual contexts, which diverges from the multi-visual scenarios commonly encountered in real-world mathematical applications. To address this gap, we introduce MV-MATH: a meticulously curated dataset of 2,009 high-quality mathematical problems. Each problem integrates multiple images interleaved with text, derived from authentic K-12 scenarios and enriched with detailed annotations. MV-MATH includes multiple-choice, free-form, and multi-step questions, covering 11 subject areas across 3 difficulty levels, and serves as a comprehensive and rigorous benchmark for assessing MLLMs\u2019 mathematical reasoning in multi-visual contexts. Through extensive experimentation, we observe that MLLMs encounter substantial challenges in multi-visual math tasks, with a considerable performance gap relative to human capabilities on MV-MATH. Furthermore, we analyze the performance and error patterns of various models, providing insights into MLLMs' mathematical reasoning capabilities within multi-visual settings.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces MV-MATH, a dataset designed to evaluate Multimodal Large Language Models (MLLMs) in multi-visual mathematical reasoning contexts. Unlike existing benchmarks that focus on single-visual scenarios, MV-MATH comprises 2,009 high-quality problems integrating multiple images with text, derived from real-world K-12 educational contexts. The dataset includes diverse question types and spans 11 subject areas across three difficulty levels. Experiments reveal that MLLMs struggle significantly with multi-visual math tasks, showing a notable performance gap compared to humans. The study also provides insights into the error patterns and reasoning capabilities of MLLMs in such settings.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Datasets and Benchmarks",
            "Multi-Visual Reasoning",
            "Mathematical Reasoning",
            "K-12 Education"
        ]
    },
    {
        "Title": "3D-SLNR: A Super Lightweight Neural Representation for Large-scale 3D Mapping",
        "Authors": "Chenhui Shi \u00b7 Fulin Tang \u00b7 Ning An \u00b7 Yihong Wu",
        "Abstract": "We propose 3D-SLNR, a new and ultra-lightweight neural representation with outstanding performance for large-scale 3D mapping. The representation defines a global signed distance function (SDF) in near-surface space based on a set of band-limited local SDFs anchored at support points sampled from point clouds. These SDFs are parameterized only by a tiny multi-layer perceptron (MLP) with no latent features, and the state of each SDF is modulated by three learnable geometric properties: position, rotation, and scaling, which make the representation adapt to complex geometries. Then, we develop a novel parallel algorithm tailored for this unordered representation to efficiently detect local SDFs where each sampled point is located, allowing for real-time updates of local SDF states during training. Additionally, a prune-and-expand strategy is introduced to enhance adaptability further. The synergy of our low-parameter model and its adaptive capabilities results in an extremely compact representation with excellent expressiveness. Extensive experiments demonstrate that our method achieves state-of-the-art reconstruction performance with less than 1/5 of the memory footprint compared with previous advanced methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces 3D-SLNR, a super lightweight neural representation designed for large-scale 3D mapping. It utilizes a global signed distance function (SDF) based on band-limited local SDFs anchored at support points from point clouds. These SDFs are parameterized by a minimal multi-layer perceptron (MLP) without latent features, with their states modulated by learnable geometric properties (position, rotation, scaling). A novel parallel algorithm enables efficient detection of local SDFs for real-time updates during training, and a prune-and-expand strategy enhances adaptability. The method achieves state-of-the-art reconstruction performance with significantly reduced memory usage compared to existing methods.",
        "Tags": [
            "3D Reconstruction",
            "Implicit Neural Representations",
            "Signed Distance Function (SDF)",
            "Parallel Algorithm",
            "Prune-and-Expand Strategy"
        ]
    },
    {
        "Title": "EnliveningGS: Active Locomotion of 3DGS",
        "Authors": "Siyuan Shen \u00b7 Tianjia Shao \u00b7 Kun Zhou \u00b7 Chenfanfu Jiang \u00b7 Yin Yang",
        "Abstract": "This paper presents a novel pipeline named EnliveningGS, which enables active locomotion of 3D models represented with 3D Gaussian splatting (3DGS). We are inspired by the fact that real-world lives pose their bodies in a natural and physically meaningful manner by compressing or elongating muscle fibers embedded in the body. EnliveningGS aims to replicate the similar functionality of 3DGS models so that the object within a 3DGS scene acts like a living creature rather than a static shape --- they walk, jump, and twist in the scene under provided motion trajectories driven by muscle activations. While the concept is straightforward, many challenging technical difficulties need to be taken care of. Synthesizing realistic locomotion of a 3DGS model embodies an inverse physics problem of very high dimensions. The core challenge is how to efficiently and robustly model frictional contacts between an ``enlivened model'' and the environment, as it is the composition of contact/collision/friction forces triggered by muscle activation that generates the final movement of the object. We propose a hybrid numerical method mixing LCP and penalty method to tackle this NP-hard problem robustly. Our pipeline also addresses the limitation of existing 3DGS deformation algorithms and inpainting the missing information when models move around.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces EnliveningGS, a novel pipeline that enables active locomotion of 3D models represented with 3D Gaussian splatting (3DGS). Inspired by the natural movements of living creatures, the pipeline allows 3DGS models to perform actions like walking, jumping, and twisting under motion trajectories driven by muscle activations. The core challenge involves modeling frictional contacts between the model and the environment, which is addressed using a hybrid numerical method combining LCP and penalty methods. The pipeline also overcomes limitations in existing 3DGS deformation algorithms and handles missing information during model movement.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "3D Reconstruction",
            "Inverse Physics Problem",
            "Frictional Contact Modeling",
            "Hybrid Numerical Method"
        ]
    },
    {
        "Title": "Hybrid Concept Bottleneck Models",
        "Authors": "Yang Liu \u00b7 Tianwei Zhang \u00b7 Shi Gu",
        "Abstract": "Concept Bottleneck Models (CBMs) provide an interpretable framework for neural networks by mapping visual features to predefined, human-understandable concepts. However, the application of CBMs is often constrained by insufficient concept annotations. Recently, multi-modal pre-trained models have shown promise in reducing annotation costs by aligning visual representations with textual concept embeddings. Nevertheless, the quality and completeness of the predefined concepts significantly affect the performance of CBMs.In this work, we propose Hybrid Concept Bottleneck Model (HybridCBM), a novel CBM framework to address the challenge of incomplete predefined concepts. Our method consists of two main components: a Static Concept Bank and a Dynamic Concept Bank. The Static Concept Bank directly leverages large language models (LLMs) for concept construction, while the Dynamic Concept Bank employs learnable vectors to capture complementary and valuable concepts continuously during training. After training, a pre-trained translator converts these vectors into human-understandable concepts, further enhancing model interpretability. Notably, HybridCBM is highly flexible and can be easily applied to any CBM to improve performance. Experimental results across multiple datasets demonstrate that HybridCBM outperforms current state-of-the-art methods and achieves comparable results to black-box models. Additionally, we propose novel metrics to evaluate the quality of the learned concepts, showing that they perform comparably to predefined concepts.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces the Hybrid Concept Bottleneck Model (HybridCBM), a novel framework designed to address the limitations of Concept Bottleneck Models (CBMs) caused by incomplete predefined concepts. HybridCBM incorporates two main components: a Static Concept Bank, which utilizes large language models (LLMs) for concept construction, and a Dynamic Concept Bank, which employs learnable vectors to capture additional valuable concepts during training. These vectors are later translated into human-understandable concepts by a pre-trained translator, enhancing model interpretability. The framework is flexible and can be integrated into any CBM to improve performance. Experimental results show that HybridCBM outperforms existing methods and achieves results comparable to black-box models. The paper also introduces new metrics to evaluate the quality of learned concepts, demonstrating their effectiveness relative to predefined concepts.",
        "Tags": [
            "Concept Bottleneck Models (CBMs)",
            "Large Language Models (LLMs)",
            "Dynamic Concept Bank",
            "Static Concept Bank",
            "Concept Quality Evaluation"
        ]
    },
    {
        "Title": "Multi-Sensor Object Anomaly Detection: Unifying Appearance, Geometry, and Internal Properties",
        "Authors": "wenqiao Li \u00b7 BoZhong Zheng \u00b7 Xiaohao Xu \u00b7 Jinye Gan \u00b7 Fading Lu \u00b7 Xiang Li \u00b7 Na Ni \u00b7 Zheng Tian \u00b7 Xiaonan Huang \u00b7 Shenghua Gao \u00b7 Yingna Wu",
        "Abstract": "Object anomaly detection is essential for industrial quality inspection, yet traditional single-sensor methods face critical limitations. They fail to capture the wide range of anomaly types, as single sensors are often constrained to either external appearance, geometric structure, or internal properties. To overcome these challenges, we introduce MulSen-AD, the first high-resolution, multi-sensor anomaly detection dataset tailored for industrial applications. MulSen-AD unifies data from RGB cameras, laser scanners, and lock-in infrared thermography, effectively capturing external appearance, geometric deformations, and internal defects. The dataset spans 15 industrial products with diverse, real-world anomalies. We also present MulSen-AD Bench, a benchmark designed to evaluate multi-sensor methods, and propose MulSen-TripleAD, a decision-level fusion algorithm that integrates these three modalities for robust, unsupervised object anomaly detection. Our experiments demonstrate that multi-sensor fusion substantially outperforms single-sensor approaches, achieving 96.1% AUROC in object-level detection accuracy. These results highlight the importance of integrating multi-sensor data for comprehensive industrial anomaly detection. The dataset and code are publicly available  to support further research.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces MulSen-AD, a high-resolution, multi-sensor anomaly detection dataset designed for industrial applications, addressing the limitations of single-sensor methods. MulSen-AD integrates data from RGB cameras, laser scanners, and lock-in infrared thermography to capture external appearance, geometric deformations, and internal defects across 15 industrial products. The authors also present MulSen-AD Bench, a benchmark for evaluating multi-sensor methods, and propose MulSen-TripleAD, a decision-level fusion algorithm for unsupervised object anomaly detection. The results show that multi-sensor fusion significantly outperforms single-sensor approaches, achieving 96.1% AUROC in object-level detection accuracy, underscoring the importance of multi-sensor data integration for comprehensive anomaly detection.",
        "Tags": [
            "Anomaly Detection",
            "Multimodal Learning",
            "Industrial Quality Inspection",
            "Decision-Level Fusion",
            "Unsupervised Anomaly Detection"
        ]
    },
    {
        "Title": "Human-Aligned Video Generation Benchmark",
        "Authors": "Hui Han \u00b7 Siyuan Li \u00b7 Jiaqi Chen \u00b7 Yiwen Yuan \u00b7 Yuling Wu \u00b7 Yufan Deng \u00b7 Chak Tou Leong \u00b7 Hanwen Du \u00b7 Junchen Fu \u00b7 Youhua Li \u00b7 Jie Zhang \u00b7 Chi Zhang \u00b7 Li-jia Li \u00b7 Yongxin Ni",
        "Abstract": "Video generation assessment is essential for ensuring that generative models produce visually realistic, high-quality videos while aligning with human expectations. Current video generation benchmarks fall into two main categories: traditional benchmarks, which use metrics and embeddings to evaluate generated video quality across multiple dimensions but often lack alignment with human judgments; and large language model (LLM)-based benchmarks, though capable of human-like reasoning, are constrained by a limited understanding of video quality metrics and cross-modal consistency.To address these challenges and establish a benchmark that better aligns with human preferences, this paper introduces HA-Video-Bench, a comprehensive benchmark featuring a rich prompt suite and extensive evaluation dimensions. This benchmark represents the first attempt to systematically leverage MLLMs across all dimensions relevant to video generation assessment in generative models. By incorporating few-shot scoring and chain-of-query techniques, HA-Video-Bench provides a structured, scalable approach to generated video evaluation. Experimental results demonstrate that MLLMs achieve superior alignment with human preferences across all dimensions. Moreover, in instances where our framework\u2019s assessments diverge from human evaluations, it consistently offers more objective and accurate insights, suggesting an even greater potential advantage over traditional human judgment.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces HA-Video-Bench, a benchmark designed to improve the alignment of video generation assessments with human preferences. Unlike traditional benchmarks that often misalign with human judgments and LLM-based benchmarks that lack a deep understanding of video quality, HA-Video-Bench leverages MLLMs across all relevant dimensions of video generation assessment. It employs few-shot scoring and chain-of-query techniques to provide a structured and scalable evaluation framework. The results show that MLLMs align more closely with human preferences and offer more objective insights when discrepancies with human evaluations occur.",
        "Tags": [
            "Video Generation",
            "Multimodal Large Language Models (MLLMs)",
            "Few-shot Learning",
            "Chain-of-Query Techniques",
            "Human Preference Alignment"
        ]
    },
    {
        "Title": "LUCAS: Layered Universal Codec Avatars",
        "Authors": "Di Liu \u00b7 Teng Deng \u00b7 Giljoo Nam \u00b7 Yu Rong \u00b7 Stanislav Pidhorskyi \u00b7 Junxuan Li \u00b7 Jason Saragih \u00b7 Dimitris N. Metaxas \u00b7 Chen Cao",
        "Abstract": "Photorealistic 3D head avatar reconstruction faces critical challenges in modeling dynamic face-hair interactions and achieving cross-identity generalization, particularly during expressions and head movements. We present LUCAS, a novel Universal Prior Model (UPM) for codec avatar modeling that disentangles face and hair through a layered representation. Unlike previous UPMs that treat hair as an integral part of the head, our approach separates the modeling of the hairless head and hair into distinct branches. LUCAS is the first to introduce a mesh-based UPM, facilitating real-time rendering on devices. LUCAS can be integrated with Gaussian Splatting to enhance visual fidelity, a feature particularly beneficial for rendering complex hairstyles. Experimental results indicate that LUCAS outperforms existing single-mesh and Gaussian-based avatar models in both quantitative and qualitative assessments, including evaluations on held-out subjects in zero-shot driving scenarios. LUCAS demonstrates superior dynamic performance in managing head pose changes, expression transfer, and hairstyle variations, thereby advancing the state-of-the-art in 3D head avatar reconstruction.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "LUCAS introduces a novel Universal Prior Model (UPM) for photorealistic 3D head avatar reconstruction, addressing challenges in dynamic face-hair interactions and cross-identity generalization. By disentangling face and hair through a layered representation, LUCAS separates the modeling of the hairless head and hair into distinct branches, enabling real-time rendering on devices. It integrates with Gaussian Splatting to enhance visual fidelity, particularly for complex hairstyles. LUCAS outperforms existing models in quantitative and qualitative assessments, demonstrating superior dynamic performance in head pose changes, expression transfer, and hairstyle variations, advancing the state-of-the-art in 3D head avatar reconstruction.",
        "Tags": [
            "Avatars",
            "3DGS (Gaussian Splatting)",
            "Layered Representation",
            "Mesh-Based UPM",
            "Real-Time Rendering"
        ]
    },
    {
        "Title": "GLASS: Guided Latent Slot Diffusion for Object-Centric Learning",
        "Authors": "Krishnakant Singh \u00b7 Simone Schaub-Meyer \u00b7 Stefan Roth",
        "Abstract": "Object-centric learning aims to decompose an input image into a set of meaningful object files (slots). These latent object representations enable a variety of downstream tasks. Yet, object-centric learning struggles on real-world datasets, which contain multiple objects of complex textures and shapes in natural everyday scenes. To address this, we introduce Guided Latent Slot Diffusion (GLASS), a novel slot attention model that learns in the space of generated images and uses semantic and instance guidance modules to learn better slot embeddings for various downstream tasks. Our experiments show that GLASS surpasses state-of-the-art slot attention methods by a wide margin on tasks such as (zero-shot) object discovery and conditional image generation for real-world scenes. Moreover, GLASS enables the first application of slot attention to compositional generation of complex, realistic scenes.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Guided Latent Slot Diffusion (GLASS), a novel slot attention model designed to improve object-centric learning in real-world datasets. GLASS operates in the space of generated images and incorporates semantic and instance guidance modules to enhance slot embeddings, facilitating better performance in downstream tasks such as object discovery and conditional image generation. The model significantly outperforms existing slot attention methods and extends the application of slot attention to the compositional generation of complex, realistic scenes.",
        "Tags": [
            "Object-Centric Learning",
            "Diffusion Models",
            "Slot Attention",
            "Semantic Guidance",
            "Instance Guidance"
        ]
    },
    {
        "Title": "MambaOut: Do We Really Need Mamba for Vision?",
        "Authors": "Weihao Yu \u00b7 Xinchao Wang",
        "Abstract": "Mamba, an architecture with RNN-like token mixer of state space model (SSM), was recently introduced to address the quadratic complexity of the attention mechanism and subsequently applied to vision tasks. Nevertheless, the performance of Mamba for vision is often underwhelming when compared with convolutional and attention-based models. In this paper, we delve into the essence of Mamba, and conceptually conclude that Mamba is ideally suited for tasks with long-sequence and autoregressive characteristics. For vision tasks, as image classification on ImageNet does not align with either characteristic, we hypothesize that Mamba is not necessary for this task; Detection and segmentation tasks on COCO or ADE20K are also not autoregressive, yet they adhere to the long-sequence characteristic, so we believe it is still worthwhile to explore Mamba's potential for these tasks. To empirically verify our hypotheses, we construct a series of models named MambaOut through stacking Mamba blocks while removing their core token mixer, SSM. Experimental results strongly support our hypotheses. Specifically, our MambaOut model surpasses all visual Mamba models on ImageNet image classification, indicating that Mamba is indeed unnecessary for this task. As for detection and segmentation, MambaOut cannot match the performance of state-of-the-art visual Mamba models, demonstrating the potential of Mamba for long-sequence visual tasks.",
        "Link": "https://github.com/yuweihao/MambaOut",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper investigates the suitability of Mamba, an architecture with a state space model (SSM) token mixer, for vision tasks. The authors argue that Mamba is best suited for tasks with long-sequence and autoregressive characteristics, which are not typically present in image classification tasks like those on ImageNet. To test this, they introduce MambaOut, a series of models that remove the core SSM token mixer from Mamba blocks. Results show that MambaOut outperforms visual Mamba models on ImageNet classification, suggesting Mamba is unnecessary for this task. However, for detection and segmentation tasks on COCO or ADE20K, which involve long sequences, MambaOut does not match the performance of state-of-the-art visual Mamba models, indicating Mamba's potential for such tasks.",
        "Tags": [
            "Mamba",
            "Vision Transformer (ViT)",
            "State Space Model (SSM)",
            "Long-Sequence Tasks",
            "Autoregressive Tasks"
        ]
    },
    {
        "Title": "Reward Fine-Tuning Two-Step Diffusion Models via Learning Differentiable Latent-Space Surrogate Reward",
        "Authors": "Zhiwei Jia \u00b7 Yuesong Nan \u00b7 Huixi Zhao \u00b7 Gengdai Liu",
        "Abstract": "Recent research has shown that fine-tuning diffusion models (DMs) with arbitrary rewards, including non-differentiable ones, is feasible with reinforcement learning (RL) techniques, offering great flexibility in model alignment. However, it is challenging to apply existing RL methods to timestep-distilled DMs for ultra-fast ($\\le2$-step) image generation.Our analysis suggests several limitations of policy-based RL methods such as PPO or DPO towards improving $\\le2$-step image generation.Based on the insights, we propose to fine-tune DMs with learned differentiable surrogate rewards.Our method, named \\textbf{LaSRO}, learns surrogate reward models in the latent space of SDXL to convert arbitrary rewards into differentiable ones for efficient reward gradient guidance.LaSRO leverages pre-trained latent DMs for reward modeling and specifically targets image generation $\\le2$ steps for reward optimization, enhancing generalizability and efficiency.We show that LaSRO is effective and stable for improving ultra-fast image generation with different reward objectives, outperforming popular RL methods including those based on PPO or DPO. We further show LaSRO's connection to value-based RL, providing theoretical insights behind it.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of fine-tuning diffusion models (DMs) for ultra-fast (\u22642-step) image generation using reinforcement learning (RL). Existing RL methods like PPO or DPO face limitations in this context. The authors propose LaSRO, a method that fine-tunes DMs with learned differentiable surrogate rewards in the latent space of SDXL. LaSRO leverages pre-trained latent DMs for reward modeling, specifically targeting \u22642-step image generation, and demonstrates superior performance and stability compared to popular RL methods. The paper also provides theoretical insights connecting LaSRO to value-based RL.",
        "Tags": [
            "Diffusion Models",
            "Reinforcement Learning (RL)",
            "Latent Space Modeling",
            "Surrogate Reward Learning",
            "Ultra-Fast Image Generation"
        ]
    },
    {
        "Title": "TreeMeshGPT: Artistic Mesh Generation with Autoregressive Tree Sequencing",
        "Authors": "Stefan Lionar \u00b7 Jiabin Liang \u00b7 Gim Hee Lee",
        "Abstract": "We introduce TreeMeshGPT, an autoregressive Transformer designed to generate high-quality artistic meshes aligned with input point clouds. Instead of the conventional next-token prediction in autoregressive Transformer, we propose a novel Autoregressive Tree Sequencing where the next input token is retrieved from a dynamically growing tree structure that is built upon the triangle adjacency of faces within the mesh. Our sequencing enables the mesh to extend locally from the last generated triangular face at each step, and therefore reduces training difficulty and improves mesh quality. Our approach represents each triangular face with two tokens, achieving a compression rate of approximately 22% compared to the naive face tokenization. Due to this efficient tokenization technique, we push the boundary of artistic mesh generation to the face limit of 5,500 triangles with a strong point cloud condition of 2,048 tokens, surpassing previous methods. Furthermore, our method generates mesh with strong normal orientation constraints, minimizing flipped normals commonly encountered in previous methods. Our experiments show that TreeMeshGPT enhances the mesh generation quality with refined details and normal orientation consistency.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "TreeMeshGPT is an autoregressive Transformer model designed for generating high-quality artistic meshes from input point clouds. It introduces Autoregressive Tree Sequencing, a novel approach where the next input token is derived from a dynamically growing tree structure based on triangle adjacency within the mesh. This method allows the mesh to extend locally from the last generated triangular face, reducing training difficulty and improving mesh quality. The model represents each triangular face with two tokens, achieving a 22% compression rate compared to naive face tokenization, enabling the generation of meshes with up to 5,500 triangles under strong point cloud conditions. TreeMeshGPT also enforces normal orientation constraints, minimizing flipped normals and enhancing detail refinement and consistency in generated meshes.",
        "Tags": [
            "3D Generation",
            "Autoregressive Models",
            "Autoregressive Tree Sequencing",
            "Mesh Compression",
            "Normal Orientation Constraints"
        ]
    },
    {
        "Title": "RICCARDO: Radar Hits Prediction and Convolution for Target Detection with Radar-Camera Fusion",
        "Authors": "Yunfei Long \u00b7 Abhinav Kumar \u00b7 Xiaoming Liu \u00b7 Daniel Morris",
        "Abstract": "Radar hits reflect from points on both  the boundary and internal to object outlines. This results in a complex distribution of radar hits that depends on factors including object category, size and orientation. Current radar-camera fusion methods implicitly account for this with a black-box neural network. In this paper, we explicitly utilize a radar hit distribution model to assist fusion.  First, we build a model to predict radar hit distributions conditioned on object properties obtained from a monocular detector.  Second, we use the predicted distribution as a kernel to match actual measured radar points in the neighborhood of the monocular detections, generating matching scores at nearby positions. Finally, a fusion stage combines context with the kernel detector to refine the matching scores. Our method achieves the state-of-the-art radar-camera detection performance on nuScenes. We will release the model and code upon publication.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces RICCARDO, a novel method for radar-camera fusion that explicitly models radar hit distributions to improve target detection. Unlike current methods that rely on black-box neural networks, RICCARDO predicts radar hit distributions based on object properties from a monocular detector. These predicted distributions are used as kernels to match actual radar points near monocular detections, generating matching scores. A fusion stage then refines these scores by combining context with the kernel detector. The method achieves state-of-the-art performance on the nuScenes dataset.",
        "Tags": [
            "Radar-Camera Fusion",
            "Object Detection",
            "Radar Hit Distribution Modeling",
            "Kernel Matching",
            "Contextual Fusion"
        ]
    },
    {
        "Title": "Your Scale Factors are My Weapon: Targeted Bit-Flip Attacks on Vision Transformers via Scale Factor Manipulation",
        "Authors": "Jialai Wang \u00b7 Yuxiao Wu \u00b7 Weiye Xu \u00b7 Yating Huang \u00b7 Chao Zhang \u00b7 Zongpeng Li \u00b7 Mingwei Xu \u00b7 Zhenkai Liang",
        "Abstract": "Vision Transformers (ViTs) have experienced significant progress and are quantized for deployment in resource-constrained applications. Quantized models are vulnerable to targeted bit-flip attacks (BFAs). A targeted BFA prepares a trigger and a corresponding Trojan/backdoor, inserting the latter (with RowHammer bit flipping) into a victim model, to mislead its classification on samples containing the trigger. Existing targeted BFAs on quantized ViTs are limited in that: (1) they require numerous bit-flips, and (2) the separation between flipped bits is below 4 KB, making attacks infeasible with RowHammer in real-world scenarios. We propose a new and practical targeted attack Flip-S against quantized ViTs. The core insight is that in quantized models, a scale factor change ripples through a batch of model weights. Consequently, flipping bits in scale factors, rather than solely in model weights, enables more cost-effective attacks. We design a Scale-Factor-Search (SFS) algorithm to identify critical bits in scale factors for flipping, and adopt a mutual exclusion strategy to guarantee a 4 KB separation between flips. We evaluate Flip-S on CIFAR-10 and ImageNet datasets across five ViT architectures and two quantization levels. Results show that Flip-S achieves attack success rate (ASR) exceeding 90.0\\% on all models with 50 bits flipped, outperforming baselines with ASR typically below 80.0\\%. Furthermore, compared to the SOTA, Flip-S reduces the number of required bit-flips by 8$\\times$-20$\\times$ while reaching equal or higher ASR. Our source code is publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Vision Transformers (ViTs) are increasingly quantized for deployment in resource-constrained environments, but they remain vulnerable to targeted bit-flip attacks (BFAs). Existing BFAs on quantized ViTs require numerous bit-flips and fail to meet the 4 KB separation requirement for RowHammer-based attacks. This paper introduces Flip-S, a novel targeted attack that manipulates scale factors in quantized models to achieve cost-effective attacks. By flipping bits in scale factors rather than model weights, Flip-S reduces the number of required bit-flips significantly. The proposed Scale-Factor-Search (SFS) algorithm identifies critical bits for flipping, ensuring a 4 KB separation between flips. Evaluations on CIFAR-10 and ImageNet datasets across five ViT architectures and two quantization levels demonstrate that Flip-S achieves an attack success rate (ASR) exceeding 90.0% with only 50 bits flipped, outperforming existing methods. Flip-S reduces the number of required bit-flips by 8\u00d7-20\u00d7 compared to state-of-the-art approaches while maintaining or improving ASR.",
        "Tags": [
            "Vision Transformer (ViT)",
            "Model Pruning",
            "Quantization",
            "RowHammer",
            "Bit-Flip Attacks"
        ]
    },
    {
        "Title": "Ref-GS: Modeling View-Dependent Appearance with Environment Gaussian",
        "Authors": "Tao Xie \u00b7 Xi Chen \u00b7 Zhen Xu \u00b7 Yiman Xie \u00b7 Yudong Jin \u00b7 Yujun Shen \u00b7 Sida Peng \u00b7 Hujun Bao \u00b7 Xiaowei Zhou",
        "Abstract": "Reconstructing complex reflections in real-world scenes from 2D images is essential for achieving photorealistic novel view synthesis. Existing methods that utilize environment maps to model reflections from distant lighting often struggle with high-frequency reflection details and fail to account for near-field reflections. In this work, we introduce RefGS, a novel environment representation that employs a set of Gaussian primitives as an explicit 3D model for capturing reflections. Our approach models all reflections, regardless of their distance, into a unified set of Gaussian primitives, effectively representing high-frequency reflections from both near and distant light sources. To efficiently render these environment Gaussians, we developed a ray-tracing-based renderer that leverages the GPU's RT core for fast rendering. This allows us to jointly optimize our model for high-quality reconstruction while maintaining real-time rendering speeds. Results from multiple real-world and synthetic datasets demonstrate that our method produces significantly more detailed reflections, achieving the best rendering quality in real-time novel view synthesis. The code will be released upon acceptance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Ref-GS introduces a novel environment representation using Gaussian primitives to model complex reflections in real-world scenes, capturing both near and distant light sources. The method employs a ray-tracing-based renderer optimized for GPU RT cores, enabling high-quality reconstruction and real-time rendering. Results show superior rendering quality in novel view synthesis compared to existing methods.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Novel View Synthesis",
            "Ray-Tracing Rendering",
            "Environment Gaussian Representation",
            "Real-Time Rendering"
        ]
    },
    {
        "Title": "Self-Supervised Cross-View Correspondence with Predictive Cycle Consistency",
        "Authors": "Alan Baade \u00b7 Changan Chen",
        "Abstract": "Learning self-supervised visual correspondence is a long-studied task fundamental to visual understanding and human perception. However, existing correspondence methods largely focus on small image transformations, such as object tracking in high-framerate videos or learning pixel-to-pixel mappings between images with high view overlap. This severely limits their application in dynamic multi-view settings such as robot imitation learning or augmented reality. In this work, we introduce Predictive Cycle Consistency for learning object correspondence between extremely disjoint views of a scene without paired segmentation data. Our technique bootstraps object correspondence pseudolabels from raw image segmentations using conditional grayscale colorization and a cycle-consistency refinement prior. We then train deep ViTs on these pseudolabels, which we use to generate higher-quality pseudolabels and iteratively train better correspondence models. We demonstrate the performance of our method under both extreme in-the-wild camera view changes and across large temporal gaps in video. Our approach beats all prior supervised and prior SoTA self-supervised correspondence models on the EgoExo4D correspondence benchmark (+6.7 IoU Exo Query) and the prior SoTA self-supervised methods SiamMAE and DINO V1&V2 on the DAVIS-2017 and LVOS datasets across large frame gaps.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Predictive Cycle Consistency, a novel method for learning object correspondence between highly disjoint views of a scene without requiring paired segmentation data. The approach leverages conditional grayscale colorization and cycle-consistency refinement to bootstrap pseudolabels from raw image segmentations. These pseudolabels are used to train deep Vision Transformers (ViTs), which in turn generate higher-quality pseudolabels for iterative model improvement. The method demonstrates superior performance under extreme camera view changes and large temporal gaps in video, outperforming prior supervised and state-of-the-art self-supervised correspondence models on benchmarks such as EgoExo4D, DAVIS-2017, and LVOS.",
        "Tags": [
            "Self-Supervised Learning",
            "Vision Transformer (ViT)",
            "Cycle-Consistency",
            "Object Correspondence",
            "Conditional Grayscale Colorization"
        ]
    },
    {
        "Title": "EgoPressure: A Dataset for Hand Pressure and Pose Estimation in Egocentric Vision",
        "Authors": "Yiming Zhao \u00b7 Taein Kwon \u00b7 Paul Streli \u00b7 Marc Pollefeys \u00b7 Christian Holz",
        "Abstract": "Estimating touch contact and pressure in egocentric vision is a central task for downstream applications in Augmented Reality, Virtual Reality, as well as many robotic applications, because it provides precise physical insights into hand-object interaction and object manipulation. However, existing contact pressure datasets lack egocentric views and hand poses, which are essential for accurate estimation during in-situ operation, both for AR/VR interaction and robotic manipulation.In this paper, we introduce  a novel dataset of touch contact and pressure interaction from an egocentric perspective, complemented with hand pose meshes and fine-grained pressure intensities for each contact. The hand poses in our dataset are optimized using our proposed multi-view sequence-based method that processes footage from our capture rig of 8 accurately calibrated RGBD cameras.  comprises 5.0 hours of touch contact and pressure interaction from 21 participants captured by a moving egocentric camera and 7 stationary Kinect cameras, which provided RGB images and depth maps at 30 Hz. In addition, we provide baselines for estimating pressure with different modalities, which will enable future developments and benchmarking on the dataset. Overall, we demonstrate that pressure and hand poses are complementary, which supports our intention to better facilitate the physical understanding of hand-object interactions in AR/VR and robotics research",
        "Link": "https://yiming-zhao.github.io/EgoPressure/",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces EgoPressure, a novel dataset for touch contact and pressure estimation in egocentric vision, which is crucial for applications in Augmented Reality (AR), Virtual Reality (VR), and robotics. The dataset includes hand pose meshes and fine-grained pressure intensities, captured using a multi-view sequence-based method from a rig of 8 RGBD cameras. It comprises 5.0 hours of interaction data from 21 participants, captured by a moving egocentric camera and 7 stationary Kinect cameras, providing RGB images and depth maps at 30 Hz. The dataset also includes baselines for pressure estimation across different modalities, aiming to enhance the physical understanding of hand-object interactions in AR/VR and robotics research.",
        "Tags": [
            "Egocentric Vision",
            "Hand Pose Estimation",
            "Pressure Estimation",
            "Multi-view Sequence-based Optimization",
            "Fine-grained Pressure Intensity",
            "Hand-Object Interaction Analysis"
        ]
    },
    {
        "Title": "PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation",
        "Authors": "Qiyao Xue \u00b7 Xiangyu Yin \u00b7 Boyuan Yang \u00b7 Wei Gao",
        "Abstract": "Text-to-video (T2V) generation has been recently enabled by transformer-based diffusion models, but current T2V models lack capabilities in adhering to the real-world common knowledge and physical rules, due to their limited understanding of physical realism and deficiency in temporal modeling. Existing solutions are either data-driven or require extra model inputs, but cannot be generalizable to out-of-distribution domains. In this paper, we present PhyT2V, a new data-independent T2V technique that expands the current T2V model's capability of video generation to out-of-distribution domains, by enabling chain-of-thought and step-back reasoning in T2V prompting. Our experiments show that PhyT2V improves existing T2V models' adherence to real-world physical rules by 2.3x, and achieves 35% improvement compared to T2V prompt enhancers.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "PhyT2V introduces a novel text-to-video (T2V) generation technique that enhances the adherence of T2V models to real-world physical rules and common knowledge. Unlike existing methods that are either data-driven or require additional model inputs, PhyT2V employs chain-of-thought and step-back reasoning in T2V prompting, enabling generalization to out-of-distribution domains. The approach significantly improves the physical realism of generated videos, outperforming existing T2V models and prompt enhancers.",
        "Tags": [
            "Text-to-Image Generation",
            "Diffusion Models",
            "Large Language Models (LLMs)",
            "Physics-Grounded Generation",
            "Chain-of-Thought Reasoning",
            "Step-Back Reasoning"
        ]
    },
    {
        "Title": "Spatial-Temporal Graph Diffusion Policy with Kinematics Modeling for Bimanual Robotic Manipulation",
        "Authors": "Qi Lv \u00b7 Hao Li \u00b7 Xiang Deng \u00b7 Rui Shao \u00b7 Yinchuan Li \u00b7 Jianye Hao \u00b7 Longxiang Gao \u00b7 MICHAEL YU WANG \u00b7 Liqiang Nie",
        "Abstract": "Despite the significant success of imitation learning in robotic manipulation, its application to bimanual tasks remains highly challenging. Existing approaches mainly learn a policy to predict a distant next-best end-effector pose (NBP) and then compute the corresponding joint rotation angles for motion using inverse kinematics. However, they suffer from two important issues: (1) rarely considering the physical robotic structure}, which may cause self-collisions or interferences, and (2) overlooking the kinematics constraint}, which may result in the predicted poses not conforming to the actual limitations of the robot joints. In this paper, we propose Kinematics enhanced Spatial-TemporAl gRaph Diffuser (KStar Diffuser). Specifically, (1)to incorporate the physical robot structure information into action prediction, KStar Diffuser maintains a dynamic spatial-temporal graph according to the physical bimanual joint motions at continuous timesteps. This dynamic graph serves as the robot-structure condition for denoising the actions; (2) to make the NBP learning objective consistent with kinematics, we introduce the differentiable kinematics to provide the reference for optimizing KStar Diffuser. This module regularizes the policy to predict more reliable and kinematics-aware next end-effector poses. Experimental results show that our method effectively leverages the physical structural information and generates kinematics-aware actions in both simulation and real-world.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenges of applying imitation learning to bimanual robotic manipulation by proposing the Kinematics enhanced Spatial-TemporAl gRaph Diffuser (KStar Diffuser). The method integrates physical robot structure information into action prediction through a dynamic spatial-temporal graph that models bimanual joint motions over time. Additionally, it employs differentiable kinematics to ensure that the predicted next-best end-effector poses are consistent with the robot's actual joint limitations. The approach demonstrates effectiveness in generating kinematics-aware actions in both simulated and real-world environments.",
        "Tags": [
            "Embodied AI",
            "Diffusion Models",
            "Bimanual Manipulation",
            "Dynamic Graph Modeling",
            "Differentiable Kinematics"
        ]
    },
    {
        "Title": "MambaVision: A Hybrid Mamba-Transformer Vision Backbone",
        "Authors": "Ali Hatamizadeh \u00b7 Jan Kautz",
        "Abstract": "We propose a novel hybrid Mamba-Transformer backbone, MambaVision, specifically tailored for vision applications. Our core contribution includes redesigning the Mamba formulation to enhance its capability for efficient modeling of visual features. Through a comprehensive ablation study, we demonstrate the feasibility of integrating Vision Transformers (ViT) with Mamba. Our results show that equipping the Mamba architecture with self-attention blocks in the final layers greatly improves its capacity to capture long-range spatial dependencies. Based on these findings, we introduce a family of MambaVision models with a hierarchical architecture to meet various design criteria. For classification on the ImageNet-1K dataset, MambaVision variants achieve state-of-the-art (SOTA) performance in terms of both Top-1 accuracy and throughput. In downstream tasks such as object detection, instance segmentation, and semantic segmentation on MS COCO and ADE20K datasets, MambaVision outperforms comparably sized backbones while demonstrating favorable performance. Code: https://anonymous.4open.science/r/mamba_vision-D073",
        "Link": "https://github.com/NVlabs/MambaVision",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces MambaVision, a hybrid Mamba-Transformer backbone designed for vision applications. The authors redesign the Mamba formulation to improve its ability to model visual features efficiently. By integrating Vision Transformers (ViT) with Mamba, particularly through the addition of self-attention blocks in the final layers, the model significantly enhances its capacity to capture long-range spatial dependencies. A family of MambaVision models with a hierarchical architecture is proposed, achieving state-of-the-art performance on the ImageNet-1K dataset for classification and demonstrating superior results in downstream tasks such as object detection, instance segmentation, and semantic segmentation on MS COCO and ADE20K datasets.",
        "Tags": [
            "Mamba",
            "Vision Transformer (ViT)",
            "Backbone",
            "Hybrid Architecture",
            "Long-Range Spatial Dependencies",
            "Hierarchical Models"
        ]
    },
    {
        "Title": "FreeUV: Ground-Truth-Free Realistic Facial UV Texture Recovery via Cross-Assembly Inference Strategy",
        "Authors": "Xingchao Yang \u00b7 Takafumi Taketomi \u00b7 Yuki Endo \u00b7 Yoshihiro Kanamori",
        "Abstract": "Recovering high-quality 3D facial textures from single-view 2D images is a challenging task, especially under constraints of limited data and complex facial details such as makeup, wrinkles, and occlusions. In this paper, we introduce FreeUV, a novel ground-truth-free UV texture recovery framework that eliminates the need for annotated or synthetic UV data. FreeUV leverages pre-trained stable diffusion model alongside a Cross-Assembly inference strategy to fulfill this objective. In FreeUV, separate networks are trained independently to focus on realistic appearance and structural consistency, and these networks are combined during inference to generate coherent textures. Our approach accurately captures intricate facial features and demonstrates robust performance across diverse poses and occlusions. Extensive experiments validate FreeUV's effectiveness, with results surpassing state-of-the-art methods in both quantitative and qualitative metrics. Additionally, FreeUV enables new applications, including local editing, facial feature interpolation, and multi-view texture recovery. By reducing data requirements, FreeUV offers a scalable solution for generating high-fidelity 3D facial textures suitable for real-world scenarios.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "FreeUV introduces a novel framework for recovering high-quality 3D facial textures from single-view 2D images without requiring annotated or synthetic UV data. The framework utilizes a pre-trained stable diffusion model and a Cross-Assembly inference strategy, where separate networks focus on realistic appearance and structural consistency, combining them during inference to generate coherent textures. FreeUV effectively captures intricate facial details and performs robustly across diverse poses and occlusions. The approach reduces data requirements and enables applications such as local editing, facial feature interpolation, and multi-view texture recovery, offering a scalable solution for real-world scenarios.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Avatars",
            "Image Editing",
            "Cross-Assembly Inference",
            "Stable Diffusion Model",
            "Ground-Truth-Free"
        ]
    },
    {
        "Title": "PromptHMR: Promptable Human Mesh Recovery",
        "Authors": "Yufu Wang \u00b7 Yu Sun \u00b7 Priyanka Patel \u00b7 Kostas Daniilidis \u00b7 Michael J. Black \u00b7 Muhammed Kocabas",
        "Abstract": "Human pose and shape (HPS) estimation presents challenges in diverse scenarios such as crowded scenes, person-person interactions, and single-view reconstruction. Existing approaches lack mechanisms to incorporate auxiliary ``side information\" that could enhance reconstruction accuracy in such challenging scenarios. Furthermore, the most accurate methods rely on cropped person detections and cannot exploit scene context while methods that process the whole image often fail to detect people and are less accurate than methods that use crops. While recent language-based methods explore HPS reasoning through large language or vision-language models, their metric accuracy is well below the state of the art. In contrast, we present PromptHMR, a transformer-based promptable method that reformulates HPS estimation through spatial and semantic prompts. Our method processes full images to maintain scene context and accepts multiple input modalities: spatial prompts like face or body bounding boxes, and semantic prompts like language descriptions or interaction labels. PromptHMR demonstrates robust performance across challenging scenarios: estimating people from bounding boxes as small as faces in crowded scenes, improving body shape estimation through language descriptions, modeling person-person interactions, and producing temporally coherent motions in videos. Experiments on benchmarks show that PromptHMR achieves state-of-the-art performance while offering flexible prompt-based control over the HPS estimation process.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "PromptHMR introduces a transformer-based approach for human pose and shape (HPS) estimation, addressing challenges in diverse scenarios like crowded scenes and person-person interactions. Unlike existing methods, PromptHMR incorporates auxiliary side information through spatial and semantic prompts, such as face or body bounding boxes and language descriptions. This method processes full images to maintain scene context and supports multiple input modalities, enhancing accuracy in challenging scenarios. PromptHMR demonstrates robust performance, including estimating people from small bounding boxes, improving body shape estimation through language descriptions, and producing temporally coherent motions in videos. It achieves state-of-the-art performance on benchmarks while offering flexible prompt-based control over the HPS estimation process.",
        "Tags": [
            "3D Human Pose Estimation",
            "3D Human Mesh Estimation",
            "Transformer-based Models",
            "Multimodal Input",
            "Temporal Coherence"
        ]
    },
    {
        "Title": "Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought",
        "Authors": "Yunze Man \u00b7 De-An Huang \u00b7 Guilin Liu \u00b7 Shiwei Sheng \u00b7 Shilong Liu \u00b7 Liangyan Gui \u00b7 Jan Kautz \u00b7 Yu-Xiong Wang \u00b7 Zhiding Yu",
        "Abstract": "Recent advances in multimodal large language models (MLLMs) have demonstrated remarkable capabilities in vision-language tasks, yet they often struggle with vision-centric scenarios where precise visual focus is needed for accurate reasoning. In this paper, we introduce Argus to address these limitations with a new visual attention grounding mechanism. Our approach employs object-centric grounding as visual chain-of-thought signals, enabling more effective goal-conditioned visual attention during multimodal reasoning tasks. Evaluations on diverse benchmarks demonstrate that Argus excels in both multimodal reasoning tasks and referring object grounding tasks. Extensive analysis further validates various design choices of Argus, and reveals the effectiveness of explicit language-guided visual region-of-interest engagement in MLLMs, highlighting the importance of advancing multimodal intelligence from a visual-centric perspective.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Argus introduces a novel visual attention grounding mechanism to enhance the performance of multimodal large language models (MLLMs) in vision-centric reasoning tasks. By employing object-centric grounding as visual chain-of-thought signals, Argus enables more effective goal-conditioned visual attention, improving accuracy in multimodal reasoning and referring object grounding tasks. The approach underscores the importance of explicit language-guided visual region-of-interest engagement, advancing multimodal intelligence from a visual-centric perspective.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Vision-Language Models (VLMs)",
            "Visual Attention Grounding",
            "Object-Centric Grounding",
            "Language-Guided Visual Engagement"
        ]
    },
    {
        "Title": "SplatFlow: Self-Supervised Dynamic Gaussian Splatting in Neural Motion Flow Field for Autonomous Driving",
        "Authors": "Su Sun \u00b7 Cheng Zhao \u00b7 Zhuoyang Sun \u00b7 Yingjie Chen \u00b7 Mei Chen",
        "Abstract": "Most existing Dynamic Gaussian Splatting methods for complex dynamic urban scenarios rely on accurate object-level supervision from expensive manual labeling, limiting their scalability in real-world applications. In this paper, we introduce SplatFlow, a Self-Supervised Dynamic Gaussian Splatting within Neural Motion Flow Fields (NMFF) to learn 4D space-time representations without requiring tracked 3D bounding boxes, enabling accurate dynamic scene reconstruction and novel view RGB/depth/flow synthesis. SplatFlow designs a unified framework to seamlessly integrate time-dependent 4D Gaussian representation within NMFF, where NMFF is a set of implicit functions to model temporal motions of both LiDAR points and Gaussians as continuous motion flow fields. Leveraging NMFF, SplatFlow effectively decomposes static background and dynamic objects, representing them with 3D and 4D Gaussian primitives, respectively.NMFF also models the status correspondences of each 4D Gaussian across time, which aggregates temporal features to enhance cross-view consistency of dynamic components. SplatFlow further improves dynamic scene identification by distilling features from 2D foundational models into 4D space-time representation. Comprehensive evaluations conducted on the Waymo Open Dataset and KITTI Dataset validate SplatFlow's state-of-the-art (SOTA) performance for both image reconstruction and novel view synthesis in dynamic urban scenarios. The code and model will be released upon the paper's acceptance.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "SplatFlow introduces a self-supervised approach for Dynamic Gaussian Splatting within Neural Motion Flow Fields (NMFF), enabling 4D space-time representations without the need for tracked 3D bounding boxes. This method facilitates accurate dynamic scene reconstruction and novel view synthesis of RGB, depth, and flow. By integrating time-dependent 4D Gaussian representation within NMFF, SplatFlow effectively separates static backgrounds from dynamic objects, using 3D and 4D Gaussian primitives respectively. NMFF also enhances cross-view consistency by modeling temporal correspondences of each 4D Gaussian. Additionally, SplatFlow improves dynamic scene identification by incorporating features from 2D foundational models into its 4D space-time representation. Evaluations on the Waymo Open Dataset and KITTI Dataset demonstrate SplatFlow's superior performance in image reconstruction and novel view synthesis in dynamic urban environments.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Autonomous Driving",
            "Neural Motion Flow Fields",
            "4D Space-Time Representation",
            "Self-Supervised Learning"
        ]
    },
    {
        "Title": "Federated Semi-Supervised Learning via Pseudo-Correction utilizing Confidence Discrepancy",
        "Authors": "Yijie Liu \u00b7 Xinyi Shang \u00b7 Yiqun Zhang \u00b7 Yang Lu \u00b7 Chen Gong \u00b7 Jing-Hao Xue \u00b7 Hanzi Wang",
        "Abstract": "Federated Semi-Supervised Learning (FSSL) aims to leverage unlabeled data across clients with limited labeled data to train a global model with strong generalization ability. Most FSSL methods rely on consistency regularization with pseudo-labels, converting predictions from local or global models into hard pseudo-labels as supervisory signals. However, we discover that the quality of pseudo-label is largely deteriorated by data heterogeneity, an intrinsic facet of federated learning. In this paper, we study the problem of FSSL in-depth and show that (1) heterogeneity exacerbates pseudo-label mismatches, further degrading model performance and convergence, and (2) local and global models' predictive tendencies diverge as heterogeneity increases. Motivated by these findings, we propose a simple and effective method called \\textbf{S}emi-supervised \\textbf{A}ggregation for \\textbf{G}lobally-Enhanced \\textbf{E}nsemble (SAGE), that can flexibly correct pseudo-labels based on confidence discrepancies. This strategy effectively mitigates performance degradation caused by incorrect pseudo-labels and enhances consensus between local and global models. Experimental results demonstrate that SAGE outperforms existing FSSL methods in both performance and convergence. Our code is available at https://anonymous.4open.science/r/CVPR2025-1926-code.",
        "Link": "https://anonymous.4open.science/r/CVPR2025-1926-code",
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Federated Semi-Supervised Learning (FSSL) leverages unlabeled data across clients with limited labeled data to train a global model with strong generalization. However, data heterogeneity in federated learning deteriorates pseudo-label quality, leading to performance degradation and convergence issues. This paper identifies that heterogeneity exacerbates pseudo-label mismatches and causes divergence in predictive tendencies between local and global models. To address this, the authors propose SAGE (Semi-supervised Aggregation for Globally-Enhanced Ensemble), a method that corrects pseudo-labels based on confidence discrepancies, improving performance and consensus between models. SAGE demonstrates superior performance and convergence compared to existing FSSL methods.",
        "Tags": [
            "Federated Learning",
            "Semi-Supervised Learning",
            "Pseudo-Label Correction",
            "Confidence Discrepancy",
            "Model Consensus"
        ]
    },
    {
        "Title": "ShapeShifter: 3D Variations Using Multiscale and Sparse Point-Voxel Diffusion",
        "Authors": "Nissim Maruani \u00b7 Wang Yifan \u00b7 Matthew Fisher \u00b7 Pierre Alliez \u00b7 Mathieu Desbrun",
        "Abstract": "This paper proposes a new 3D generative model that learns to synthesize shape variations based on a single example. While generative methods for 3D objects have recently attracted much attention, current techniques often lack geometric details and/or require long training times and large resources. Our approach remedies these issues by combining sparse voxel grids and multiscale point, normal, and color sampling within an encoder-free neural architecture that can be trained efficiently and in parallel. We show that our resulting variations better capture the fine details of their original input and can capture more general types of surfaces than previous SDF-based methods. Moreover, we offer interactive generation of 3D shape variants, allowing more human control in the design loop if needed.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces ShapeShifter, a novel 3D generative model capable of synthesizing shape variations from a single input example. The model addresses limitations in current 3D generative techniques, such as insufficient geometric detail and high computational demands, by integrating sparse voxel grids with multiscale point, normal, and color sampling in an encoder-free neural architecture. This design enables efficient and parallel training. The proposed method excels in capturing fine details and a broader range of surface types compared to previous SDF-based approaches. Additionally, it supports interactive generation of 3D shape variants, enhancing user control in the design process.",
        "Tags": [
            "3D Generation",
            "Diffusion Models",
            "Sparse Voxel Grids",
            "Multiscale Sampling",
            "Interactive 3D Generation"
        ]
    },
    {
        "Title": "Task Preference Optimization: Improving Multimodal Large Language Models Performance with Vision Task Alignment",
        "Authors": "ziang yan \u00b7 Zhilin Li \u00b7 Yinan He \u00b7 Chenting Wang \u00b7 Kunchang Li \u00b7 Xinhao Li \u00b7 Xiangyu Zeng \u00b7 Zilei Wang \u00b7 Yali Wang \u00b7 Yu Qiao \u00b7 Limin Wang \u00b7 Yi Wang",
        "Abstract": "Current multimodal large language models (MLLMs) struggle with fine-grained or precise understanding of visuals though they give comprehensive perception and reasoning in a spectrum of vision applications. Recent studies either develop tool-using or unify specific visual tasks into the autoregressive framework, often at the expense of overall multimodal performance. To address this issue and enhance MLLMs with visual tasks in a scalable fashion, we propose Task Preference Optimization (TPO), a novel method that utilizes differentiable task preferences derived from typical fine-grained visual tasks. TPO introduces learnable task tokens that establish connections between multiple task-specific heads and the MLLM. By leveraging rich visual labels during training, TPO significantly enhances the MLLM's multimodal capabilities and task-specific performance. Through multi-task co-training within TPO, we observe synergistic benefits that elevate individual task performance beyond what is achievable through single-task training methodologies. Our instantiation of this approach with VideoChat and LLaVA demonstrates an overall 14.6\\% improvement in multimodal performance compared to baseline models. Additionally, MLLM-TPO demonstrates robust zero-shot capabilities across various tasks, performing comparably to state-of-the-art supervised models.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Task Preference Optimization (TPO), a novel method designed to enhance the performance of multimodal large language models (MLLMs) by aligning them with fine-grained visual tasks. TPO employs learnable task tokens that connect multiple task-specific heads to the MLLM, leveraging rich visual labels during training to improve both multimodal capabilities and task-specific performance. The approach, demonstrated with VideoChat and LLaVA, shows a 14.6% improvement in multimodal performance over baseline models and exhibits strong zero-shot capabilities across various tasks, rivaling state-of-the-art supervised models.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Vision-Language Models (VLMs)",
            "Task Preference Optimization",
            "Learnable Task Tokens",
            "Zero-Shot Learning"
        ]
    },
    {
        "Title": "IDOL: Instant Photorealistic 3D Human Creation from a Single Image",
        "Authors": "Yiyu Zhuang \u00b7 Jiaxi Lv \u00b7 Hao Wen \u00b7 Qing Shuai \u00b7 Ailing Zeng \u00b7 Hao Zhu \u00b7 Shifeng Chen \u00b7 Yujiu Yang \u00b7 Xun Cao \u00b7 Wei Liu",
        "Abstract": "Creating a high-fidelity, animatable 3D full-body avatar from a single image is a challenging task due to the diverse appearance and poses of humans and the limited availability of high-quality training data. To achieve fast and high-quality human reconstruction, this work rethinks the task from the perspectives of dataset, model, and representation. First, we introduce a large-scale HUman GEnerated training dataset, HuGe100K, consisting of 100K diverse, photorealistic human images with corresponding 24-view in a static pose or dynamic pose frames generated via a pose-controllable image-to-video model. Next, leveraging the diversity in views, poses, and appearances within HuGe100K, we develop a scalable feed-forward transformer model to predict a 3D human Gaussian representation in a uniform space of a given human image. This model is trained to disentangle human pose, shape, clothing geometry, and texture. Accordingly, the estimated Gaussians can be animated robustly without post-processing. We conduct comprehensive experiments to validate the effectiveness of the proposed dataset and method. Our model demonstrates the generalizable ability to efficiently reconstruct photorealistic humans in under 1 second using a single GPU. Additionally, it seamlessly supports various applications, including animation, shape, and texture editing tasks.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of creating high-fidelity, animatable 3D full-body avatars from a single image by introducing a novel dataset, HuGe100K, and a scalable feed-forward transformer model. The dataset consists of 100K diverse, photorealistic human images with corresponding multi-view frames, generated using a pose-controllable image-to-video model. The proposed transformer model predicts a 3D human Gaussian representation, disentangling pose, shape, clothing geometry, and texture, enabling robust animation without post-processing. The method achieves efficient, photorealistic human reconstruction in under 1 second on a single GPU and supports applications such as animation and texture editing.",
        "Tags": [
            "Avatars",
            "3DGS (Gaussian Splatting)",
            "3D Human Reconstruction",
            "Pose-Controllable Image-to-Video",
            "Feed-Forward Transformer"
        ]
    },
    {
        "Title": "Integral Fast Fourier Color Constancy",
        "Authors": "Wenjun Wei \u00b7 Yanlin Qian \u00b7 Huaian Chen \u00b7 Junkang Dai \u00b7 Yi Jin",
        "Abstract": "Traditional auto white balance (AWB) algorithms typically assume a single global illuminant source, which leads to color distortions in multi-illuminant scenes. While recent neural network-based methods have shown excellent accuracy in such scenarios, their high parameter count and computational demands limit their practicality for real-time video applications. The Fast Fourier Color Constancy (FFCC) algorithm was proposed for single-illuminant-source scenes, predicting a global illuminant source with high efficiency. However, it cannot be directly applied to multi-illuminant scenarios unless specifically modified. To address this, we propose Integral Fast Fourier Color Constancy (IFFCC), an extension of FFCC tailored for multi-illuminant scenes. IFFCC leverages the proposed integral UV histogram to accelerate histogram computations across all possible regions in Cartesian space and parallelizes Fourier-based convolution operations, resulting in a spatially-smooth illumination map. This approach enables high-accuracy, real-time AWB in multi-illuminant scenes. Extensive experiments show that IFFCC achieves accuracy that is on par with or surpasses that of pixel-level neural networks, while reducing the parameter count by over 400\u00d7 and processing speed by 20 - 100\u00d7 faster than network-based approaches.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Traditional auto white balance (AWB) algorithms often fail in multi-illuminant scenes due to their assumption of a single global illuminant source. While neural network-based methods improve accuracy, their computational demands hinder real-time applications. The Fast Fourier Color Constancy (FFCC) algorithm, efficient for single-illuminant scenes, is unsuitable for multi-illuminant scenarios without modification. To address this, the authors propose Integral Fast Fourier Color Constancy (IFFCC), an extension of FFCC designed for multi-illuminant scenes. IFFCC introduces an integral UV histogram to accelerate histogram computations and parallelizes Fourier-based convolution operations, producing a spatially-smooth illumination map. This method achieves high-accuracy, real-time AWB in multi-illuminant scenes, matching or surpassing pixel-level neural networks in accuracy while significantly reducing parameters and processing time.",
        "Tags": [
            "Color Constancy",
            "Real-Time Processing",
            "Integral UV Histogram",
            "Fourier-Based Convolution",
            "Multi-Illuminant Scenes"
        ]
    },
    {
        "Title": "MC$^2$: Multi-concept Guidance for Customized  Multi-concept Generation",
        "Authors": "Jiaxiu Jiang \u00b7 Yabo Zhang \u00b7 Kailai Feng \u00b7 Xiaohe Wu \u00b7 Wenbo Li \u00b7 Renjing Pei \u00b7 Fan Li \u00b7 Wangmeng Zuo",
        "Abstract": "Customized text-to-image generation, which synthesizes images based on user-specified concepts, has made significant progress in handling individual concepts. However, when extended to multiple concepts, existing methods often struggle with properly integrating different models and avoiding the unintended blending of characteristics from distinct concepts. In this paper, we propose MC$\\mathbf{^2}$, a novel approach for multi-concept customization that enhances flexibility and fidelity through inference-time optimization. MC$\\mathbf{^2}$ enables the integration of multiple single-concept models with heterogeneous architectures. By adaptively refining attention weights between visual and textual tokens, our method ensures that image regions accurately correspond to their associated concepts while minimizing interference between concepts. Extensive experiments demonstrate that MC$\\mathbf{^2}$ outperforms training-based methods in terms of prompt-reference alignment. Furthermore, MC$\\mathbf{^2}$ can be seamlessly applied to text-to-image generation, providing robust compositional capabilities. To facilitate the evaluation of multi-concept customization, we also introduce a new benchmark, MC++. The code will be publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces MC$^2$, a novel approach for customized text-to-image generation that addresses the challenge of integrating multiple user-specified concepts without unintended blending. MC$^2$ enhances flexibility and fidelity by enabling the integration of multiple single-concept models with heterogeneous architectures through inference-time optimization. It adaptively refines attention weights between visual and textual tokens to ensure accurate correspondence between image regions and their associated concepts while minimizing interference. The method outperforms training-based approaches in prompt-reference alignment and demonstrates robust compositional capabilities. A new benchmark, MC++, is introduced to evaluate multi-concept customization.",
        "Tags": [
            "Text-to-Image Generation",
            "Multimodal Learning",
            "Attention Mechanism",
            "Inference-Time Optimization",
            "Multi-Concept Integration"
        ]
    },
    {
        "Title": "From Head to Tail: Efficient Black-box Model Inversion Attack via Long-tailed Learning",
        "Authors": "Ziang Li \u00b7 Hongguang Zhang \u00b7 Juan Wang \u00b7 Meihui Chen \u00b7 Hongxin Hu \u00b7 Wenzhe Yi \u00b7 Xiaoyang Xu \u00b7 Mengda Yang \u00b7 Chenjun Ma",
        "Abstract": "Model Inversion Attacks (MIAs) aim to reconstruct private training data from models, leading to privacy leakage, particularly in facial recognition systems. Although many studies have enhanced the effectiveness of white-box MIAs, less attention has been paid to improving efficiency and utility under limited attacker capabilities. Existing black-box MIAs necessitate an impractical number of queries, incurring significant overhead. Therefore, we analyze the limitations of existing MIAs and introduce Surrogate Model-based Inversion with Long-tailed Enhancement (SMILE), a high-resolution oriented and query-efficient MIA for the black-box setting. We begin by analyzing the initialization of MIAs from a data distribution perspective and propose a long-tailed surrogate training method to obtain high-quality initial points. We then enhance the attack's effectiveness by employing the gradient-free black-box optimization algorithm selected by NGOpt. Our experiments show that SMILE outperforms existing state-of-the-art black-box MIAs while requiring only about 5% of the query overhead.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Model Inversion Attacks (MIAs) aim to reconstruct private training data from models, posing significant privacy risks, especially in facial recognition systems. While white-box MIAs have seen advancements, black-box MIAs remain inefficient, requiring excessive queries. This paper introduces Surrogate Model-based Inversion with Long-tailed Enhancement (SMILE), a query-efficient and high-resolution black-box MIA. By analyzing MIA initialization from a data distribution perspective, the authors propose a long-tailed surrogate training method to improve initial point quality. They further enhance attack effectiveness using a gradient-free black-box optimization algorithm. SMILE outperforms existing black-box MIAs while reducing query overhead by 95%.",
        "Tags": [
            "Long-Tail Learning",
            "Model Inversion Attacks",
            "Black-box Optimization",
            "Privacy Leakage",
            "Query Efficiency"
        ]
    },
    {
        "Title": "Dual Exposure Stereo for Extended Dynamic Range 3D Imaging",
        "Authors": "Juhyung Choi \u00b7 Jinneyong Kim \u00b7 Seokjun Choi \u00b7 Jinwoo Lee \u00b7 Samuel Brucker \u00b7 Mario Bijelic \u00b7 Felix Heide \u00b7 Seung-Hwan Baek",
        "Abstract": "Achieving robust stereo 3D imaging under diverse illumination conditions is an importat however challenging task, largely due to the limited dynamic ranges (DRs) of cameras, which are significantly smaller than real world DR. As a result, the accuracy of existing stereo depth estimation methods is often compromised by under- or over-exposed images.In this work, we introduce dual-exposure stereo for extended dynamic range 3D imaging. We develop automatic dual-exposure control method that adjusts the dual exposures, diverging them when the scene DR exceeds the camera DR, thereby providing information about broader DR. From the captured dual-exposure stereo images, we estimate depth by developing a motion-aware dual-exposure stereo depth network.To validate our proposed method, we develop a robot-vision system, collect real-world stereo video datasets, and generate a synthetic dataset. Our approach outperforms traditional exposure control and depth estimation methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of achieving robust stereo 3D imaging under varying illumination conditions, which is hindered by the limited dynamic range (DR) of cameras. The authors propose a dual-exposure stereo method for extended DR 3D imaging, featuring an automatic dual-exposure control mechanism that adjusts exposures to capture broader DR information. A motion-aware dual-exposure stereo depth network is developed to estimate depth from the captured images. The method is validated through a robot-vision system, real-world stereo video datasets, and a synthetic dataset, demonstrating superior performance over traditional exposure control and depth estimation techniques.",
        "Tags": [
            "Stereo Matching",
            "Low-Light Image Enhancement",
            "Dynamic Range Enhancement",
            "Depth Estimation",
            "Robot Vision"
        ]
    },
    {
        "Title": "EmoDubber: Towards High Quality and Emotion Controllable Movie Dubbing",
        "Authors": "Gaoxiang Cong \u00b7 Jiadong Pan \u00b7 Liang Li \u00b7 Yuankai Qi \u00b7 Yuxin Peng \u00b7 Anton van den Hengel \u00b7 Jian Yang \u00b7 Qingming Huang",
        "Abstract": "Given a piece of text, a video clip, and a reference audio, the movie dubbing task aims to generate speech that aligns with the video while cloning the desired voice. The existing methods have two primary deficiencies: (1) They struggle to simultaneously hold audio-visual sync and achieve clear pronunciation; (2) They lack the capacity to express user-defined emotions. To address these problems, we propose EmoDubber, an emotion-controllable dubbing architecture that allows users to specify emotion type and emotional intensity while satisfying high-quality lip sync and pronunciation. Specifically, we first design Lip-related Prosody Aligning (LPA), which focuses on learning the inherent consistency between lip motion and prosody variation by duration level contrastive learning to incorporate reasonable alignment. Then, we design Pronunciation Enhancing (PE) strategy to fuse the video-level phoneme sequences by efficient conformer to improve speech intelligibility. Next, the speaker identity adapting module aims to decode acoustics prior and inject the speaker style embedding. After that, the proposed Flow-based User Emotion Controlling (FUEC) is used to synthesize waveform by flow matching prediction network conditioned on acoustics prior. In this process, the FUEC determines the gradient direction and guidance scale based on the user's emotion instructions by the positive and negative guidance mechanism, which focuses on amplifying the desired emotion while suppressing others. Extensive experimental results on three benchmark datasets demonstrate favorable performance compared to several state-of-the-art methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "EmoDubber introduces a novel architecture for emotion-controllable movie dubbing, addressing the limitations of existing methods in achieving audio-visual synchronization and clear pronunciation while allowing users to specify emotion type and intensity. The proposed system includes Lip-related Prosody Aligning (LPA) for aligning lip motion with prosody variation, Pronunciation Enhancing (PE) for improving speech intelligibility, a speaker identity adapting module for incorporating speaker style, and Flow-based User Emotion Controlling (FUEC) for synthesizing waveforms based on user-defined emotions. The system demonstrates superior performance on benchmark datasets.",
        "Tags": [
            "Multimodal Learning",
            "Video Understanding",
            "Emotion Control",
            "Lip Sync",
            "Speech Synthesis"
        ]
    },
    {
        "Title": "High-quality Point Cloud Oriented Normal Estimation via Hybrid Angular and Euclidean Distance Encoding",
        "Authors": "Yuanqi Li \u00b7 Jingcheng Huang \u00b7 Hongshen Wang \u00b7 Peiyuan Lv \u00b7 Yansong Liu \u00b7 Jiuming Zheng \u00b7 Jie Guo \u00b7 Yanwen Guo",
        "Abstract": "The proliferation of Light Detection and Ranging (LiDAR) technology has facilitated the acquisition of three-dimensional point clouds, which are integral to applications in VR, AR, and Digital Twin. Oriented normals, critical for 3D reconstruction and scene analysis, cannot be directly extracted from scenes using LiDAR due to its operational principles. Previous traditional or learning-based methods are prone to inaccuracies due to uneven distribution and noise due to the dependence on local geometry features. This paper addresses the challenge of estimating oriented point normals by introducing a point cloud normal estimation framework via hybrid angular and Euclidean distance encoding (HAE). Our method overcomes the limitations of local geometric information by combining angular and Euclidean spaces to extract features from both point cloud coordinates and light rays, leading to more accurate normal estimation. The core of our network consists of an angular distance encoding module, which leverages both ray directions and point coordinates for unoriented normal refinement, and a ray feature fusion module for normal orientation, that is robust to noise. We also provide a point cloud dataset with ground truth normals, generated a virtual scanner, which reflects real scanning distributions and noise profiles.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a novel framework for high-quality oriented normal estimation in 3D point clouds, addressing the limitations of existing methods that rely solely on local geometric features. The proposed Hybrid Angular and Euclidean Distance Encoding (HAE) method combines angular and Euclidean spaces to extract features from both point cloud coordinates and light rays, enhancing the accuracy of normal estimation. The framework includes an angular distance encoding module for refining unoriented normals and a ray feature fusion module for robust normal orientation. Additionally, a new point cloud dataset with ground truth normals, generated using a virtual scanner, is provided to support the development and evaluation of the method.",
        "Tags": [
            "3D Point Cloud",
            "3D Reconstruction",
            "Normal Estimation",
            "Hybrid Encoding",
            "Virtual Scanner Dataset"
        ]
    },
    {
        "Title": "Beyond Background Shift: Rethinking Instance Replay in Continual Semantic Segmentation",
        "Authors": "Hongmei Yin \u00b7 Tingliang Feng \u00b7 Fan Lyu \u00b7 Fanhua Shang \u00b7 Hongying Liu \u00b7 Wei Feng \u00b7 Liang Wan",
        "Abstract": "In this work, we focus on continual semantic segmentation (CSS), where segmentation networks are required to continuously learn new classes without erasing knowledge of previously learned ones. Although storing images of old classes and directly incorporating them into the training of new models has proven effective in mitigating catastrophic forgetting in classification tasks, this strategy presents notable limitations in CSS. Specifically, the stored and new images with partial category annotations leads to confusion between unannotated categories and the background, complicating model fitting. To tackle this, this paper proposes EIR, which not only preserves knowledge of old classes while simultaneously eliminating background confusion by instance storage of old classes, but also mitigates background shifts present in the new images by integrating stored instances with new images. By effectively resolving background shifts in both stored and new images, EIR alleviates catastrophic forgetting in the CSS task, thereby enhancing the model's capacity for CSS. Experimental results validate the efficacy of our approach, which significantly outperforms state-of-the-art CSS methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of continual semantic segmentation (CSS), where models must learn new classes without forgetting previously learned ones. The authors identify a key issue with existing methods: storing and reusing old images with partial annotations causes confusion between unannotated categories and the background, hindering model performance. To overcome this, they propose EIR (Eliminating Instance Replay), a method that stores instances of old classes to preserve knowledge while eliminating background confusion. EIR also integrates stored instances with new images to mitigate background shifts, effectively reducing catastrophic forgetting in CSS. The approach demonstrates superior performance compared to state-of-the-art CSS methods.",
        "Tags": [
            "Semantic Segmentation",
            "Continual Learning",
            "Background Shift Mitigation",
            "Instance Replay",
            "Catastrophic Forgetting"
        ]
    },
    {
        "Title": "Crab: A Unified Audio-Visual Scene Understanding Model with Explicit Cooperation",
        "Authors": "Henghui Du \u00b7 Guangyao Li \u00b7 Chang Zhou \u00b7 Chunjie Zhang \u00b7 Alan Zhao \u00b7 Di Hu",
        "Abstract": "In recent years, numerous tasks have been proposed to encourage model to develop specified capability in understanding audio-visual scene, primarily categorized into temporal localization, spatial localization, spatio-temporal reasoning, and pixel-level understanding. Instead, human possesses a unified understanding ability for diversified tasks. Therefore, designing an audio-visual model with general capability to unify these tasks is of great value. However, simply joint training for all tasks can lead to interference due to the heterogeneity of audiovisual data and complex relationship among tasks. We argue that this problem can be solved through explicit cooperation among tasks. To achieve this goal, we propose a unified learning method which achieves explicit inter-task cooperation from both the perspectives of data and model thoroughly. Specifically, considering the labels of existing datasets are simple words, we carefully refine these datasets and construct an Audio-Visual Unified Instruction-tuning dataset with Explicit reasoning process (AV-UIE), which clarifies the cooperative relationship among tasks. Subsequently, to facilitate concrete cooperation in learning stage, an interaction-aware LoRA structure with multiple LoRA heads is designed to learn different aspects of audiovisual data interaction. By unifying the explicit cooperation across the data and model aspect, our method not only surpasses existing unified audio-visual model on multiple tasks, but also outperforms most specialized models for certain tasks. Furthermore, we also visualize the process of explicit cooperation and surprisingly find that each LoRA head has certain audio-visual understanding ability. Our dataset and code will be open-sourced.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Crab, a unified audio-visual scene understanding model designed to address the challenge of integrating diverse tasks such as temporal localization, spatial localization, spatio-temporal reasoning, and pixel-level understanding. The authors argue that joint training for these tasks often leads to interference due to the heterogeneity of audio-visual data and complex task relationships. To overcome this, they propose a method that achieves explicit inter-task cooperation through both data and model perspectives. Specifically, they refine existing datasets to create the Audio-Visual Unified Instruction-tuning dataset with Explicit reasoning process (AV-UIE), which clarifies task relationships. Additionally, they design an interaction-aware LoRA structure with multiple LoRA heads to facilitate learning different aspects of audio-visual data interaction. The proposed method outperforms existing unified models and specialized models on multiple tasks, and the authors visualize the explicit cooperation process, revealing that each LoRA head exhibits specific audio-visual understanding capabilities.",
        "Tags": [
            "Multimodal Learning",
            "Audio-Visual Scene Understanding",
            "LoRA Structure",
            "Audio-Visual Unified Instruction-tuning",
            "Explicit Inter-task Cooperation"
        ]
    },
    {
        "Title": "DA-VPT: Semantic-Guided Visual Prompt Tuning for Vision Transformers",
        "Authors": "Li Ren \u00b7 Chen Chen \u00b7 Liqiang Wang \u00b7 Kien A. Hua",
        "Abstract": "Visual Prompt Tuning (VPT) has become a promising solution for Parameter-Efficient Fine-Tuning (PEFT) approach for Vision Transformer (ViT) models by partially fine-tuning learnable tokens while keeping most model parameters frozen. Recent research has explored modifying the connection structures of the prompts. However, the fundamental correlation and distribution between the prompts and image tokens remain unexplored. In this paper, we leverage \\textit{metric learning} techniques to investigate how the distribution of prompts affects fine-tuning performance. Specifically, we propose a novel framework, \\textbf{D}istribution \\textbf{A}ware \\textbf{V}isual \\textbf{P}rompt Tuning (DA-VPT), to guide the distributions of the prompts by learning the distance metric from their class-related semantic data. Our method demonstrates that the prompts can serve as an effective bridge to share semantic information between image patches and the class token. We extensively evaluated our approach on popular benchmarks in both recognition and segmentation tasks. The results demonstrate that our approach enables more effective and efficient fine-tuning of ViT models by leveraging semantic information to guide the learning of the prompts, leading to improved performance on various downstream vision tasks. The code will be released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Visual Prompt Tuning (VPT) is a promising approach for fine-tuning Vision Transformer (ViT) models efficiently by partially updating learnable tokens while keeping most parameters frozen. This paper introduces DA-VPT, a novel framework that leverages metric learning to guide the distribution of prompts using class-related semantic data. DA-VPT demonstrates that prompts can effectively share semantic information between image patches and the class token, improving fine-tuning performance on recognition and segmentation tasks. The approach enhances the efficiency and effectiveness of ViT fine-tuning by incorporating semantic guidance into prompt learning.",
        "Tags": [
            "Vision Transformer (ViT)",
            "Parameter-Efficient Fine-Tuning (PEFT)",
            "Metric Learning",
            "Semantic-Guided Learning",
            "Visual Prompt Tuning"
        ]
    },
    {
        "Title": "Keep the Balance: A Parameter-Efficient Symmetrical Framework for RGB+X Semantic Segmentation",
        "Authors": "Jiaxin Cai \u00b7 Jingze Su \u00b7 Qi Li \u00b7 Wenjie Yang \u00b7 Shu Wang \u00b7 Tiesong Zhao \u00b7 Shengfeng He \u00b7 Wenxi Liu",
        "Abstract": "Multimodal semantic segmentation is a critical challenge in computer vision, with early methods suffering from high computational costs and limited transferability due to full fine-tuning of RGB-based pre-trained parameters. Recent studies, while leveraging additional modalities as supplementary prompts to RGB, still predominantly rely on RGB, which restricts the full potential of other modalities. To address these issues, we propose a novel symmetric parameter-efficient fine-tuning framework for multimodal segmentation, featuring with a modality-aware prompting and adaptation scheme, to simultaneously adapt the capabilities of a powerful pre-trained model to both RGB and X modalities. Furthermore, prevalent approaches use the global cross-modality correlations of attention mechanism for modality fusion, which inadvertently introduces noise across modalities. To mitigate this noise, we propose a dynamic sparse cross-modality fusion module to facilitate effective and efficient cross-modality fusion. To further strengthen the above two modules, we propose a training strategy that leverages accurately predicted dual-modality results to self-teach the single-modality outcomes. In comprehensive experiments, we demonstrate that our method outperforms previous state-of-the-art approaches across six multimodal segmentation scenarios with minimal computation cost.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a parameter-efficient symmetrical framework for RGB+X semantic segmentation, addressing the limitations of early methods that suffer from high computational costs and restricted transferability. The proposed framework features a modality-aware prompting and adaptation scheme, enabling the adaptation of a pre-trained model to both RGB and X modalities. To reduce noise introduced by global cross-modality correlations, a dynamic sparse cross-modality fusion module is introduced. Additionally, a training strategy is proposed that uses accurately predicted dual-modality results to self-teach single-modality outcomes. The method demonstrates superior performance across six multimodal segmentation scenarios with minimal computational cost.",
        "Tags": [
            "Semantic Segmentation",
            "Multimodal Learning",
            "Parameter-Efficient Fine-Tuning",
            "Dynamic Sparse Fusion",
            "Self-Teaching Training Strategy"
        ]
    },
    {
        "Title": "Spk2SRImgNet: Super-Resolve Dynamic Scene from Spike Stream via Motion Aligned Collaborative Filtering",
        "Authors": "Yuanlin Wang \u00b7 Yiyang Zhang \u00b7 Ruiqin Xiong \u00b7 Jing Zhao \u00b7 Jian Zhang \u00b7 Xiaopeng Fan \u00b7 Tiejun Huang",
        "Abstract": "Spike camera is a kind of neuromorphic camera that records dynamic scenes by firing a stream of binary spikes with extremely high temporal resolution. It demonstrates great potential for vision tasks in high-speed scenarios. One limitation in its current implementation is the relatively low spatial resolution. This paper develops a network called Spk2SRImgNet to super-resolve high resolution images from low resolution spike stream. However, fluctuations in spike stream hinder the performance of spike camera super resolution. To address this issue, we propose a motion aligned collaborative filtering (MACF) module, which is motivated by key ideas in classic image restoration schemes to mitigate fluctuations in spike data. MACF leverages the temporal similarity of spike stream to acquire similar features from neighboring moments via motion alignment. To separate disturbances from features, MACF filters these similar features jointly in transform domain to exploit representation sparsity, and generates refinement features that will be used to update initial fluctuated features. Specifically, MACF designs an inverse motion alignment operation to map these refinement features back to their original positions. The initial features are aggregated with the repositioned refinement features to enhance reliability. Experimental results demonstrate that the proposed method achieves state-of-the-art performance compared with existing methods. The code will be made publicly available.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Spk2SRImgNet, a network designed to super-resolve high-resolution images from low-resolution spike streams captured by spike cameras, which are neuromorphic cameras with high temporal resolution but limited spatial resolution. To address the challenge of fluctuations in spike streams, the authors propose a Motion Aligned Collaborative Filtering (MACF) module. MACF utilizes temporal similarity in spike streams to align and filter similar features from neighboring moments, mitigating disturbances and enhancing feature reliability. The method involves motion alignment, transform domain filtering, and inverse motion alignment to refine and aggregate features, resulting in improved super-resolution performance. The proposed approach achieves state-of-the-art results compared to existing methods.",
        "Tags": [
            "Super-Resolution",
            "Low-Level Vision",
            "Spike Camera",
            "Motion Alignment",
            "Transform Domain Filtering",
            "Neuromorphic Vision"
        ]
    },
    {
        "Title": "Graph Neural Network Combining Event Stream and Periodic Aggregation for Low-Latency Event-based Vision",
        "Authors": "Manon Dampfhoffer \u00b7 Thomas Mesquida \u00b7 Damien Joubert \u00b7 Thomas Dalgaty \u00b7 Pascal Vivet \u00b7 Christoph Posch",
        "Abstract": "Event-based cameras asynchronously detect changes in light intensity with high temporal resolution, making them a promising alternative to RGB camera for low-latency and low-power optical flow estimation. However, state-of-the-art convolutional neural network methods create frames from the event stream, therefore losing the opportunity to exploit events for both sparse computations and low-latency prediction. On the other hand, asynchronous event graph methods could leverage both, but at the cost of avoiding any form of time accumulation, which limits the prediction accuracy. In this paper, we propose to break this accuracy-latency trade-off with a novel architecture combining an asynchronous accumulation-free event branch and a periodic aggregation branch. The periodic branch performs feature aggregations on the event graphs of past data to extract global context information, which improves accuracy without introducing any latency.The solution could predict optical flow per event with a latency of tens of microseconds on asynchronous hardware, which represents a gain of three orders of magnitude with respect to state-of-the-art frame-based methods, with 48x less operations per second. We show that the solution can detect rapid motion changes faster than a periodic output. This work proposes, for the first time, an effective solution for ultra low-latency and low-power optical flow prediction from event cameras.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a novel architecture for low-latency optical flow prediction using event-based cameras, which asynchronously detect light intensity changes. The proposed method combines an asynchronous event branch, which avoids time accumulation to maintain low latency, with a periodic aggregation branch that extracts global context from past event graphs to enhance accuracy. This dual-branch approach significantly reduces latency to tens of microseconds and decreases computational operations by 48 times compared to traditional frame-based methods, enabling rapid motion detection and offering a groundbreaking solution for ultra low-latency and low-power optical flow prediction.",
        "Tags": [
            "Graph Neural Networks (GNNs)",
            "Low-Latency Event-based Vision",
            "Optical Flow Estimation",
            "Asynchronous Processing",
            "Event-based Cameras"
        ]
    },
    {
        "Title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models",
        "Authors": "Matt Deitke \u00b7 Christopher Clark \u00b7 Sangho Lee \u00b7 Rohun Tripathi \u00b7 Yue Yang \u00b7 Jae Sung Park \u00b7 Reza Salehi \u00b7 Niklas Muennighoff \u00b7 Kyle Lo \u00b7 Luca Soldaini \u00b7 Jiasen Lu \u00b7 Taira Anderson \u00b7 Erin Bransom \u00b7 Kiana Ehsani \u00b7 Huong Ngo \u00b7 Yen-Sung Chen \u00b7 Ajay Patel \u00b7 Mark Yatskar \u00b7 Chris Callison-Burch \u00b7 Andrew Head \u00b7 Rose Hendrix \u00b7 Favyen Bastani \u00b7 Eli VanderBilt \u00b7 Nathan Lambert \u00b7 Yvonne Chou \u00b7 Arnavi Chheda-Kothary \u00b7 Jenna Sparks \u00b7 Sam Skjonsberg \u00b7 Michael Schmitz \u00b7 Aaron Sarnat \u00b7 Byron Bischoff \u00b7 Pete Walsh \u00b7 Christopher Newell \u00b7 Piper Wolters \u00b7 Tanmay Gupta \u00b7 Kuo-Hao Zeng \u00b7 Jon Borchardt \u00b7 Dirk Groeneveld \u00b7 Crystal Nam \u00b7 Sophie Lebrecht \u00b7 Caitlin Wittlif \u00b7 Carissa Schoenick \u00b7 Oscar Michel \u00b7 Ranjay Krishna \u00b7 Luca Weihs \u00b7 Noah A. Smith \u00b7 Hannaneh Hajishirzi \u00b7 Ross Girshick \u00b7 Ali Farhadi \u00b7 Aniruddha Kembhavi",
        "Abstract": "Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present \\textbf{Molmo}, a new family of VLMs that are state-of-the-art in their class of openness.  Our key contribution is a collection of new datasets, including a dataset of highly detailed image captions for pre-training called \\textbf{PixMo}, a free-form image Q\\&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and  on a large human evaluation. Our model weights, new datasets, and source code will all be released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces Molmo, a family of state-of-the-art vision-language models (VLMs) that are open-weight and open-data, addressing the gap in foundational knowledge for building performant VLMs from scratch. The key contribution includes new datasets such as PixMo for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without relying on external VLMs. The success of Molmo is attributed to careful modeling choices, a well-tuned training pipeline, and high-quality datasets. The 72B model outperforms other open-weight and data models and even larger proprietary models like Claude 3.5 Sonnet and Gemini 1.5 Pro, ranking second only to GPT-4o in both academic benchmarks and human evaluations. The model weights, datasets, and source code will be released.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Datasets and Benchmarks",
            "Open-Weight Models",
            "Dataset Creation",
            "Human Evaluation"
        ]
    },
    {
        "Title": "NN-Former: Rethinking Graph Structure in Neural Architecture Representation",
        "Authors": "Ruihan Xu \u00b7 Haokui Zhang \u00b7 Yaowei Wang \u00b7 Wei Zeng \u00b7 Shiliang Zhang",
        "Abstract": "The growing use of deep learning necessitates efficient network design and deployment, making neural predictors vital for estimating attributes such as accuracy and latency. Recently, Graph Neural Networks (GNNs) and transformers have shown promising performance in representing neural architectures. However, each method has its disadvantages. GNNs lack the capabilities to represent complicated features, while transformers face poor generalization when the depth of architecture grows. To mitigate the above problems, we rethink neural architecture topology and show that sibling nodes are pivotal while overlooked in previous research. Thus we propose a novel predictor leveraging the strengths of GNNs and transformers to learn the enhanced topology. We introduce a novel token mixer that considers siblings, and a new channel mixer named bidirectional graph isomorphism feed-forward network. Our approach consistently achieves promising performance in both accuracy and latency prediction, providing valuable insights for learning Directed Acyclic Graph (DAG) topology. The code will be released.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the limitations of Graph Neural Networks (GNNs) and transformers in representing neural architectures, particularly in terms of handling complex features and generalization with increasing depth. The authors propose a novel predictor that combines the strengths of GNNs and transformers, focusing on the importance of sibling nodes in neural architecture topology. They introduce a new token mixer that considers siblings and a bidirectional graph isomorphism feed-forward network as a channel mixer. This approach demonstrates superior performance in accuracy and latency prediction, offering new insights into learning Directed Acyclic Graph (DAG) topology.",
        "Tags": [
            "Graph Neural Networks (GNNs)",
            "Neural Architecture Search (NAS)",
            "Directed Acyclic Graph (DAG)",
            "Bidirectional Graph Isomorphism",
            "Token Mixer"
        ]
    },
    {
        "Title": "IRGS: Inter-Reflective Gaussian Splatting with 2D Gaussian Ray Tracing",
        "Authors": "Chun Gu \u00b7 Xiaofei Wei \u00b7 Zixuan Zeng \u00b7 Yuxuan Yao \u00b7 Li Zhang",
        "Abstract": "In inverse rendering, accurately modeling visibility and indirect radiance for incident light is essential for capturing secondary effects. Due to the absence of a powerful Gaussian ray tracer, previous 3DGS-based methods have either adopted a simplified rendering equation or used learnable parameters to approximate incident light, resulting in inaccurate material and lighting estimations. To this end, we introduce the inter-reflective Gaussian splatting (IRGS) framework for inverse rendering. To capture inter-reflection, we apply the full rendering equation without simplification and compute incident radiance on the fly using the proposed differentiable 2D Gaussian ray tracing. Additionally, we present an efficient optimization scheme to handle the computational demands of Monte Carlo sampling for rendering equation evaluation. Furthermore, we introduce a novel strategy for querying the indirect radiance of incident light when relighting the optimized scenes. Extensive experiments on multiple standard benchmarks validate the effectiveness of IRGS, demonstrating its capability to accurately model complex inter-reflection effects.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces the Inter-Reflective Gaussian Splatting (IRGS) framework for inverse rendering, addressing the challenge of accurately modeling visibility and indirect radiance for incident light. Unlike previous methods that simplify the rendering equation or use learnable parameters, IRGS applies the full rendering equation and employs differentiable 2D Gaussian ray tracing to compute incident radiance dynamically. The framework also includes an efficient optimization scheme to manage the computational demands of Monte Carlo sampling and a novel strategy for querying indirect radiance during scene relighting. Experimental results on standard benchmarks demonstrate IRGS's effectiveness in accurately capturing complex inter-reflection effects.",
        "Tags": [
            "3DGS (Gaussian Splatting)",
            "Inverse Rendering",
            "Differentiable Ray Tracing",
            "Monte Carlo Sampling",
            "Scene Relighting"
        ]
    },
    {
        "Title": "LiVOS: Light Video Object Segmentation with Gated Linear Matching",
        "Authors": "Qin Liu \u00b7 Jianfeng Wang \u00b7 Zhengyuan Yang \u00b7 Linjie Li \u00b7 Kevin Lin \u00b7 Marc Niethammer \u00b7 Lijuan Wang",
        "Abstract": "Semi-supervised video object segmentation (VOS) has been largely driven by space-time memory (STM) networks, which store past frame features in a spatiotemporal memory to segment the current frame via softmax attention. However, STM networks face memory limitations due to the quadratic complexity of softmax matching, restricting their applicability as video length and resolution increase. To address this, we propose LiVOS, a lightweight memory network that employs linear matching via linear attention, reformulating memory matching into a recurrent process that reduces the quadratic attention matrix to a constant-size, spatiotemporal-agnostic 2D state. To enhance selectivity, we introduce gated linear matching, where a data-dependent gate matrix is multiplied with the state matrix to control what information to retain or discard. Experiments on diverse benchmarks demonstrated the effectiveness of our method. It achieved 64.8 J&F on MOSE and 85.1 J&F on DAVIS, surpassing all non-STM methods and narrowing the gap with STM-based approaches. For longer and higher-resolution videos, it matched STM-based methods with 53% less GPU memory and supports 4096p inference on a 32G consumer-grade GPU--a previously cost-prohibitive capability--opening the door for long and high-resolution video foundation models.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces LiVOS, a lightweight memory network designed for semi-supervised video object segmentation (VOS). Unlike traditional space-time memory (STM) networks that use softmax attention with quadratic complexity, LiVOS employs linear matching via linear attention, reformulating memory matching into a recurrent process. This reduces the quadratic attention matrix to a constant-size, spatiotemporal-agnostic 2D state. To improve selectivity, the method incorporates gated linear matching, where a data-dependent gate matrix controls information retention. LiVOS achieves competitive performance on benchmarks, surpassing non-STM methods and significantly reducing GPU memory usage, enabling high-resolution video inference on consumer-grade GPUs.",
        "Tags": [
            "Video Object Segmentation",
            "Self-Supervised Learning",
            "Linear Attention",
            "Gated Matching",
            "Memory Efficiency"
        ]
    },
    {
        "Title": "Understanding multi-layered transmission matrices",
        "Authors": "Marina Alterman \u00b7 Anat Levin",
        "Abstract": "Transmission matrices, mapping the propagation of light from one end of the tissue to the other, form an important mathematical tool in the analysis of tissue scattering and the design of wavefront shaping systems. To understand the relationship between their content and the volumetric structure of the tissue, we wish to fit them with multi-slice models, composed of a set of planar aberrations spaced throughout the volume. The number of layers used in such a model would largely affect the amount of information compression and the ease in which we can use such layered models in a wavefront-shaping system. This work offers a theoretical study of such multi-layered models. We attempt to understand how many layers are required for a good fit, and how does the approximation degrade when a smaller number of such layers is used.  We show analytically that transmission matrices can be well fitted with very sparse layers. This leads to optimistic predictions on our ability to use them to design future wavefront shaping systems which can correct tissue aberration over a wide field-of-view.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper presents a theoretical study of multi-layered models for transmission matrices, which map light propagation through tissue. The study focuses on understanding the relationship between the number of layers in the model and the accuracy of fitting the transmission matrices. It is shown analytically that transmission matrices can be well approximated with sparse layers, offering optimistic implications for designing wavefront shaping systems capable of correcting tissue aberrations over a wide field-of-view.",
        "Tags": [
            "3D Reconstruction",
            "Low-Level Vision",
            "Wavefront Shaping",
            "Tissue Scattering",
            "Information Compression"
        ]
    },
    {
        "Title": "GLane3D : Detecting Lanes with Graph of 3D Keypoints",
        "Authors": "Halil \u0130brahim \u00d6zt\u00fcrk \u00b7 Muhammet Esat Kalfaoglu \u00b7 Ozsel Kilinc",
        "Abstract": "Accurate and efficient lane detection in 3D space is essential for autonomous driving systems, where robust generalization is the foremost requirement for 3D lane detection algorithms. Considering the extensive variation in lane structures worldwide, achieving high generalization capacity is particularly challenging, as algorithms must accurately identify a wide variety of lane patterns worldwide. Traditional top-down approaches rely heavily on learning lane characteristics from training datasets, often struggling with lanes exhibiting previously unseen attributes. To address this generalization limitation, we propose a method that detects keypoints of lanes and subsequently predicts sequential connections between them to construct complete 3D lanes. Each key point is essential for maintaining lane continuity, and we predict multiple proposals per keypoint by allowing adjacent grids to predict the same keypoint using an offset mechanism. PointNMS is employed to eliminate overlapping proposal keypoints, reducing redundancy in the estimated BEV graph and minimizing computational overhead from connection estimations. Our model surpasses previous state-of-the-art methods on both the Apollo and OpenLane datasets, demonstrating superior F1 scores and a strong generalization capacity when models trained on OpenLane are evaluated on the Apollo dataset, compared to prior approaches.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces GLane3D, a novel method for 3D lane detection that addresses the generalization challenges faced by traditional top-down approaches. By detecting keypoints of lanes and predicting sequential connections between them, the method constructs complete 3D lanes. It employs an offset mechanism for predicting multiple proposals per keypoint and uses PointNMS to eliminate overlapping keypoints, enhancing efficiency and reducing redundancy. GLane3D outperforms existing methods on the Apollo and OpenLane datasets, showcasing superior F1 scores and generalization capabilities.",
        "Tags": [
            "Lane Detection",
            "Autonomous Driving",
            "3D Point Cloud",
            "Graph Neural Networks (GNNs)",
            "Keypoint Detection",
            "PointNMS",
            "Generalization in Autonomous Systems"
        ]
    },
    {
        "Title": "R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning",
        "Authors": "Lijun Sheng \u00b7 Jian Liang \u00b7 Zilei Wang \u00b7 Ran He",
        "Abstract": "Vision-language models (VLMs), such as CLIP, have gained significant popularity as foundation models, with numerous fine-tuning methods developed to enhance performance on downstream tasks. However, due to their inherent vulnerability and the common practice of selecting from a limited set of open-source models, VLMs suffer from a higher risk of adversarial attacks than traditional visual models. Existing defense techniques typically rely on adversarial fine-tuning during training, which requires labeled data and is often difficult to generalize across tasks. To address these limitations, we propose robust test-time prompt tuning (R-TPT), which mitigates the impact of adversarial attacks during the inference stage. We first reformulate the classic marginal entropy objective by eliminating the term that introduces conflicts under adversarial conditions, retaining only the pointwise entropy minimization. Furthermore, we introduce a plug-and-play reliability-based weighted ensembling strategy, which aggregates useful information from reliable augmented views to strengthen the defense. R-TPT enhances defense against adversarial attacks without requiring labeled training data while offering high flexibility for inference tasks. Extensive experiments on widely used benchmarks with various attacks demonstrate the effectiveness of R-TPT. The code is available in supplementary materials.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Vision-language models (VLMs), like CLIP, are widely used as foundation models but are vulnerable to adversarial attacks. Existing defense methods often require adversarial fine-tuning during training, which relies on labeled data and lacks generalization. To address this, the authors propose robust test-time prompt tuning (R-TPT), a method that improves adversarial robustness during inference without labeled data. R-TPT reformulates the marginal entropy objective by removing conflicting terms under adversarial conditions and introduces a reliability-based weighted ensembling strategy to aggregate useful information from reliable augmented views. This approach enhances defense against adversarial attacks and offers flexibility for inference tasks.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Adversarial Robustness",
            "Test-Time Prompt Tuning",
            "Entropy Minimization",
            "Reliability-Based Ensembling"
        ]
    },
    {
        "Title": "TinyFusion: Diffusion Transformers Learned Shallow",
        "Authors": "Gongfan Fang \u00b7 Kunjun Li \u00b7 Xinyin Ma \u00b7 Xinchao Wang",
        "Abstract": "Diffusion Transformers have demonstrated remarkable capabilities in image generation but often come with excessive parameterization, resulting in considerable inference overhead in real-world applications. In this work, we present TinyFusion, a depth pruning method designed to remove redundant layers from diffusion transformers via end-to-end learning. The core principle of our approach is to create a pruned model with high recoverability, allowing it to regain strong performance after fine-tuning. To accomplish this, we introduce a differentiable sampling technique to make pruning learnable, paired with a co-optimized parameter to simulate future fine-tuning. While prior works focus on minimizing loss or error after pruning, our method explicitly models and optimizes the post-fine-tuning performance of pruned models. Experimental results indicate that this learnable paradigm offers substantial benefits for layer pruning of diffusion transformers, surpassing existing importance-based and error-based methods. Additionally, TinyFusion exhibits strong generalization across diverse architectures, such as DiTs, MARs, and SiTs. Experiments with DiT-XL show that TinyFusion can craft a shallow diffusion transformer at less than 7% of the pre-training cost, achieving a 2$\\times$ speedup with an FID score of 2.86, outperforming competitors with comparable efficiency.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "TinyFusion introduces a depth pruning method for diffusion transformers, aiming to reduce inference overhead by removing redundant layers through end-to-end learning. The approach focuses on creating pruned models with high recoverability, enabling strong performance post-fine-tuning. A differentiable sampling technique and co-optimized parameter are employed to simulate fine-tuning effects during pruning. TinyFusion outperforms existing importance-based and error-based methods, demonstrating strong generalization across architectures like DiTs, MARs, and SiTs. For instance, it achieves a 2\u00d7 speedup with an FID score of 2.86 using less than 7% of the pre-training cost.",
        "Tags": [
            "Diffusion Models",
            "Model Pruning",
            "Depth Pruning",
            "Differentiable Sampling",
            "Recoverability Optimization"
        ]
    },
    {
        "Title": "Is this Generated Person Existed in Real-world? Fine-grained Detecting and Calibrating Abnormal Human-body",
        "Authors": "Zeqing Wang \u00b7 Qingyang Ma \u00b7 Wentao Wan \u00b7 Haojie Li \u00b7 Keze Wang \u00b7 Yonghong Tian",
        "Abstract": "Recent improvements in visual synthesis have significantly enhanced the depiction of generated human photos, which are pivotal due to their wide applicability and demand. Nonetheless, the existing text-to-image or text-to-video models often generate low-quality human photos that might differ considerably from real-world body structures, referred to as ``abnormal human bodies''. Such abnormalities, typically deemed unacceptable, pose considerable challenges in the detection and repair of them within human photos. These challenges require precise abnormality recognition capabilities, which entail pinpointing both the location and the abnormality type. Intuitively, Visual Language Models (VLMs) that have obtained remarkable performance on various visual tasks are quite suitable for this task. However, their performance on abnormality detection in human photos is quite poor.Hence, it is quite important to highlight this task for the research community. In this paper, we first introduce a simple yet challenging task, i.e., \\textbf{F}ine-grained \\textbf{H}uman-body \\textbf{A}bnormality \\textbf{D}etection \\textbf{(FHAD)}, and construct two high-quality datasets for evaluation. Then, we propose a meticulous framework, named HumanCalibrator, which identifies and repairs abnormalities in human body structures while preserving the other content. Experiments indicate that our HumanCalibrator achieves high accuracy in abnormality detection and accomplishes an increase in visual comparisons while preserving the other visual content.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper addresses the challenge of detecting and repairing 'abnormal human bodies' in generated human photos, which often deviate from real-world body structures due to limitations in text-to-image or text-to-video models. The authors introduce a new task called Fine-grained Human-body Abnormality Detection (FHAD) and develop two high-quality datasets for evaluation. They propose a framework, HumanCalibrator, designed to identify and repair abnormalities in human body structures while preserving other visual content. The framework demonstrates high accuracy in abnormality detection and improves visual quality in generated images.",
        "Tags": [
            "Vision-Language Models (VLMs)",
            "Image Editing",
            "Abnormality Detection",
            "Human Body Synthesis",
            "Image Repair"
        ]
    },
    {
        "Title": "TKG-DM: Training-free Chroma Key Content Generation Diffusion Model",
        "Authors": "Ryugo Morita \u00b7 Stanislav Frolov \u00b7 Brian Bernhard Moser \u00b7 Takahiro Shirakawa \u00b7 Ko Watanabe \u00b7 Andreas Dengel \u00b7 Jinjia Zhou",
        "Abstract": "Diffusion models have enabled the generation of high-quality images with a strong focus on realism and textual fidelity. Yet, large-scale text-to-image models, such as Stable Diffusion, struggle to generate images where foreground objects are placed over a chroma key background, limiting their ability to separate foreground and background elements without fine-tuning. To address this limitation, we present a novel Training-Free Chroma Key Content Generation Diffusion Model (TKG-DM), which optimizes the initial random noise to produce images with foreground objects on a specifiable color background. Our proposed method is the first to explore the manipulation of the color aspects in initial noise for controlled background generation, enabling precise separation of foreground and background without fine-tuning. Extensive experiments demonstrate that our training-free method outperforms existing methods in both qualitative and quantitative evaluations, matching or surpassing fine-tuned models. Finally, we successfully extend it to other tasks (e.g., consistency models and text-to-video), highlighting its transformative potential across various generative applications where independent control of foreground and background is crucial.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces TKG-DM, a Training-Free Chroma Key Content Generation Diffusion Model that generates images with foreground objects on a specifiable color background without requiring fine-tuning. By optimizing the initial random noise, TKG-DM enables precise separation of foreground and background elements, outperforming existing methods in both qualitative and quantitative evaluations. The method is also extended to other generative tasks, demonstrating its versatility and potential for applications requiring independent control of foreground and background.",
        "Tags": [
            "Diffusion Models",
            "Image Generation",
            "Chroma Key Background",
            "Noise Optimization",
            "Training-Free Generation"
        ]
    },
    {
        "Title": "Differentiable Inverse Rendering with Interpretable Basis BRDFs",
        "Authors": "Hoon-Gyu Chung \u00b7 Seokjun Choi \u00b7 Seung-Hwan Baek",
        "Abstract": "Inverse rendering seeks to reconstruct both geometry and spatially varying BRDFs (SVBRDFs) from captured images. To address the inherent ill-posedness of inverse rendering, basis BRDF representations are commonly used, modeling SVBRDFs as spatially varying blends of a set of basis BRDFs. However, existing methods often yield basis BRDFs that lack intuitive separation and have limited scalability to scenes of varying complexity.In this paper, we introduce a differentiable inverse rendering method that produces interpretable basis BRDFs. Our approach models a scene using 2D Gaussians, where the reflectance of each Gaussian is defined by a weighted blend of basis BRDFs.We efficiently render an image from the 2D Gaussians and basis BRDFs using differentiable rasterization and impose a rendering loss with the input images.During this analysis-by-synthesis optimization process of differentiable inverse rendering, we dynamically adjust the number of basis BRDFs to fit the target scene while encouraging sparsity in the basis weights. This ensures that the reflectance of each Gaussian is represented by only a few basis BRDFs.This approach enables the reconstruction of accurate geometry and interpretable basis BRDFs that are spatially separated. Consequently, the resulting scene representation, comprising basis BRDFs and 2D Gaussians, supports physically-based novel-view relighting and intuitive scene editing.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces a differentiable inverse rendering method that produces interpretable basis BRDFs for reconstructing geometry and spatially varying BRDFs (SVBRDFs) from captured images. The approach models scenes using 2D Gaussians, with reflectance defined by a weighted blend of basis BRDFs. Differentiable rasterization is used to render images, and a rendering loss is imposed with input images. The method dynamically adjusts the number of basis BRDFs to fit the scene, encouraging sparsity in basis weights, ensuring each Gaussian's reflectance is represented by only a few basis BRDFs. This results in accurate geometry reconstruction and spatially separated, interpretable basis BRDFs, enabling physically-based novel-view relighting and intuitive scene editing.",
        "Tags": [
            "Inverse Rendering",
            "Differentiable Rendering",
            "2D Gaussians",
            "Sparse Basis Weights",
            "Novel-View Relighting"
        ]
    },
    {
        "Title": "UMFN: Unified Multi-Domain Face Normalization for Joint Cross-domain Prototype Learning and Heterogeneous Face Recognition",
        "Authors": "Meng Pang \u00b7 WenjunZhang \u00b7 Nanrun Zhou \u00b7 Shengbo Chen \u00b7 Hong Rao",
        "Abstract": "Face normalization aims to enhance the robustness and effectiveness of face recognition systems by mitigating intra-personal variations in expressions, poses, occlusions, illuminations, and domains. Existing methods face limitations in handling multiple variations and adapting to cross-domain scenarios. To address these challenges, we propose a novel Unified Multi-Domain Face Normalization Network (UMFN) model, which can process face images with various types of facial variations from different domains, and reconstruct frontal, neutral-expression facial prototypes in the target domain. As an unsupervised domain adaptation model, UMFN facilitates concurrent training on multiple datasets across domains and demonstrates strong prototype reconstruction capabilities. Notably, UMFN serves as a joint prototype and feature learning framework, enabling the simultaneous extraction of domain-agnostic identity features through a decoupling mapping network and a feature domain classifier for adversarial training. Moreover, we design an efficient Heterogeneous Face Recognition (HFR) network that fuses domain-agnostic and identity-discriminative features for HFR, and introduce contrastive learning to enhance identity recognition accuracy. Empirical studies on diverse cross-domain face datasets validate the effectiveness of our proposed method.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "The paper introduces the Unified Multi-Domain Face Normalization Network (UMFN), a novel model designed to enhance face recognition systems by addressing intra-personal variations such as expressions, poses, occlusions, illuminations, and domains. UMFN reconstructs frontal, neutral-expression facial prototypes across different domains and supports unsupervised domain adaptation, allowing for concurrent training on multiple datasets. The model integrates a joint prototype and feature learning framework, utilizing a decoupling mapping network and a feature domain classifier for adversarial training to extract domain-agnostic identity features. Additionally, an efficient Heterogeneous Face Recognition (HFR) network is developed, which combines domain-agnostic and identity-discriminative features and employs contrastive learning to improve identity recognition accuracy. The effectiveness of UMFN is demonstrated through empirical studies on various cross-domain face datasets.",
        "Tags": [
            "Face Normalization",
            "Heterogeneous Face Recognition",
            "Unsupervised Domain Adaptation",
            "Contrastive Learning",
            "Prototype Reconstruction",
            "Domain-Agnostic Features",
            "Adversarial Training"
        ]
    },
    {
        "Title": "Improve Representation for Imbalanced Regression through Geometric Constraints",
        "Authors": "Zijian Dong \u00b7 Yilei Wu \u00b7 Chongyao Chen \u00b7 Yingtian Zou \u00b7 Yichi Zhang \u00b7 Juan Helen Zhou",
        "Abstract": "In representation learning, uniformity refers to the uniform feature distribution in the latent space (i.e., unit hypersphere). Previous work has shown that improving uniformity contributes to the learning of under-represented classes. However, most of the previous work focused on classification; the representation space of imbalanced regression remains unexplored. Classification-based methods are not suitable for regression tasks because they cluster features into distinct groups without considering the continuous and ordered nature essential for regression. In a geometric aspect, we uniquely focus on ensuring uniformity in the latent space for imbalanced regression through two key losses: enveloping and homogeneity. The enveloping loss encourages the induced trace to uniformly occupy the surface of a hypersphere, while the homogeneity loss ensures smoothness, with representations evenly spaced at consistent intervals. Our method integrates these geometric principles into the data representations via a Surrogate-driven Representation Learning (SRL) framework. Experiments with real-world regression and operator learning tasks highlight the importance of uniformity in imbalanced regression and validate the efficacy of our geometry-based loss functions.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper addresses the challenge of improving representation learning for imbalanced regression tasks by focusing on geometric constraints in the latent space. Unlike classification tasks, regression requires continuous and ordered representations, which are not adequately addressed by existing methods. The authors propose two novel loss functions\u2014enveloping and homogeneity\u2014to ensure uniformity and smoothness in the latent space. These losses are integrated into a Surrogate-driven Representation Learning (SRL) framework. The approach is validated on real-world regression tasks, demonstrating the importance of geometric principles in enhancing representation quality for imbalanced regression.",
        "Tags": [
            "Imbalanced Regression",
            "Representation Learning",
            "Geometric Constraints",
            "Surrogate-driven Representation Learning",
            "Hypersphere Uniformity"
        ]
    },
    {
        "Title": "Period-LLM: Extending the Periodic Capability of Multimodal Large Language Model",
        "Authors": "Yuting Zhang \u00b7 Hao Lu \u00b7 Qingyong Hu \u00b7 Yin Wang \u00b7 Kaishen Yuan \u00b7 Xin Liu \u00b7 Kaishun Wu",
        "Abstract": "Periodic or quasi-periodic phenomena reveal intrinsic characteristics in various natural processes, such as weather patterns, movement behaviors, traffic flows, and biological signals. Given that these phenomena span multiple modalities, the capabilities of Multimodal Large Language Models (MLLMs) offer promising potential to effectively capture and understand their complex nature. However, current MLLMs struggle with periodic tasks due to limitations in: 1) lack of temporal modelling and 2) conflict between short and long periods. This paper introduces Period-LLM, a multimodal large language model designed to enhance the performance of periodic tasks across various modalities, and constructs a benchmark of various difficulty for evaluating the cross-modal periodic capabilities of large models. Specially, We adopt an ``Easy to Hard Generalization\" paradigm, starting with relatively simple text-based tasks and progressing to more complex visual and multimodal tasks, ensuring that the model gradually builds robust periodic reasoning capabilities. Additionally, we propose a Resisting Logical Oblivion optimization strategy to maintain periodic reasoning abilities during semantic alignment. Extensive experiments demonstrate the superiority of the proposed Period-LLM over existing MLLMs in periodic tasks. The code will be available on GitHub.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "Periodic or quasi-periodic phenomena are intrinsic to various natural processes, such as weather patterns, movement behaviors, and biological signals. Multimodal Large Language Models (MLLMs) have potential to understand these complex phenomena, but current models face challenges in temporal modeling and handling conflicts between short and long periods. This paper introduces Period-LLM, a model designed to enhance performance in periodic tasks across modalities. It employs an 'Easy to Hard Generalization' paradigm, starting with simple text-based tasks and progressing to complex visual and multimodal tasks, ensuring robust periodic reasoning. A Resisting Logical Oblivion optimization strategy is proposed to maintain periodic reasoning during semantic alignment. Experiments show Period-LLM's superiority over existing MLLMs in periodic tasks.",
        "Tags": [
            "Multimodal Large Language Models (MLLMs)",
            "Periodic Phenomena Analysis",
            "Temporal Modeling",
            "Cross-Modal Reasoning",
            "Semantic Alignment Optimization"
        ]
    },
    {
        "Title": "PSBD: Prediction Shift Uncertainty Unlocks Backdoor Detection",
        "Authors": "Wei Li \u00b7 Pin-Yu Chen \u00b7 Sijia Liu \u00b7 Ren Wang",
        "Abstract": "Deep neural networks are susceptible to backdoor attacks, where adversaries manipulate model predictions by inserting malicious samples into the training data. Currently, there is still a significant challenge in identifying suspicious training data to unveil potential backdoor samples. In this paper, we propose a novel method, Prediction Shift Backdoor Detection (PSBD), leveraging an uncertainty-based approach requiring minimal unlabeled clean validation data. PSBD is motivated by an intriguing Prediction Shift (PS) phenomenon, where poisoned models' predictions on clean data often shift away from true labels towards certain other labels with dropout applied during inference, while backdoor samples exhibit less PS. We hypothesize PS results from neuron bias effect, making neurons favor features of certain classes. PSBD identifies backdoor training samples by computing the Prediction Shift Uncertainty (PSU), the variance in probability values when dropout layers are toggled on and off during model inference. Extensive experiments have been conducted to verify the effectiveness and efficiency of PSBD, which achieves state-of-the-art results among mainstream detection methods.",
        "Link": null,
        "Year": 2025,
        "Conference": "cvpr",
        "Abstract_Summary": "This paper introduces Prediction Shift Backdoor Detection (PSBD), a novel method for detecting backdoor attacks in deep neural networks. PSBD utilizes an uncertainty-based approach that requires minimal unlabeled clean validation data. The method is based on the Prediction Shift (PS) phenomenon, where poisoned models' predictions on clean data shift away from true labels when dropout is applied during inference, while backdoor samples show less PS. PSBD identifies backdoor samples by computing Prediction Shift Uncertainty (PSU), which measures the variance in probability values when dropout layers are toggled on and off. The method achieves state-of-the-art results in backdoor detection.",
        "Tags": [
            "Backbone",
            "Anomaly Detection",
            "Uncertainty Estimation",
            "Dropout Analysis",
            "Backdoor Attack Detection"
        ]
    }
]
