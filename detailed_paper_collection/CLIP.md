# **CLIP - Full Paper Collection**

## Logits DeConfusion with CLIP for Few-Shot Learning
- **Tags:** CLIP, Few-Shot Learning, Multi-level Adapter Fusion, Inter-Class Deconfusion, Residual Structure
## SpatialCLIP: Learning 3D-aware Image Representations from Spatially Discriminative Language
- **Tags:** CLIP, Vision-Language Models (VLMs), 3D-aware Image Representations, Spatial Reasoning, 3D-inspired ViT, Hard Negative Captions, SpatialBench
## Towards More General Video-based Deepfake Detection through Facial Feature Guided Adaptation for Foundation Model
- **Tags:** Deepfake Detection, CLIP, Facial Component Guidance, Spatial-Temporal Cues, Cross-Dataset Generalization
## Dr. Splat: Directly Referring 3D Gaussian Splatting via Direct Language Embedding Registration
- **Tags:** 3DGS (Gaussian Splatting), CLIP, 3D Semantic Segmentation, Open-Vocabulary 3D Understanding, Product Quantization, Language Feature Registration
## Semantic Library Adaptation: LoRA Retrieval and Fusion for Open-Vocabulary Semantic Segmentation
- **Tags:** Open-Vocabulary Semantic Segmentation, CLIP, Domain Adaptation, LoRA-based Adapters, Training-Free Adaptation, Dynamic Adapter Fusion
## Occlusion-aware Text-Image-Point Cloud Pretraining for Open-World 3D Object Recognition
- **Tags:** 3D Point Cloud, Zero-Shot Learning, CLIP, Occlusion-aware Pretraining, DuoMamba, Space-filling Curves
## ImagineFSL: Self-Supervised Pretraining Matters on Imagined Base Set for VLM-based Few-shot Learning
- **Tags:** CLIP, Self-Supervised Learning, Few-Shot Learning, Text-to-Image Generation, Chain-of-Thought Learning, In-Context Learning
## SciBench: Addressing Scientific Illusions in Image Synthesis
- **Tags:** Image Generation, CLIP, Data Augmentation, Scientific Knowledge Integration, Adversarial Dataset, Reward Model
## SAIST: Segment Any Infrared Small Target Model Guided by Contrastive Language-Image Pretraining
- **Tags:** Infrared Small Target Detection, Vision-Language Models (VLMs), Multimodal Learning, Contrastive Language-Image Pretraining (CLIP), Infrared Imaging
## SketchFusion: Learning Universal Sketch Features through Fusing Foundation Models
- **Tags:** Sketch Understanding, CLIP, Stable Diffusion, Feature Fusion, Semantic-Level Aggregation, Frequency-Domain Bias Correction
## ResCLIP: Residual Attention for Training-free Dense Vision-language Inference
- **Tags:** CLIP, Vision-Language Models (VLMs), Self-Supervised Learning, Residual Attention, Semantic Feedback Refinement, Dense Vision-Language Inference
## FineLIP: Extending CLIP’s Reach via Fine-Grained Alignment with Longer Text Inputs
- **Tags:** CLIP, Vision-Language Models (VLMs), Zero-Shot Learning, Fine-Grained Alignment, Dynamic Token Aggregation, Longer Text Inputs
## Vision-Language Model IP Protection via Prompt-based Learning
- **Tags:** Vision-Language Models (VLMs), CLIP, Intellectual Property (IP) Protection, Prompt-based Learning, Feature Transfer Prevention, Style-Enhancement Branch, Performance Metrics for IP Protection
## GOAL: Global-local Object Alignment Learning
- **Tags:** Vision-Language Models (VLMs), CLIP, Local Image-Sentence Matching, Token Similarity-based Learning, Image-Lengthy Text Retrieval
## Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions
- **Tags:** Text-to-Image Generation, CLIP, Token-Level Directions, Subject-Specific Attribute Control, Prompt Augmentation
- **Link:** [Link](https://compvis.github.io/attribute-control/)

## DPC: Dual-Prompt Collaboration for Tuning Vision-Language Models
- **Tags:** Vision-Language Models, CLIP, Prompt Tuning, Weighting-Decoupling, Dynamic Hard Negative Optimizer, Feature Channel Invariance
## Towards Smart Point-and-Shoot Photography
- **Tags:** Image Quality Assessment, CLIP, Camera Pose Adjustment, Image Composition, Learnable Text Embedding, Mixture-of-Experts Model, Gated Loss Function
## DistinctAD: Distinctive Audio Description Generation in Contexts
- **Tags:** Vision-Language Models (VLMs), Data Augmentation, CLIP-AD adaptation, Contextual Expectation-Maximization Attention, Distinctive word prediction loss
## GuardSplat:  Efficient and Robust Watermarking for 3D Gaussian Splatting
- **Tags:** 3DGS (Gaussian Splatting), CLIP, Watermarking, Message Decoupling Optimization, Spherical-harmonic-aware Embedding, Anti-distortion Extraction
- **Link:** [Link](https://narcissusex.github.io/GuardSplat/)

## Scaling Vision Pre-Training to 4K Resolution
- **Tags:** CLIP, Multimodal Large Language Models (MLLMs), High-Resolution Vision, Selective Region Processing, 4K Benchmark
## Conformal Prediction for Zero-Shot Models
- **Tags:** Zero-Shot Learning, CLIP, Conformal Prediction, Optimal Transport, Domain Adaptation
## Forensics Adapter: Adapting CLIP for Generalizable Face Forgery Detection
- **Tags:** CLIP, Face Forgery Detection, Adapter Networks, Knowledge Interaction Strategy, Generalizability
## Reproducible Vision-Language Models Meet Concepts Out of Pre-Training
- **Tags:** Vision-Language Models (VLMs), Zero-Shot Learning, Name-Tuning, Image-Text Alignment, OpenCLIP
## Language Guided Concept Bottleneck Models for Interpretable Continual Learning
- **Tags:** Continual Learning, Interpretability, Concept Bottleneck Models, CLIP, Semantic Consistency, Transparent Decision-Making, Concept Visualization
## Retaining Knowledge and Enhancing Long-Text Representations in CLIP through Dual-Teacher Distillation
- **Tags:** CLIP, Long-Tail Learning, Dual-Teacher Distillation, Long-Text Representation, Knowledge Retention
## CLIP is Almost All You Need: Towards Parameter-Efficient Scene Text Retrieval without OCR
- **Tags:** CLIP, Scene Text Retrieval, Parameter-Efficient Transfer Learning, Cross-Attention Mechanism, Visual Context Dropout
## Enhancing Few-Shot Class-Incremental Learning via Training-Free Bi-Level Modality Calibration
- **Tags:** Few-Shot Learning, CLIP, Vision-Language Models (VLMs), Bi-level Modality Calibration, Training-Free Framework, Intra-modal and Inter-modal Calibration
## Reconstructing Animals and the Wild
- **Tags:** 3D Reconstruction, CLIP, Single Image Reconstruction, Synthetic Dataset, Environmental Context Integration
## Domain Generalization in CLIP via Learning with Diverse Text Prompts
- **Tags:** Domain Generalization, Vision-Language Models (VLMs), CLIP, Text Prompts, Feature Suppression, Feature Diversification
## Incorporating Dense Knowledge Alignment into Unified Multimodal Representation Models
- **Tags:** Multimodal Large Language Models (MLLMs), CLIP, Multimodal Alignment, Cross-Modal Retrieval, Contrastive Learning
## Parameter-efficient Fine-tuning in Hyperspherical Space for Open-vocabulary Semantic Segmentation
- **Tags:** CLIP, Semantic Segmentation, Parameter-efficient Fine-tuning, Hyperspherical Space, Dual Cross-relation Communication
## SLADE: Shielding against Dual Exploits in Large Vision-Language Models
- **Tags:** Vision-Language Models (VLMs), Self-Supervised Learning, Adversarial Robustness, Adversarial Fine-Tuning, Dual-Level Contrastive Learning, CLIP-Based Encoders
## OSLoPrompt: Bridging Low-Supervision Challenges and Open-Set Domain Generalization in CLIP
- **Tags:** CLIP, Open-Set Domain Generalization, Low-Shot Learning, Prompt-Learning, Cross-Attention Module, Pseudo-Open Sample Synthesis
- **Link:** [Link](https://has97.github.io/projects/osloprompt/)

## Harnessing Frozen Unimodal Encoders for Flexible Multimodal Alignment
- **Tags:** Multimodal Learning, CLIP, Frozen Encoders, Efficient Multimodal Alignment, MLP Projectors
## Understanding Fine-tuning CLIP for Open-vocabulary Semantic Segmentation in Hyperbolic Space
- **Tags:** CLIP, Semantic Segmentation, Hyperbolic Space, Hierarchical Alignment, Parameter Efficiency
## Targeted Forgetting of Image Subgroups in CLIP Models
- **Tags:** CLIP, Model Unlearning, Knowledge Distillation, Fine-Grained Unlearning, Model Souping, Distribution Disparity Handling
## SMILE: Infusing Spatial and Motion Semantics in Masked Video Learning
- **Tags:** Self-Supervised Learning, Video Understanding, CLIP, Synthetic Motion Patterns, Video Representation Learning
## Preserve or Modify? Context-Aware Evaluation for Balancing Preservation and Modification in Text-Guided Image Editing
- **Tags:** Vision-Language Models (VLMs), Image Editing, Context-Aware Metrics, CLIP Space Analysis, Multi-Modal Large Language Models
## Vision-Language Models Do Not Understand Negation
- **Tags:** Vision-Language Models (VLMs), Negation Understanding, NegBench, CLIP Fine-Tuning, Synthetic Data
## Classifier-guided CLIP Distillation for Unsupervised Multi-label Classification
- **Tags:** CLIP, Unsupervised Learning, Multi-label Classification, Class Activation Mapping (CAM), Pseudo-label Debias, Multi-view Learning
## MARBLE: Material Recomposition and Blending in CLIP-Space
- **Tags:** CLIP, Image Editing, Material Recomposition, Material Blending, Material Attributes, Denoising UNet, Parametric Control
## Zero-Shot 4D Lidar Panoptic Segmentation
- **Tags:** Zero-Shot Learning, 3D Semantic Segmentation, Panoptic Segmentation, Multi-Modal Learning, Video Object Segmentation, CLIP Tokens
## Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations
- **Tags:** Out-of-Distribution Detection (OoDD), Multimodal Learning, CLIP, Cross-Modal Alignment, Energy-Based Model, Hyperspherical Representation Space
## Advancing Myopia To Holism: Fully Contrastive Language–Image Pre-training
- **Tags:** Vision-Language Models (VLMs), Contrastive Language-Image Pre-training (CLIP), Multi-Text Generation, Multi-Branch Image Encoder, Part-to-Part Matching
## Panorama Generation From NFoV Image Done Right
- **Tags:** Panorama Generation, Diffusion Models, CLIP, Distortion Evaluation, Virtual Reality (VR), Distort-CLIP, PanoDecouple, Distortion Correction
## Hybrid Global-Local Representation with Augmented Spatial Guidance for Zero-Shot Referring Image Segmentation
- **Tags:** Zero-Shot Learning, Semantic Segmentation, CLIP, Spatial Guidance Augmentation, Cross-Modal Alignment, Mask Region Representation
## GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery
- **Tags:** CLIP, Multimodal Learning, Generalized Category Discovery, Text Embedding Synthesizer, Dual-Branch Framework, Vision-Language Alignment
## SmartCLIP: Modular Vision-language Alignment with Identification Guarantees
- **Tags:** CLIP, Multimodal Learning, Modular Alignment, Disentangled Representations, Cross-Modal Semantic Preservation
## CLIP Under the Microscope: A Fine-Grained Analysis of Multi-Object Representation
- **Tags:** CLIP, Multimodal Learning, Bias Analysis, Text-to-Image Models, Prompt Engineering
## NeighborRetr: Balancing Hub Centrality in Cross-Modal Retrieval
- **Tags:** Cross-Modal Retrieval, CLIP, Hubness Problem, Adaptive Neighbor Relations, Generalization to New Domains
## Seeing What Matters: Empowering CLIP with Patch Generation-to-Selection
- **Tags:** CLIP, Self-Supervised Learning, Data Augmentation, Patch Selection, Optimal Transport, Edge Detection
## Open Ad-hoc Categorization with Contextualized Feature Learning
- **Tags:** CLIP, Self-Supervised Learning, Contextualized Feature Learning, Ad-hoc Categorization, Interpretable Saliency Maps
## Exploring Semantic Feature Discrimination for Perceptual Image Super-Resolution and Opinion-Unaware No-Reference Image Quality Assessment
- **Tags:** Generative Adversarial Networks (GANs), Super-Resolution, CLIP, Image Quality Assessment, Semantic Feature Discrimination, Text-Guided Discrimination, No-Reference Image Quality Assessment
## Preserving Clusters in Prompt Learning for Unsupervised Domain Adaptation
- **Tags:** CLIP, Unsupervised Domain Adaptation (UDA), Optimal Transport Theory, Multi-modal Embeddings, Cluster Preservation
## Hyperbolic Uncertainty-Aware Few-Shot Incremental Point Cloud Segmentation
- **Tags:** 3D Point Cloud, Few-Shot Learning (FSL), Class Incremental Learning (CIL), Hyperbolic Embeddings, Poincaré Hyperbolic Sphere, CLIP-derived Semantics
## EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation
- **Tags:** Diffusion Models, Text-to-Image Generation, Latent Space Manipulation, CLIP-based Attention, Grid-based Generation
## AA-CLIP: Enhancing Zero-shot Anomaly Detection via Anomaly-Aware CLIP
- **Tags:** Anomaly Detection, CLIP, Zero-Shot Learning, Patch-Level Feature Alignment, Linear Residual Adapters, Anomaly-Aware Text Anchors
## CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole Slide Image Analysis in Computational Pathology
- **Tags:** Multimodal Large Language Models (MLLMs), Medical Image Analysis, Unified Pathology Model, CLIP-based Visual Processor, Zero-Shot Learning
## Unbiased General Annotated Dataset Generation
- **Tags:** Data Augmentation, Diffusion Models, CLIP, Unbiased Dataset Generation, Semantic Alignment Loss, Image Quality Assurance
## SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images
- **Tags:** Remote Sensing Image Analysis, Semantic Segmentation, Open-Vocabulary Segmentation, CLIP-based Bias Correction, Feature Upsampling
## SGC-Net: Stratified Granular Comparison Network for Open-Vocabulary HOI Detection
- **Tags:** Open-Vocabulary HOI Detection, CLIP, Large Language Models (LLMs), Granularity Sensing Alignment, Hierarchical Group Comparison, OV-HOI Detection
## Exploring CLIP's Dense Knowledge for Weakly Supervised Semantic Segmentation
- **Tags:** CLIP, Weakly Supervised Semantic Segmentation, Patch-Text Alignment, Text Semantic Enrichment, Visual Calibration
## GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill
- **Tags:** Large Language Models (LLMs), Vision-Language Models (VLMs), Pose2CLIP, Iterative Reward Design, Open-Vocabulary Learning
## Mask-Adapter: The Devil is in the Masks for Open-Vocabulary Segmentation
- **Tags:** Semantic Segmentation, CLIP, Open-Vocabulary Segmentation, Mask Pooling, Semantic Activation Maps, Mask Consistency Loss
## CLIP is Strong Enough to Fight Back: Test-time Counterattacks towards Zero-shot Adversarial Robustness of CLIP
- **Tags:** CLIP, Adversarial Robustness, Zero-Shot Learning, Test-Time Defense, Adversarial Perturbations, Training-Free Robustness
