# **Knowledge Distillation - Full Paper Collection**

## Distilling Long-tailed Datasets
- **Tags:** Long-Tail Learning, Knowledge Distillation, Dataset Distillation, Soft-label Initialization, Expert Decoupling
## SDGOCC: Semantic and Depth-Guided Bird’s-Eye View Transformation for 3D Multimodal Occupancy Prediction
- **Tags:** 3D Semantic Segmentation, Autonomous Driving, Multimodal Learning, Depth Estimation, Knowledge Distillation, Real-Time Processing
- **Link:** [Link](https://github.com/)

## MoVE-KD: Knowledge Distillation for VLMs with Mixture of Visual Encoders
- **Tags:** Knowledge Distillation, Vision-Language Models (VLMs), Low-Rank Adaptation (LoRA), Mixture-of-Experts (MoEs), Attention-Based Distillation
## Distilling Spatially-Heterogeneous Distortion Perception for Blind Image Quality Assessment
- **Tags:** Image Quality Assessment, Knowledge Distillation, Spatially Heterogeneous Distortions, Block-wise Degradation Modelling, Contrastive Knowledge Distillation
## What Makes a Good Dataset for Knowledge Distillation?
- **Tags:** Knowledge Distillation, Data Augmentation, Synthetic Data, Model Compression, Surrogate Datasets
## See Further When Clear: Curriculum Consistency Model
- **Tags:** Diffusion Models, Knowledge Distillation, Curriculum Learning, Image-Text Alignment, Single-Step Sampling
## Incremental Object Keypoint Learning
- **Tags:** Object Detection, Self-Supervised Learning, Incremental Learning, Keypoint Estimation, Knowledge Distillation
## Hierarchical Knowledge Prompt Tuning for Multi-task Test-Time Adaptation
- **Tags:** Vision-Language Models (VLMs), Test-Time Adaptation, Prompt Tuning, Knowledge Distillation, Multi-task Learning
## Knowledge Bridger: Towards Training-free Missing Multi-modality Completion
- **Tags:** Multimodal Learning, Knowledge Distillation, Knowledge Graphs, Modality-Agnostic Framework, Out-of-Domain Generalization
## Align-KD: Distilling Cross-Modal Alignment Knowledge for Mobile Vision-Language Model
- **Tags:** Knowledge Distillation, Vision-Language Models (VLMs), Cross-Modal Alignment, Mobile AI, Model Compression
## Taming High-Resolution Text-to-Image Models for Mobile Devices with Efficient Architectures and Training
- **Tags:** Text-to-Image Generation, Knowledge Distillation, Mobile Optimization, Adversarial Guidance, Few-Step Generation
## VL2Lite: Task-Specific Knowledge Distillation from Large Vision-Language Models to Lightweight Networks
- **Tags:** Knowledge Distillation, Vision-Language Models (VLMs), Image Classification, Lightweight Networks, Multi-Modal Knowledge Transfer
## Learning Compatible Multi-Prize Subnetworks for Asymmetric Retrieval
- **Tags:** Knowledge Distillation, Model Pruning, Post-Training Pruning, Conflict-Aware Gradient Integration, Self-Compatible Networks
## DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any Architecture
- **Tags:** Diffusion Models, Knowledge Distillation, Data-Free Training, Dynamic Iterative Distillation, Generative Model Efficiency
## Separation of powers: On segregating knowledge from observation in LLM-enabled knowledge-based visual question answering
- **Tags:** Large Language Models (LLMs), Vision-Language Models (VLMs), Knowledge Distillation, Visual Question Answering, Question-Aware Captioning, GPT-4 Augmented Dataset, State-of-the-Art Benchmarks
## CustomKD: Customizing Large Vision Foundation for Edge Model Improvement via Knowledge Distillation
- **Tags:** Knowledge Distillation, Vision-Language Models (VLMs), Model Discrepancy Reduction, Unsupervised Domain Adaptation, Semi-Supervised Learning
## Random Conditioning for Diffusion Model Compression with Distillation
- **Tags:** Diffusion Models, Knowledge Distillation, Random Conditioning, Image-Free Distillation, Conditional Diffusion Models
## Task Singular Vectors: Reducing Task Interference in Model Merging
- **Tags:** Model Pruning, Knowledge Distillation, Singular Value Decomposition, Task Interference Reduction, Layer-Level Analysis
## Multi-modal Knowledge Distillation-based Human Trajectory Forecasting
- **Tags:** Knowledge Distillation, Autonomous Driving, Multimodal Learning, Pedestrian Trajectory Forecasting, Vision-Language Models, Resource-Constrained Systems
## Curriculum Coarse-to-Fine Selection for High-IPC Dataset Distillation
- **Tags:** Data Augmentation, Knowledge Distillation, Curriculum Learning, High-IPC Dataset Distillation, Synthetic Data Integration
## CoMBO: Conflict Mitigation via Branched Optimization for Class Incremental Segmentation
- **Tags:** Class Incremental Segmentation, Knowledge Distillation, Query Conflict Reduction, Half-Learning Half-Distillation, Importance-Based Knowledge Distillation
## Plug-and-Play Interpretable Responsible Text-to-Image Generation via Dual-Space Multi-facet Concept Control
- **Tags:** Text-to-Image Generation, Responsible AI, Knowledge Distillation, Concept Whitening, Plug-and-Play Mechanism, Diffusion Model Integration
## Accurate Scene Text Recognition with Efficient Model Scaling and Cloze Self-Distillation
- **Tags:** Scene Text Recognition (STR), Knowledge Distillation, Label Noise Mitigation, Differential Cross-Attention, Model Scaling
## Learning Occlusion-Robust Vision Transformers for Real-Time UAV Tracking
- **Tags:** Vision Transformer (ViT), Visual Tracking, Occlusion-Robust Representations, Knowledge Distillation, Real-Time Tracking
- **Link:** [Link](https://github.com/qtyz-ogvm/ORTrack)

## Skip Tuning: Pre-trained Vision-Language Models are Effective and Efficient Adapters Themselves
- **Tags:** Vision-Language Models (VLMs), Knowledge Distillation, Layer-wise Skipping, Class-wise Skipping, Feature-gradient Propagation
- **Link:** [Link](https://github.com/anonymity-007/SkipT)

## Simplification Is All You Need against Out-of-Distribution Overconfidence
- **Tags:** Knowledge Distillation, Out-of-Distribution Detection, Model Simplification, ReLU Reduction, OOD Overconfidence Mitigation
## Towards RAW Object Detection in Diverse Conditions
- **Tags:** Object Detection, RAW Image Processing, Knowledge Distillation, Domain Adaptation, Adverse Conditions
## Distilling Monocular Foundation Model for Fine-grained Depth Completion
- **Tags:** Depth Estimation, Knowledge Distillation, Monocular Depth Estimation, Scale- and Shift-Invariant Loss, LiDAR Simulation
## BiM-VFI: Bidirectional Motion Field-Guided Frame Interpolation for Video with Non-uniform Motions
- **Tags:** Video Frame Interpolation (VFI), Knowledge Distillation, Bidirectional Motion Field, Content-Aware Upsampling, VFI-centric Flow Supervision
## U-Know-DiffPAN: An Uncertainty-aware Knowledge Distillation Diffusion Framework with Details Enhancement for PAN-Sharpening
- **Tags:** Knowledge Distillation, Diffusion Models, Frequency Selective Attention, Wavelet Transforms, Uncertainty Map
- **Link:** [Link](https://github.com/xxx/yyy)

## Decoupled Distillation to Erase: A General Unlearning Method for Any Class-centric Tasks
- **Tags:** Knowledge Distillation, Unlearning, Mask Distillation, Dark Knowledge, Class-centric Unlearning
## ADU: Adaptive Detection of Unknown Categories in Black-Box Domain Adaptation
- **Tags:** Black-box Domain Adaptation, Knowledge Distillation, Unknown Category Detection, Selective Amplification Knowledge Distillation, Entropy-Driven Label Differentiation, Adaptive Thresholding
## Heterogeneous Teacher Distillation
- **Tags:** Knowledge Distillation, 3D Understanding, Multi-teacher Distillation, Generalization Capabilities, Compact Encoder
## Targeted Forgetting of Image Subgroups in CLIP Models
- **Tags:** CLIP, Model Unlearning, Knowledge Distillation, Fine-Grained Unlearning, Model Souping, Distribution Disparity Handling
## Distilling Multi-modal Large Language Models for Autonomous Driving
- **Tags:** Autonomous Driving, Large Language Models (LLMs), Motion Planning, Knowledge Distillation, Long-Tail Learning
## Two is Better than One: Efficient Ensemble Defense for Robust and Compact Models
- **Tags:** Model Pruning, Knowledge Distillation, Adversarial Robustness, Ensemble Learning, Resource Efficiency
## EditAR: Unified Conditional Generation with Autoregressive Models
- **Tags:** Image Editing, Autoregressive Models, Unified Framework, Knowledge Distillation, Text-to-Image Alignment
## BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models
- **Tags:** Vision-Language Models (VLMs), Biomedical Image Analysis, Prompt Learning, Few-Shot Learning, Knowledge Distillation
## RADIO Amplified: Improved Baselines for Agglomerative Vision Foundation Models
- **Tags:** Vision-Language Models (VLMs), Knowledge Distillation, Multi-Teacher Distillation, Token Compression, Resolution Mode Shifts
## Enhancing Adversarial Transferability with Checkpoints of a Single Model’s Training
- **Tags:** Adversarial Attacks, Knowledge Distillation, Training Checkpoints, Task-Intrinsic Knowledge, Accuracy Gap-Based Selection
## CL-LoRA: Continual Low-Rank Adaptation for Rehearsal-Free Class-Incremental Learning
- **Tags:** Continual Learning, Low-Rank Adaptation, Dual-Adapter Architecture, Knowledge Distillation, Gradient Reassignment
## Efficient Fine-Tuning and Concept Suppression for Pruned Diffusion Models
- **Tags:** Diffusion Models, Model Pruning, Bilevel Optimization, Concept Unlearning, Knowledge Distillation
## Efficient ANN-Guided Distillation: Aligning Rate-based Features of Spiking Neural Networks through Hybrid Block-wise Replacement
- **Tags:** Knowledge Distillation, Spiking Neural Networks (SNNs), Rate-based Features, Hybrid Block-wise Replacement, ANN-to-SNN Conversion
## MonoTAKD: Teaching Assistant Knowledge Distillation for Monocular 3D Object Detection
- **Tags:** Knowledge Distillation, 3D Object Detection, Autonomous Driving, Depth Estimation, Residual Features, Modality Gap Bridging
## Subspace Constraint and Contribution Estimation for Heterogeneous Federated Learning
- **Tags:** Federated Learning, Knowledge Distillation, Subspace Constraint, Adaptive Weighted Aggregation, Dynamic Subspace Update
## Towards Stable and Storage-efficient Dataset Distillation: Matching Convexified Trajectory
- **Tags:** Dataset Distillation, Knowledge Distillation, Convex Optimization, Training Trajectory Optimization, Storage Efficiency
## Learning on Model Weights using Tree Experts
- **Tags:** Neural Architecture Search (NAS), Knowledge Distillation, Model Trees, Weight-Language Embedding, Zero-Shot Classification
## VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models
- **Tags:** Vision-Language Models (VLMs), Knowledge Distillation, Layer-wise Distillation, Intermediate Verbalizers, Efficient VLMs
## UniGraspTransformer: Simplified Policy Distillation for Scalable Dexterous Robotic Grasping
- **Tags:** Embodied AI, Knowledge Distillation, Transformer-based Networks, Reinforcement Learning, Robotic Grasping
## WAVE: Weight Templates for Adaptive Initialization of Variable-sized Models
- **Tags:** Knowledge Distillation, Model Pruning, Kronecker Product, Learngene Framework, Variable-sized Models
## Less is More: Efficient Model Merging with Binary Task Switch
- **Tags:** Model Pruning, Knowledge Distillation, Binarized Task Vectors, Automated Switch Combination, Storage Efficiency
## Quantization without Tears
- **Tags:** Model Pruning, Knowledge Distillation, Network Quantization, Lightweight Structures, Closed-Form Solution
## Enhancing Dataset Distillation via Non-Critical Region Refinement
- **Tags:** Dataset Distillation, Knowledge Distillation, Non-Critical Region Refinement, Distance-Based Knowledge Transfer, Synthetic Data Optimization
- **Link:** [Link](https://anonymous.4open.science/r/NRR-DD)

## Temporal Separation with Entropy Regularization for Knowledge Distillation in Spiking Neural Networks
- **Tags:** Knowledge Distillation, Spiking Neural Networks (SNNs), Temporal Separation, Entropy Regularization, Logit Distillation
## Scaling Down Text Encoders of Text-to-Image Diffusion Models
- **Tags:** Diffusion Models, Text-to-Image Generation, Knowledge Distillation, Model Efficiency, Text Encoder Optimization
## Distilled Prompt Learning for Incomplete Multimodal Survival Prediction
- **Tags:** Multimodal Learning, Large Language Models (LLMs), Survival Prediction, Incomplete Modalities, Knowledge Distillation
## Tripartite Weight-Space Ensemble for Few-Shot Class-Incremental Learning
- **Tags:** Few-Shot Learning, Class-Incremental Learning, Weight-Space Ensemble, Knowledge Distillation, Catastrophic Forgetting Mitigation
## Automated Joint Structured Pruning and Quantization for Efficient Neural Network Training and Compression
- **Tags:** Model Pruning, Knowledge Distillation, Quantization-aware Training, Structured Pruning, Joint Optimization
## ESC: Erasing Space Concept for Knowledge Deletion
- **Tags:** Knowledge Distillation, Data Augmentation, Privacy in Deep Learning, Feature Space Manipulation, Learnable Mask
## High Temporal Consistency through Semantic Similarity Propagation in Semi-Supervised Video Semantic Segmentation for Autonomous Flight
- **Tags:** Semantic Segmentation, Autonomous Driving, Knowledge Distillation, Temporal Consistency, Aerial Data, Real-Time Inference
## Sketch Down the FLOPs: Towards Efficient Networks for Human Sketch
- **Tags:** Sketch Data, Efficient Networks, Cross-Modal Knowledge Distillation, Reinforcement Learning, Fine-Grained Sketch-Based Image Retrieval (FG-SBIR), FLOPs Reduction, Dynamic Abstraction Adjustment
## Autoregressive Distillation of Diffusion Transformers
- **Tags:** Diffusion Models, Knowledge Distillation, Transformer Architecture, ODE Trajectory Prediction, Exposure Bias Mitigation
## Ground-V: Teaching VLMs to Ground Complex Instructions in Pixels
- **Tags:** Vision-Language Models (VLMs), Object Detection, Pixel-Level Grounding, Instruction-Following Data, Visual-Language Knowledge Distillation
## Active Data Curation Effectively Distills Large-Scale Multimodal Models
- **Tags:** Knowledge Distillation, Multimodal Learning, Contrastive Multimodal Pretraining, Inference Efficiency, Zero-Shot Learning
